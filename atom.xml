<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LeijieZhang</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://leijiezhang001.github.io/"/>
  <updated>2020-05-22T10:04:17.044Z</updated>
  <id>https://leijiezhang001.github.io/</id>
  
  <author>
    <name>Leijie</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>[paper_reading]-&quot;SA-SSD: Structure Aware Single-stage 3D Object Detection from Point Cloud&quot;</title>
    <link href="https://leijiezhang001.github.io/SA-SSD/"/>
    <id>https://leijiezhang001.github.io/SA-SSD/</id>
    <published>2020-05-22T03:27:38.000Z</published>
    <updated>2020-05-22T10:04:17.044Z</updated>
    
    <content type="html"><![CDATA[<p>　　Voxel-based 3D Detection 相比 <a href="/Point-based-3D-Det/" title="Point-based 3D Detection">Point-based 3D Detection</a> 的缺点是特征提取不仅在 Voxel 阶段损失了一定的点云信息，而且 Voxel 化后丢失了点云之间的拓扑关系。<a href="/Point-based-3D-Det/" title="Point-based 3D Detection">Point-based 3D Detection</a> 中详细描述了几种 Point-based 方法，这种方法目前比较棘手的地方是，即使作 Inference 时，也需要作 kd-tree 搜索与采样等运算量较大的操作。那么如何榨干 Voxel-based 的性能对工业界落地就显得比较重要了，本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种单阶段的 Voxel-based 3D 检测方法，并借助了 Point 级别特征提取的相关策略，使得检测性能有较大提升。</p><h2 id="framework">1. Framework</h2><p><img src="/SA-SSD/framework.png" width="100%" height="100%" title="图 1. Framework of SA-SSD"> 　　如图 1. 所示，SA-SSD 由三部分组成：Backbone，Detection Head，Auxiliary Network。<br>　　Backbone 的输入是栅格化后的点云表示方式，文中栅格大小设定为 \(0.05m,0.05m,0.1m\)。Backbone 由一系列的 3D convolution 组成，因为需要保留空间三维位置信息，作 Voxel-to-Point 的映射。这里如果用 2D convolution 代替，那么 Auxiliary Network 估计也只能作 BridView 的分割了。<br>　　Detection Head 主体就是传统 Anchor-Free 结构，一个分支用于预测每个特征层像素点的 Confidence，另一个分支用于预测基于每个特征层像素点的 BBox 属性，如，以该点为 &quot;Anchor&quot; 的四个顶点坐标。此外，为了消除 One-Stage 方法中目标框与置信度不对齐的问题，本文引入 Part-sensitive Warping 来实现与 PSRoiAlign 类似的作用，实现两者的对齐。<br>　　Auxiliary Network 只在训练的阶段起作用，Inference 阶段不需要计算。该模块的作用是训练时通过 Voxel-to-Point 特征映射来反向传播监督 Backbone 中的 Voxel 特征学习 Point 级别的特征，包括点云的空间拓扑关系。<strong>当然 Inference 时也可以保留该分割模块，那么还可以增加点级别的特征反映射到 Voxel 的模块(Point-to-Voxel)，进一步作特征增强。</strong></p><h2 id="detachable-auxiliary-network">2. Detachable Auxiliary Network</h2><p><img src="/SA-SSD/sa.png" width="60%" height="60%" title="图 2. Structured Aware Feature Learning"> 　　如图 2. 所示，随着 Backbone 特征提取的感受野增大(特征分辨率下降)，背景点会接近目标的边缘，使得目标框大小不容易预测准确。本文提出的 Auxiliary Network，通过增加点级别分割及目标中心坐标预测任务，来监督 Backbone 特征层捕捉这种结构信息，从而达到更准确的目标检测的目的。<br>　　Auxiliary Network 的输入来自 Backbone 各个分辨率的特征层。将特征层上不为零的特征点，通过 Voxel-to-Point 反栅格化映射到三维空间，设该特征点表示为 \(\{(f _ j,p _ j):j=1,...,M\}\)，其中 \(f\) 为特征向量，\(p\) 为坐标向量。有了栅格对应的伪三维坐标点下的特征表示后，即可插值出实际点云中每个点的特征向量。设点云中点的插值特征为：\(\{(\hat{f} _ i,p _ i):i=1,...,N\}\)，采用 Inverse Distance Weighted 方法进行插值： <span class="math display">\[ \hat{f} _ i = \frac{\sum _ {j=1}^Mw _ j(p _ i)f _ j}{\sum _ {j=1}^Mw _ j(p _ i)} \tag{1}\]</span> 其中： <span class="math display">\[w _ j(p _ i)=\left\{\begin{array}{l}\frac{1}{\Vert p _ i-p _ j\Vert _ 2} &amp; \mathrm{if} p _ j\in\mathcal{N}(p _ i)\\0 &amp; \mathrm{otherwise}\end{array}\tag{2}\right.\]</span> \(\mathcal{N}(p _ i)\) 为球状区域，本文在四个分辨率下分别设定为：0.05m，0.1m，0.2m，0.4m。然后通过 cross-stage link 对各个分辨率下的点特征进行 concatenate 融合。最后通过感知机进行点云分割及目标中心点预测任务的构建。<br>　　对于点级别前景分割的任务，经过 sigmoid 函数后，应用二分类的 Focal Loss： <span class="math display">\[ \mathcal{L} _ {seg} = \frac{1}{N _ {pos}}\sum _ i^N -\alpha(1-\hat{s} _ i)^{\gamma}\mathrm{log}(\hat{s} _ i) \tag{3}\]</span> 该分割任务使得目标检测的框更加准确，如图 2.c 所示。但是还得优化其尺度与形状。<br>　　中心点的预测任务则能有效约束目标框的尺度与形状，具体的，预测的是每个属于目标的点云与中心点的相对位置(残差)。可用 Smooth-l1 来构建预测的中心点与实际中心点的 Loss。</p><h2 id="part-sensitive-warping">3. Part-sensitive Warping</h2><p><img src="/SA-SSD/psw.png" width="60%" height="60%" title="图 3. Part-sensitive Warping"> 　　One-Stage 方法都会有 Confidence 和 BBox 错位的现象，本文提出一种类似 PSROIAlign 但更有高效的 PSW 方法，具体步骤为：</p><ol type="1"><li>对于分类分支，修改为 \(K\) 个 Part-sensitive 的 cls maps，每个 map 包含目标的部分信息，比如当 \(K=4\) 时，可以理解为将目标切分为 \(2\times 2\) 部分；</li><li>对于回归分支，将每个目标框的 Feature map 划分为 \(K\) 个子区域，每个区域的中心点作为采样点；</li><li>如图 3. 所示，通过采样得到最终 cls map 的平均值。</li></ol><h2 id="experiment">4. Experiment</h2><p><img src="/SA-SSD/ablation.png" width="60%" height="60%" title="图 4. Ablation Study"> 　　如图 4. 所示，Auxiliary Network 能有效提升网络的定位精度，PSWarp 也能有效消除 Confidence 与 BBox 的错位影响。</p><h2 id="reference">5. Reference</h2><p><a id="1" href="#1ref">[1]</a> henhang, et al. &quot;Structure Aware Single-stage 3D Object Detection from Point Cloud.&quot;</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Voxel-based 3D Detection 相比 &lt;a href=&quot;/Point-based-3D-Det/&quot; title=&quot;Point-based 3D Detection&quot;&gt;Point-based 3D Detection&lt;/a&gt; 的缺点是特征提取不仅在 Vo
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>非线性最小二乘</title>
    <link href="https://leijiezhang001.github.io/Non-linear-Least-Squares/"/>
    <id>https://leijiezhang001.github.io/Non-linear-Least-Squares/</id>
    <published>2020-05-18T01:19:54.000Z</published>
    <updated>2020-05-24T04:47:24.293Z</updated>
    
    <content type="html"><![CDATA[<p>　　非线性最小二乘(Non-linear Least Squares)问题应用非常广泛，尤其是在 SLAM 领域。<a href="/LOAM/" title="LOAM">LOAM</a>，，<a href="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/" title="Stereo-RCNN">Stereo-RCNN</a>，<a href="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/" title="Stereo Vision-based Semantic and Ego-motion Tracking for Autonomous Driving">Stereo Vision-based Semantic and Ego-motion Tracking for Autonomous Driving</a> 等均需要求解非线性最小二乘问题。其中 <a href="/LOAM/" title="LOAM">LOAM</a> 作为非常流行的激光 SLAM 框架，其后端是一个典型的非线性最优化问题，本文会作为实践进行代码级讲解。</p><h2 id="问题描述">1. 问题描述</h2><p>　　在前端观测-后端优化框架下，设观测数据对集合为：\(\{y _ i,z _ i\} _ {i=1}^m\)，待求解的变量参数 \(x\in\mathbb{R}^n\) 定义了观测数据对的映射关系，即 \(z _ i=h(y _ i;x)\)，由此得到有 \(m\) 个参数方程 \(F(x)=[f _ 1(x),...,f _ m(x)]^T\)，其中 \(f _ i(x) = z _ i-h(y _ i;x)\)。我们要找到最优的参数 \(x\) 来描述观测数据对之间的关系，即求解的最优化问题为： <span class="math display">\[\begin{align}\mathop{\arg\min}\limits _ x \frac{1}{2}\Vert F(x)\Vert ^2 \iff \mathop{\arg\min}\limits _ x\frac{1}{2}\sum _ i \rho _ i\left(\Vert f _ i(x)\Vert ^ 2\right)\\L\leq x \leq U\end{align}\tag{1}\]</span> 其中 \(f _ i(\cdot)\) 为 Cost Function，\(\rho _ i(\cdot)\) 为 Loss Function，即核函数，用来减少离群点对非线性最小二乘优化的影响；\(L,U\) 分别为参数 \(x\) 的上下界。当核函数 \(\rho _ i(x)=x\) 时，就是常见的非线性最小二乘问题。<br>　　《视觉 SLAM 十四讲》<a href="#1" id="1ref"><sup>[1]</sup></a>在 SLAM 的状态估计问题中，从概率学角度导出了最大似然估计求解状态的方法，并进一步引出了最小二乘问题。回过头来看，本文很多内容在《视觉 SLAM 十四讲》中已经有非常清晰的描述，可作进一步参考。</p><h2 id="问题求解">2. 问题求解</h2><p>　　根据 \(F(x)\) 求得雅克比矩阵(Jacobian)：\(J(x) \in\mathbb{R}^{m\times n}\)，即 \(J _ {ij}(x)=\frac{\partial f _ i(x)}{\partial x _ j}\)。目标函数的梯度向量为 \(g(x) = \nabla\frac{1}{2}\Vert F(x)\Vert ^ 2=J(x)^TF(x)\)。在 \(x\) 处将目标函数线性化：\(F(x+\Delta x)\approx F(x)+J(x)\Delta x\)。由此非线性最小二乘问题可转换为线性最小二乘求解残差量 \(\Delta x\) 来近似求解： <span class="math display">\[\mathop{\arg\min}\limits _ {\Delta x}\frac{1}{2}\Vert J(x)\Delta x+F(x)\Vert ^ 2\tag{2}\]</span> 根据如何控制 \(\Delta x\) 的大小，非线性优化算法可分为两大类：</p><ul><li>Line Search<ul><li>Gradient Descent</li><li>Gaussian-Newton</li></ul></li><li>Trust Region<ul><li>Levenberg-Marquardt</li><li>Dogleg</li><li>Inner Iterations</li><li>Non-monotonic Steps</li></ul></li></ul><p>Line Search 首先确定迭代方向，然后最小化 \(\Vert f(x+\alpha \Delta x)\Vert ^2\) 确定迭代步长；Trust Region 则划分一个局部区域，在该区域内求解最优值，然后根据近似程度，扩大或缩减该局部区域范围。Trust Region 相比 Linear Search，数值迭代会更加稳定。这里介绍几种有代表性的方法：属于 Line Search 的梯度下降法，高斯牛顿法，以及属于 Trust Region 的 LM 法。</p><h3 id="梯度下降法">2.1. 梯度下降法</h3><p>　　将目标函数式(1)在 \(x\) 附近泰勒展开： <span class="math display">\[ \Vert F(x+\Delta x)\Vert ^2 \approx \Vert F(x)\Vert ^2 + J(x)\Delta x+\frac{1}{2}\Delta x^TH\Delta x \tag{3}\]</span> 其中 \(H\) 是二阶导数(Hessian 矩阵)。<br>　　如果保留一阶导数，那么增量的解就为： <span class="math display">\[\Delta x = -\lambda J^T(x) \tag{4}\]</span> 其中 \(\lambda\) 为步长，可预先由相关策略设定。<br>　　如果保留二阶导数，那么增量方程为： <span class="math display">\[\mathop{\arg\min}\limits _ {\Delta x} \Vert F(x)\Vert ^2+J(x)\Delta x+\frac{1}{2}\Delta x^TH\Delta x\tag{5}\]</span> 对 \(\Delta x\) 求导即可求解增量的解为： <span class="math display">\[\Delta x = -H^{-1}J^T \tag{6}\]</span> 　　一阶梯度法又称为最速下降法，二阶梯度法又称为牛顿法。一阶和二阶法都是将函数在当前值下泰勒展开，然后线性得求解增量值。最速下降法过于贪心，容易走出锯齿路线，反而增加迭代步骤。牛顿法需要计算 \(H\) 矩阵，计算量较大且困难。</p><h3 id="高斯牛顿法">2.2. 高斯牛顿法</h3><p>　　 将式(2)对 \(\Delta x\) 求导并令其为零，可得： <span class="math display">\[\begin{align}&amp;J(x)^TJ(x)\Delta x=-J(x)^TF(x)\\\iff &amp; H\Delta x=g\end{align}\tag{7}\]</span> 相比牛顿法，高斯牛顿法不用计算 \(H\) 矩阵，直接用 \(J^TJ\) 来近似，所以节省了计算量。但是高斯牛顿法要求 \(H\) 矩阵是可逆且正定的，而实际计算的 \(J^TJ\) 是半正定的，所以 \(J^TJ\) 会出现奇异或病态的情况，此时增量的稳定性就会变差，导致迭代发散。另一方面，增量较大时，目标近似函数式(2)就会产生较大的误差，也会导致迭代发散。这是高斯牛顿法的缺陷。高斯牛顿法的步骤为：</p><ol type="1"><li>根据式 (7) 求解迭代步长 \(\Delta x\)；</li><li>变量迭代：\(x ^ * \leftarrow x+\Delta x\)；</li><li>如果 \(\Vert F(x ^ * )-F(x)\Vert &lt; \epsilon\)，则收敛，退出迭代，否则重复步骤 1.；</li></ol><p>高斯牛顿法简单的将 \(\alpha\) 置为 1，而其它 Line Search 方法会最小化 \(\Vert f(x+\alpha \Delta x)\Vert ^2\) 来确定 \(\alpha\) 值。</p><h3 id="lm-法">2.3. LM 法</h3><p>　　Line Search 依赖线性化近似有较高的拟合度，但是有时候线性近似效果较差，导致迭代不稳定；Region Trust 就是解决了这种问题。高斯牛顿法中采用的近似二阶泰勒展开只在该点附近有较好的近似结果，对 \(\Delta x\) 添加一个信赖域区域，就变为 Trust Region 方法。其最优化问题转换为： <span class="math display">\[\begin{align}\mathop{\arg\min}\limits _ x \frac{1}{2}\Vert J(x)\Delta x+F(x)\Vert ^2 \\\Vert D(x)\Delta x\Vert ^2 \leq \mu\\L\leq x \leq U\\\end{align}\tag{8}\]</span> 用 Lagrange 乘子将其转换为无约束优化问题： <span class="math display">\[\mathop{\arg\min}\limits _ {\Delta x}\frac{1}{2}\Vert J(x)\Delta x+F(x)\Vert ^ 2+\frac{1}{\mu}\Vert D(x)\Delta x\Vert ^2 \tag{9}\]</span> 其中 Levenberg 提出的方法中 \(D=I\)，相当于把 \(\Delta x\) 约束在球中；Marquart 提出的方法中将 \(D\) 取为非负数对角阵，通常为 \(J(x)^TJ(x)\) 的对角元素平方根。<br>　　对于信赖域区域 \(\mu\) 的定义，一个比较好的方式是根据近似模型与实际函数之间的差异来确定这个范围：如果差异小，那么增大信赖域；反之减小信赖域。因此，考虑： <span class="math display">\[\rho = \frac{\Vert F(x+\Delta x)\Vert ^2-\Vert F(x)\Vert ^2}{\Vert J(x)\Delta x+F(x)\Vert ^2-\Vert F(x)\Vert ^2} \tag{10}\]</span> 　　 将式(9)对 \(\Delta x\) 求导并令其为零，可得： <span class="math display">\[\begin{align}&amp;\left(J(x)^TJ(x)+\frac{2}{\mu}D^T(x)D(x)\right)\Delta x=-J(x)^TF(x)\\\iff &amp; (H+\lambda D^TD)\Delta x=g\end{align}\tag{11}\]</span> 当 \(\lambda\) 较小时，接近于高斯牛顿法；当 \(\lambda\) 较大时，接近于最速下降法。LM 法的步骤为：</p><ol type="1"><li>根据式(11)求解迭代步长 \(\Delta x\);</li><li>根据式(10)求解 \(\rho\);</li><li>若 \(\rho &gt; \eta _ 1\)，则 \(\mu = 2\mu\);</li><li>若 \(\rho &lt; \eta _ 2\)，则 \(\mu = 0.5\mu\);</li><li>若 \(\rho &gt; \epsilon\)，则 \(x ^ * \leftarrow x+\Delta x\)；</li><li>如果满足收敛条件，则结束，否则继续步骤1.；</li></ol><h2 id="ceres-实践">3. Ceres 实践</h2><p>　　Ceres 是谷歌开发的一个用于非线性优化的库，使用 Ceres 库有以下几个步骤：</p><ul><li>构建 Cost Function，式(1)中的 \(\rho _ i\left(\Vert f _ i(x)\Vert ^ 2\right)\) 即为代码中需要增加的 ResidualBlock；</li><li>累加的 Cost Function 构成最终的 Loss Function 目标函数；</li><li>配置求解器参数并求解问题；</li></ul><h3 id="例子-曲线拟合">3.1. 例子-曲线拟合</h3><p>　　以下代码为拟合曲线参数的简单例子：</p><p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// copy from http://zhaoxuhui.top/blog/2018/04/04/ceres&amp;ls.html</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/core/core.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;ceres/ceres.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cv;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> ceres;</span><br><span class="line"></span><br><span class="line"><span class="comment">//vector,用于存放x、y的观测数据</span></span><br><span class="line"><span class="comment">//待估计函数为y=3.5x^3+1.6x^2+0.3x+7.8</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; xs;</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; ys;</span><br><span class="line"></span><br><span class="line"><span class="comment">//定义CostFunctor结构体用于描述代价函数</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">CostFunctor</span>&#123;</span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">double</span> x_guan,y_guan;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//构造函数，用已知的x、y数据对其赋值</span></span><br><span class="line">  CostFunctor(<span class="keyword">double</span> x,<span class="keyword">double</span> y)</span><br><span class="line">  &#123;</span><br><span class="line">    x_guan = x;</span><br><span class="line">    y_guan = y;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//重载括号运算符，两个参数分别是估计的参数和由该参数计算得到的残差</span></span><br><span class="line">  <span class="comment">//注意这里的const，一个都不能省略，否则就会报错</span></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">  <span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> T* <span class="keyword">const</span> params,T* residual)</span><span class="keyword">const</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    residual[<span class="number">0</span>]=y_guan-(params[<span class="number">0</span>]*x_guan*x_guan*x_guan+params[<span class="number">1</span>]*x_guan*x_guan+params[<span class="number">2</span>]*x_guan+params[<span class="number">3</span>]);</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;  </span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//生成实验数据</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">generateData</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  RNG rng;</span><br><span class="line">  <span class="keyword">double</span> w_sigma = <span class="number">1.0</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">100</span>;i++)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">double</span> x = i;</span><br><span class="line">    <span class="keyword">double</span> y = <span class="number">3.5</span>*x*x*x+<span class="number">1.6</span>*x*x+<span class="number">0.3</span>*x+<span class="number">7.8</span>;</span><br><span class="line">    xs.push_back(x);</span><br><span class="line">    ys.push_back(y+rng.gaussian(w_sigma));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;xs.size();i++)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"x:"</span>&lt;&lt;xs[i]&lt;&lt;<span class="string">" y:"</span>&lt;&lt;ys[i]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//简单描述我们优化的目的就是为了使我们估计参数算出的y'和实际观测的y的差值之和最小</span></span><br><span class="line"><span class="comment">//所以代价函数(CostFunction)就是y'-y，其对应每一组观测值与估计值的残差。</span></span><br><span class="line"><span class="comment">//由于我们优化的是残差之和，因此需要把代价函数全部加起来，使这个函数最小，而不是单独的使某一个残差最小</span></span><br><span class="line"><span class="comment">//默认情况下，我们认为各组的残差是等权的，也就是核函数系数为1。</span></span><br><span class="line"><span class="comment">//但有时可能会出现粗差等情况，有可能不等权，但这里不考虑。</span></span><br><span class="line"><span class="comment">//这个求和以后的函数便是我们优化的目标函数</span></span><br><span class="line"><span class="comment">//通过不断调整我们的参数值，使这个目标函数最终达到最小，即认为优化完成</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">  generateData();</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//创建一个长度为4的double数组用于存放参数</span></span><br><span class="line">  <span class="keyword">double</span> params[<span class="number">4</span>]=&#123;<span class="number">1.0</span>&#125;;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//第一步，创建Problem对象，并对每一组观测数据添加ResidualBlock</span></span><br><span class="line">  <span class="comment">//由于每一组观测点都会得到一个残差，而我们的目的是最小化所有残差的和</span></span><br><span class="line">  <span class="comment">//所以采用for循环依次把每个残差都添加进来</span></span><br><span class="line">  Problem problem;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;xs.size();i++)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="comment">//利用我们之前写的结构体、仿函数，创建代价函数对象，注意初始化的方式</span></span><br><span class="line">    <span class="comment">//尖括号中的参数分别为误差类型，输出维度(因变量个数)，输入维度(待估计参数的个数)</span></span><br><span class="line">    CostFunction* cost_function = <span class="keyword">new</span> AutoDiffCostFunction&lt;CostFunctor,<span class="number">1</span>,<span class="number">4</span>&gt;(<span class="keyword">new</span> CostFunctor(xs[i],ys[i]));</span><br><span class="line">    <span class="comment">//三个参数分别为代价函数、核函数和待估参数</span></span><br><span class="line">    problem.AddResidualBlock(cost_function,<span class="literal">NULL</span>,params);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//第二步，配置Solver</span></span><br><span class="line">  Solver::Options options;</span><br><span class="line">  <span class="comment">//配置增量方程的解法</span></span><br><span class="line">  options.linear_solver_type=ceres::DENSE_QR;</span><br><span class="line">  <span class="comment">//是否输出到cout</span></span><br><span class="line">  options.minimizer_progress_to_stdout=<span class="literal">true</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//第三步，创建Summary对象用于输出迭代结果</span></span><br><span class="line">  Solver::Summary summary;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//第四步，执行求解</span></span><br><span class="line">  Solve(options,&amp;problem,&amp;summary);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//第五步，输出求解结果</span></span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;summary.BriefReport()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;<span class="string">"p0:"</span>&lt;&lt;params[<span class="number">0</span>]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;<span class="string">"p1:"</span>&lt;&lt;params[<span class="number">1</span>]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;<span class="string">"p2:"</span>&lt;&lt;params[<span class="number">2</span>]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;<span class="string">"p3:"</span>&lt;&lt;params[<span class="number">3</span>]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="例子-loam">3.2. 例子-LOAM</h3><p>　　<a href="/LOAM/" title="LOAM">LOAM</a> 前端提取线和面特征，后端最小化线和面的匹配误差。其源码实现了整个最优化过程，ALOAM<a href="#2" id="2ref"><sup>[2]</sup></a> 将后端代码用 Ceres 实现，这里对其作理解与分析。</p><p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">LidarEdgeFactor</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">LidarEdgeFactor(Eigen::Vector3d curr_point_, Eigen::Vector3d last_point_a_,</span><br><span class="line">Eigen::Vector3d last_point_b_, <span class="keyword">double</span> s_)</span><br><span class="line">: curr_point(curr_point_), last_point_a(last_point_a_), last_point_b(last_point_b_), s(s_) &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> T *q, <span class="keyword">const</span> T *t, T *residual)</span> <span class="keyword">const</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; cp&#123;T(curr_point.x()), T(curr_point.y()), T(curr_point.z())&#125;;</span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; lpa&#123;T(last_point_a.x()), T(last_point_a.y()), T(last_point_a.z())&#125;;</span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; lpb&#123;T(last_point_b.x()), T(last_point_b.y()), T(last_point_b.z())&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Eigen::Quaternion&lt;T&gt; q_last_curr&#123;q[3], T(s) * q[0], T(s) * q[1], T(s) * q[2]&#125;;</span></span><br><span class="line">Eigen::Quaternion&lt;T&gt; q_last_curr&#123;q[<span class="number">3</span>], q[<span class="number">0</span>], q[<span class="number">1</span>], q[<span class="number">2</span>]&#125;;</span><br><span class="line">Eigen::Quaternion&lt;T&gt; q_identity&#123;T(<span class="number">1</span>), T(<span class="number">0</span>), T(<span class="number">0</span>), T(<span class="number">0</span>)&#125;;</span><br><span class="line">q_last_curr = q_identity.slerp(T(s), q_last_curr);</span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; t_last_curr&#123;T(s) * t[<span class="number">0</span>], T(s) * t[<span class="number">1</span>], T(s) * t[<span class="number">2</span>]&#125;;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; lp;</span><br><span class="line">lp = q_last_curr * cp + t_last_curr;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; nu = (lp - lpa).cross(lp - lpb);</span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; de = lpa - lpb;</span><br><span class="line"></span><br><span class="line">residual[<span class="number">0</span>] = nu.x() / de.norm();</span><br><span class="line">residual[<span class="number">1</span>] = nu.y() / de.norm();</span><br><span class="line">residual[<span class="number">2</span>] = nu.z() / de.norm();</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> ceres::<span class="function">CostFunction *<span class="title">Create</span><span class="params">(<span class="keyword">const</span> Eigen::Vector3d curr_point_, <span class="keyword">const</span> Eigen::Vector3d last_point_a_,</span></span></span><br><span class="line"><span class="function"><span class="params">   <span class="keyword">const</span> Eigen::Vector3d last_point_b_, <span class="keyword">const</span> <span class="keyword">double</span> s_)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> (<span class="keyword">new</span> ceres::AutoDiffCostFunction&lt;</span><br><span class="line">LidarEdgeFactor, <span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>&gt;(</span><br><span class="line"><span class="keyword">new</span> LidarEdgeFactor(curr_point_, last_point_a_, last_point_b_, s_)));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Eigen::Vector3d curr_point, last_point_a, last_point_b;</span><br><span class="line"><span class="keyword">double</span> s;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>　　对于 Point2Line 误差，为了衡量该线特征上的点是否在地图对应的线特征上，在地图线特征上采样两个点，加上该点，组成两个向量，向量叉乘即可描述匹配误差。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">LidarPlaneFactor</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">LidarPlaneFactor(Eigen::Vector3d curr_point_, Eigen::Vector3d last_point_j_,</span><br><span class="line"> Eigen::Vector3d last_point_l_, Eigen::Vector3d last_point_m_, <span class="keyword">double</span> s_)</span><br><span class="line">: curr_point(curr_point_), last_point_j(last_point_j_), last_point_l(last_point_l_),</span><br><span class="line">  last_point_m(last_point_m_), s(s_)</span><br><span class="line">&#123;</span><br><span class="line">ljm_norm = (last_point_j - last_point_l).cross(last_point_j - last_point_m);</span><br><span class="line">ljm_norm.normalize();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> T *q, <span class="keyword">const</span> T *t, T *residual)</span> <span class="keyword">const</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; cp&#123;T(curr_point.x()), T(curr_point.y()), T(curr_point.z())&#125;;</span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; lpj&#123;T(last_point_j.x()), T(last_point_j.y()), T(last_point_j.z())&#125;;</span><br><span class="line"><span class="comment">//Eigen::Matrix&lt;T, 3, 1&gt; lpl&#123;T(last_point_l.x()), T(last_point_l.y()), T(last_point_l.z())&#125;;</span></span><br><span class="line"><span class="comment">//Eigen::Matrix&lt;T, 3, 1&gt; lpm&#123;T(last_point_m.x()), T(last_point_m.y()), T(last_point_m.z())&#125;;</span></span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; ljm&#123;T(ljm_norm.x()), T(ljm_norm.y()), T(ljm_norm.z())&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Eigen::Quaternion&lt;T&gt; q_last_curr&#123;q[3], T(s) * q[0], T(s) * q[1], T(s) * q[2]&#125;;</span></span><br><span class="line">Eigen::Quaternion&lt;T&gt; q_last_curr&#123;q[<span class="number">3</span>], q[<span class="number">0</span>], q[<span class="number">1</span>], q[<span class="number">2</span>]&#125;;</span><br><span class="line">Eigen::Quaternion&lt;T&gt; q_identity&#123;T(<span class="number">1</span>), T(<span class="number">0</span>), T(<span class="number">0</span>), T(<span class="number">0</span>)&#125;;</span><br><span class="line">q_last_curr = q_identity.slerp(T(s), q_last_curr);</span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; t_last_curr&#123;T(s) * t[<span class="number">0</span>], T(s) * t[<span class="number">1</span>], T(s) * t[<span class="number">2</span>]&#125;;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; lp;</span><br><span class="line">lp = q_last_curr * cp + t_last_curr;</span><br><span class="line"></span><br><span class="line">residual[<span class="number">0</span>] = (lp - lpj).dot(ljm);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> ceres::<span class="function">CostFunction *<span class="title">Create</span><span class="params">(<span class="keyword">const</span> Eigen::Vector3d curr_point_, <span class="keyword">const</span> Eigen::Vector3d last_point_j_,</span></span></span><br><span class="line"><span class="function"><span class="params">   <span class="keyword">const</span> Eigen::Vector3d last_point_l_, <span class="keyword">const</span> Eigen::Vector3d last_point_m_,</span></span></span><br><span class="line"><span class="function"><span class="params">   <span class="keyword">const</span> <span class="keyword">double</span> s_)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> (<span class="keyword">new</span> ceres::AutoDiffCostFunction&lt;</span><br><span class="line">LidarPlaneFactor, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>&gt;(</span><br><span class="line"><span class="keyword">new</span> LidarPlaneFactor(curr_point_, last_point_j_, last_point_l_, last_point_m_, s_)));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Eigen::Vector3d curr_point, last_point_j, last_point_l, last_point_m;</span><br><span class="line">Eigen::Vector3d ljm_norm;</span><br><span class="line"><span class="keyword">double</span> s;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>　　对于 Point2Plane 误差，为了衡量该面特征上的点是否在地图对应的面特征上，在地图面特征上采样一个点，加上该点，组成向量，然后点乘面的法向量即可衡量匹配误差。</p><h3 id="例子-ba">3.3. 例子-BA</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// copy from https://www.jianshu.com/p/3df0c2e02b4c</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"ceres/ceres.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"ceres/rotation.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Read a Bundle Adjustment in the Large dataset.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BALProblem</span> &#123;</span></span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  ~BALProblem() &#123;</span><br><span class="line">    <span class="keyword">delete</span>[] point_index_;</span><br><span class="line">    <span class="keyword">delete</span>[] camera_index_;</span><br><span class="line">    <span class="keyword">delete</span>[] observations_;</span><br><span class="line">    <span class="keyword">delete</span>[] parameters_;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">int</span> <span class="title">num_observations</span><span class="params">()</span>       <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> num_observations_;               &#125;</span><br><span class="line">  <span class="function"><span class="keyword">const</span> <span class="keyword">double</span>* <span class="title">observations</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> observations_;                   &#125;</span><br><span class="line">  <span class="function"><span class="keyword">double</span>* <span class="title">mutable_cameras</span><span class="params">()</span>          </span>&#123; <span class="keyword">return</span> parameters_;                     &#125;</span><br><span class="line">  <span class="function"><span class="keyword">double</span>* <span class="title">mutable_points</span><span class="params">()</span>           </span>&#123; <span class="keyword">return</span> parameters_  + <span class="number">9</span> * num_cameras_; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">double</span>* <span class="title">mutable_camera_for_observation</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> mutable_cameras() + camera_index_[i] * <span class="number">9</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">double</span>* <span class="title">mutable_point_for_observation</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> mutable_points() + point_index_[i] * <span class="number">3</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">bool</span> <span class="title">LoadFile</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>* filename)</span> </span>&#123;</span><br><span class="line">    FILE* fptr = fopen(filename, <span class="string">"r"</span>);</span><br><span class="line">    <span class="keyword">if</span> (fptr == <span class="literal">NULL</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    FscanfOrDie(fptr, <span class="string">"%d"</span>, &amp;num_cameras_);</span><br><span class="line">    FscanfOrDie(fptr, <span class="string">"%d"</span>, &amp;num_points_);</span><br><span class="line">    FscanfOrDie(fptr, <span class="string">"%d"</span>, &amp;num_observations_);</span><br><span class="line"></span><br><span class="line">    point_index_ = <span class="keyword">new</span> <span class="keyword">int</span>[num_observations_];</span><br><span class="line">    camera_index_ = <span class="keyword">new</span> <span class="keyword">int</span>[num_observations_];</span><br><span class="line">    observations_ = <span class="keyword">new</span> <span class="keyword">double</span>[<span class="number">2</span> * num_observations_];</span><br><span class="line"></span><br><span class="line">    num_parameters_ = <span class="number">9</span> * num_cameras_ + <span class="number">3</span> * num_points_;</span><br><span class="line">    parameters_ = <span class="keyword">new</span> <span class="keyword">double</span>[num_parameters_];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_observations_; ++i) &#123;</span><br><span class="line">      FscanfOrDie(fptr, <span class="string">"%d"</span>, camera_index_ + i);</span><br><span class="line">      FscanfOrDie(fptr, <span class="string">"%d"</span>, point_index_ + i);</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">2</span>; ++j) &#123;</span><br><span class="line">        FscanfOrDie(fptr, <span class="string">"%lf"</span>, observations_ + <span class="number">2</span>*i + j);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_parameters_; ++i) &#123;</span><br><span class="line">      FscanfOrDie(fptr, <span class="string">"%lf"</span>, parameters_ + i);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">FscanfOrDie</span><span class="params">(FILE *fptr, <span class="keyword">const</span> <span class="keyword">char</span> *format, T *value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> num_scanned = <span class="built_in">fscanf</span>(fptr, format, value);</span><br><span class="line">    <span class="keyword">if</span> (num_scanned != <span class="number">1</span>) &#123;</span><br><span class="line">      LOG(FATAL) &lt;&lt; <span class="string">"Invalid UW data file."</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> num_cameras_;</span><br><span class="line">  <span class="keyword">int</span> num_points_;</span><br><span class="line">  <span class="keyword">int</span> num_observations_;</span><br><span class="line">  <span class="keyword">int</span> num_parameters_;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span>* point_index_;</span><br><span class="line">  <span class="keyword">int</span>* camera_index_;</span><br><span class="line">  <span class="keyword">double</span>* observations_;</span><br><span class="line">  <span class="keyword">double</span>* parameters_;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Templated pinhole camera model for used with Ceres.  The camera is</span></span><br><span class="line"><span class="comment">// parameterized using 9 parameters: 3 for rotation, 3 for translation, 1 for</span></span><br><span class="line"><span class="comment">// focal length and 2 for radial distortion. The principal point is not modeled</span></span><br><span class="line"><span class="comment">// (i.e. it is assumed be located at the image center).</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">SnavelyReprojectionError</span> &#123;</span></span><br><span class="line">  SnavelyReprojectionError(<span class="keyword">double</span> observed_x, <span class="keyword">double</span> observed_y)</span><br><span class="line">      : observed_x(observed_x), observed_y(observed_y) &#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">  <span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> T* <span class="keyword">const</span> camera,</span></span></span><br><span class="line"><span class="function"><span class="params">                  <span class="keyword">const</span> T* <span class="keyword">const</span> point,</span></span></span><br><span class="line"><span class="function"><span class="params">                  T* residuals)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">    <span class="comment">// camera[0,1,2] are the angle-axis rotation.</span></span><br><span class="line">    T p[<span class="number">3</span>];</span><br><span class="line">    ceres::AngleAxisRotatePoint(camera, point, p);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// camera[3,4,5] are the translation.</span></span><br><span class="line">    p[<span class="number">0</span>] += camera[<span class="number">3</span>];</span><br><span class="line">    p[<span class="number">1</span>] += camera[<span class="number">4</span>];</span><br><span class="line">    p[<span class="number">2</span>] += camera[<span class="number">5</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Compute the center of distortion. The sign change comes from</span></span><br><span class="line">    <span class="comment">// the camera model that Noah Snavely's Bundler assumes, whereby</span></span><br><span class="line">    <span class="comment">// the camera coordinate system has a negative z axis.</span></span><br><span class="line">    T xp = - p[<span class="number">0</span>] / p[<span class="number">2</span>];</span><br><span class="line">    T yp = - p[<span class="number">1</span>] / p[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Apply second and fourth order radial distortion.</span></span><br><span class="line">    <span class="keyword">const</span> T&amp; l1 = camera[<span class="number">7</span>];</span><br><span class="line">    <span class="keyword">const</span> T&amp; l2 = camera[<span class="number">8</span>];</span><br><span class="line">    T r2 = xp*xp + yp*yp;</span><br><span class="line">    T distortion = <span class="number">1.0</span> + r2  * (l1 + l2  * r2);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Compute final projected point position.</span></span><br><span class="line">    <span class="keyword">const</span> T&amp; focal = camera[<span class="number">6</span>];</span><br><span class="line">    T predicted_x = focal * distortion * xp;</span><br><span class="line">    T predicted_y = focal * distortion * yp;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// The error is the difference between the predicted and observed position.</span></span><br><span class="line">    residuals[<span class="number">0</span>] = predicted_x - observed_x;</span><br><span class="line">    residuals[<span class="number">1</span>] = predicted_y - observed_y;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Factory to hide the construction of the CostFunction object from</span></span><br><span class="line">  <span class="comment">// the client code.</span></span><br><span class="line">  <span class="keyword">static</span> ceres::<span class="function">CostFunction* <span class="title">Create</span><span class="params">(<span class="keyword">const</span> <span class="keyword">double</span> observed_x,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">const</span> <span class="keyword">double</span> observed_y)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (<span class="keyword">new</span> ceres::AutoDiffCostFunction&lt;SnavelyReprojectionError, <span class="number">2</span>, <span class="number">9</span>, <span class="number">3</span>&gt;(</span><br><span class="line">                <span class="keyword">new</span> SnavelyReprojectionError(observed_x, observed_y)));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">double</span> observed_x;</span><br><span class="line">  <span class="keyword">double</span> observed_y;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span> </span>&#123;</span><br><span class="line">  google::InitGoogleLogging(argv[<span class="number">0</span>]);</span><br><span class="line">  <span class="keyword">if</span> (argc != <span class="number">2</span>) &#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cerr</span> &lt;&lt; <span class="string">"usage: simple_bundle_adjuster &lt;bal_problem&gt;\n"</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  BALProblem bal_problem;</span><br><span class="line">  <span class="keyword">if</span> (!bal_problem.LoadFile(argv[<span class="number">1</span>])) &#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cerr</span> &lt;&lt; <span class="string">"ERROR: unable to open file "</span> &lt;&lt; argv[<span class="number">1</span>] &lt;&lt; <span class="string">"\n"</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">double</span>* observations = bal_problem.observations();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Create residuals for each observation in the bundle adjustment problem. The</span></span><br><span class="line">  <span class="comment">// parameters for cameras and points are added automatically.</span></span><br><span class="line">  ceres::Problem problem;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; bal_problem.num_observations(); ++i) &#123;</span><br><span class="line">    <span class="comment">// Each Residual block takes a point and a camera as input and outputs a 2</span></span><br><span class="line">    <span class="comment">// dimensional residual. Internally, the cost function stores the observed</span></span><br><span class="line">    <span class="comment">// image location and compares the reprojection against the observation.</span></span><br><span class="line"></span><br><span class="line">    ceres::CostFunction* cost_function =</span><br><span class="line">        SnavelyReprojectionError::Create(observations[<span class="number">2</span> * i + <span class="number">0</span>],</span><br><span class="line">                                         observations[<span class="number">2</span> * i + <span class="number">1</span>]);</span><br><span class="line">    problem.AddResidualBlock(cost_function,</span><br><span class="line">                             <span class="literal">NULL</span> <span class="comment">/* squared loss */</span>,</span><br><span class="line">                             bal_problem.mutable_camera_for_observation(i),</span><br><span class="line">                             bal_problem.mutable_point_for_observation(i));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Make Ceres automatically detect the bundle structure. Note that the</span></span><br><span class="line">  <span class="comment">// standard solver, SPARSE_NORMAL_CHOLESKY, also works fine but it is slower</span></span><br><span class="line">  <span class="comment">// for standard bundle adjustment problems.</span></span><br><span class="line">  ceres::Solver::Options options;</span><br><span class="line">  options.linear_solver_type = ceres::DENSE_SCHUR;</span><br><span class="line">  options.minimizer_progress_to_stdout = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">  ceres::Solver::Summary summary;</span><br><span class="line">  ceres::Solve(options, &amp;problem, &amp;summary);</span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; summary.FullReport() &lt;&lt; <span class="string">"\n"</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>　　这里使用了 Bundle Adjustment in the Large<a href="#3" id="3ref"><sup>[3]</sup></a> 数据集，观测量为图像坐标系下路标(特征)的像素坐标系，待优化的参数为各路标的 3D 坐标以及相机内外参，这里相机内外参有 9 个，其中位置及姿态 6 个，畸变系数 2 个，焦距 1 个。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> 高翔. 视觉 SLAM 十四讲: 从理论到实践. 电子工业出版社, 2017.<br><a id="2" href="#2ref">[2]</a> https://github.com/HKUST-Aerial-Robotics/A-LOAM<br><a id="3" href="#3ref">[3]</a> http://grail.cs.washington.edu/projects/bal/</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　非线性最小二乘(Non-linear Least Squares)问题应用非常广泛，尤其是在 SLAM 领域。&lt;a href=&quot;/LOAM/&quot; title=&quot;LOAM&quot;&gt;LOAM&lt;/a&gt;，，&lt;a href=&quot;/[paper_reading]-Stereo-RCNN-ba
      
    
    </summary>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/categories/SLAM/"/>
    
    
      <category term="Optimization" scheme="https://leijiezhang001.github.io/tags/Optimization/"/>
    
  </entry>
  
  <entry>
    <title>如何搭建一个 ADAS 产品</title>
    <link href="https://leijiezhang001.github.io/How-to-Build-An-ADAS/"/>
    <id>https://leijiezhang001.github.io/How-to-Build-An-ADAS/</id>
    <published>2020-05-07T01:36:23.000Z</published>
    <updated>2020-05-15T10:22:19.245Z</updated>
    
    <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="本文介绍了基于视觉的 ADAS 产品构建方法，全文 5K 字，请输入密码查看：" />    <label for="pass">本文介绍了基于视觉的 ADAS 产品构建方法，全文 5K 字，请输入密码查看：</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      从零到一构建视觉 ADAS 产品。
    
    </summary>
    
      <category term="Autonomous Driving System" scheme="https://leijiezhang001.github.io/categories/Autonomous-Driving-System/"/>
    
      <category term="ADAS" scheme="https://leijiezhang001.github.io/categories/Autonomous-Driving-System/ADAS/"/>
    
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="ADAS" scheme="https://leijiezhang001.github.io/tags/ADAS/"/>
    
  </entry>
  
  <entry>
    <title>OctoMap</title>
    <link href="https://leijiezhang001.github.io/OctoMap/"/>
    <id>https://leijiezhang001.github.io/OctoMap/</id>
    <published>2020-04-23T02:23:05.000Z</published>
    <updated>2020-04-27T02:31:31.376Z</updated>
    
    <content type="html"><![CDATA[<p>　　地图是机器人领域非常重要的模块，也可以认为是自动驾驶保障安全的基础模块。根据存储建模类型，地图可分为拓扑地图，栅格地图，点云地图等。<a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 就是一种能在线检测静态障碍物的栅格地图。自动驾驶领域，地图的用处有：</p><ul><li><strong>高精度定位</strong>，一般是 3D 栅格地图，但是栅格中近似存储点云原始信息；</li><li><strong>路径规划</strong>，不同规划算法依赖不同地图，自动驾驶中比较靠谱又简单的规划算法一般依赖拓扑地图，俗称高精度语义地图，描述一些车道线等路面拓扑关系；而在室内或低速无道路信息场景，则会用如 \(A ^ * \) 算法在栅格地图上进行路径规划；</li><li><strong>辅助感知检测未定义类别的障碍物</strong>，有人称之为静态地图，一般是 2.5D 栅格地图，图层可以自定义一些语义信息；</li></ul><p>下游不同模块对不同存储方式的利用效率是不同的，所以需要针对不同下游任务设计不同地图建模方式。本文<a href="#1" id="1ref"><sup>[1]</sup></a>介绍了一种基于八叉树的栅格地图建模方法。<br>　　对于机器人而言，类似 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 能建模 FREE，OCCUPIED，UNMAPPED AREAS 的地图是信息量比较丰富的，但是 Grid-Mapping 是 2D 的。这里对 3D 地图有以下要求：</p><ul><li><strong>Probabilistic Representation</strong><br>测量都会有不确定性，这种不确定性需要用概率表征出来；另外多传感器融合也需要基于概率的表示；</li><li><strong>Modeling of Unmapped Areas</strong><br>对机器人导航而言，显式得表示哪些区域是观测未知的也非常重要；</li><li><strong>Efficiency</strong><br>地图构建与存储需要非常高效，一般而言，地图的内存消耗会是瓶颈；</li></ul><p><img src="/OctoMap/maps.png" width="90%" height="90%" title="图 1. Different Representations of Maps"> 　　如图 1. 所示，原始点云地图信息量丰富，但是不能结构化存储；Elevation Maps 与 Multi-level Surface Maps 虽然高效，但是不能表征未观测的区域信息。OctoMap 可以认为是 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 的 3D 版本，信息量丰富且高效。</p><h2 id="octomap-mapping-framework">1. OctoMap Mapping Framework</h2><h3 id="octrees">1.1. Octrees</h3><p><img src="/OctoMap/OctoMap.png" width="40%" height="40%" title="图 2. 八叉树地图存储"> 　　如图 2. 所示，八叉树是将空间递归得等分成八份(QuadTree 四叉树则等分为四份)，每个节点可以存储 Occupied，Free，Unknown 信息(Occupied 概率即可)。此外，如果子节点的状态都一样，那么可以进行剪枝，只保留大节点低分辨率的 Voxel，达到紧凑存储的目的。<br>　　时间复杂度上，对于有 \(n\) 个节点，深度为 \(d\) 的八叉树，那么单次查询的时间复杂度为 \(\mathcal{O}(d)=\mathcal{O}(\mathrm{log}\,n)\)；遍历节点的时间复杂度为 \(\mathcal{O}(n)\)。\(d = 16, r = 1cm\)，可以覆盖 \((655.36m)^3\)的区域。</p><h3 id="probabilistic-sensor-fusion">1.2. Probabilistic Sensor Fusion</h3><p>　　时序概率融合也是基于贝叶斯滤波，详见 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a>，只不过这里是 3D Mapping，作 Raycasting 的时候采用 <a href="/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/" title="What You See is What You Get">What You See is What You Get</a> 中提到的 Fast Voxel Traversal 算法。实际应用中，一般都会采用上下界限制概率值，这种限制也能提高八叉树的剪枝率。</p><h3 id="multi-resolution-queries">1.3. Multi-Resolution Queries</h3><p>　　由于八叉树的特性，OctoMap 支持低于最高分辨率的 Voxel 概率查询，即父节点是子节点的平均概率，或是子节点的最大概率: <span class="math display">\[\bar{l}(n)=\frac{1}{8}\sum _ {i=1}^8 L (n _ i)\\\hat{l}(n)=\max\limits _ iL(n _ i)\tag{1}\]</span> 其中 \(l\) 是测量模型下概率的 log-odds 值。</p><h2 id="implementation-details-statics">2. Implementation Details &amp; Statics</h2><h3 id="memory-efficient-node-map-file-generation">2.1. Memory-Efficient Node &amp; Map File Generation</h3><p><img src="/OctoMap/save.png" width="60%" height="60%" title="图 3. Node Memory Consumption and Serialization"> 　　如图 3. 左图所示，每个节点只分配一个 float 型的数据存储以及指向子节点地址数组的地址指针(而不是直接包含子节点地址的指针)，只有存在子节点时，才会分配子节点的地址数组空间。由此在 32-bit 系统中(4 字节对齐)，每个父节点需要 40B，子节点需要 8B；在 64-bit 系统中(8 字节对齐)，每个父节点需要 80B(\(4+9\times 8\))，子节点需要 16B(\(4+8)\)。<br>　　地图存储需要在信息量损失最小的情况下进行压缩。如图 3. 右图所示，存储序列化时，每个叶子节点总共需要 4B 概率值，不需要状态量；每个父节点总共需要 2B，表示 8 个子节点的 2bit 状态量(貌似与论文有出入，其不是最优的压缩)。在这种压缩方式下，大范围地图的存储大小一般也能接受。根据存储的地图重建地图时，只需要知道坐标原点即可。</p><h3 id="accessing-data-memory-consumption">2.2. Accessing Data &amp; Memory Consumption</h3><p><img src="/OctoMap/memusage1.png" width="60%" height="60%" title="图 4. Memory Usage VS. Scan Num."> 　　Freiburg 建图大小为 \((202\times 167\times 28) m^3\)，如图 4. 所示，随着点云扫描区域扩大，OctoMap 表示方式能有效降低建图大小。 <img src="/OctoMap/memusage2.png" width="60%" height="60%" title="图 5. Memory Usage VS. Resolution"> 　　图 5. 则说明建图大小与分辨率的关系。 <img src="/OctoMap/inserttime.png" width="60%" height="60%" title="图 6. Insert Date Time VS. Resolution"> <img src="/OctoMap/traversetime.png" width="60%" height="60%" title="图 7. Traverse Data Time VS. Depth"> 　　图 6. 显示了往图中插入一个节点所需时间，1000 个节点在毫秒级；图 7. 显示了遍历所有节点所需的时间，基本也在毫秒级。 <img src="/OctoMap/compress.png" width="60%" height="60%" title="图 8. Compression Ratio"> 　　通过限制概率上下界，可以剪枝压缩图，用 KL-diverge 来评估压缩前后图的分布相似性，图 8. 显示了压缩比与网络大小及相似性的关系。</p><h3 id="some-strategies">2.3. Some Strategies</h3><p><img src="/OctoMap/case.png" width="60%" height="60%" title="图 9. Corner Case Handle"> 　　如图 9. 所示，前后帧位姿的抖动，会导致 Occupied 持续观测的不稳定，所以需要一些领域约束策略来保证 Occupied 的稳定观测。这种类似的策略在 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 工程实现中也需要采用，因为实际的 Pose 肯定会有噪声，导致同一目标的栅格前后有一定概率不能完全命中。</p><h2 id="rethinking">3. ReThinking</h2><p>　　对于自动驾驶来说，高度方向的范围不需要很大，甚至四叉树足矣，如果采用八叉树，那么需要将高度方向的分辨率降低，从而更加紧凑的构建地图。<br>　　此外自动驾驶肯定是需要大范围建图的，如平方千公里级别。所以切片式的地图存储与查询就显得尤为重要，换句话说，需要动态得载入局部地图，这就有两种思路：</p><ul><li>动态载入完全局部地图<br>要求前后局部地图有一定的重叠，通过索引式的存储可以不存储重叠区域的地图信息；</li><li>动态载入部分局部地图<br>随着机器人本体的运动，实时动态载入前方更远处的地图，丢掉后方远处的历史地图。这对在线地图结构的灵活性要求比较高，如果基于八叉树，那么需要作片区域剪枝及插入的操作，效率不一定高；</li></ul><p>　　在自动驾驶领域，目前用于高精度定位的栅格地图与用于 PNC 规划控制的拓扑地图(高精地图)已经比较成熟；而用于环境感知的静态语义地图还没形成大范围的共识。不管从工程实现效果及效率上，还是语义信息描述定义上，还需作很多探索与实践。比如，可以定义最底层的语义信息：地面高度，此外也可以把车道线信息打到栅格图层中去(但是可能加大对 PNC 的搜索计算量)，等等。所以可能最优的存储查询方式并不是八叉树，<strong>可能还是栅格化后并对每个栅格哈希化，牺牲一定的内存空间，然后作 \(O(1)\) 的快速插入与查询</strong>。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Hornung, Armin, et al. &quot;OctoMap: An efficient probabilistic 3D mapping framework based on octrees.&quot; Autonomous robots 34.3 (2013): 189-206.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　地图是机器人领域非常重要的模块，也可以认为是自动驾驶保障安全的基础模块。根据存储建模类型，地图可分为拓扑地图，栅格地图，点云地图等。&lt;a href=&quot;/Grid-Mapping/&quot; title=&quot;Grid-Mapping&quot;&gt;Grid-Mapping&lt;/a&gt; 就是一种能在
      
    
    </summary>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/categories/SLAM/"/>
    
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/tags/SLAM/"/>
    
      <category term="Mapping" scheme="https://leijiezhang001.github.io/tags/Mapping/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;What You See is What You Get, Exploiting Visibility for 3D Object Detection&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/"/>
    <id>https://leijiezhang001.github.io/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/</id>
    <published>2020-04-22T01:19:59.000Z</published>
    <updated>2020-04-23T02:34:12.937Z</updated>
    
    <content type="html"><![CDATA[<p>　　Bird-View 3D Detection 都是将点云离散化到 Voxel，有点的 Voxel 提取区域特征，无点的 Voxel 则置为空。而 LiDAR 的测量特性其实还包含更多的信息，<a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 中较详细的阐述了 LiDAR 的测量模型，每个栅格可以标记为三个状态：UNKNOWN，FREE，OCCUPIED。传统的 Bird-View 3D Detection 没有显式得提取 UNKNOW 与 FREE 的信息(即没有提取 Visibility 信息)，而 UNKNOW 与 FREE 对数据增广及检测效果非常重要。 <img src="/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/visibility.png" width="90%" height="90%" title="图 1. Visibility or Freespace from LiDAR"> 　　如图 1. 所示，左图是传统的点云表示方式，无法区分红色区域是否有车，而右图则非常容易得区分哪个区域不可能有车，哪个区域可能有车。所以本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了显式提取点云 UNKNOWN 与 FREE 信息来辅助数据增广与提高目标检测精度的方法。</p><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/framework.png" width="90%" height="90%" title="图 2. Framework"> 　　如图 2. 所示，本文的 3D 检测框架与传统的差不多，是 Anchor-Based 方法，主要不同点是输入网络的特征，即点云栅格化后提取出的特征不一样以及融合时序信息。并且，训练过程中，对数据增广做了精心的设计。 <img src="/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/fusion.png" width="90%" height="90%" title="图 3. Frusion Strategy"> 　　如图 3. 所示，点云栅格化后提取的特征不一样是指增加了 Visibility 图层。有两种融合方式，前融合是与点云栅格化后提取的特征作 Concate，然后输入到主干网络；后融合则是二者分别通过主干网络，然后再作 Concate。实验表明前融合效果较好。</p><h3 id="object-augmentation">1.1. Object Augmentation</h3><p>　　传统的数据增广关注在全帧点云的平移，旋转，翻转变换。本文则采用目标级别的数据增广。首先生成目标的点云集合，可以用 CAD 模型，也可以直接扣实际的目标点云(扣出来的点云增广能力有限)；然后将目标点云集合随机得放到全帧点云中。在放置的过程中需要模拟 LiDAR 的测量模型，也就是 Visibility 计算过程，这在第 2. 节中详细描述。实验表面能提升 ~9 个点。</p><h3 id="temporal-aggregation">1.2. Temporal Aggregation</h3><p>　　时序点云信息的利用可以有以下几种方法：</p><ul><li>将每帧点云栅格化，然后直接在 Chanel 层作 Concate，之后作 3D 卷积，或者先在 Chanel 维度作 1D 卷积，然后作 2D 卷积；</li><li>将点云中的点增加相对时间戳属性，然后作整体的栅格化，之后直接作传统的 2D 卷积；</li></ul><p>本文采用第二种方法，实验表明能提升 ~8 个点。</p><h2 id="visibility-computing">2. Visibility Computing</h2><p>　　<a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 中已经应用了 Raycasting 来计算 Visibility/Free。对于点云中的每一个点，我们不仅能获得该点探测到障碍物的这个信息，还能知道，传感器与该点之间的连线上是 Free 的。这就要求能高效得计算该连线相交 Voxel 的集合。该计算模型也用来修正 Object Augmentation 时的点云。</p><h3 id="efficient-voxel-traversal">2.1. Efficient Voxel traversal</h3><p>　　对每个点，都需要遍历传感器原点到该点所经过的 Voxel，采用 Fast Voxel Traversal<a href="#2" id="2ref"><sup>[2]</sup></a>方法来进行高效的 Voxel 遍历。</p><h3 id="raycasting-with-augmented-objects">2.2. Raycasting with Augmented Objects</h3><p><img src="/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/augment.png" width="90%" height="90%" title="图 4. Rectify Object Augmentation"> 　　如图 4. 所示，本文设计了两种策略来修正物体增广：</p><ul><li>Culling，如果该物体是被遮挡的，那么直接去掉，这样会极大减少增广的物体；</li><li>Drilling，如果该物体是被遮挡的，那么将遮挡物去掉，即置为 Free；</li></ul><p>实验表明 Drilling 效果较好，在训练时采用该策略进行物体增广后的点云修正，作 Inference 时就直接计算 Freespace 即可。</p><h3 id="online-occupancy-mapping">2.3. Online Occupancy Mapping</h3><p>　　栅格内点云提取特征时融合了时序信息，Visibility 也需要融合时序信息，最直观的方式是将 3D Occupancy Map 进行时间维度的堆叠，获得 4D Map，这样对后续的计算量较大。本文采用 OctoMap<a href="#3" id="3ref"><sup>[3]</sup></a> 计算方式，作贝叶斯滤波，得到时序滤波的 3D Occupancy Map。原理与 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 一样，只不过这里是 3D 的。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Hu, Peiyun, et al. &quot;What You See is What You Get: Exploiting Visibility for 3D Object Detection.&quot; arXiv preprint arXiv:1912.04986 (2019).<br><a id="2" href="#2ref">[2]</a> Amanatides, John, and Andrew Woo. &quot;A fast voxel traversal algorithm for ray tracing.&quot; Eurographics. Vol. 87. No. 3. 1987.<br><a id="3" href="#3ref">[3]</a> Hornung, Armin, et al. &quot;OctoMap: An efficient probabilistic 3D mapping framework based on octrees.&quot; Autonomous robots 34.3 (2013): 189-206.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Bird-View 3D Detection 都是将点云离散化到 Voxel，有点的 Voxel 提取区域特征，无点的 Voxel 则置为空。而 LiDAR 的测量特性其实还包含更多的信息，&lt;a href=&quot;/Grid-Mapping/&quot; title=&quot;Grid-Map
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Reconfigurable Voxels, A New Representation for LiDAR-Based Point Clouds&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-Reconfigurable-Voxels/"/>
    <id>https://leijiezhang001.github.io/paper-reading-Reconfigurable-Voxels/</id>
    <published>2020-04-20T01:54:31.000Z</published>
    <updated>2020-04-21T03:38:54.663Z</updated>
    
    <content type="html"><![CDATA[<p>　　Voxel-based 点云特征提取虽然损失了一定的信息，但是计算高效。Voxel-based 方法一个比较大的问题是，由于<strong>点云分布的不均匀性</strong>，作卷积时会导致可能计算的区域没有点，从而不能有效提取局部信息。为了解决栅格化后栅格中点云分布的不均匀问题，目前看到的有以下几种方法：</p><ol type="1"><li>Deformable Convolution，采用可变形卷积方法，自动学习卷积核的连接范围，理论上应该能更有效得使卷积核连接到点密度较高的栅格；</li><li><a href="/paper-reading-PolarNet/" title="PolarNet">PolarNet</a>提出了一种极坐标栅格化方式，因为点云获取的特性，这种方法获得的栅格中点数较为均匀;</li><li>手动设计不同分辨率的栅格，作特征提取，然后融合。比如近处分辨率较高，远处较低的方式；</li><li>本文<a href="#1" id="1ref"><sup>[1]</sup></a> 提出了一种自动选择栅格领域及分辨率，从而最大化卷积区域点数的方法；</li></ol><p><img src="/paper-reading-Reconfigurable-Voxels/reconfig.png" width="80%" height="80%" title="图 1. Reconfig Voxels"> 　　如图 1. 所示，本文提出的 Reconfigurable Voxel 方法，能自动选择领域内点数较多的栅格特征提取，进而作卷积运算，避免点数较少，从而信息量较少的栅格作特征提取操作；此外还可根据点数自动调整分辨率以获得合适的栅格点数。通过这种方法，每个栅格输入到网络前都能有效提取周围点数较多区域的特征信息。</p><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-Reconfigurable-Voxels/pipeline.png" width="80%" height="80%" title="图 2. Framework"> 　　如图 2. 所示，本文以检测任务为例，分三部分：Voxel/Pillar Feature Extraction，Backbone，RPN/Detection Head。后两个采用传统的方法，本文主要是改进 Voxel/Pillar Feature Extraction，这是输入到网络前的特征提取阶段。</p><h2 id="voxelpillar-feature-extraction">2. Voxel/Pillar Feature Extraction</h2><p>　　传统的输入到 2D 卷积网络的特征要么是手工提取的，要么是用 <a href="/paperreading-PointPillars/" title="PointPillars">PointPillars</a> 网络去学习每个 Voxel 的特征。由此输入到网络的特征不是最优的，因为点云的稀疏性会导致后面的 2D 卷积网络作特征提取时遇到很多“空”的 Voxel。本文提出的方法就能显式得搜索每个 Voxel 周围有点的区域作特征提取，使得之后 2D 卷积特征提取更加有效。其步骤为：</p><ul><li>点云栅格化，并存储每个 Voxel 周围 Voxel 的索引；</li><li>每个 Voxel 周围 Voxel 作 Biased Random Walk，去搜索有更稠密点云的 Voxel；</li><li>将每个 Voxel 与新搜索到的周围 Voxel 作特征提取与融合，得到该 Voxel 特征；</li></ul><h3 id="biased-random-walking-neighbors">2.1. Biased Random Walking Neighbors</h3><p>　　邻域 Voxel 搜索目标是：<strong>在距离较近的情况下寻找较稠密的 Voxel</strong>。由此设计几种策略：</p><ul><li>点数越少的 Voxel，有更高概率作 Random Walk，以及更多 Step 去周围相邻的 Voxel；</li><li>点数越多的 Voxel，有更高概率被其它 Voxel Random Walk 到；</li></ul><p>　　将以上策略数学化。设第 \(j\) 个 Voxel 有 \(N(j)\) 个点，最大点数为 \(n\)，其作 Random Walk 的概率为 \(P _ w(j)\)，步数 Step 为 \(S(j)\)，第 \(i\) 步到达的 Voxel 为 \(w _ j(i)\)，其四领域 Voxel 集合为 \(V(w _ j(i))\)，从该 Voxel 走到下一个 Voxel 的概率为 \(P(w _ j(i+1)|w _ j(i))\)。由此得到以上策略的数学描述： <span class="math display">\[P _ w(j)=\frac{1}{N(j)} \tag{1}\]</span> <span class="math display">\[S(j)=n-N(j)\tag{2}\]</span> <span class="math display">\[P\left(w _ j(i+1)|w _ j(i)\right) = \frac{N\left(w _ j(i+1)\right)}{\sum _ {v\in V(w _ j(i))}N(v)}\tag{3}\]</span> 需要注意的是，\(S(j)\) 是在开始时计算的，此后每走一步就减1。 <img src="/paper-reading-Reconfigurable-Voxels/random_walk.png" width="90%" height="90%" title="图 3. Random walk"> 　　如图 3. 所示，左边为单分辨率下 Voxel 搜索过程。</p><h3 id="reconfigurable-voxels-encoder">2.2. Reconfigurable Voxels Encoder</h3><p>　　每个 Voxel \(v _ i\) 搜索到最优的 4 领域 Voxel 集 \(V(v _ i)\) 后，需要融合得到该 Voxel 的特征： <span class="math display">\[\begin{align}F(v _ i) &amp;= \psi\left(f _ {v _ i}, f _ {V(v _ i)}\right)\\&amp;= \varphi _ 1\left[\varphi _ 2(f _ {v _ i}), \varphi _ 2\left(\sum _ {j=1}^4 W _ j(f _ {v _ i})f _ {V _ {j(v _ i)}}\right)\right] _ f\tag{4}\end{align}\]</span> 其中 \(\varphi _ 1\) 为 low-level 操作，如 average pooling，\(\varphi _ 2\) 为 high-level 操作，如 MLP。</p><h3 id="multi-resolution-reconfigurable-voxels">2.3. Multi-resolution Reconfigurable Voxels</h3><p>　　图 3. 左边是单分辨率情况，Random Walking 可以拓展到多分辨率情形。当点云非常稀疏的时候，就很有必要降低栅格的分辨率。如图 3. 所示，\(P _ w\) 计算时除以 4，以维持与高分辨率的一致性；高分辨率到低分辨率搜索概率为 \(0.25P _ w\)，低分辨率到高分辨率搜索概率为 \(0.5P _ w\)。其余准则与单分辨率一致。实验结果表面多分辨率有一定提升，但是相比单分辨率提升不明显。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Wang, Tai, Xinge Zhu, and Dahua Lin. &quot;Reconfigurable Voxels: A New Representation for LiDAR-Based Point Clouds.&quot; arXiv preprint arXiv:2004.02724 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Voxel-based 点云特征提取虽然损失了一定的信息，但是计算高效。Voxel-based 方法一个比较大的问题是，由于&lt;strong&gt;点云分布的不均匀性&lt;/strong&gt;，作卷积时会导致可能计算的区域没有点，从而不能有效提取局部信息。为了解决栅格化后栅格中点云分布
      
    
    </summary>
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;PolarNet&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-PolarNet/"/>
    <id>https://leijiezhang001.github.io/paper-reading-PolarNet/</id>
    <published>2020-04-16T01:19:12.000Z</published>
    <updated>2020-04-17T02:09:42.539Z</updated>
    
    <content type="html"><![CDATA[<p>　　Point-wise 特征提取在 <a href="/PointCloud-Feature-Extraction/" title="PointCloud-Feature-Extraction">PointCloud-Feature-Extraction</a> 中已经有较为详细的描述，虽然 Point-wise 提取的特征更加精细，但是一般都有 KNN 构建及索引操作，计算量较大，而且实践中发现学习收敛较慢。Voxel-based 虽然理论上损失了一定的信息，但是能直接应用 2D 卷积网络，网络学习效率很高。本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种在极坐标下栅格化后进行点云 Semantic Segmentation 的方法，相比传统的笛卡尔坐标系下栅格化有一定的优势。</p><h2 id="voxelization">1. Voxelization</h2><p><img src="/paper-reading-PolarNet/pts.png" width="98%" height="98%" title="图 1. Cartesian VS. Polar"> 　　如图 1. 所示，传统的笛卡尔坐标系下栅格化的栅格是矩形，而极坐标系下栅格是饼状的。激光雷达是在极坐标方式下获取点云的，所以由图可知，<strong>极坐标栅格化下，每个栅格拥有的点数更加均匀</strong>，有利于网络学习并减少计算量。此外，本文统计后显示，相比笛卡尔坐标栅格，极坐标的栅格内点属于同一目标的概率更大。</p><h2 id="polarnet-framework">2. PolarNet Framework</h2><p><img src="/paper-reading-PolarNet/framework.png" width="98%" height="98%" title="图 2. PolarNet"> 　　如图 2. 所示，点云经过 Polar 栅格化后，对每个栅格首先进行 PointNet 特征提取，然后对所有栅格作 ring-convolution 操作。<br>　　ring-convolution 是指卷积在环形方向进行，没有边缘截断效应。实现上，将栅格从某处展开，然后边缘处用另一边对应的栅格进行 padding，即可用普通的卷积进行运算。<br>　　网络是作 Voxel-wise 的分割，然后直接将预测的类别应用到栅格内的点云中。统计上，同一栅格内的点云属于不同类别的概率很低，所以本文并没进一步作 Point-wise 的分割。</p><h2 id="rethinking">3. Rethinking</h2><p>　　PolarNet 作 Semantic Segmentation 比其它方法提升很多。但是实际应用时，PolarNet 不能指定各个方向的范围，所以计算效率较低。比如，自动驾驶中，我们可以设定前 100m，后 60m，左右各 30m 的检测范围，笛卡尔坐标系下很容易进行栅格化，而极坐标下则没法搞。所以为了解决点云的分布不均匀问题，另一种思路是在笛卡尔坐标系下，近处打高分辨率的栅格，远处打低分辨率的栅格。具体实现，可以先用低分辨率过一遍网络，然后再对感兴趣的特定区域作高分辨率检测。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Zhang, Yang, et al. &quot;PolarNet: An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation.&quot; arXiv preprint arXiv:2003.14032 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Point-wise 特征提取在 &lt;a href=&quot;/PointCloud-Feature-Extraction/&quot; title=&quot;PointCloud-Feature-Extraction&quot;&gt;PointCloud-Feature-Extraction&lt;/a&gt; 中已经有
      
    
    </summary>
    
      <category term="Semantic Segmentation" scheme="https://leijiezhang001.github.io/categories/Semantic-Segmentation/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/tags/Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>PointFlowNet</title>
    <link href="https://leijiezhang001.github.io/PointFlowNet/"/>
    <id>https://leijiezhang001.github.io/PointFlowNet/</id>
    <published>2020-04-13T01:54:59.000Z</published>
    <updated>2020-04-15T03:07:01.997Z</updated>
    
    <content type="html"><![CDATA[<p>　　点云的 Scene Flow 与 Semantic 一样是一个较低层的信息，通过 Point-Wise Semantic 信息可以作物体级别的检测，这种方式有很高的召回率，且超参数较少。同样，通过 Point-Wise Scene Flow 作目标级别的运动估计(当然也可作物体点级别聚类检测的线索)，也会非常鲁棒。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 将点级别/Voxel 级别的 Scene Flow 与 3D 目标检测融合在一起，作物体级别的运动估计，工作系统性较强。</p><h2 id="问题描述">1. 问题描述</h2><p>　　设 \(t\) 时刻点云 \(\mathbf{P} _ t\in\mathbb{R}^{M\times 3}\)，那么需要求解的未知量有：</p><ul><li>每个点的 Scene Flow: \(\mathbf{v} _ i\in\mathbb{R} ^3\);</li><li>每个点集的 Rigid Motion: \(\mathbf{R} _ i\in\mathbb{R}^{3\times 3}\)，\(\mathbf{t} _ i\in\mathbb{R}^{3}\);</li><li>每个物体的 3D 属性：Location，Orientation，Size，Rigid Motion;</li></ul><h2 id="算法框架">2. 算法框架</h2><p><img src="/PointFlowNet/framework.png" width="90%" height="90%" title="图 1. PoingFlowNet Framework"> 　　如图 1. 所示，PointFlowNet 由四部分组成，分别为：Feature Encoder，Scene Flow/Ego-motion Estimation and 3D Object Detection，Rigid Motion Estimation，Object Motion Decoder。Feature Encoder 将前后帧点云栅格化后作特征提取，然后 Context Encoder 作进一步的特征融合去提取；输出的特征第一个分支作 Voxel 级别的 Scene Flow 预测，进一步作每个点的 Rigid Motion 预测(<strong>每个点属于对应物体的 Motion 在该 Voxel 坐标系下的表示</strong>)；第二个分支作 Ego-Motion 的预测；第三个分支作 3D 目标检测，进一步作目标的 Motion Decoder。</p><h3 id="feature-encoder">2.1. Feature Encoder</h3><p>　　不同的点云特征提取方式都可采用，本文采用传统的 Bird-View Voxel 表示方式，然后作 2D/3D 卷积。同时还需要将前后帧的点云作特征融合，这里也完全可以采用 <a href="/paperreading-FlowNet3D/" title="FlowNet3D">FlowNet3D</a> 的特征提取形式。</p><h3 id="scene-flowrigid-motion-decoder">2.2. Scene Flow/Rigid Motion Decoder</h3><p>　　Scene Flow 是作 Bird-View 下 Voxel 级别的场景流预测，然后再预测 Rigid Motion。 <img src="/PointFlowNet/rigid-motion.png" width="80%" height="80%" title="图 2. Rigid MOtion Estimation"> 　　如图 2. 所示，世界坐标系 \(\mathbf{W}\) 下点 \(\mathbf{p}\) 的 scene flow 表示为 \(\mathbf{v}\)，刚体物体的局部坐标系从 \(\mathbf{A}\) 经过 \((\mathbf{R _ A, t _ A})\) 运动到 \(\mathbf{B}\) ，那么其 scene flow 可表示为： <span class="math display">\[\mathbf{v=[R _ A(p-o _ A)+t _ A]-(p-o _ A)} \tag{1}\]</span> 本文论证了两个定理：</p><ol type="1"><li>scene flow 只能通过刚体局部坐标系的运动导出，不能直接通过世界坐标系下的刚体运动导出(除非运动无旋转量)。所以如图 1. 所示，通过 scene flow 预测出的 voxel motion 是局部坐标系下的，还需通过坐标变换到世界坐标系下。<strong>这里每个 Voxel 预测量的局部坐标系采用 Voxel 中心点</strong>。作目标运动估计时，&quot;世界坐标系&quot;其实可以定义为物体坐标系(Voxel 为局部坐标系)，最后再通过 Ego-motion 变换到世界坐标系。</li><li>不管是局部坐标系 \(\mathbf{A}\) 还是 \(\mathbf{B}\)，都能导出 scene flow。</li></ol><p>　　如图 2. 所示，实验也验证了 scene flow 不能直接学习到世界坐标系下的 translation 运动。</p><h3 id="ego-motion-regressor">2.3. Ego-motion Regressor</h3><p>　　根据前后帧的点云回归本车的运动(ego-motion)，ego-motion 建立局部坐标系与世界坐标系的联系。如果有更精准的外部模块估计的 ego-motion，则可以直接替换采用。</p><h3 id="d-object-detection-and-object-motion-decoder">2.4. 3D Object Detection and Object Motion Decoder</h3><p>　　Bird-view 下 Voxel 后的 3D 检测方法很多，可以是 Anchor-based，Anchor-free，Semantic Segmentation 等方法，其中如果采用 Semantic Segmentation + cluster 方法，那么 scene flow 的结果也可作为 cluster 的线索。<br>　　有了 3D 目标以及目标内 Voxel 的 Rigid Motion 后，取平均或中值即可得到目标的 Motion。<br>　　<strong>Voxel Rigid Motion 可以有两种回归方法：</strong></p><ol type="1"><li>translation 真值为实际该 Voxel 的位移，rotation 为对应刚体的旋转量；</li><li>translation 与 rotation 均为对应刚体的位移与旋转量；</li></ol><p>我理解的本文是采用方法 1. 这种形式，这种形式的好处是回归的就是真实 Voxel 的位移，与输入的特征是 Voxel 级别对应的，但是简单的对目标内的 Voxel 取平均或中值只是目标位移的近似，实际目标的真实位移应该为旋转中心 Voxel 的位移。而方法 2. 是物体级别的回归量，均值即可反应物体的运动，只要构建物体级别的 Loss，用 Voxel 去学习物体级别的运动应该问题不大，所以可能方法 2. 更合理。</p><h2 id="loss-functions">3. Loss Functions</h2><p>　　采用 Voxel 级别的 Loss，总的 Loss 为： <span class="math display">\[\mathcal{L}=\alpha\mathcal{L} _ {flow}+\beta\mathcal{L} _ {rigmo} + \gamma\mathcal{L} _ {ego}+\mathcal{L} _ {det}\tag{2}\]</span> 这四部分具体的形式为：</p><ol type="1"><li>Scene Flow Loss<br>对于有效的 Voxel，作预测值与真值的 \(\mathcal{l} _ 1\) 误差： <span class="math display">\[\mathcal{L} _ {flow}=\frac{1}{K}\sum _ j\Vert \mathbf{v} _ j-\mathbf{v} _ j ^ * \Vert \tag{3}\]</span></li><li>Rigid Motion Loss<br>对于有效的 Voxel，作预测值与真值(真值有两种形式，详见 2.4 讨论)的 \(\mathcal{l} _ 1\) 误差： <span class="math display">\[\mathcal{L} _ {rigmo} = \frac{1}{K}\sum _ j\Vert\mathbf{t} _ j-\mathbf{t} _ j^ * \Vert+\lambda\Vert\theta _ j-\theta _ j^ * \Vert\tag{4}\]</span></li><li>Ego-motion Loss<br>同样的对预测值与真值作 \(\mathcal{l} _ 1\) Loss: <span class="math display">\[\mathcal{L} _ {ego}=\Vert\mathbf{t} _ {BG}-\mathbf{t} _ {BG}^ * \Vert+\lambda\Vert\theta _ {BG}-\theta _ {BG}^ * \Vert \tag{5}\]</span></li><li>Detection Loss<br>不作赘述。</li></ol><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Behl, Aseem, et al. &quot;Pointflownet: Learning representations for rigid motion estimation from point clouds.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　点云的 Scene Flow 与 Semantic 一样是一个较低层的信息，通过 Point-Wise Semantic 信息可以作物体级别的检测，这种方式有很高的召回率，且超参数较少。同样，通过 Point-Wise Scene Flow 作目标级别的运动估计(当然也
      
    
    </summary>
    
      <category term="Scene Flow" scheme="https://leijiezhang001.github.io/categories/Scene-Flow/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Scene Flow" scheme="https://leijiezhang001.github.io/tags/Scene-Flow/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
  </entry>
  
  <entry>
    <title>The Normal Distributions Transform for Laser Scan Matching</title>
    <link href="https://leijiezhang001.github.io/paper-reading-The-Normal-Distributions-Transform/"/>
    <id>https://leijiezhang001.github.io/paper-reading-The-Normal-Distributions-Transform/</id>
    <published>2020-04-10T01:39:54.000Z</published>
    <updated>2020-04-11T11:29:19.873Z</updated>
    
    <content type="html"><![CDATA[<p>　　机器人系统中，定位是非常重要的模块。基于 SLAM/VO/VIO 技术的算法能实时作机器人的自定位，但是这种开环下的里程计方案很容易累积绝对误差，使得定位漂移。而离线建立的地图因为有闭环检测，精度很高，所以基于地图的定位方法有很高的绝对定位精度。<br>　　<a href="/LOAM/" title="LOAM">LOAM</a> 是一种基于点云的实时建图与定位方法，其中当前帧点云与前序建立的地图点云配准的方法，采用了提取线、面特征并建立点-线，点-面特征匹配误差函数，从而最小二乘非线性优化求解位姿。这种方案如果特征点噪声较大无匹配对，那么就会有较大的误差。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 将地图点云栅格化，每个栅格又统计点云的高斯分布，匹配的时候计算该帧点云在每个栅格的概率，从而迭代至最优匹配位姿。<br>　　<strong>有闭环检测</strong>的 SLAM 建立的地图即可作为离线定位地图，定位的过程就是当前时刻点云与地图配准的过程，当然后续可以融合其它传感器(GPS，IMU)输出最终的绝对位姿。<strong>点云与地图配准的过程与建图时点云与局部地图或上一时刻点云配准的过程非常相似</strong>。本文介绍一种区别于 <a href="/LOAM/" title="LOAM">LOAM</a> 特征匹配的基于概率统计优化的 NDT 配准方法。</p><h2 id="点云配准算法过程">1. 点云配准算法过程</h2><p>　　考虑二维情况，本文点云配准算法过程为：</p><ol type="1"><li>建立 \(t-1\) 帧点云的 NDT；</li><li>初始化待优化的相对位姿参数 \(T\);</li><li>用 \(T\) 将 \(t\) 帧点云变换到 \(t-1\) 坐标系；</li><li>找到变换每个变换点对应的 \(t-1\) 帧栅格的高斯分布；</li><li>该变换 \(T\) 的度量分数为变换点在高斯分布下的概率和；</li><li>用 Newton 法迭代优化 \(T\);</li><li>重复 3. 直到收敛；</li></ol><p>　　这里主要涉及 NDT，目标函数构建(即 \(T\) 的度量分数)，Newton 法优化三个内容。</p><h3 id="ndt">1.1. NDT</h3><p>　　NDT 是点云栅格化后一系列高斯分布的表示，其过程为：</p><ol type="1"><li>将点云进行栅格化；</li><li>统计每个栅格的点 \(\mathbf{x} _ {i=1..n}\)；</li><li>计算每个栅格高斯分布的 Mean: \(\mathbf{q} = \frac{1}{n}\sum _ i\mathbf{x} _ i\);</li><li>计算 Covariance Matrix: \(\Sigma = \frac{1}{n}\sum _ i(\mathbf{x} _ i -\mathbf{q})(\mathbf{x} _ i-\mathbf{q})^t\)；</li></ol><p>　　由此，<strong>NDT 描述了栅格内每个位置出现点的概率</strong>，即 \(\mathbf{x}\) 有点的概率为： <span class="math display">\[ p(\mathbf{x}) \sim \mathrm{exp}\left(-\frac{(\mathbf{x-q})^t\sum ^ {-1}(\mathbf{x-q})}{2}\right) \tag{1}\]</span> 需要注意的是 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 描述的是每个栅格有点的概率，NDT 描述的是每个栅格点云的概率分布。为了更准确的建模，采用重叠栅格化的设计以消除离散化的影响，以及限定 Covariance 矩阵的最小奇异值。</p><h3 id="目标函数构建">1.2. 目标函数构建</h3><p>　　考虑二维情况，需要优化的位姿参数为 \(\mathbf{p}=(t _ x, t _ y, \varphi)^t\)，第2个点云(待配准点云)中的点为 \(\mathbf{x} _ i\)，其变换到第1个点云坐标系后的表示为 \(\mathbf{x}' _ i\)，对应的第1个点云栅格的 NDT 表示为 \(\mathbf{\Sigma} _ i, \mathbf{q} _ i\)。由此可计算该变换位姿下，其度量分数为： <span class="math display">\[\mathrm{score}(\mathbf{p})=\sum _ i\mathrm{exp}\left(-\frac{(\mathbf{x}&#39; _ i-\mathbf{q} _ i)^t\sum _ i ^ {-1}(\mathbf{x}&#39; _ i-\mathbf{q} _ i)}{2}\right) \tag{2}\]</span> 最大化度量函数即可求解最优的位姿，优化过程一般都是最小化目标函数，所以设定目标函数为 \(-\mathrm{score}\)。</p><h3 id="newton-法优化迭代">1.3. Newton 法优化迭代</h3><p>　　设 \(\mathbf{q}=\mathbf{x}' _ i-\mathbf{q} _ i\)，那么目标函数为： <span class="math display">\[ s = -\mathrm{exp}\frac{-\mathbf{q^t\sum ^ {-1}q}}{2} \tag{3}\]</span> 每次迭代过程为： <span class="math display">\[\mathbf{p\gets p+\Delta p} \tag{4}\]</span> 而 \(\mathbf{\Delta p}\) 来自： <span class="math display">\[\mathbf{H\Delta p} = \mathbf{-g} \tag{5}\]</span> 其中 \(\mathbf{g}\) 是目标函数对优化参数的导数，\(\mathbf{H}\) 为目标函数的 Hessian 矩阵： <span class="math display">\[\left\{\begin{array}{l}g _ i=\frac{\partial s}{\partial p _ i}\\H _ {ij} = \frac{\partial s}{\partial p _ i\partial p _ j}\end{array}\tag{6}\right.\]</span></p><h2 id="建图与定位">2. 建图与定位</h2><p>　　本文的建图是通过<strong>关键帧集合与关键帧之间的位姿变化实现的</strong>，定位的时候去找重合度最高的关键帧作点云配准。此外，当找不到重合度较高的关键帧时，可以实时更新当前帧作为关键帧添加到地图中，还可以对地图作进一步的全局，半全局优化。</p><h2 id="一些思考">3. 一些思考</h2><p>　　本文建图是关键帧的形式，更鲁棒的做法是将点云配准到一起，在世界坐标系下获得场景的稠密点云，然后再 NDT 化，这样能更准确的建模点云分布。<br>　　<a href="/LOAM/" title="LOAM">LOAM</a> 维护的是栅格化的地图，每个栅格限制特征点的数量，所以本质上存储的是原始点云图(被选出是特征点的点云)。为了更好的描述栅格内的特征分布，可以对其作类似 NDT 近似，同时加入能描述该分布的特征，比如对于面特征，加入法向量。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Biber, Peter &amp; Straßer, Wolfgang. (2003). The Normal Distributions Transform: A New Approach to Laser Scan Matching. IEEE International Conference on Intelligent Robots and Systems. 3. 2743 - 2748 vol.3. 10.1109/IROS.2003.1249285.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　机器人系统中，定位是非常重要的模块。基于 SLAM/VO/VIO 技术的算法能实时作机器人的自定位，但是这种开环下的里程计方案很容易累积绝对误差，使得定位漂移。而离线建立的地图因为有闭环检测，精度很高，所以基于地图的定位方法有很高的绝对定位精度。&lt;br&gt;
　　&lt;a hr
      
    
    </summary>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/categories/SLAM/"/>
    
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/tags/SLAM/"/>
    
      <category term="Localization" scheme="https://leijiezhang001.github.io/tags/Localization/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Probabilistic 3D Multi-Object Tracking for Autonomous Driving&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-Probabilistic-3D-Multi-Object-Tracking-for-Autonomous-Driving/"/>
    <id>https://leijiezhang001.github.io/paper-reading-Probabilistic-3D-Multi-Object-Tracking-for-Autonomous-Driving/</id>
    <published>2020-04-07T09:28:57.000Z</published>
    <updated>2020-05-06T06:32:45.376Z</updated>
    
    <content type="html"><![CDATA[<p>　　<a href="/卡尔曼滤波器在三维目标状态估计中的应用/" title="卡尔曼滤波器在三维目标状态估计中的应用">卡尔曼滤波器在三维目标状态估计中的应用</a>中已经较详细得阐述了 3D MOT 状态估计过程，文章末提到观测过程的协方差矩阵初始化问题可以用观测的不确定性解决，<a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 就是通过贝叶斯深度神经网络来建模该不确定性。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 提供了另一种简化的观测不确定性计算方法，同时估计运动模型与观测模型的不确定性，即过程噪声与测量噪声。</p><h2 id="kalman-filter">1. Kalman Filter</h2><p><img src="/paper-reading-Probabilistic-3D-Multi-Object-Tracking-for-Autonomous-Driving/framework.png" width="80%" height="80%" title="图 1. MOT Framework"> 　　如图 1. 所示，本文采用的卡尔曼滤波框架与传统的一样，分为预测与更新。预测阶段，根据上一时刻结果通过 Motion Model(Process Model) 预测当前时刻的状态(先验)；数据关联阶段，将预测的状态与观测的状态作目标数据关联，出 ID；更新阶段，融合预测与观测的状态，得到状态的后验估计。</p><h3 id="predict-step">1.1. Predict Step</h3><p>　　本文采用 CTRV(Constant Turn Rate and Velocity) 运动模型。不同与<a href="/卡尔曼滤波器在三维目标状态估计中的应用/" title="卡尔曼滤波器在三维目标状态估计中的应用">卡尔曼滤波器在三维目标状态估计中的应用</a>中描述的 CTRV，本文作了<strong>线性简化</strong>，其运动方程为： <span class="math display">\[\begin{align}&amp;\begin{bmatrix}\hat{x}\\\hat{y}\\\hat{z}\\\hat{a}\\\hat{l}\\\hat{w}\\\hat{h}\\\hat{d} _ x\\\hat{d} _ y\\\hat{d} _ z\\\hat{d} _ a\\\end{bmatrix} _ {t+1}=\begin{bmatrix}1 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0\\0 &amp;1 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0\\0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0\\0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1\\0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0\\0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0\\0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0 &amp;0\\0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0\\0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0\\0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0\\0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1\\\end{bmatrix}\begin{bmatrix}x\\y\\z\\a\\l\\w\\h\\d _ x\\d _ y\\d _ z\\d _ a\\\end{bmatrix} _ {t}  +\begin{bmatrix}q _ x\\q _ y\\q _ z\\q _ a\\0\\0\\0\\q _ {d _ x}\\q _ {d _ y}\\q _ {d _ z}\\q _ {d _ a}\\\end{bmatrix} _ {t}\\\Longleftrightarrow &amp; \\&amp;\hat{\mu} _ {t+1} = \mathbf{A}\mu _ t \\\end{align}\tag{1}\]</span> 其中未知的线加速度与角加速度 \((q _ x, q _ y, q _ z, q _ a)\)，\((q _ {d _ x},q _ {d _ y},q _ {d _ z},q _ {d _ a})\) 符合\((0,\mathbf{Q})\)高斯分布。<br>　　根据 Motion Model，卡尔曼的预测过程计算状态量的先验： <span class="math display">\[\begin{align}\hat{\mu} _ {t+1} &amp;= \mathbf{A}\mu _ t \\\hat{\Sigma} _ {t+1} &amp;= \mathbf{A}\Sigma _ t\mathbf{A}^T + \mathbf{Q}\\\end{align}\tag{2}\]</span> 　　观测模型为每一时刻检测的结果，包括位置，朝向，目标框尺寸，即观测矩阵 \(\mathbf{H} _ {7\times 11} = [\mathbf{I}, \mathbf{0}]\)。观测噪声也符合高斯分布，由此得到预测的观测量： <span class="math display">\[\begin{align}\hat{o} _ {t+1} &amp;= \mathbf{H}\hat{\mu} _ {t+1} \\\mathbf{S} _ {t+1} &amp;= \mathbf{H}\hat{\Sigma} _ {t+1}\mathbf{H}^T + \mathbf{R}\\\end{align}\tag{3}\]</span></p><h3 id="update-step">1.2. Update Step</h3><p>　　首先将预测的观测量与实际的观测量作数据关联。基本思想是将预测目标与观测目标作 Cost Matrix，然后用匈牙利/贪心算法求解最优匹配对。本文采用 Mahalanobis distance： <span class="math display">\[ m = \sqrt{(o _ {t+1}- \mathbf{H}\hat{\mu} _ {t+1})^T\mathbf{S} _ {t+1} ^{-1}(o _ {t+1}-\mathbf{H}\hat{\mu} _ {t+1})} \tag{4}\]</span> 需要注意的是，计算距离前先做角度矫正，如果两个目标框角度相差大于 90 度，那么作 180 度旋转。<br>　　得到预测与观测的匹配对后，计算后验概率更新该目标的状态： <span class="math display">\[\begin{align}\mathbf{K} _ {t+1} &amp;= \hat{\Sigma} _ {t+1}\mathbf{H} ^T\mathbf{S} _ {t+1}^{-1}\\\mu _ {t+1} &amp;= \hat{\mu} _ {t+1} + \mathbf{K} _ {t+1}(o _ {t+1}-\mathbf{H}\hat{\mu} _ {t+1})\\\Sigma _ {t+1} &amp;=(\mathbf{I}-\mathbf{K} _ {t+1}\mathbf{H})\hat{\Sigma} _ {t+1}\\\end{align}\tag{5}\]</span> 　　以上卡尔曼过程与<a href="/卡尔曼滤波器在三维目标状态估计中的应用/" title="卡尔曼滤波器在三维目标状态估计中的应用">卡尔曼滤波器在三维目标状态估计中的应用</a>，以及<a href="/卡尔曼滤波详解/" title="卡尔曼滤波详解">卡尔曼滤波详解</a>完全一致。</p><h2 id="covariance-matrices-estimation">2. Covariance Matrices Estimation</h2><p>　　如何确定卡尔曼滤波过程中的 \(\Sigma _ 0, \mathbf{Q, R}\)？传统方法是直接用一个确定的经验矩阵赋值；理想的是用<a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 建模处理，但是会相对较复杂；本文用更简单的基于统计方法来确定协方差矩阵。<br>　　<strong>观测量的方差(不确定性)与目标的属性有关</strong>，如距离，遮挡，类别等。本文没有区分这些属性，只统计了一种观测量的方差，<strong>更好的处理方式是按照不同属性，统计不同的方差</strong>。而 <a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 是 Instance 级别的方差预测。<strong>这种统计出来的方差虽然细粒度差一点，但是非常合理，因为只要模型训练好后，模型预测的分布是与训练集分布相似的(理想情况)</strong>，所以用训练集的方差来直接代替模型预测的方差也较为合理。<br>　　<span style="color:red"><strong>更准确的来说，不确定性与物体的属性以及标注误差有关，这里只统计了标注误差(标注误差在大多数情况下都是同分布的)，而实际上遮挡大的目标，是更难学习的(目标学习有难易之分，即预测分布与训练集分布会有偏差)，即预测结果会有额外量的不确定性，所以这种离线统计方法也有很大的局限性</strong>。</span><br>　　设训练集的真值标签：\(\left\{\left\{x _ t^m, y _ t^m, z _ t^m, a _ t^m\right\} _ {m=1}^M\right\} _ {t = 1}^T\)。</p><h2 id="motionprocess-noise-model">2.1. Motion/Process Noise Model</h2><p>　　假设各状态量的噪声独立同分布，那么对于位置与朝向噪声，有： <span class="math display">\[\begin{align}Q _ {xx} &amp;= \mathbf{Var}\left(\left(x _ {t+1}^m-x _ t^m\right)-\left(x _ t^m-x _ {t-1}^m\right)\right)\\Q _ {yy} &amp;= \mathbf{Var}\left(\left(y _ {t+1}^m-y _ t^m\right)-\left(y _ t^m-y _ {t-1}^m\right)\right)\\Q _ {zz} &amp;= \mathbf{Var}\left(\left(z _ {t+1}^m-z _ t^m\right)-\left(z _ t^m-z _ {t-1}^m\right)\right)\\Q _ {aa} &amp;= \mathbf{Var}\left(\left(a _ {t+1}^m-a _ t^m\right)-\left(a _ t^m-a _ {t-1}^m\right)\right)\\\end{align}\tag{6}\]</span> 　　对于线速度与角速度，因为： <span class="math display">\[\begin{align}q _ {x _ t} &amp;\approx x _ {x+1} - x _ t - d _ {x _ t}\\&amp; \approx (x _ {t+1}-x _ t) - (x _ t-x _ {t-1})\\q _ {d _ {x _ t}} &amp;\approx d _ {x _ {t+1}} - d _ {x _ t}\\&amp; \approx (x _ {t+1}-x _ t) - (x _ t-x _ {t-1})\\\end{align}\tag{7}\]</span> 所以： <span class="math display">\[ (Q _ {d _ xd _ x}, Q _ {d _ yd _ y}, Q _ {d _ zd _ z}, Q _ {d _ ad _ a}) = (Q _ {xx}, Q _ {yy}, Q _ {zz}, Q _ {aa})\tag{8}\]</span></p><h2 id="observation-noise-model">2.2. Observation Noise Model</h2><p>　　在训练集上，找到检测与真值的匹配对 \(\left\{\left\{(D _ t^k, G _ t^k)\right\} _ {k=1}^K\right\} _ {t=1}^T\)，从而计算观测噪声： <span class="math display">\[\begin{align}&amp;R _ {xx} = \mathbf{Var}\left(D _ {x _ t}^k-G _ {x _ t}^k\right)\\&amp;R _ {yy} = \mathbf{Var}\left(D _ {y _ t}^k-G _ {y _ t}^k\right)\\&amp;R _ {zz} = \mathbf{Var}\left(D _ {z _ t}^k-G _ {z _ t}^k\right)\\&amp;R _ {aa} = \mathbf{Var}\left(D _ {a _ t}^k-G _ {a _ t}^k\right)\\&amp;R _ {ll} = \mathbf{Var}\left(D _ {l _ t}^k-G _ {l _ t}^k\right)\\&amp;R _ {ww} = \mathbf{Var}\left(D _ {w _ t}^k-G _ {w _ t}^k\right)\\&amp;R _ {hh} = \mathbf{Var}\left(D _ {h _ t}^k-G _ {h _ t}^k\right)\\\end{align}\tag{8}\]</span> 初始的状态协方差 \(\Sigma _ 0 = \mathbf{R}\)。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Chiu, Hsu-kuang, et al. &quot;Probabilistic 3D Multi-Object Tracking for Autonomous Driving.&quot; arXiv preprint arXiv:2001.05673 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　&lt;a href=&quot;/卡尔曼滤波器在三维目标状态估计中的应用/&quot; title=&quot;卡尔曼滤波器在三维目标状态估计中的应用&quot;&gt;卡尔曼滤波器在三维目标状态估计中的应用&lt;/a&gt;中已经较详细得阐述了 3D MOT 状态估计过程，文章末提到观测过程的协方差矩阵初始化问题可以用观测的不
      
    
    </summary>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/categories/MOT/"/>
    
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="Uncertainty" scheme="https://leijiezhang001.github.io/tags/Uncertainty/"/>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/tags/MOT/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;LaserNet&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-LaserNet-An-Efficient-Probabilistic-3D-Object-Detector-for-Autonomous-Driving/"/>
    <id>https://leijiezhang001.github.io/paper-reading-LaserNet-An-Efficient-Probabilistic-3D-Object-Detector-for-Autonomous-Driving/</id>
    <published>2020-04-06T07:36:13.000Z</published>
    <updated>2020-04-07T09:22:24.202Z</updated>
    
    <content type="html"><![CDATA[<p>　　3D 目标检测中，目标定位的不确定性也很关键，<a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 中已经较为详细的描述了在 Bayesian Deep Networks 中如何建模异方差偶然不确定性(Aleatoric Uncertainty)。在贝叶斯深度神经网络框架下，网络不仅预测目标的位置(Mean)，还预测出该预测位置的方差(Variance)。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 延续了 <a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 中预测 Corner 点位置方差的思路，提出了一种预测目标位置方差的方法。</p><h2 id="算法框架">1. 算法框架</h2><p><img src="/paper-reading-LaserNet-An-Efficient-Probabilistic-3D-Object-Detector-for-Autonomous-Driving/framework.png" width="90%" height="90%" title="图 1. LaserNet Framework"> 　　如图 1. 所示，输入为激光点云的 Sensor Range View 表示方式，输出为点级别的目标框3D属性，框顶点位置方差，以及类别概率。最后在 Bird View 下作目标框的聚类与 NMS 等后处理。</p><h3 id="点云输入方式">1.1. 点云输入方式</h3><p>　　不同于目前主流的 Bird View 点云栅格化方式，本文将点云直接根据线束在 Sensor Range View 下进行表示，高为激光线数量，宽为 HFOV 除以角度分辨率。设计 5 个 channel：距离，高度，角度，反射值，以及是否有点的标志位。<br>　　本文认为这种点云表示方式的优点被忽视了，该视角下，点云的表达是紧促的，而且能高效得取得局部区域点，此外，能保留点云获取方式的信息。另一方面，该表达方式的缺点有，访问局部区域时，并不是空间一致的；以及需要处理物体的不同形状和遮挡问题。本文实验结果是，在 Kitti 上效果不如 Bird View 方法，但是在一个较大数据集上，能克服这些缺点。</p><h3 id="网络输出">1.2. 网络输出</h3><p>　　网络输出为点级别的预测，由三部分组成：</p><ol type="1"><li><strong>类别概率</strong><br>每个类别的概率；</li><li><strong>3D 框属性</strong><br>包括相对中心距离 \((d _ x, d _ y)\)；相对朝向 \((\omega _ x, \omega _ y)=(\mathrm{cos}\omega, \mathrm{sin}\omega)\)；以及尺寸 \((l,w)\)。最终目标框中心点位置及朝向表示为： <span class="math display">\[\left\{\begin{array}{l}\mathbf{b} _ c = [x,y]^T+\mathbf{R} _ \theta [d _ x,d _ y]^T \\\varphi = \theta + \mathrm{atan2}(\omega _ y,\omega _ x)\end{array}\tag{1}\right.\]</span> 其中 \(\theta\) 为该点的雷达扫描角度。由此可得到四个目标框角点坐标： <span class="math display">\[\left\{\begin{array}{l}\mathbf{b} _ 1 = \mathbf{b} _ c + \frac{1}{2}\mathbf{R} _ \varphi [l,w]^T\\\mathbf{b} _ 2 = \mathbf{b} _ c + \frac{1}{2}\mathbf{R} _ \varphi [l,-w]^T\\\mathbf{b} _ 3 = \mathbf{b} _ c + \frac{1}{2}\mathbf{R} _ \varphi [-l,-w]^T\\\mathbf{b} _ 4 = \mathbf{b} _ c + \frac{1}{2}\mathbf{R} _ \varphi [-l,w]^T\end{array}\tag{2}\right.\]</span></li><li><strong>顶点位置方差</strong><br>当观测不完全时(遮挡，远处)，目标框的概率分布是多模态的，所以如 <a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 中所述，输出为混合高斯模型。对于每个点的每个类别，输出 \(K\) 个目标框属性：\(\{d _ {x,k}, d _ {y,k}, \omega _ {x,k}, \omega _ {y,k}, l _ k, w _ k\} _ {k=1}^K\)；对应的方差 \(\{s _ k\} _ {k=1}^K\)；以及模型权重 \(\{\alpha _ k\} _ {k=1}^K\)。</li></ol><h3 id="bird-view-后处理">1.3. Bird View 后处理</h3><p>　　网络其实就做了一个点级别的分割，接下来需要作聚类以得到目标框。本文采用 Mean-Shift 方法作聚类。由于是点级别的概率分布，得到目标点集后，需要用 BCN(详见 <a href="/MOT-Fusion/" title="MOT-Fusion">MOT-Fusion</a>) 转换为目标级别的概率分布： <span class="math display">\[\left\{\begin{array}{l}\hat{\mathbf{b}} _ i = \frac{\sum _ {j\in S _ i} w _ j\mathbf{b} _ j}{\sum _ {j\in S _ i}w _ j}\\\hat{\sigma} _ i^2 = \left(\sum _ {j\in S _ i}\frac{1}{\sigma ^2 _ j}\right)^{-1}\end{array}\tag{3}\right.\]</span> 其中 \(w=\frac{1}{\sigma ^ 2}\)。</p><h2 id="loss-形式">2. Loss 形式</h2><p>　　分类采用 Focal Loss。对于每个点 3D 属性的回归，首先找到最靠近真值的预测模型： <span class="math display">\[k ^ * = \mathrm{arg}\min \limits _ k\Vert\hat{\mathbf{b}} _ k-\mathbf{b} ^{gt}\Vert\tag{4}\]</span> 对该预测模型作 Loss： <span class="math display">\[\mathcal{L} _ {box}=\sum _ n\frac{1}{\hat{\sigma} _ {k ^ * }} \left\vert\hat{\mathbf{b}} _ {n,k^ * }-\mathbf{b} _ n^{gt}\right\vert + \mathrm{log}\hat{\sigma} _ {k ^ * }\tag{5}\]</span> 实际回归的是 \(s:=\mathrm{log} \sigma\)。然后对混合模型的权重 \(\{\alpha _ k\} _ {k=1}^K\) 作 cross entry loss \(\mathcal{L} _ {mix}\)。最终的回归 Loss 为： <span class="math display">\[\mathcal{L} _ {reg} = \frac{1}{N}\sum _ i \frac{\mathcal{L} _ {box, i} + \lambda \mathcal{L} _ {mix,i}}{n _ i} \tag{6}\]</span></p><h2 id="adaptive-nms">3. Adaptive NMS</h2><p>　　类别概率不能反应目标框的质量，所以本文采用预测的目标框方差作为 NMS 的参考量。将目标框方差转换为目标框的质量分数：\(\alpha _ k/2\hat{\sigma} _ k\)。<br><img src="/paper-reading-LaserNet-An-Efficient-Probabilistic-3D-Object-Detector-for-Autonomous-Driving/nms.png" width="50%" height="50%" title="图 2. Adaptive NMS"> 　　此外不同目标在 Bird-View 下 IoU 最大值有一定的限制，如图 2. 所示，最坏的情况，Bird-View 下两个框的 IoU 最大限制为设计为： <span class="math display">\[t=\left\{\begin{array}{l}\frac{\sigma _ 1+\sigma _ 2}{2w-\sigma _ 1 - \sigma _ 2} &amp; \sigma _ 1+\sigma _ 2 &lt; w\\1 &amp; otherwise\end{array}\tag{7}\right.\]</span> 当两个目标框的 IoU 大于阈值时，可能的情况是：1. 目标框错误，则删除低分数的目标框；2. 方差估计错误，那么增大方差使最大阈值满足 IoU 条件。</p><h2 id="预测分布的分析">4. 预测分布的分析</h2><p><img src="/paper-reading-LaserNet-An-Efficient-Probabilistic-3D-Object-Detector-for-Autonomous-Driving/calibration.png" width="80%" height="80%" title="图 3. Uncertainty(Variance) Calibration"> 　　评价 Variance(Uncertainty) 预测的好坏，可以画 Calibration 图。如图 3. 所示，横坐标为预测的 Mean 与真值形成的高斯概率分布下的 CDF，而纵坐标为预测的 Variance 统计出的高斯分布下的 CDF。理想情况下，两者是 \(y=x\) 的关系，如图所示，在 ATG4D 大数据集上，预测的 Variance 效果更好。</p><h2 id="一些思考">5. 一些思考</h2><p>　　不管是 2D 检测还是 3D 检测，这种先(语义)分割后聚类出目标的思想，有很强的优势：召回率高，超参数少，自带分割信息等。本文又应用 Aleatoric Uncertainty 来建模检测的不确定性--位置方差(不确定性干嘛用，怎么用，不多说了)，有很好的借鉴意义。</p><h2 id="reference">6. Reference</h2><p><a id="1" href="#1ref">[1]</a> Meyer, Gregory P., et al. &quot;Lasernet: An efficient probabilistic 3d object detector for autonomous driving.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　3D 目标检测中，目标定位的不确定性也很关键，&lt;a href=&quot;/Heteroscedastic-Aleatoric-Uncertainty/&quot; title=&quot;Heteroscedastic Aleatoric Uncertainty&quot;&gt;Heteroscedastic 
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="Uncertainty" scheme="https://leijiezhang001.github.io/tags/Uncertainty/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Learning to See in the Dark&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-Learning-to-See-in-the-Dark/"/>
    <id>https://leijiezhang001.github.io/paper-reading-Learning-to-See-in-the-Dark/</id>
    <published>2020-04-03T03:03:20.000Z</published>
    <updated>2020-04-03T05:57:54.579Z</updated>
    
    <content type="html"><![CDATA[<p>　　无监督低光照图像增强更有应用价值，<a href="/Unsupervised-Low-Light-Image-Enhancement/" title="Unsupervised Low Light Image Enhancement">Unsupervised Low Light Image Enhancement</a> 中介绍了几种无监督方法。本文则是有监督方法，但是值得一读。在 Sensor，曝光时间，光圈，ISO 等(在线调节通过 AE 完成)确定后，图像低光照下曝光不足主要是因为 ISP 过程对图像的亮度矫正不理想。本文直接重构 ISP 过程，对 Raw 图像进行一系列操作，以增强亮度。</p><h2 id="算法过程">1. 算法过程</h2><p><img src="/paper-reading-Learning-to-See-in-the-Dark/ISP.png" width="90%" height="90%" title="图 1. Raw Image Processing Pipeline 对比"> 　　如图 1. 所示，传统 ISP 过程包括：White Balance, Demosaic, Denoise/Sharpen, Color Space Conversion, Gamma Correction(与亮度变化相关)等。L3 与 Burst 是其它 ISP pipeline 学习的方法，本文网络算法过程如图 1.b 所示，首先提取 RGB sensor 值并放大一定比例(该放大系数用来控制最终增强的曝光级别)，然后经过网络层，最终输出全尺寸的 RGB 图像。<br>　　训练数据采集自室内静态场景，每对数据由短曝光的低光照图像与长曝光的标签图像构成，由此可进行有监督训练。</p><h2 id="reference">2. Reference</h2><p><a id="1" href="#1ref">[1]</a> Chen, Chen, et al. &quot;Learning to see in the dark.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　无监督低光照图像增强更有应用价值，&lt;a href=&quot;/Unsupervised-Low-Light-Image-Enhancement/&quot; title=&quot;Unsupervised Low Light Image Enhancement&quot;&gt;Unsupervised Low
      
    
    </summary>
    
      <category term="Low-Light Image Enhancement" scheme="https://leijiezhang001.github.io/categories/Low-Light-Image-Enhancement/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Low-Light Image Enhancement" scheme="https://leijiezhang001.github.io/tags/Low-Light-Image-Enhancement/"/>
    
  </entry>
  
  <entry>
    <title>Unsupervised Low-Light Image Enhancement</title>
    <link href="https://leijiezhang001.github.io/Unsupervised-Low-Light-Image-Enhancement/"/>
    <id>https://leijiezhang001.github.io/Unsupervised-Low-Light-Image-Enhancement/</id>
    <published>2020-03-28T04:21:55.000Z</published>
    <updated>2020-04-03T06:02:16.620Z</updated>
    
    <content type="html"><![CDATA[<p>　　在自动驾驶中，相机能捕捉丰富的纹理信息，是不可或缺的传感器。但是受限于相机 Sensor 及 ISP 性能，其动态范围有限，往往会出现过曝或欠曝的情况。过曝的情况还能通过 3A(AE, AF, AW) 中的 AE 调节，而欠曝的情况，AE 中要么提高增益或 ISO 但是会增加噪声，要么增加曝光时间但是撑死 50ms(按照 20Hz)，光圈则一般是固定的，不会调节。所以在低光照自动驾驶场景下，对欠曝的图像进行亮度增强则显得尤其重要（当然也可用夜视相机如红外相机等辅助)。<br>　　基于学习的图像增强方法，由于很难获得大量的欠爆图像与对应的增强图像。所以无监督的图像增强方法就更有应用价值，本文介绍几种无监督图像增强方法。</p><h2 id="zero-dce1">1. Zero-DCE<a href="#1" id="1ref"><sup>[1]</sup></a></h2><p>　　无监督图像增强方法主要是指基于 GAN 的方法，基于 GAN 的方法还是需要选择欠爆图像及正常图像两个分布的数据集，选择不当也会导致性能下降。而 Zero-DCE 则无需选择正常图像数据集，消除了数据分布下过拟合或欠拟合的风险。<br>　　Zero-DCE 基本思想是对每个像素作亮度变换，每个像素的变换方程为： <span class="math display">\[LE(I(\mathrm{x});\alpha) = I(\mathrm{x}) + \alpha I(\mathrm{x})(1-I(\mathrm{x})) \tag{1}\]</span> 其中 \(\alpha\in[-1,1]\) 是变换系数。对图像的每个通道每个像素分别作不同系数的迭代变换，可得： <span class="math display">\[LE _ n(\mathrm{x}) = LE _ {n-1}(\mathrm{x}) + \mathcal{A} _ n LE _ {n-1}(\mathrm{x})(1-LE _ {n-1}(\mathrm{x})) \tag{2}\]</span> 其中 \(\mathcal{A} _ n\) 是变换系数集，与图像大小一致。 <img src="/Unsupervised-Low-Light-Image-Enhancement/Zero-DCE.png" width="90%" height="90%" title="图 1. Zero-DCE Framework"> 　　如图 1. 所示，Zero-DCE 框架中，一个基本网络预测几组 \(\mathcal{A} _ n\) 集合，然后对原图每个通道进行迭代的亮度变换。LE-curves 不仅能增强暗处的曝光量，还能减弱过曝处的亮度值。<br>　　该方法最重要的是 Loss 函数的设计，一共有以下 Loss 组成：</p><ol type="1"><li><strong>Spatial Consisiency Loss</strong><br>增强后的图像要求其与原图具有空间一致性： <span class="math display">\[ L _ {spa} = \frac{1}{K}\sum _ {i=1}^K\sum _ {j\in\Omega (i)}\left(\Vert Y _ i-Y _ j\Vert-\Vert I _ i-I _ j\Vert\right)^2 \tag{3}\]</span> 其中 \(\Omega\) 为某像素的领域集，可为四领域；\(K\) 为局部区域数量，可设定为 \(4\times 4\) 大小；\(Y,I\) 分别为增强后与原始的像素亮度值。</li><li><strong>Exposure Control Loss</strong><br>曝光控制 Loss 相当于设定曝光量去监督训练每个像素亮度，实现“无监督”的效果： <span class="math display">\[ L _ {exp} = \frac{1}{M}\sum _ {k=1}^M\Vert Y _ k-E\Vert \tag{4}\]</span> 其中 \(M\) 为无重合的局部区域数量，可设定为 \(16\times 16\) 大小；\(Y _ k\) 为局部区域的平均亮度值。作者实验中，设定 \(E\in[0.4,0.7]\) 均能获得相似的较好的结果。</li><li><strong>Color Constancy Loss</strong><br>根据 Gray-World color constancy 假设：rgb 每个通道的平均亮度值与 gray 灰度值一致。所以为了保证颜色不失真，构造： <span class="math display">\[ L _ {col}=\sum _ {\forall (p,q)\in \epsilon}(J^p-J^q), \epsilon=\{R,G,B\} \tag{5}\]</span> 其中 \(p,q\) 表示一对不同的颜色通道，\(J\) 表示该通道的平均亮度值。</li><li><strong>Illumination Smoothness Loss</strong><br>增强的过程要求相邻亮度值是平滑的，对增强变换系数作约束： <span class="math display">\[ L _ {tv _ {\mathcal{A}}} = \frac{1}{N}\sum _ {n=1}^N\sum _ {c\in\epsilon}(\nabla _ x\mathcal{A} _ n^c+\nabla _ y\mathcal{A} _ n^c)^2, \epsilon = \{R,G,B\}\tag{6}\]</span> 其中 \(N\) 为增强迭代数；\(\nabla _ x,\nabla _ y\) 分别表示水平与垂直方向的求导操作。</li></ol><p>最终 Loss 构成为： <span class="math display">\[ L _ {total} = L _ {spa} + L _ {exp} + W _ {col}L _ {col} + W _ {tv _ {\mathcal{A}}}L _ {tv _ {\mathcal{A}}} \tag{7}\]</span></p><h2 id="enlightengan2">2. EnlightenGAN<a href="#2" id="2ref"><sup>[2]</sup></a></h2><p>　　图像增强本质上是作 domain transfer，所以能用 GAN 处理，实现无监督训练。 <img src="/Unsupervised-Low-Light-Image-Enhancement/EnlightenGAN.png" width="90%" height="90%" title="图 2. EnlightenGAN Framework"> 　　如图 2. 所示，EnlightenGAN 由 Generator 和 Discriminator 构成。Generator 是一个 attention-guided U-Net，因为我们期望欠曝的区域能增强，所以将亮度值归一化后，用 1 减去亮度值作为注意力图，与原图一起输入网络。Discriminator 由 Global Discriminator 与 Local Discriminator 组成，因为经常只需要局部区域的亮度，所以设计 Local Discriminator 就很有必要。<br>　　Loss 的设计非常关键，EnlightenGAN 一共有以下 Loss 组成：</p><ol type="1"><li><strong>Adversarial Loss</strong><br>用于直接训练 Generator 以及 Discriminator 的 Loss，与传统的 GAN Loss 类似；</li><li><strong>Self Feature Preserving Loss</strong><br>注意到，调整输入图像值的范围，对最终的高层任务影响不是很大，所以引入网络特征 Loss 来保证增强后图像的准确性。对原始图像与生成的图像，分别输入到在 ImageNet 上预训练的 VGG-16 模型，提取特征集合，将对应的特征对作 L1 Loss。</li></ol><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Guo, Chunle, et al. &quot;Zero-Reference Deep Curve Estimation for Low-Light Image Enhancement.&quot; arXiv preprint arXiv:2001.06826 (2020).<br><a id="2" href="#2ref">[2]</a> Jiang, Yifan, et al. &quot;Enlightengan: Deep light enhancement without paired supervision.&quot; arXiv preprint arXiv:1906.06972 (2019).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　在自动驾驶中，相机能捕捉丰富的纹理信息，是不可或缺的传感器。但是受限于相机 Sensor 及 ISP 性能，其动态范围有限，往往会出现过曝或欠曝的情况。过曝的情况还能通过 3A(AE, AF, AW) 中的 AE 调节，而欠曝的情况，AE 中要么提高增益或 ISO 但是
      
    
    </summary>
    
      <category term="Low-Light Image Enhancement" scheme="https://leijiezhang001.github.io/categories/Low-Light-Image-Enhancement/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Low-Light Image Enhancement" scheme="https://leijiezhang001.github.io/tags/Low-Light-Image-Enhancement/"/>
    
      <category term="GAN" scheme="https://leijiezhang001.github.io/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>GAN</title>
    <link href="https://leijiezhang001.github.io/GAN/"/>
    <id>https://leijiezhang001.github.io/GAN/</id>
    <published>2020-03-08T08:45:51.000Z</published>
    <updated>2020-04-03T06:15:25.726Z</updated>
    
    <content type="html"><![CDATA[<p>　　Generative Adversarial Nets(GAN) 能将某个分布的数据映射到另一组数据形成的分布空间内。这在某些领域非常有用，如：图像去噪，图像去雨雾，图像去模糊，图像低光照增强等。<strong>自动驾驶中，图像去雨雾与低光照增强非常关键，GAN 能在没有模拟器的情况下，根据有限的数据，自动生成某一分布的数据，为后续感知做准备</strong>。目前还没看到针对点云的 GAN，未来 3D GAN 可能会有大进展。<br>　　本文介绍几个 GAN 的基础性工作。</p><h2 id="gan-基础网络">1. GAN 基础网络</h2><h3 id="generative-adversarial-nets1">1.1. Generative Adversarial Nets<a href="#1" id="1ref"><sup>[1]</sup></a></h3><p>　　对抗网络由生成模型和判别模型构成。生成模型输入随机噪声，输出以假乱真的图像，判别模型则对图像作分类。其优化函数为： <span class="math display">\[ \min\limits _ G \max\limits _ D V(D,G) = E _ {x\sim p _ {data}(x)}[log(D(x))] + E _ {x\sim p _ z(z)}[log(1-D(G(z)))] \tag{1}\]</span> 该优化过程有两部分组成：</p><ol type="1"><li><strong>优化判别模型</strong><br><span class="math display">\[ \max\limits _ D V(D,G) = E _ {x\sim p _ {data}(x)}[log(D(x))] + E _ {x\sim p _ z(z)}[log(1-D(G(z)))] \tag{2}\]</span> 其中第一项表示输入为真样本时，那么判别模型输出越大越好，即越接近 1；而对于已经生成的假样本 \(G(z)\)，判别模型输出越小越好，即接近 0。</li><li><strong>优化生成模型</strong><br><span class="math display">\[ \min\limits _ GV(D,G) =E _ {x\sim p _ z(z)}[log(1-D(G(z)))] \tag{3}\]</span> 优化生成模型时，希望生成的假样本接近真样本，所以生成的假样本经过判别模型后越大越好，即\(D(G(z))\)要接近 1。由此统一成上式。</li></ol><p>　　对抗网络的优化由这两步迭代组成。</p><h3 id="conditional-generative-adversarial-nets2">1.2. Conditional Generative Adversarial Nets<a href="#2" id="2ref"><sup>[2]</sup></a></h3><p>　　条件对抗网络中的生成模型输入不在是随机噪声，而是特定的数据分布，如真值标签。其优化函数为： <span class="math display">\[ \min\limits _ G \max\limits _ D V(D,G) = E _ {x\sim p _ {data}(x)}[log(D(x|y))] + E _ {x\sim p _ z(z)}[log(1-D(G(z|y)))] \tag{4}\]</span> 　　其优化过程与 GAN 类似。</p><h3 id="cycle-consistent-adversarial-nets3">1.3. Cycle-Consistent Adversarial Nets<a href="#3" id="3ref"><sup>[3]</sup></a></h3><p>　　Cycle GAN 使得高分辨率图像的 domain-transfer 成为可能。对于两个图像分布 \(X,Y\)，设计两个映射函数(生成模型): \(G:X\to Y\) 和 \(F:Y\to X\)；设计两个判别模型: \(D _ X\) 和 \(D _ Y\)，\(D _ X\) 用于判别 \(x\) 与 \(F(y)\), \(D _ Y\) 用于判别 \(y\) 与 \(G(x)\)。为了还原高分辨率图像，设计两部分 Loss：</p><ol type="1"><li><strong>Adversarial Loss</strong><br>就是传统的对抗网络 Loss: <span class="math display">\[\begin{align}\mathcal{L} _ {GAN}&amp;=\mathcal{L} _ {GAN}(G, D _ Y,X,Y)+\mathcal{L} _ {GAN}(F, D _ X,Y,X)\\&amp;= E _ {y\sim p _ {data}(y)}[log(D _ Y(y))] + E _ {x\sim p _ {data}(x)}[log(1-D _ Y(G(x)))]\\&amp;+ E _ {x\sim p _ {data}(x)}[log(D _ X(x))] + E _ {y\sim p _ {data}(y)}[log(1-D _ X(F(Y)))]\end{align} \tag{5}\]</span></li><li><strong>Cycle Consistency Loss</strong><br>为了保证映射网络的映射准确性，考虑到 \(x\to G(x)\to F(G(x))\approx x \) 以及 \(y\to F(y)\to G(F(y))\approx y \)，设计 cycle loss： <span class="math display">\[\mathcal{L} _ {cyc}(G,F)= E _ {x\sim p _ {data}(x)}\Vert F(G(x))-x\Vert + E _ {y\sim p _ {data}(y)}\Vert G(F(y))-y\Vert \tag{6}\]</span></li></ol><p>总的 Loss 为： <span class="math display">\[\mathcal{L} _ (G,F,D _ X, D _ Y)=\mathcal{L} _ {GAN}(G, D _ Y,X,Y)+\mathcal{L} _ {GAN}(F, D _ X,Y,X)+\lambda \mathcal{L} _ {cyc}(G,F) \tag{7}\]</span></p><h2 id="其它资料">2. 其它资料</h2><p>　　上面介绍了三个 GAN 基本网络，尤其是 Cycle-GAN，是高分辨率图像无监督 domain-transfer 的基础，应用相当广泛。本文介绍相对较简单，<a href="#4" id="4ref">[4]</a> 详细介绍了 GAN 的来龙去脉。代码则可以参考 <a href="#5" id="5ref">[5]</a> ，收录的 GAN 网络非常详细。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Goodfellow, Ian, et al. &quot;Generative adversarial nets.&quot; Advances in neural information processing systems. 2014.<br><a id="2" href="#2ref">[2]</a> Mirza, Mehdi, and Simon Osindero. &quot;Conditional generative adversarial nets.&quot; arXiv preprint arXiv:1411.1784 (2014).<br><a id="3" href="#3ref">[3]</a> Zhu, Jun-Yan, et al. &quot;Unpaired image-to-image translation using cycle-consistent adversarial networks.&quot; Proceedings of the IEEE international conference on computer vision. 2017.<br><a id="4" href="#4ref">[4]</a> http://www.gwylab.com/note-gans.html<br><a id="5" href="#5ref">[5]</a> https://github.com/eriklindernoren/PyTorch-GAN</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Generative Adversarial Nets(GAN) 能将某个分布的数据映射到另一组数据形成的分布空间内。这在某些领域非常有用，如：图像去噪，图像去雨雾，图像去模糊，图像低光照增强等。&lt;strong&gt;自动驾驶中，图像去雨雾与低光照增强非常关键，GAN 能在没
      
    
    </summary>
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Point-based 3D Detetection</title>
    <link href="https://leijiezhang001.github.io/Point-based-3D-Det/"/>
    <id>https://leijiezhang001.github.io/Point-based-3D-Det/</id>
    <published>2020-02-29T04:30:15.000Z</published>
    <updated>2020-03-01T07:42:25.374Z</updated>
    
    <content type="html"><![CDATA[<p>　　基于激光点云的 3D 目标检测是自动驾驶系统中的核心感知模块。由于点云的稀疏性以及空间结构的无序性，一系列 Voxel-based 3D 检测方法得以发展：<a href="/paperreading-PointPillars/" title="PointPillars">PointPillars</a>，<a href="/paperreading-Fast-and-Furious/" title="FaF">FaF</a>，<a href="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/" title="MVF">MVF</a> 等。然而 Voxel-based 方法需要预定义空间栅格的分辨率，其特征提取的有效性依赖于空间分辨率。同时在点云语义分割领域，对点云的点级别特征提取方法研究较为广泛，<a href="/PointCloud-Feature-Extraction/" title="PointCloud Feature Extraction">PointCloud Feature Extraction</a> 中已经较详细的介绍了针对点云的点级别特征提取方法，<a href="/paper-reading-Grid-GCN-for-Fast-and-Scalable-Point-Cloud-Learning/" title="Grid-GCN">Grid-GCN</a> 提出了几种策略来加速特征提取。<br>　　由此高效的 Point-based 3D 检测方法成为可能，这种方法首先提取点级别的特征(相比 Voxel-based，理论上没有信息损失)，然后用点级别的 Anchor-based 或 Anchor-free 方法作 3D 检测。</p><h2 id="anchor-based">1. Anchor-based</h2><h3 id="ipod1">1.1. IPOD<a href="#1" id="1ref"><sup>[1]</sup></a></h3><p><img src="/Point-based-3D-Det/ipod.png" width="90%" height="90%" title="图 1. IPOD Framework"> 　　如图 1. 所示， IPOD 与 F-PointNet 类似，只不过 IPOD 在俯视图下生成 Proposal 取点，而 F-PointNet 是直接在锥形视野的点云中作分割。IPOD 由三部分组成：</p><ol type="1"><li><strong>Semantic Segmentation</strong><br>目的是将点云中的背景点过滤掉，只生成前景点的 Anchor。作者采用图像语义分割的方法，这里也可直接用点云分割来做；</li><li><strong>Point-based Proposal Generation</strong><br>生成点级别的候选框，去掉冗余的候选框；</li><li><strong>Head for Classification and Regression</strong><br>根据候选框，提取特征，作分类和回归；</li></ol><p>这里的前两步是要得到少量但又能保证召回率的 Proposal，其中 Anchor 是根据每个点来设置的，然后作 NMS 操作，这里不做展开。 <img src="/Point-based-3D-Det/proposal_feat.png" width="80%" height="80%" title="图 2. Proposal Feature Generation"> 　　如图 2. 所示，每个 Proposal 提取出点云信息，然后通过 PointNet++ 直接来预测该 Proposal 的 3D 属性。这里用到了 T-Net(Spatial Transformation Network 的一种) 将点云变换到规范坐标系(Canonical coordinates)，这个套路用的也比较多。其它细节就是正常的 3D 属性回归策略，不作展开。</p><h3 id="std2">1.2. STD<a href="#2" id="2ref"><sup>[2]</sup></a></h3><p><img src="/Point-based-3D-Det/STD.png" width="80%" height="80%" title="图 3. STD Framework"> 　　如图 3. 所示，STD 模块有：</p><ol type="1"><li><strong>Backbone</strong><br>用 PointNet++ 提取点级别特征以及作点级别的 Classification；</li><li><strong>PGM(Proposal Generation Module)</strong><br>根据点级别的分类结果，对目标点设计球状 Spherical Anchor；不同类别设计不同的球状 Anchor 半径。将球状 Anchor 里面的点收集起来，作坐标规范化并且 concate 点级别特征，然后用 PointNet 来预测实际的矩形 proposal：包括中心 Offsets 以及 size offsets。同时对角度进行预测，角度预测通过分类加预测 Offsets 实现。</li><li><strong>Proposal Feature Generation</strong><br>有了 proposal 后，其实可以直接通过 PointNet 作进一步的预测及分类，但是作者为了加速，这时候采用了 Voxel Feature Encoding。将 proposal 里面的点都转换到中心点坐标系，然后栅格化提取特征；</li><li><strong>Box Prediction</strong><br>除了通常的类别预测以及 3D Box 相关属性的 Offsets 预测，作者还加入了与真值的 IoU 预测，该 IoU 值与类别分数相乘作为最终的该预测分数(这个在 2D Detection 中已经有应用)。</li></ol><h2 id="anchor-free">2. Anchor-free</h2><h3 id="pointrcnn3">2.1. PointRCNN<a href="#3" id="3ref"><sup>[3]</sup></a></h3><p><img src="/Point-based-3D-Det/PointRCNN.png" width="80%" height="80%" title="图 4. PointRCNN Framework"> 　　如图 4. 所示，PointRCNN 是一个 two-stage 3D 检测方法，类似 Faster-RCNN，其由 Bottom-up 3D Proposal Generation 和 Canonical 3D Box Refinement 两个模块组成。</p><h4 id="bottom-up-3d-proposal-generation">2.1.1 Bottom-up 3D Proposal Generation</h4><p>　　Proposal 的生成要求是，数量少，召回率高。3D Anchor 由于要覆盖 3D 空间，所以数量会很大(如 AVOD)，本文采用目标点生成 Proposal 的方法。与 IPOD，STD 类似，首先对点云进行点级别的特征提取并作前景分割(或语义分割)，对前景的每个点用 Bin-based 方法生成 3D proposal。由此在生成尽量少的 Proposal 下，保证目标的高召回率。<br>　　点级别的特征提取及前景分割，可以采用任意的语义分割网络，这里前景的真值即为目标框内的点云，用 Focal Loss 来平衡正负样本。<br><img src="/Point-based-3D-Det/bin-based.png" width="60%" height="60%" title="图 5. Bin-based Localization"> 　　如图 5. 所示，对每个前景点用 Bin-based 方法生成 proposal。将平面的 \(x,z\) (与一般的雷达坐标系不同) 方向分成若干个 bin，然后对每个前景点，预测目标中心点属于哪个 bin，以及中心点与该 bin 的 Offsets(与角度处理的方式非常像)。针对尺寸，预测该类别平均尺寸的 Residual；针对角度，还是分解成分类加回归任务进行处理。最后再作 NMS 即可得到较少的 Proposal，给到下一模块作 refine。本模块的 Loss 设计为： <span class="math display">\[\begin{align}\mathcal{L} _ 1 &amp;= \mathcal{L} _ {seg} + \mathcal{L} _ {proposal} \\&amp;= \mathcal{L} _ {seg} + \frac{1}{N _ {pos}} \sum _ {p\in pos} \left(\mathcal{L} _ {bin} ^ {(p)} + \mathcal{L} _ {res} ^ {(p)}\right) \\&amp;= \mathcal{L} _ {seg} + \sum _ {u\in{\{x,z,\theta\}}} \left(\mathcal{F} _ {cls}(\widehat{bin} _ u^{(p)}, bin _ u^{(p)})+\mathcal{F} _ {reg}(\widehat{res} _ u^{(p)}, res _ u^{(p)})\right) + \sum _ {v\in\{y,h,w,l\}} \mathcal{F} _ {reg}(\widehat{res} _ v^{(p)}, res _ v^{(p)})\\\tag{1}\end{align}\]</span> 其中 \(\mathcal{F} _ {cls}, \mathcal{F} _ {reg}\) 分别为 cross-entropy Loss 和 smooth L1 Loss。</p><h4 id="canonical-3d-box-refinement">2.1.2 Canonical 3D Box Refinement</h4><p>　　有了 3D proposal 后，经过 Point Cloud Region Pooling 提取该 proposal 的点特征，步骤如下：先对 proposal 进行一定程度的扩大，然后提取内部点的 semantic features，foreground mask score，Point distance等。由此获得每个 proposal 的点及点特征，用来作 3D Box Refinement。<br><img src="/Point-based-3D-Det/canonical.png" width="60%" height="60%" title="图 6. Canonical Transformation"> 　　如图 4. 所示，为了更好的学习 proposal 的局部空间特征，增加每个 proposal 在自身 Canonical 坐标系下的空间点。Canonical 变换如图 6. 所示，因为这里每个 proposal 的位置及角度已经有了，所以直接对其内的点作变换。如果没有，那就需要 STN(T-Net) 来学习这个变换。<br>　　Loss 也是在 Canonical 坐标系下计算的，假设 proposal：\(\mathrm{b _ i} = (x _ i,y _ i,z _ i,h _ i,w _ i,l _ i,\theta _ i)\)，真值: \(\mathrm{b} _ i^{gt} = (x _ i^{gt}, y _ i^{gt},z _ i^{gt},h _ i^{gt},w _ i^{gt},l _ i^{gt},\theta _ i^{gt})\)。那么两者变换到 Canonical 坐标系后： <span class="math display">\[\begin{align}\mathrm{\tilde{b}} _ i &amp;=(0,0,0,h _ i,w _ i,l _ i,0) \\\mathrm{\tilde{b}} _ i^{gt} &amp;= (x _ i^{gt}-x _ i, y _ i^{gt}-y _ i,z _ i^{gt}-z _ i,h _ i^{gt},w _ i^{gt},l _ i^{gt},\theta _ i^{gt}-\theta _ i)\tag{2}\end{align}\]</span> 对于中心点，还是 bin 分类加 Residual 回归，但是可以减少 bin 的尺度；对于尺寸，还是回归 Residual；对于角度，由于限定 positive 与 gt 的 IoU&gt;0.55，所以可以将回归的角度限定为 \((-\frac{\pi}{4},\frac{\pi}{4})\) 的范围，由此进行 bin 分类及 Residual 回归。最终本阶段的 Loss 为： <span class="math display">\[ \mathcal{L} _ 2= \frac{1}{N _ {pos}+ N _ {neg}} \sum _ {p\in all} \mathcal{L} _ {label} ^{(p)}+ \frac{1}{N _ {pos}} \sum _ {p\in pos} \left(\mathcal{\tilde{L}} _ {bin} ^ {(p)} + \mathcal{\tilde{L}} _ {res} ^ {(p)}\right) \tag{3}\]</span></p><h3 id="dssd4">2.2. 3DSSD<a href="#4" id="4ref"><sup>[4]</sup></a></h3><p><img src="/Point-based-3D-Det/3DSSD.png" width="100%" height="100%" title="图 7. 3DSSD Framework"> 　　如图 7. 所示，3DSSD 是 one-stage 网络，由 Backbone，Candidate Generation Layer，Head 构成。Backbone 作者提出了 Fusion Sampling 以提升前景点在采样时候的召回率。Candidate Generation Layer 中根据前景点，生成 3D box 预测的 Candidate 锚点。最后 Head 根据锚点，作 Anchor-free 的 3D Box 预测。</p><h4 id="fusion-sampling">2.2.1 Fusion Sampling</h4><p>　　为了扩大感受野提取局部特征，点云通常需要作下采样处理，一般采用 D-FPS 方法(点空间距离作为采样度量)，但是这样会使前景点大量丢失。前面几种方法不管是用图像分割还是点云分割，都会去除背景点云，保留前景点云以提高生成 Proposal 的召回率。<br>　　这里作者提出了 Feature-FPS，加入特征间的距离作为采样的度量方式。对于地面等背景，其特征基本类似，所以很容易就去除了；而对于目标区域，其点特征都不太一样，又得以保留。如果只保留同一目标的点，也会产生冗余，所以融合点特征距离及空间距离，设计采样度量方式为： <span class="math display">\[ C(A,B) = \lambda L _ d(A,B) + L _ f(A,B) \tag{4}\]</span> 　　因为 F-FPS 去除了大量的背景点，虽然有利于回归，但是不利于分类，所以设计了融合 D-FPS 和 F-FPS 的 Fusion Sampling 方法。如图 7. 所示，最终分别输出 F-FPS 与 D-FPS 的特征点。</p><h4 id="candidate-generation-layer">2.2.2 Candidate Generation Layer</h4><p><img src="/Point-based-3D-Det/candidate_pts.png" width="60%" height="60%" title="图 8. Candidate Generation"> 　　如图 8. 所示，根据 F-FPS 采样的点，在真值框中心点的监督下，用一个 T-Net 去学习采样点与中心点的变换。变换后的点即作为 Candidate 锚点。对每个 Candidate 点提取周围一定距离的 F-FPS 与 D-FPS(大量背景点利于分类)中点集的特征(空间坐标作归一化或变换到 Candidate 坐标系，类似 Canonical 坐标系)，然后作 MaxPool 提取该 Candidate 对应区域的特征。</p><h4 id="prediction-head">2.2.3 Prediction Head</h4><p>　　对于每个 Candidate 特征，作 3D Box 属性的回归。本文采用 Anchor-free 的方法。对于中心点，直接回归 Candidate 坐标点与真值框中心点的 Offsets；对于尺寸，直接回归与该类别平均尺寸的 Residual；对于角度，还是采用 bin 分类加 Residual 回归的策略。<br>　　这里期望的是 Candidate 点能接近目标框中心点，所以作者借鉴 FCOS(详见 <a href="/Anchor-Free-Detection/" title="Anchor-Free Detection">Anchor-Free Detection</a>)中的 Center-ness Loss 来选取靠近中心点的 Candidate，真值 Label 为: <span class="math display">\[l _ {ctrness}=\sqrt[3]{\frac{\mathrm{min}(f,b)}{\mathrm{max}(f,b)}+\frac{\mathrm{min}(l,r)}{\mathrm{max}(l,r)}+\frac{\mathrm{min}(t,d)}{\mathrm{max}(t,d)}} \tag{5}\]</span> 其中 \(f,b,l,r,t,d\) 分别表示前后左右上下与中心点的距离。FCOS 中，加了一个与分类平行的分支来预测 Center-ness，最终的预测 Score 是分类 Score 乘以 Center-ness 得到(与预测 IoU 套路一样，本质上都是引入与真值的距离度量)，该预测 Score 用于之后的 NMS 等处理。本文则没有显示的预测 Center-ness，其直接将真值 Center-ness 与真值类别相乘，作为类别真值，所以一个类别分支即得到最终的预测 Score。<br>　　最终的 Loss 为： <span class="math display">\[L = \frac{1}{N _ c}\sum _ iL _ c(s _ i, u _ i) + \lambda _ 1\frac{1}{N _ p}\sum _ i[u _ i&gt;0]L _ r + \lambda _ 2\frac{1}{N _ p}L _ s \tag{5}\]</span> 其中 \(s _ i\) 为预测的类别 Score，\(u _ i\) 为经过 Center-ness 处理后的类别真值；\(L _ c\) 表示类别预测 Loss；\(L _ r\) 表示 3D Box Loss，包括中心点距离，尺寸，角度，8个角点位置；\(L _ s\) 表示生成 Candidate 点的 shift 变换 Loss。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Yang, Zetong, et al. &quot;Ipod: Intensive point-based object detector for point cloud.&quot; arXiv preprint arXiv:1812.05276 (2018).<br><a id="2" href="#2ref">[2]</a> Yang, Zetong, et al. &quot;Std: Sparse-to-dense 3d object detector for point cloud.&quot; Proceedings of the IEEE International Conference on Computer Vision. 2019.<br><a id="3" href="#3ref">[3]</a> Shi, Shaoshuai, Xiaogang Wang, and Hongsheng Li. &quot;Pointrcnn: 3d object proposal generation and detection from point cloud.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br><a id="4" href="#4ref">[4]</a> Yang, Zetong, et al. &quot;3DSSD: Point-based 3D Single Stage Object Detector.&quot; arXiv preprint arXiv:2002.10187 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　基于激光点云的 3D 目标检测是自动驾驶系统中的核心感知模块。由于点云的稀疏性以及空间结构的无序性，一系列 Voxel-based 3D 检测方法得以发展：&lt;a href=&quot;/paperreading-PointPillars/&quot; title=&quot;PointPillars
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
  </entry>
  
  <entry>
    <title>MOT Multimodal Fusion</title>
    <link href="https://leijiezhang001.github.io/MOT-Fusion/"/>
    <id>https://leijiezhang001.github.io/MOT-Fusion/</id>
    <published>2020-02-19T04:35:40.000Z</published>
    <updated>2020-02-23T09:24:02.959Z</updated>
    
    <content type="html"><![CDATA[<p>　　同一传感器的目标状态估计在<a href="/卡尔曼滤波器在三维目标状态估计中的应用/" title="卡尔曼滤波器在三维目标状态估计中的应用">卡尔曼滤波器在三维目标状态估计中的应用</a>中已经有较详细的介绍。不同传感器在不同光照不同天气情况下，有不同的表现，比如相机在低光照下可靠性较差，而激光雷达能弥补这个缺陷。所以在目标状态估计中，多传感器融合非常重要，可以是<strong>数据前融合，特征级融合，目标状态后融合</strong>。本文关注目标状态后融合过程。</p><h2 id="问题描述">1. 问题描述</h2><p>　　考虑两个传感器 \(A,B\) (传感器可为相机，激光雷达，毫米波雷达等)检测输出的(也可以是经过滤波的)多目标分别为：\(A=\{A _ i\in\mathbb{R}^D|i=1,...,M\}\)，\(B=\{B _ i\in\mathbb{R}^D|i=1,...,N\}\)，其中 \(\mathbb{R}^D\) 表示目标状态的维数，如位置，速度，朝向，类别等。MOT 的多模态后融合问题即由此求解融合后结果 \(C=\{C _ i\in\mathbb{R}^D|i=1,...,L\}\)，该过程主要有三步：</p><ol type="1"><li>目标匹配/数据关联：从 \(A,B\) 中找出同一目标的两个多模态观测量，设匹配数为 \(K\)；</li><li>目标状态的多模态融合：对匹配上的同一目标的两个多模态观测进行融合；</li><li>整合目标，经过滤波输出最终结果，目标数目为 \(L=M+N-K\)；</li></ol><h2 id="目标匹配">2. 目标匹配</h2><p>　　本质上与单传感器下目标状态估计中前后帧的数据关联问题一致，这里的关键步骤是：</p><ol type="1"><li>提取每个目标的特征向量：可以是位置，速度，角度，CNN特征层等；</li><li>构建 cost function：对两个目标集合建立 Cost 矩阵；</li><li>匈牙利算法找出最优匹配；</li></ol><p>　　传统的 cost function 基本是向量的 Euclidean 距离或是 cosine 距离，<a href="#1" id="1ref">[1]</a> 提出了一种 Deep Affinity Network 来一次性解决两个目标集合的匹配问题。 <img src="/MOT-Fusion/affinity.png" width="70%" height="70%" title="图 1. Affinity Network"> 　　如图 1. 所示，两个目标集 \(A\in\mathbb{R}^{M\times D}\)，\(B\in\mathbb{R}^{N\times D}\)，扩展到维度 \(\mathbb{R}^{M\times N\times D}\)，相减后输入到网络中，预测出 affinity matrix，\(C\in\mathbb{R}^{M\times N}\)，其中 \(C _ {ij}=1\) 表示匹配上同一目标，否则认为是两个目标。这里关键是 Loss 的设计，最简单的 Loss 为： <span class="math display">\[L(A,B)=\frac{1}{MN}\sum _ {i=1} ^ {M}\sum _ {j=1}^N |C _ {ij}-G _ {ij}| \tag{1}\]</span> 其中 \(G\) 为亲和度矩阵的 groundtruth。实际对亲和度矩阵并没有 0-1 要求，最终是通过匈牙利算法找出匹配的，所以只要将同一目标的分数增大，不同目标的分数减小，最终即可选出匹配。由此设计 Loss： <span class="math display">\[L(A,B)=\sum _ {i,\,j;\,G _ {ij}=1} \left(\sum _ {k;\,G _ {ik}\neq 1}\mathrm{max}(0,C _ {ik}-C _ {ij}+m)+\sum _ {p;\,G _ {pj}\neq 1}\mathrm{max}(0,C _ {pj}-C _ {ij}+m)\right)\tag{2}\]</span> 其中 \(m\) 控制正负样本的相对大小。式(2)更容易使网络收敛。</p><h2 id="多模态融合">3. 多模态融合</h2><p>　　当多传感器检测的同一目标匹配上后，需要融合出一个最终的观测。可以采用卡尔曼滤波的方法，<a href="/卡尔曼滤波器在三维目标状态估计中的应用/" title="卡尔曼滤波器在三维目标状态估计中的应用">卡尔曼滤波器在三维目标状态估计中的应用</a>中的式(1)~(6)是时序下状态估计的迭代过程。对于多模态融合，虽然是同时获取的观测，但是融合过程类似，令测量矩阵 \(H _ k\) 为单位阵，所以可得卡尔曼增益： <span class="math display">\[K _ k=\frac{\bar{P} _ k}{\bar{P} _ k+R _ k} \tag{3}\]</span> 由此计算后验概率<a href="#2" id="2ref"><sup>[2]</sup></a>： <span class="math display">\[\begin{align}\hat{x} _ k &amp;=\bar{x} _ k+K(z_k-\bar{x}) = \frac{\bar{P} _ kz _ k + \bar{x} _ kR _ k}{\bar{P} _ k+R _ k} \tag{4}\\\hat{P} _ k &amp;=(I-KH _ k)\bar{P} _ k =\frac{\bar{P} _ kR _ k}{\bar{P} _ k+R _ k}\tag{5}\end{align}\]</span> 对于多模态输入 \(A,B\)，令 \(A = \bar{x} _ k,\sigma _ A^2 = \bar{P} _ k\)，\(B=z _ k,\sigma _ B^2 =R _ k\)，可得多模态融合结果为： <span class="math display">\[\begin{align}C &amp;= \frac{\sigma _ A^2B+\sigma _ B^2A}{\sigma _ A^2+\sigma _ B^2}\\\sigma _ C^2 &amp;= \frac{\sigma _ A^2\sigma _ B^2}{\sigma _ A^2+\sigma _ B^2}\\\tag{6}\end{align}\]</span> 式(6)等价于： <span class="math display">\[\begin{align}\sigma _ C^2 &amp;= \frac{\sigma _ A^2\sigma _ B^2}{\sigma _ A^2+\sigma _ B^2}\\C &amp;= \sigma _ C^2\left(\frac{A}{\sigma _ A^2}+\frac{B}{\sigma _ B^2}\right)\\\tag{7}\end{align}\]</span> 这是 BCM<a href="#3" id="3ref"><sup>[3]</sup></a>！卡尔曼滤波器也是在贝叶斯概率模型下导出来的，可见两个高斯分布的同一状态的观测量，均可通过 BCM 进行融合。<br>　　得到当前时刻多模态融合后的目标状态后，即可进一步作时序卡尔曼平滑获得最终估计的目标状态。<br>　　另一种融合方法是在 JPDAF(Joint Probabilistic Data Association Filter)<a href="#4" id="4ref"><sup>[4]</sup></a>框架下作两次 PDA 融合<a href="#5" id="5ref"><sup>[5]</sup></a>，JPDAF 是另一种数据关联(目标匹配)的方法，这里不作展开。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Kuang, Hongwu, et al. &quot;Multi-Modality Cascaded Fusion Technology for Autonomous Driving.&quot; arXiv preprint arXiv:2002.03138 (2020).<br><a id="2" href="#2ref">[2]</a> Fankhauser, Péter, et al. &quot;Robot-centric elevation mapping with uncertainty estimates.&quot; Mobile Service Robotics. 2014. 433-440.<br><a id="3" href="#3ref">[3]</a> Tresp, Volker. &quot;A Bayesian committee machine.&quot; Neural computation 12.11 (2000): 2719-2741.<br><a id="4" href="#4ref">[4]</a> Arya Senna Abdul Rachman, Arya. &quot;3D-LIDAR Multi Object Tracking for Autonomous Driving: Multi-target Detection and Tracking under Urban Road Uncertainties.&quot; (2017).<br><a id="5" href="#5ref">[5]</a> JRMOT: A Real-Time 3D Multi-Object Tracker and a New Large-Scale Dataset</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　同一传感器的目标状态估计在&lt;a href=&quot;/卡尔曼滤波器在三维目标状态估计中的应用/&quot; title=&quot;卡尔曼滤波器在三维目标状态估计中的应用&quot;&gt;卡尔曼滤波器在三维目标状态估计中的应用&lt;/a&gt;中已经有较详细的介绍。不同传感器在不同光照不同天气情况下，有不同的表现，比如相
      
    
    </summary>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/categories/MOT/"/>
    
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/tags/MOT/"/>
    
      <category term="tracking" scheme="https://leijiezhang001.github.io/tags/tracking/"/>
    
  </entry>
  
  <entry>
    <title>Model Compression - &#39;Quantization&#39;</title>
    <link href="https://leijiezhang001.github.io/Quantization/"/>
    <id>https://leijiezhang001.github.io/Quantization/</id>
    <published>2020-02-11T08:25:15.000Z</published>
    <updated>2020-02-29T04:25:32.683Z</updated>
    
    <content type="html"><![CDATA[<p>　　量化(Quantization)是模型压缩主要技术之一。因为模型训练后的权重及特征图基本符合高斯分布(特征图可能是混合高斯分布)，所以将 32-bit 的张量量化到低比特后也能保持模型输出的准确度。如果只量化模型的权重，那么只是减少了模型的存储及传输大小；只有同时量化权重及特征图(Weight &amp; Activation)，才能同时减少计算量。本文来详细描述下模型量化的细节。</p><h2 id="quantization-scheme">1. Quantization Scheme</h2><h3 id="fixed-point-approximation">1.1. Fixed Point Approximation</h3><p>　　设 Fixed Point 近似法中表示整数与小数的比特数分别为 \(\mathrm{IL,FL}\)，那么其可表达的浮点数范围为<a href="#1" id="1ref"><sup>[1]</sup></a><a href="#2" id="2ref"><sup>[2]</sup></a>：\([-2^{\mathrm{IL-1}}, 2 ^ {\mathrm{IL-1}}-2 ^ {-\mathrm{FL}}]\)。这种方法很明显，精度较差且表达的浮点数范围有限。更进一步，可以针对不同的张量，用不同的 \(\mathrm{IL,FL}\)，即 Dynamic Fixed Point 近似法<a href="#1" id="1ref"><sup>[1]</sup></a>。综上，Fixed Point 近似法将一个浮点数表示为： <span class="math display">\[(-1)^s\cdot 2^{-\mathrm{FL}}\sum _ {i=0}^{\mathrm{IL+FL-2}}2^i\cdot x _ i \tag{1}\]</span> 其中 \(x_i\) 为第 \(i\) 比特位的值。<br>　　对于 Dynamic Fixed Point，首先保证整数部分不溢出，所以量化张量 \(X\) 时设计： <span class="math display">\[\mathrm{IL}=\lceil\mathrm{lg} _ 2(\mathop{\max}\limits _ {S} X + 1)\rceil \tag{2}\]</span> 剩下的比特位即为符号位与小数位。<br>　　用这种定点方式量化后，由式(1)可知，两数相乘可以转换为 bits shifts &amp; add 操作，极大提升计算效率。<br>　　Fixed Point 近似法精度有限，尤其是当所要表示的值较大时，小数位 \(\mathrm{FL}\) 只能分到很小，所以精度必然有较大损失。</p><h3 id="range-based-linear-approximation">1.2. Range-Based Linear Approximation</h3><p>　　不同于 Fixed Point 近似中小数位有一定限制(导致精度较差)，Range-Based Linear 近似法直接将浮点数通过一个高精度的 Scale 值映射到对应量化位数中，所以能保持非常高的精度。</p><h4 id="asymmetric-mode">1.2.1. Asymmetric Mode</h4><p><img src="/Quantization/asymmetric.png" width="40%" height="40%" title="图 1. Asymmetric Quantization"> 　　如图 1. 所示，设浮点数为 \(r\)，那么 Asymmetric Linear Approximation 过程为<a href="#3" id="3ref"><sup>[3]</sup></a>： <span class="math display">\[q = round\left((r-r _ {min})\cdot\frac{2^n-1}{r _ {max}-r _ {min}}\right) = round(\frac{r}{S}-\frac{r _ {min}}{S}) \tag{3}\]</span> 等价于<a href="#4" id="4ref"><sup>[4]</sup></a>： <span class="math display">\[r = S(q-Z) \tag{4}\]</span> 其中 \(S\) 为映射的 Scale 参数，\(Z\) 表示零值被量化的值。 <img src="/Quantization/conv.png" width="40%" height="40%" title="图 2. Convolution Operator"> 　　如图 2. 所示，卷积操作可转化为矩阵相乘运算，接下来我们来推导量化后的矩阵相乘运算。假设两个 \(N\times N\) 矩阵相乘：\(r _ 3=r _ 1\cdot r _ 2\)。令 \(r _ \alpha ^{(i,j)}\) 表示矩阵 \(r _ \alpha\) 第 \((i,j)\) 个元素，\(1\leq i,j\leq N\)。矩阵张量对应的量化参数为 \(S _ \alpha,Z _ \alpha\)，对应的量化后的元素表示为 \(q _ \alpha ^{(i,j)}\)： <span class="math display">\[r _ \alpha ^{(i,j)} = S _ \alpha\left(q _ \alpha ^{(i,j)}-Z _ \alpha\right) \tag{5}\]</span> bias 量化参数设为 \(S _ b=S _ 1S _ 2,Z _ b=0\)，那么卷积运算(矩阵相乘)可表示为： <span class="math display">\[S _ 3\left(q _ 3 ^{(i,k)}-Z _ 3\right) = \sum _ {j=1} ^N S _ 1\left(q _ 1 ^{(i,j)}-Z _ 1\right)S _ 2\left(q _ 2 ^{(j,k)}-Z _ 2\right) + S _ b(q _ b^{(i)} - Z _ b)\tag{6}\]</span> 等价于： <span class="math display">\[\begin{align}q _ 3 ^{(i,k)} &amp;= Z _ 3+M\left(\sum _ {j=1} ^N \left(q _ 1 ^{(i,j)}-Z _ 1\right)\left(q _ 2 ^{(j,k)}-Z _ 2\right)+ \frac{S _ b}{S _ 1S _ 2}q _ b^{(i)}\right) \\&amp;= Z _ 3+M\left(NZ _ 1Z _ 2- Z _ 1\sum _ {j=1}^Nq _ 2^{(j,k)}-Z _ 2\sum _ {j=1}^Nq _ 1^{(i,j)}+\sum _ {j=1}^N q _ 1^{(i,j)}q _ 2^{(j,k)}+ \frac{S _ b}{S _ 1S _ 2}q _ b^{(i)}\right)  \\&amp;= Z _ 3+M\left(NZ _ 1Z _ 2- Z _ 1\sum _ {j=1}^Nq _ 2^{(j,k)}-Z _ 2\sum _ {j=1}^Nq _ 1^{(i,j)}+\sum _ {j=1}^N q _ 1^{(i,j)}q _ 2^{(j,k)}+ q _ b^{(i)}\right)\tag{7}\end{align}\]</span> 其中 \(M=\frac{S _ 1S _ 2}{S _ 3}\) 可以离线计算，为上式唯一的浮点数。经验上可知 \(M\in(0,1)\)，进一步可将其表示为： <span class="math display">\[M\approx 2^{-n}M _ 0 \tag{8}\]</span> 假设 \(m\) 是能表示 \(M _ 0\) 的位数( int32 硬件下，\(m\) 可为 32)，那么有 \(2 ^ {n} M \leq 2 ^m -1\)，故： <span class="math display">\[\left\{\begin{array}{l}n = \left\lfloor\mathrm{log} _ 2\frac{2 ^ m-1}{M}\right\rfloor \\M _ 0 = \left\lfloor 2 ^ nM\right\rfloor\end{array}\tag{9}\right.\]</span> 由此，乘以 \(M _ 0\) 可以用定点乘法实现，乘以 \(2 ^{-n}\) 可以用高效的位运算实现。式(7)中核心的计算为两个量化向量的乘加运算：\(\sum _ {j=1}^N q _ 1^{(i,j)}q _ 2^{(j,k)}\)，其可通过传统的特定位数的 BLAS 库完成。<br>　　具体的，令矩阵张量(卷积滤波器权重及特征图)量化为 8-bit，那么 8-bit 乘法需要用 32-bit 的累加器，即： <span class="math display">\[\mathrm{int32 += uint8 * uint8} \tag{10}\]</span> 所以式(7)中每一项累加时都是 32-bit 的，bias 也是量化为 32-bit 或是 rescale 到 32-bit，即 \(S _ b=S _ 1S _ 2,Z _ b=0\)。</p><h4 id="symmetric-mode">1.2.2. Symmetric Mode</h4><p><img src="/Quantization/symmetric.png" width="40%" height="40%" title="图 3. Symmetric Quantization"> 　　这种模式下最大最小值绝对值取相同值 \(R\) (该值可为任意值)，那么量化表示为： <span class="math display">\[r = Sq \tag{11}\]</span> Full Range 下 \(S = \frac{R}{(2^n-1)/2}\)(8-bit 则量化范围为 [-128,127]，Range 范围为 255)，Restricted Range 则 \(S = \frac{R}{2^{n-1}-1}\)(8-bit 量化范围为[-127,127]，Range 范围为 254)。Full Range 精度更高，PyTorch，ONNX 采用这种方式；TensorFlow，TensorRT，MKL-DNN 则采用 Restricted Range 量化方式。<br>　　由此式(7)简化为： <span class="math display">\[q _ 3 ^{(i,k)} = M\left(\sum _ {j=1}^N q _ 1^{(i,j)}q _ 2^{(j,k)}+ q _ b^{(i)}\right) \tag{12}\]</span> 实现更加简单。</p><h2 id="quantization-alogorithm">2. Quantization Alogorithm</h2><h3 id="post-training-quantization">2.1. Post-Training Quantization</h3><p>　　训练好的模型，可以直接对其权重进行量化，而对于特征的量化，则需要一个 Calibration 数据集来统计特征数值的分布，然后对其进行量化。<br>　　量化参数的搜索，可以根据量化后的模型好坏进行 Loss 构建：</p><ol type="1"><li><strong>任务级别损失函数</strong>：直接根据特定任务的指标来搜索及评价量化参数；</li><li><strong>张量级别损失函数</strong>：设计量化后的张量与原始张量的分布相似度，或者说信息损失度，如 KL-divergence 等度量方法；</li></ol><h3 id="quantization-aware-training">2.2. Quantization-Aware Training</h3><p>　　将训练好的模型直接进行量化，可能会导致对应的任务准确度下降，尤其对表达能力有限的小模型而言，以下情况会导致量化后模型准确度下降：</p><ol type="1"><li>权重张量中数值差异 100 倍以上，导致小数值的量化误差较大；</li><li>权重张量中有 outlier 值，导致其它值的量化误差较大；</li></ol><p>而直接在训练的时候进行量化，可以保证完成模型训练也就得到了对应的高准确率的量化模型。 <img src="/Quantization/quantization-aware.png" width="80%" height="80%" title="图 4. Quantization-Aware Training Framework"> 　　如图 4. 所示，<a href="#4" id="3ref">[4]</a> 提出了一种 Quantization-Aware Training 的框架，权重和特征图均维护 float32 及 int8 数值，前向传播采用 int8 伪量化运算，反向传播更新权重的 float32 值，并作量化。 <img src="/Quantization/quantized_alg.png" width="60%" height="60%" title="图 5. Quantization-Aware Training Pipline"> 　　如图 5. 所示，<a href="#4" id="3ref">[4]</a> 基于 TensorFlow 实现了一种 Quantization-Aware Training 的算法，其步骤为：</p><ol type="1"><li>建立一个浮点模型的 graph；</li><li>在 graph 中加入伪量化操作；</li><li>用伪量化的方式训练得到精度与浮点模型差不多的量化模型；</li><li>建立并优化量化的 Inference 模型 graph；</li><li>在量化引擎上作模型的 Inference；</li></ol><h4 id="simulated-quantization">2.2.1. Simulated Quantization</h4><p>　　这里采用 Asymmetric Linear Approximation 量化策略。对于权重，卷积运算时，先做伪量化操作，并且如果有 batch-normalization，则将其合并入卷积核权重中；对于特征图(Activations)，前向传播时都先做伪量化操作。伪量化操作如下<a href="#4" id="4ref"><sup>[4]</sup></a><a href="#5" id="5ref"><sup>[5]</sup></a>： <span class="math display">\[\begin{align}\mathrm{clamp}(r\;;a,b) &amp;:= \mathrm{min}(\mathrm{max}(r,a),b) \\s(a,b,n) &amp;:= \frac{b-a}{2 ^n-1} \\q(r\;;a,b,n) &amp;:= \left\lfloor\frac{\mathrm{clamp}(r\;;a,b)-a}{s(a,b,n)}\right\rceil s(a,b,n)+a\\\tag{13}\end{align}\]</span> 其中 \([a,b]\) 是 被量化的浮点范围(可以是 \([r _ {min}, r _ {max}]\))，\(q(r\;;a,b,n)\) 即为浮点数 \(r\) 的伪量化表示，也是浮点数。</p><h4 id="learning-quantization-ranges">2.2.2. Learning Quantization Ranges</h4><p>　　训练时，每次迭代，权重与特征图都要作伪量化处理，所以每次要确定量化参数。对于权重，因为其服从均值为零的高斯分布，所以 \([a,b]\) 直接设为其最大值与最小值即可；对于特征图，其数值与输入相关，所以策略为：刚开始训练的时候不对其作量化处理，之后用 EMA(Exponential Moving Averages) 对量化参数进行平滑，去除特征图输出突变的影响。</p><h4 id="batch-normalization-folding">2.2.3. Batch Normalization Folding</h4><p>　　作 Inference 或者说前向传播时，BN 可以合并入卷积核权重中，所以在量化前，先要将其合并，然后权重就仅限于卷积操作中。对于每个卷积 filter，其生成特征图以及 BN 过程如下： <span class="math display">\[\begin{align}\hat{x} _ i &amp;\gets wx _ i+b\\\mu _ B &amp;\gets \frac{1}{m}\sum _ {i=1}^m \hat{x} _ i\\\sigma^2 _ B &amp;\gets \frac{1}{m}\sum _ {i=1}^m(\hat{x} _ i-\mu _ B)^2\\y _ i &amp;\gets \gamma\frac{\hat{x} _ i-\mu _ B}{\sqrt{\sigma^2 _ B+\epsilon}} + \beta\\\tag{14}\end{align}\]</span> 由此可得： <span class="math display">\[\begin{align}y _ i &amp;\gets \gamma\frac{\hat{x} _ i-\mu _ B}{\sqrt{\sigma^2 _ B+\epsilon}} + \beta\\&amp;\gets \gamma\frac{wx _ i+b-\mu _ B}{\sqrt{\sigma^2 _ B+\epsilon}} + \beta\\&amp;\gets \frac{\gamma wx _ i}{\sqrt{\sigma^2 _ B+\epsilon}} +\frac{\gamma(b-\mu _ B)}{\sqrt{\sigma^2 _ B+\epsilon}}+ \beta\\\tag{15}\end{align}\]</span> 由此可知作 Inference 时，BN 参数 \(\mu _ B,\sigma^2 _ B,\gamma, \beta\) 可合并到卷积 Filter 参数中： <span class="math display">\[\left\{\begin{array}{l}\hat{w} = \frac{\gamma w}{\sqrt{\sigma^2 _ B+\epsilon}}\\\hat{b} = \frac{\gamma(b-\mu _ B)}{\sqrt{\sigma^2 _ B+\epsilon}}+ \beta\\\end{array}\tag{16}\right.\]</span></p><h3 id="trained-quantization-thresholds">2.3. Trained Quantization Thresholds</h3><p>　　Post-Training Quantization 以及 Quantization-Aware Training 都是直接对张量的分析来搜索或近似求解量化参数的，Trained Quantization Thresholds 则在训练的时候同时训练得到量化参数。</p><h4 id="pact">2.3.1. PACT</h4><p>　　PACT<a href="#13" id="13ref"><sup>[13]</sup></a> 定义了激活函数输出的最大值，该最大值就是 Symmetric 量化中的激活层量化参数 Scale。具体的，改进 Relu： <span class="math display">\[ y = \mathrm{PACT}(x) = 0.5(|x|-|x-\alpha|+\alpha)=\left\{\begin{array}{l}0, \;\;x\in(-\infty,0)\\x, \;\;x\in[0,\alpha]\\\alpha, \;\;x\in[\alpha, +\infty)\end{array}\tag{17}\right.\]</span> 对应的量化参数偏导为： <span class="math display">\[\frac{\partial y _ q(x;\,\alpha)}{\partial \alpha}=\left\{\begin{array}{l}0, \;\;x\in(-\infty, \alpha)\\1, \;\;x\in[\alpha,+\infty)\end{array}\tag{18}\right.\]</span></p><h4 id="tqt">2.3.2. TQT</h4><p>　　TQT(Trained Quantization Thresholds)<a href="#14" id="14ref"><sup>[14]</sup></a>则提出了一种同时学习权重和激活函数的量化参数的方法。为了简化，其采用 Linear Symmetric Approximation，且 Scale 参数限定为 \(s=2 ^ {-f}\)，由式(8,9)可知，消除了定点乘法运算。前向传播与式(13)并无差异，对每个权重即激活层作 scale，round，saturate，de-quant 操作。反向传播则需要对量化值 \(q(x;s)\) 求导，量化值表示为： <span class="math display">\[q(x;s)=\left\{\begin{array}{l}\left\lfloor\frac{x}{s}\right\rceil \cdot s, \;\; n\leq\left\lfloor\frac{x}{s}\right\rceil\leq p\\n\cdot s, \;\;\;\;\left\lfloor\frac{x}{s}\right\rceil &lt; n\\p\cdot s, \;\;\;\;\left\lfloor\frac{x}{s}\right\rceil &gt; p\\\end{array}\tag{19}\right.\]</span> 其中 \(n,p\) 分别为量化值域的最小最大值。定义 \(\frac{\partial \lfloor x\rceil}{\partial x} = 1\)，那么对 Scale 的偏导为： <span class="math display">\[\nabla _ sq(x;s)=\left\{\begin{array}{l}\left\lfloor\frac{x}{s}\right\rceil - \frac{x}{s}, &amp;\; n\leq\left\lfloor\frac{x}{s}\right\rceil\leq p\\n, &amp;\;\left\lfloor\frac{x}{s}\right\rceil &lt; n\\p, &amp;\;\left\lfloor\frac{x}{s}\right\rceil &gt; p\\\end{array}\tag{20}\right.\]</span> 为了稳定性，令 \(\nabla _ {(\mathrm{log} _ 2 t)} s = s\, \mathrm{In}(2)\)，则： <span class="math display">\[\nabla _ {(\mathrm{log} _ 2t)}q(x;s)= s\,\mathrm{In}(2)\cdot\left\{\begin{array}{l}\left\lfloor\frac{x}{s}\right\rceil - \frac{x}{s}, &amp;\; n\leq\left\lfloor\frac{x}{s}\right\rceil\leq p\\n, &amp;\;\left\lfloor\frac{x}{s}\right\rceil &lt; n\\p, &amp;\;\left\lfloor\frac{x}{s}\right\rceil &gt; p\\\end{array}\tag{21}\right.\]</span> 对应的，对输入 \(x\) 的偏导数为： <span class="math display">\[\nabla _ xq(x;s)=\left\{\begin{array}{l}1,&amp;\; n\leq\left\lfloor\frac{x}{s}\right\rceil\leq p\\0, &amp;\;otherwise\\\end{array}\tag{22}\right.\]</span></p><p>　　由此可与网络权重一起训练得到量化参数。Graffitist<a href="#15" id="15ref"><sup>[15]</sup></a>基于 TensorFlow 实现了上述算法；NNCF<a href="#16" id="16ref"><sup>[16]</sup></a>基于 Pytorch 实现了类似算法。</p><h2 id="quantized-framework">3. Quantized Framework</h2><p>　　不管是 Post-Training Quantization 还是 Quantization-Aware Training，算法端都还是用伪量化操作实现的，部署时就必须用 INT8 引擎。据我所知目前 INT8 引擎有：</p><ol type="1"><li>DSP/加速芯片平台<br>目测没有开源的，大家自个玩自个的；</li><li>CPU 平台<br>Google 的 TensorFlow Lite<a href="#6" id="6ref"><sup>[6]</sup></a>，Facebook 的 QNNPACK<a href="#8" id="8ref"><sup>[8]</sup></a>，Tencent 的 NCNN<a href="#9" id="9ref"><sup>[9]</sup></a>。</li><li>GPU 平台<br>NVIDIA 的 TensorRT<a href="#10" id="10ref"><sup>[10]</sup></a>，TVM<a href="#11" id="11ref"><sup>[11]</sup></a>。</li></ol><p>而伪量化框架则在深度学习框架(caffe，pytorch，tensorflow)中开源的较多，如基于 pytorch 的 distiller<a href="#3" id="3ref"><sup>[3]</sup></a>，NNCF<a href="#16" id="16ref"><sup>[16]</sup></a>。<br>　　对于 ARM 平台，INT8 引擎会通过 NEON 指令集加速；对于 x86 平台，INT8 引擎会通过 SSE 加速；对于 NVIDIA GPU 平台，则通过 dp4a<a href="#12" id="12ref"><sup>[12]</sup></a> 矩阵运算库加速。dp4a 实现了基础的 INT8 矩阵相乘操作，目前 cuDNN，cuBLAS，TensorRT 均采用该指令集。下面对 INT8 引擎作简要阐述。</p><h3 id="ristretto1">3.1. Ristretto<a href="#1" id="1ref"><sup>[1]</sup></a></h3><p>　　Ristretto 是一种基于 (Dynamix) Fixed Point Approximation, Post-Training Quantization 的量化框架，其精度有限，量化的 Inference 引擎可用 bits shifts &amp; add 操作实现，比较适合应用于 DSP 等嵌入式平台。</p><h3 id="tensorflow-lite6qnnpack8ncnn9">3.2. TensorFlow Lite<a href="#6" id="6ref"><sup>[6]</sup></a>/QNNPACK<a href="#8" id="8ref"><sup>[8]</sup></a>/NCNN<a href="#9" id="9ref"><sup>[9]</sup></a></h3><p>　　TensorFlow Lite 是 Google 基于 TensorFlow 开发的针对移动嵌入式 CPU 平台的模型(量化)加速框架，其实现在 2.2 小节中已有详细的描述，有较高精度，<a href="#4" id="4ref">[4]</a> 实现了 Quantization-Aware Training。其中 INT8 矩阵运算采用了 gemmlowp<a href="#7" id="7ref"><sup>[7]</sup></a>。<br>　　移动端的 CPU 的量化计算引擎开源的也比较多，如 Facebook 的 QNNPACK<a href="#8" id="8ref"><sup>[8]</sup></a>，腾讯的 ncnn-int8<a href="#9" id="9ref"><sup>[9]</sup></a>。</p><h3 id="tensorrt10">3.3. TensorRT<a href="#10" id="10ref"><sup>[10]</sup></a></h3><p>　　TensorRT 是 NVIDIA 基于 GPU 平台的模型(量化)加速框架，其基于 Symmetric Linear Approximation 量化策略，并且只支持 Post-Training Quantization，其内部可能直接调用 dp4a，也可能调用 cuDNN 或 cuBLAS。TVM<a href="#11" id="11ref"><sup>[11]</sup></a> 调用 dp4a 实现了基于 python 的 INT8 引擎，对于部署来讲没有 TensorRT 高效。<br>　　对于特征图的量化参数 \(S\) 的搜索，其使用张量级别的损失函数，最小化量化前后特征图值分布差异性的方式，KL-divergency，即两个分布的相对熵。假设连个分布 \(P,Q\)，那么两者的相对熵为： <span class="math display">\[E(P,Q) = \sum _ i P(i)\cdot\mathrm{log}\left(\frac{P(i)}{Q(i)}\right) \tag{23}\]</span> 熵越大，表示两个分布差异性越大，即量化后信息损失越大。这里也可以采用其它能描述两个分布差异性的方式，如 EMD。整个量化参数搜索过程为：</p><ol type="1"><li>准备训练好的 FP32 模型，以及一个作校正(Calibration)的数据集；</li><li>用 FP32 模型跑数据集，统计每个特征图的值分布；</li><li>对不同的量化参数，根据式(17)计算量化前后的相对熵；选择最优的量化参数；</li><li>根据最优的量化参数量化特征图得到量化模型(权重值分布比较集中，所以可以直接用最大值作为量化参数，具体还得看 TensorRT 怎么做的)；</li><li>保存量化参数为 Calibration Table，载入该值即可启动 INT8 引擎作量化 Inference；</li></ol><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Gysel, Philipp. &quot;Ristretto: Hardware-oriented approximation of convolutional neural networks.&quot; arXiv preprint arXiv:1605.06402 (2016).<br><a id="2" href="#2ref">[2]</a> Gupta, Suyog, et al. &quot;Deep learning with limited numerical precision.&quot; International Conference on Machine Learning. 2015.<br><a id="3" href="#3ref">[3]</a> https://nervanasystems.github.io/distiller/index.html<br><a id="4" href="#4ref">[4]</a> Jacob, Benoit, et al. &quot;Quantization and training of neural networks for efficient integer-arithmetic-only inference.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.<br><a id="5" href="#5ref">[5]</a> Krishnamoorthi, Raghuraman. &quot;Quantizing deep convolutional networks for efficient inference: A whitepaper.&quot; arXiv preprint arXiv:1806.08342 (2018).<br><a id="6" href="#6ref">[6]</a> https://www.tensorflow.org/mobile/tflite<br><a id="7" href="#7ref">[7]</a> https://github.com/google/gemmlowp<br><a id="8" href="#8ref">[8]</a> https://github.com/pytorch/QNNPACK<br><a id="9" href="#9ref">[9]</a> https://github.com/Tencent/ncnn/pull/487<br><a id="10" href="#10ref">[10]</a> Migacz, Szymon. &quot;8-bit inference with tensorrt.&quot; GPU technology conference. Vol. 2. No. 4. 2017.<br><a id="11" href="#11ref">[11]</a> https://tvm.apache.org/2019/04/29/opt-cuda-quantized<br><a id="12" href="#12ref">[12]</a> https://devblogs.nvidia.com/mixed-precision-programming-cuda-8/<br><a id="13" href="#13ref">[13]</a> Choi, Jungwook, et al. &quot;Pact: Parameterized clipping activation for quantized neural networks.&quot; arXiv preprint arXiv:1805.06085 (2018).<br><a id="14" href="#14ref">[14]</a> Jain, Sambhav R., et al. &quot;Trained quantization thresholds for accurate and efficient neural network inference on fixed-point hardware.&quot; arXiv preprint arXiv:1903.08066 (2019).<br><a id="15" href="#15ref">[15]</a> https://github.com/Xilinx/graffitist<br><a id="16" href="#16ref">[16]</a> Kozlov, Alexander, et al. &quot;Neural Network Compression Framework for fast model inference.&quot; arXiv preprint arXiv:2002.08679 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　量化(Quantization)是模型压缩主要技术之一。因为模型训练后的权重及特征图基本符合高斯分布(特征图可能是混合高斯分布)，所以将 32-bit 的张量量化到低比特后也能保持模型输出的准确度。如果只量化模型的权重，那么只是减少了模型的存储及传输大小；只有同时量化权
      
    
    </summary>
    
      <category term="Model Compression" scheme="https://leijiezhang001.github.io/categories/Model-Compression/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Model Compression" scheme="https://leijiezhang001.github.io/tags/Model-Compression/"/>
    
  </entry>
  
  <entry>
    <title>LOAM(Lidar Odometry and Mapping)</title>
    <link href="https://leijiezhang001.github.io/LOAM/"/>
    <id>https://leijiezhang001.github.io/LOAM/</id>
    <published>2020-02-06T07:11:08.000Z</published>
    <updated>2020-02-08T08:02:01.527Z</updated>
    
    <content type="html"><![CDATA[<p>　　SLAM 是机器人领域非常重要的一个功能模块，而基于激光雷达的 SLAM 算法，LOAM(Lidar Odometry and Mapping)，则应用也相当广泛。本文从经典的 LOAM 出发，详细描述下激光 SLAM<a href="#1" id="1ref"><sup>[1]</sup></a><a href="#2" id="2ref"><sup>[2]</sup></a> 中的一些模块细节。</p><h2 id="问题描述">1. 问题描述</h2><h3 id="scan-定义">1.1. Scan 定义</h3><p>　　针对旋转式机械雷达，Scan 为单个激光头旋转一周获得的点云，类似 VLP-16 旋转一周则是“几乎”同时获得了 16 个 Scan。针对棱镜旋转而激光头不旋转的雷达(Solid State LiDARs)，如大疆 Livox 系列，Scan 则可定义为一定时间下累积获得的点云。</p><h3 id="sweep-定义">1.2. Sweep 定义</h3><p>　　Sweep 定义为静止的机器人平台上激光雷达能覆盖到所有空间的点云。<br>　　针对旋转式机械雷达，Sweep 即为旋转一周获得的由一个或多个 Scan 组成的点云。针对棱镜旋转而激光头不旋转的雷达，由于其属于非重复性扫描(Non-repetitive Scanning)结构，所以 Sweep 理论上为时间趋于无穷大时获得的点云，但是狭义上，可以认为一段较长时间下(相对于 Scan 时间)，获得的点云。<br><img src="/LOAM/motor_lidar.png" width="60%" height="60%" title="图 1. 3D Lidar Updated from 2D Lidar with a Motor"> 　　那么，如果给激光雷达加上一个马达呢？如图 1. 所示，<a href="#1" id="1ref">[1]</a> 中设计了一种 3D Lidar 装置，由一个只有一个激光头的 2D Lidar 和一个马达组成，激光扫描频率为 40Hz，马达转速为 180°/s。这种装置下，Scan 意义不变，Sweep 则为 1s 内该装置获得的点云(因为 1s 的时间内，该装置获得的点云可覆盖所有能覆盖的空间)。</p><h3 id="非重复性扫描激光雷达">1.2. 非重复性扫描激光雷达</h3><p><img src="/LOAM/livox.png" width="60%" height="60%" title="图 2. Livox Scanning Pattern"> 　　其实，大疆的 Livox 非重复性扫描雷达相当于把这马达移到了内部的棱镜中，而且加上非对称，所以随着时间的累积，可获得相当稠密的点云。<br>　　Livox 这种非重复式扫描的激光雷达价格低廉，相对于传统的多线激光雷达有很多优点，但是有个致命的缺点：<strong>只能准确捕捉静态物体，无法准确捕捉动态物体；对应的，只能作 Mapping，很难作动态障碍物的估计。</strong>因为在一帧点云的扫描周期 \(T\) 内，如果目标速度为 \(v\)，那么 Livox 式雷达在扫描周期内都会扫到目标，目标的尺寸会被放大 \(Tv\)，而传统旋转的线束雷达真正扫到目标的时间为 \(t\ll T\)。当 \(T=0.1s\)，\(v=20m/s\) 时，尺寸放大为 2m，而一般小汽车车长也就几米。<strong>所以尺寸是估不准的，但是其它属性，如位置，速度，在目标加速度不是很大的情况下，可能还是有技巧可以估准的，具体就得看实验效果。另一种思路：直接对其进行物理建模，先假设已知目标速度，那么所有点即可恢复出目标的真实尺寸，然后可进一步估计速度，由此迭代至最优值</strong>。<br>　　由于本车的状态可以通过其它方式(如 IMU)获得，所以本车运动所引起的点云畸变(即 Motion Blur，基本所有雷达都会有这个问题，详见 2.3，4.1 章节)可以很容易得到补偿，所以对于静态目标，点云是能准确捕捉到其物理属性的。</p><h3 id="符号定义">1.3. 符号定义</h3><p>　　本文首先基于图 1. 的装置进行 LOAM 算法的描述，一般的多线激光雷达或是 Livox 雷达则可以认为是图 1. 的特殊形式，算法过程很容易由此导出。<br>　　设第 \(k\) 次 Sweep 的点云为 \(\mathcal{P} _ k\)，Lidar 坐标系定义为此次 Sweep 初始扫描(也可定义为结束扫描)时刻 \(t_k\) 时， Lidar 位置下的坐标系 \(L\)，Sweep 由 \(S\) 个 Scan 组成，或由 \(I\) 个点组成，归纳为： <span class="math display">\[\mathcal{P} _ k = \{\mathcal{P}_{(k,s)}\}_{s=1}^S = \{\mathit{X}_{(k,i)}^L\}_{i=1}^I  \tag{1}\]</span> 定义 \(\mathit{T} _ k^L(t)\) 为 Lidar 从时间 \(t_k\to t\) 的位姿变换；定义 \(\mathit{T} _ {k}^L(t_{(k,i)})\)(简写为 \(\mathit{T} _ {(k,i)}^L\)) 为 \(t_{(k,i)}\) 时刻接收到的点 \(\mathit{X} _ {(k,i)}\) 变换到坐标系 \(L\)，即 Sweep 初始时刻 Lidar 位置，的变换矩阵。<br>　　<strong>运动补偿问题</strong>： <span class="math display">\[\{\mathit{T} _ {(k,i)}^L\} _ {i=1}^I \tag{2}\]</span> 　　<strong>里程计问题</strong>： <span class="math display">\[\mathit{T} _ K^L(t) \prod _ {k=1}^K\mathit{T} _ {k-1}^L(t _ {k}) \tag{3}\]</span></p><h2 id="loam-for-2d-lidar-with-motor12">2. LOAM for 2D Lidar with Motor<a href="#1" id="1ref"><sup>[1]</sup></a><a href="#2" id="2ref"><sup>[2]</sup></a></h2><p><img src="/LOAM/loam.png" width="70%" height="70%" title="图 3. LOAM Software System"> 　　硬件装置如图 1. 所示，这里不再赘述，软件算法流程如图 3. 所示，\(\mathcal{\hat{P}} _ k=\{\mathcal{P} _ {(k,s)}\}\) 为累积的 Scan 点云，其都会注册到 \(L\) 坐标系，得到 \(\mathcal{P} _ k\)。Lidar Odometry 由 \(\mathcal{\hat{P}} _ k\) 注册到 \(\mathcal{P} _ {k-1}\) 生成高频低精度的位姿，并且生成运动补偿后的 Sweep 点云(这里也可以用其它的里程计实现，如 IMU 等)；Lidar Mapping 则由 \(\mathcal{P}_k\) 注册到世界坐标系 \(W\) 下的地图 \(\mathcal{P}_m\) 中，生成低频高精度的位姿和地图；Transform Integration 则插值出高精度高频的位姿。</p><h3 id="feature-extraction">2.1. Feature Extraction</h3><p>　　这里提取的特征并没有描述子，更确切的说是找出有代表性的点。定义一种描述局部平面曲率的的变量： <span class="math display">\[c = \frac{1}{\vert \mathcal{S}\vert\cdot \Vert\mathit{X} _ {(k,i)}^L\Vert} \left\Vert\sum _ {j\in\mathcal{S},j\ne i}\left(\mathit{X} _ {(k,i)}^L-\mathit{X} _ {(k,j)}^L\right)\right\Vert \tag{3}\]</span> 其中 \(\mathcal{S}\) 为点 \(\mathit{X} _ {(k,i)}^L\) 相邻的同一 Scan 的点，其前后时序上各一半。根据 \(c\) 的值，由大到小选出 Edge Points 集，由小到大选出 Planar Points 集。最终选出的点需满足以下条件：</p><ol type="1"><li>为了特征点的均匀分布，将空间进行栅格化，每个栅格最多容纳特定的点数；</li><li>被选择的点的周围点不会被选择；</li><li>对于 Planar Points 集中的点，如果其平面与雷达射线接近平行，那么则不予采用；</li><li>对于 Edge Points 集中的点，如果其处于被遮挡的区域边缘，那么也不予采用；</li></ol><h3 id="feature-registration">2.2. Feature Registration</h3><p><img src="/LOAM/icp.png" width="50%" height="50%" title="图 4. Registration"> 　　如图 4. 所示，Lidar Odometry 模块的作用是将累积的 Scan 注册到上一时刻的 Sweep 中。设 \(\mathcal{\bar{P}} _ {k-1}\) 为点云 \(\mathcal{P} _ {k-1}\) 投影到 \(t _ {k}\) 的 Lidar 坐标系 \(L _ k\) 后的表示。\(\mathcal{\tilde{E}} _ k, \mathcal{\tilde{H}} _ k\) 为 \(\mathcal{\hat{P}} _ k\) 中提取的 Edge Points 与 Planar Points 集，并转换到了 \(L _ k\) 坐标系。 <img src="/LOAM/loss.png" width="50%" height="50%" title="图 4. Edge & Planar Points Correspondence"></p><ol type="1"><li><strong>Point to Edge</strong><br>对于点 \(i\in\mathcal{\tilde{E}} _ k\)，如图 4. 所示，找到其最近的点 \(j\in\mathcal{\bar{P}} _ {k-1}\)，并在点 \(j\) 前后相邻的两个 Scan 中找到与点 \(i\) 最近的点，记为 \(l\)（同一 Scan 不会打到同一 Edge 处）。通过式 (3) 进一步确认 \(j,l\) 是否满足 Edge Points 的条件，如果满足，那么直线 \((j,l)\) 则就是点 \(i\) 的对应直线，误差函数为： <span class="math display">\[d _ {\mathcal{E}} = \frac{\left\vert \left(\mathit{\tilde{X}} _ {(k,i)}^L-\mathit{\bar{X}} _ {(k-1,j)}^L\right)\times\left(\mathit{\tilde{X}} _ {(k,i)}^L-\mathit{\bar{X}} _ {(k-1,l)}^L\right) \right\vert}{\left\vert\left(\mathit{\bar{X}} _ {(k-1,j)}^L-\mathit{\bar{X}} _ {(k-1,l)}^L\right)\right\vert} \tag{4}\]</span></li><li><strong>Point to Plane</strong><br>对于点 \(i\in\mathcal{\tilde{H}} _ k\)，如图 4. 所示，找到其最近的点 \(j\in\mathcal{\bar{P}} _ {k-1}\)，并在点 \(j\) 同一 Scan 中找到与点 \(i\) 第二近的点 \(l\)，在其前后相邻的两个 Scan 中找到与点 \(i\) 最近的点，记为 \(m\)。通过式 (3) 进一步确认 \(j,l,m\) 是否满足 Planar Points 的条件，如果满足，那么平面 \((j,l,m)\) 则就是点 \(i\) 的对应面，误差函数为： <span class="math display">\[d _ {\mathcal{H}} = \frac{\left\vert \left(\mathit{\tilde{X}} _ {(k,i)}^L-\mathit{\bar{X}} _ {(k-1,j)}^L\right)^T\cdot\left(\left(\mathit{\bar{X}} _ {(k-1,j)}^L-\mathit{\bar{X}} _ {(k-1,l)}^L\right)\times\left(\mathit{\bar{X}} _ {(k-1,j)}^L-\mathit{\bar{X}} _ {(k-1,m)}^L\right)\right) \right\vert}{\left\vert\left(\mathit{\bar{X}} _ {(k-1,j)}^L-\mathit{\bar{X}} _ {(k-1,l)}^L\right)\times\left(\mathit{\bar{X}} _ {(k-1,j)}^L-\mathit{\bar{X}} _ {(k-1,m)}^L\right)\right\vert} \tag{5}\]</span></li></ol><h3 id="motion-estimation">2.3. Motion Estimation</h3><p>　　首先进行运动补偿，即求式(2)。记 \(\mathit{T} _ k^L(t) = [\mathit{R} _ k^L(t)\; \mathit{\tau} _ k^L(t)]\)。假设 \(t_k\to t\) 雷达为匀速运动，那么根据每个点的时间戳进行运动插值: <span class="math display">\[\mathit{T} _ {(k,i)}^L = \begin{bmatrix}\mathit{R} _ {(k,i)}^L &amp; \mathit{\tau} _ {(k,i)}^L\end{bmatrix} = \begin{bmatrix}e^{\hat{\omega}\theta s} &amp; s\mathit{\tau} _ k^L(t)\end{bmatrix} = \begin{bmatrix}e^{\hat{\omega}\theta \frac{t _ {(k,i)}-t _ k}{t-t _ k}} &amp; \frac{t _ {(k,i)}-t _ k}{t-t _ k}\mathit{\tau} _ k^L(t)\end{bmatrix} =\begin{bmatrix}\mathbf{I} + \hat{\omega} \mathrm{sin}\left(s\theta\right) + \hat{\omega}^2\left(1-\mathrm{cos}\left(s\theta\right)\right) &amp; s\mathit{\tau} _ k^L(t)\end{bmatrix}\tag{6}\]</span> 其中 \(\theta, \omega\) 分别是 \(\mathit{R} _ k^L(t)\) 的幅度与旋转角，\(\hat{\omega}\) 是 \(\omega\) 的 Skew Symmetric Matrix。<br>　　由此，对于特征点集，有如下关系： <span class="math display">\[\begin{align}\mathit{\tilde{X}} _ {(k,i)}^L &amp;= \mathit{T} _ {(k,i)}^L\mathit{X} _ {(k,i)} \\\tag{7}\end{align}\]</span> 带入式(4)(5)，可简化为以下非线性最小二乘优化函数： <span class="math display">\[f(\mathit{T} _ {k}^L(t)) = \mathbf{d} \tag{8}\]</span> 其中每一行表示一个特征点及对应的误差，用非线性优化使 \(\mathbf{d}\to \mathbf{0}\)： <span class="math display">\[\mathit{T} _ {k}^L(t)\gets \mathit{T} _ {k}^L(t) - (\mathbf{J}^T\mathbf{J}+\lambda\mathrm{diag(\mathbf{J}^T\mathbf{J})})^{-1}\mathbf{J}^T\mathbf{d} \tag{9}\]</span> 其中雅克比矩阵 \(\mathbf{J}=\frac{\partial f}{\partial \mathit{T} _ {k}^L(t)}\)；\(\lambda\) 由优化方法决定，如 LM，Gaussian-Newton 等。</p><h3 id="lidar-odometry">2.4. Lidar Odometry</h3><p><img src="/LOAM/loam_alg.png" width="40%" height="40%" title="图 5. Lidar Odometry Algorithm"> 　　Lidar Odometry 模块生成 10Hz 的高频低精度雷达位姿(雷达 Scan 频率为 40Hz)，1Hz 的去畸变的点云帧，算法过程如图 5. 所示，优化时对每个特征点根据匹配距离作了权重处理。这里求取雷达位姿 \(\mathit{T} _ k^L(t)\) 是通过点云注册实现的，<strong>也完全可以采用其它里程计，如 IMU 等</strong>。</p><h3 id="lidar-mapping">2.5. Lidar Mapping</h3><p>　　Lidar Mapping 模块生成 1Hz 的低频高精度雷达位姿以及地图。式(3)后半部分表示的就是本模块要求的第 \(t_k\) 时刻在世界坐标系下的低频高精度位姿 \(\mathit{T} _ {k-1}^W(t _ k)\)。设累积到第 \(k-1\) 个 Sweep 的地图为 \(\mathcal{Q} _ {k-1}\)，第 \(k\) 次 Sweep 点云 \(\mathcal{\bar{P}} _ k\) 在世界坐标系下的表示为 \(\mathcal{\bar{Q}} _ k \)，将 \(\mathcal{\bar{Q}} _ k \) 注册到世界地图 \(\mathcal{Q} _ {k-1}\) 中，就求解出了位姿 \(\mathit{T} _ {k}^W(t _ {k+1})\)。<br>　　算法过程与 Lidar Odometry 类似，不同的是：</p><ol type="1"><li>为了提升精度，特征点数量增加了好几倍(点云量也增多了，Sweep VS. Map)；</li><li>由于 Map 中无法区分相邻的 Scan，所以找 Map 中对应的 Edge 或 Planar 时，采用以下方法：找到该特征点在对应 Map 中最近的点集 \(\mathcal{S'}\)，计算该点集的协方差矩阵 \(\mathbf{M}\)，其特征值与特征向量为 \(\mathbf{V,E}\)。如果该点集分布属于 Edge Line，那么有一个显著较大的特征值，对应的特征向量代表该直线的方向；如果该点集分布属于 Planar Patch，那么有两个显著较大的特征值，最小特征值对应的特征向量表示了该平面的方向。由此找到 Point-to-Edge，Point-to-Plane 匹配。</li></ol><p>　　建图时需要对 Map 进行采样，通过 Voxel-Grid Filter 保持栅格内点的密度，由此减少内存及运算量，Edge Points 的栅格应该要比 Planar Points 的小。<br>　　得到低频高精度雷达位姿后，结合 Lidar Odometry(式(3))，即可输出高频高精度(精度相对世界坐标系而言)的雷达位姿。</p><h2 id="loam-for-livox3">3. LOAM for Livox<a href="#3" id="3ref"><sup>[3]</sup></a></h2><p>　　1.2 小节中已经阐述了 Livox 雷达的特性，这里整理如下：</p><ol type="a"><li><strong>Small FoV</strong><br>包括 MEMS 这种 Solid State LiDARs，一般都有较小的视场角，不像旋转式机械雷达可达 360°；</li><li><strong>Irregular Scanning Pattern</strong><br>如图 2. 所示，雷达扫描出的 Pattern 是无规则的，这就导致有效特征提取的难度提升；</li><li><strong>Non-repetitive Scanning</strong><br>非重复性扫描，有利有弊；</li><li><strong>Motion Blur</strong><br>包括自身运动及目标运动所产生的点云畸变。自身运动所导致的点云畸变可以通过估计自身运动后，对点云进行运动补偿来矫正；而由于帧内周期均会扫描到目标，所以目标运动所产生的点云畸变影响较大，且基本无法消除。</li></ol><h3 id="workflow">3.1. Workflow</h3><p><img src="/LOAM/livox_loam.png" width="90%" height="90%" title="图 6. Livox Loam"> 　　Livox LOAM 可以认为是 LOAM 的简化版，直接从每帧的点云中提取出 Edge Points 和 Planar Points，经过线性插值的运动补偿后，在 Map 中找到对应的 Edge Line 与 Planar Patch，由此建立优化函数。相比于 LOAM，本文干掉了高频低精度的 Lidar Odometry(因为 Livox 没有前后 Scan 概念，很难做 Scan-to-Sweep 的点云注册)，直接出 20Hz 高频高精度的 Odometry 与 Map(计算平台强+软件多线程)。<br>　　此外本文针对雷达特性还作了更细致的工程改进，包括：</p><ol type="1"><li>更严格的特征点选取<br>去除视场边缘处的特征点；去除较大或较小反射强度的点；</li><li>改进的特征提取<br>为了增多提取的特征点，将周围反射率变化较大的点也列入 Edge Points；</li><li>Outlier Rejection<br>在优化迭代时，先迭代两步，然后去除掉有较大误差的点，最后作进一步迭代；</li><li>Dynamic Objects Filtering<br>扣除掉动态障碍物的点云，这需要动态障碍物检测模块的支持；</li></ol><h2 id="loam-for-vlp-164">4. LOAM for VLP-16<a href="#4" id="4ref"><sup>[4]</sup></a></h2><h3 id="motion-blur">4.1. Motion Blur</h3><p>　　运动导致的点云畸变主要有两种：自身运动与目标运动。对于旋转式线束雷达来说，目标运动所导致的畸变基本可考虑不计(只有目标正好处于初始扫描与结束扫描的交界处时会有影响；Mapping 时则已扣掉动态障碍物，所以不影响)，这里主要讨论自身运动所导致的点云畸变影响。<br>　　每帧激光雷达数据(即一次 Sweep)都会标记到同一时间戳，假设标记到初始扫描的时刻。假设激光雷达旋转一周的扫描周期为 \(T\)，考虑一次 Sweep：\(t\in [0,T]\)。假设在扫描周期内自身为匀速运动，速度为 \(v\)，那么场景中点云的最大偏移畸变为 \(vT\)。考虑两次 Sweep: \(t _ 1,t _ 2\)，对应的速度为 \(v _ 1, v _ 2\)，那么两个时刻对同一物体的点云偏差量为 \(v _ 1T,v _ 2T\)。在世界坐标系下，该物体观测的点云最坏的不一致量可达到 \(|v _ 1T+v _ 2T|\)(自身运动有旋转的时候)，当然大多数情况可能是 \(|v _ 1T-v _ 2T|\)。</p><ol type="a"><li><strong>单帧情况</strong><br>当 \(T=0.1s,v=20m/s\) 时，畸变量为 2m，对于目标检测算法，虽然目标整体漂移了约 2m，不影响检测(尺寸未变)，但是直接导致观测的目标位置漂了约 2m！如果目标正好处于初始扫描和结束扫描的位置，那么目标的尺寸也会失真。</li><li><strong>多帧情况</strong><br>这种情况指 Mapping 的过程。如果 \(t _ 1, t _ 2\) 时间跨度大，那么世界坐标系下同一物体的不一致性会相当高。如果是相邻 \(n\) 帧，假设自身加速度为 \(a = 5m/s^2\)，那么不一致量为 \(|v _ 1T-v _ 2T|=nTaT=0.05n\)，相邻帧可达 5cm ！</li></ol><p>由此可见，不管是单帧任务还是多帧任务，点云的运动补偿不可不做。</p><h3 id="other">4.2. Other</h3><p>　　<a href="#4" id="4ref">[4]</a> 根据代码详细描述了 LOAM 应用到旋转式多线激光雷达的诸多细节，代码中采用了 IMU 里程计作为高频低精度的位姿估计。其它内容在以上章节中都有描述，这里就不再展开了。</p><h2 id="reference">5. Reference</h2><p><a id="1" href="#1ref">[1]</a> Zhang, Ji, and Sanjiv Singh. &quot;LOAM: Lidar Odometry and Mapping in Real-time.&quot; Robotics: Science and Systems. Vol. 2. No. 9. 2014.<br><a id="2" href="#2ref">[2]</a> Zhang, Ji, and Sanjiv Singh. &quot;Low-drift and real-time lidar odometry and mapping.&quot; Autonomous Robots 41.2 (2017): 401-416.<br><a id="3" href="#3ref">[3]</a> Lin, Jiarong, and Fu Zhang. &quot;Loam_livox: A fast, robust, high-precision LiDAR odometry and mapping package for LiDARs of small FoV.&quot; arXiv preprint arXiv:1909.06700 (2019).<br><a id="4" href="#4ref">[4]</a> https://zhuanlan.zhihu.com/p/57351961</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　SLAM 是机器人领域非常重要的一个功能模块，而基于激光雷达的 SLAM 算法，LOAM(Lidar Odometry and Mapping)，则应用也相当广泛。本文从经典的 LOAM 出发，详细描述下激光 SLAM&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;su
      
    
    </summary>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/categories/SLAM/"/>
    
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/tags/SLAM/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
  </entry>
  
  <entry>
    <title>Filter Pruning</title>
    <link href="https://leijiezhang001.github.io/Filter-Pruning/"/>
    <id>https://leijiezhang001.github.io/Filter-Pruning/</id>
    <published>2020-02-03T06:56:14.000Z</published>
    <updated>2020-02-04T12:08:14.148Z</updated>
    
    <content type="html"><![CDATA[<p>　　文章 <a href="/pruning/" title="pruning">pruning</a> 中详细阐述了模型压缩中 Pruning 的基本方法与理论。Pruning 可分为 Structured Pruning 与 Unstructured Pruning 两种，由于 Structured Pruning 不需要特定的芯片支持，可直接在现有 CPU/GPU 架构下进行加速，所以值得作研究及应用。而 Structured Pruning 主要指 Filter Pruning，以及伴随的 Channel Pruning。本文对近期 Filter Pruning 的进展作一个阐述及思考。<br>　　<a href="#1" id="1ref">[1]</a> 得出结论：<strong>Pruning 的本质并不应该是选择重要的 filter/channel，而应该是确定 filter/channel 的数量，在此基础上，从零开始训练也能达到原来的性能</strong>。所以 Pruning 其实只是 AutoML/NAS 领域的一个子任务，即用 AutoML/NAS 是能解决 Pruning 问题的，但是 AutoML/NAS 方法又相对复杂且耗时，所以短期内可能传统的预定义剪枝方法更容易得到应用。本文从预定义剪枝方法和自动学习剪枝方法两大块来作归纳思考。</p><h2 id="问题描述">1. 问题描述</h2><p>　　假设预训练好的网络 \(F\)，其有 \(L\) 层卷积，所有卷积层的 Filter 表示为： <span class="math display">\[ W=\{W^i\} _ {i=1}^L= \left\{\{W^i_j\} _ {j=1}^{c_i}\in\mathbb{R}^{d_i\times c_i}\right\} _ {i=1}^L \tag{1} \]</span> 其中 \(d_i=c_{i-1}\times h_i\times w_i\)；\(c_i,h_i,w_i\) 分别是第 \(i\) 层卷积的 filter 数量，高，宽；\(W_j^i\) 是第 \(i\) 层卷积第 \(j\) 个 filter。<br>　　目标是搜索被剪枝的网络 \(\mathcal{F}\)，剪枝后的 Filter 表示为： <span class="math display">\[ \mathcal{W}=\{\mathcal{W}^i\} _ {i=1}^L= \left\{\{\mathcal{W}^i_j\} _ {j=1}^{\tilde{c}_i}\in\mathbb{R}^{d_i\times \tilde{c} _ i}\right\} _ {i=1}^L \tag{2} \]</span> 其中 \(\tilde{c} _ i=\lfloor p_i\cdot c_i\rceil\)，\(p_i\) 为 Pruning Rate。<br>　　Filter Pruning 会导致输出的特征 Channel 数减少，对应的下一层的每个 Filter 参数需要相应的裁剪，如 <a href="/pruning/" title="pruning">pruning</a> 中提到的三种结构下的 Pruning，尤其需要注意后两种有交点的结构，剪枝时需要作一定的约束(为了简单，交点对应的 Filter 可以选择不剪枝)。</p><h2 id="预定义剪枝方法">2. 预定义剪枝方法</h2><p>　　预定义剪枝网络方法通常预定义的是 \(P=\{p_i\} _ {i=1}^L\)，其剪枝步骤为：</p><ol type="1"><li>Training<br>根据任务训练网络；</li><li>Pruning<br>设计 Filter 重要性度量准则，然后根据预定义的剪枝率，进行 Filter 剪枝；</li><li>Fine-tuning<br>对剪枝好的网络，进行再训练；</li></ol><h3 id="soft-filter-pruning212">2.1. Soft Filter Pruning<a href="#2" id="2ref"><sup>[2]</sup></a><a href="#12" id="12ref"><sup>[12]</sup></a></h3><p><img src="/Filter-Pruning/soft_filter_pruning.png" width="50%" height="50%" title="图 1. Soft Filter Pruning"> 　　如图 1. 所示，其核心思想就是剪枝后的 Filter 在 Fine-tuning 阶段还是保持更新，由此 Pruning，Fine-tuning 迭代获得较优剪枝结果。Filter 重要性度量准则为： <span class="math display">\[\left\Vert W_j^i\right\Vert _ p = \sqrt[p]{\sum_{cc=0}^{c_{i-1}-1}\sum_{k_1=0}^{h_i-1}\sum_{k_2=0}^{w_i-1}\left\vert W_j^i(cc,k_1,k_2)\right\vert ^p} \tag{3}\]</span></p><h3 id="filter-sketch313">2.2. Filter Sketch<a href="#3" id="3ref"><sup>[3]</sup></a><a href="#13" id="13ref"><sup>[13]</sup></a></h3><p>　　选择 Filter 进行剪枝，另一种思路是，如何选择一部分 Filter，使得该 Filter 集合的信息量与原 Filter 集合信息量近似: <span class="math display">\[\Sigma_{W^i}\approx \Sigma_{\mathcal{W}^i} \tag{4}\]</span> 这里的信息量表达方式采用了协方差矩阵: <span class="math display">\[\begin{align}\Sigma_{W^i} &amp;= \left(W^i-\bar{W}^i \right)\left(W^i-\bar{W}^i \right)^T \\\Sigma_{\mathcal{W}^i} &amp;= \left(\mathcal{W}^i-\mathcal{\bar{W}}^i \right)\left(\mathcal{W}^i-\mathcal{\bar{W}}^i \right)^T \\\end{align} \tag{5}\]</span> 其中 Filter 权重符合高斯分布，即 \(\bar{W}^i=\frac{1}{c_i}\sum _ {j=1}^{c _ i}W _ j ^ i\approx 0\)，\(\mathcal{\bar{W}} ^ i=\frac{1}{\tilde{c} _ i}\sum _ {j=1}^{\tilde{c} _ i}\mathcal{W} _ j^i\approx 0\)。由式(4)(5)，构建最小化目标函数： <span class="math display">\[\mathop{\arg\min}\limits_{\mathcal{W}^i}\left\Vert W^i(W^i)^T-\mathcal{W}^i(\mathcal{W}^i)^T \right\Vert \tag{6}\]</span> 将该问题转换为求取 \(W^i\) 矩阵的 Sketch 问题，则： <span class="math display">\[\left\Vert W^i(W^i)^T-\mathcal{W}^i(\mathcal{W}^i)^T \right\Vert _F \leq \epsilon\left\Vert W^i\right\Vert^2_F \tag{7}\]</span> <img src="/Filter-Pruning/sketch.png" width="50%" height="50%" title="图 2. Frequent Direction"> <img src="/Filter-Pruning/filter_sketch.png" width="50%" height="50%" title="图 3. FilterSketch"> 　　式(7)可用图 2. 所示的算法求解，最终的 Pruning 算法过程如图 3. 所示，改进的地方主要是 Filter 选择的部分，采用了 Matrix Sketch 算法。 <img src="/Filter-Pruning/pruning.png" width="60%" height="60%" title="图 4. 网络裁剪示意图"> 　　<a href="/pruning/" title="pruning">pruning</a> 中提到有分支结构的裁剪会比较麻烦，所以如图 4. 所示，本方法对分支节点的 Filter 不做裁剪处理，简化了问题。</p><h3 id="filter-pruning-via-geometric-median414">2.3. Filter Pruning via Geometric Median<a href="#4" id="4ref"><sup>[4]</sup></a><a href="#14" id="14ref"><sup>[14]</sup></a></h3><p>　　在预定义剪枝网络方法的三个步骤中，大家普遍研究步骤二中 Filter 的重要性度量设计。Filter 重要性度量基本是 Smaller-norm-less-informative 思想，<a href="#5" id="5ref">[5]</a> 中则验证了该思想并不一定正确。<strong>Smaller-norm-less-informative 假设成立的条件是</strong>：</p><ol type="1"><li>Filter 权重的规范偏差(norm deviation)要大；</li><li>Filter 权重的最小规范要小；</li></ol><p>只有满足这两个条件，该假设才成立，即可以裁剪掉规范数较小的 Filter。 <img src="/Filter-Pruning/norm_dist.png" width="60%" height="60%" title="图 5. Filter Norm Distribution"> 　　但是，如图 5. 所示，实际 Filter 的权重分布和理想的并不一致，当 Filter 分布是绿色区域时，采用 Smaller-norm-less-informative 就不合理了，而这种情况还比较多。一般性的，前几层网络的权重规范数偏差会比较大，后几层则比较小。<br><img src="/Filter-Pruning/criterion.png" width="50%" height="50%" title="图 6. Criterion for Filter Pruning"> 　　由此，本方法提出一种基于 Geometric Median 的 Filter 选择方法，如图 6. 所示，基于 Smaller-norm-less-informative 的裁剪后留下的均是规范数较大的 Filter，这还存在一定的冗余性，本方法则通过物理距离测算，剪掉冗余的 Filter。<strong>另一个角度可理解为最大程度的保留 Filter 集合的大概及具体信息，其思想与 FilterSketch 类似</strong>。<br>　　根据 Geometric Median 思想，第 \(i\) 层卷积要裁剪掉的 Filter 为： <span class="math display">\[W^i_{j^\ast}=\mathop{\arg\min}\limits_{W^i_{j^\ast}\,|\,j^\ast\in[0,c_i-1]}\sum_{j&#39;=0}^{c_i-1}\left\Vert W^i_{j^\ast}-W^i_{j&#39;}\right\Vert_2 \tag{8}\]</span> 由此裁剪掉满足条件的 \(W _ {j^*}^i\)，直至符合裁剪比率。<strong>本方法的思想非常类似于 Farthest Point Sampling 采样，留下的 Filter 即为原 Filter 集合采样的结果，且最大程度的保留了集合的信息</strong>。</p><h2 id="自动学习剪枝方法">3. 自动学习剪枝方法</h2><h3 id="abcpruner616">3.1. ABCPruner<a href="#6" id="6ref"><sup>[6]</sup></a><a href="#16" id="16ref"><sup>[16]</sup></a></h3><p><img src="/Filter-Pruning/ABCPruner.png" width="60%" height="60%" title="图 7. ABCPruner"> 　　出于<a href="#1" id="1ref">[1]</a>的结论：<strong>剪枝的本质应该是直接找到每层卷积最优的 Filter 数量，在此基础上从零开始训练也能达到原来的性能</strong>。ABCPruner 的目标就是搜索每层最优的 Filter 数量，如图 7. 所示，ABCPruner 步骤为：</p><ol type="1"><li>初始化一系列不同 Filter 数量的网络结构；</li><li>每个网络结构从 pre-trained 网络中继承权重值，fine-tune 获得每个网络的 fitness(即 accuracy)；</li><li>用 ABC 算法更新网络结构；</li><li>重复迭代 2,3 步骤，获取最高的 fitness 网络作为最终网络结构；</li></ol><h3 id="metapruning717">3.2. MetaPruning<a href="#7" id="7ref"><sup>[7]</sup></a><a href="#17" id="17ref"><sup>[17]</sup></a></h3><p><img src="/Filter-Pruning/metapruning.png" width="50%" height="50%" title="图 8. MetaPruning"> 　　同样，本方法也是基于<a href="#1" id="1ref">[1]</a>的结论。这里设计 PruningNet 来控制裁剪，步骤为：</p><ol type="1"><li>Training PruningNet<br>PruningNet 输入为网络编码向量，即每层卷积的 Filter 数量，输出为产生网络权重的编码量，如 size reshape，crop。每次训练时随机生成网络编码量，网络编码量与 PruningNet 输出共同决定了 PrunedNet 权重，两个网络联合训练；</li><li>Searching for the Best Pruned Net<br>即 Inference 过程，寻找最优的网络编码量，使得 PrunedNet 精度最高；得到最优网络后，不需要 fine-tuning。</li></ol><h3 id="generative-adversarial-learning8">3.3. Generative Adversarial Learning<a href="#8" id="8ref"><sup>[8]</sup></a></h3><p><img src="/Filter-Pruning/GAL.png" width="90%" height="90%" title="图 9. Generative Adversarial Learning"> 　　本方法主要思想来自知识蒸馏(Knowledge Distillation)和生成对抗网络(Generative Adversarial Network)，如图 9. 所示，Baseline 为完整的原始网络，PrunedNet 是为了学习一个 soft mask 来动态选择 block，branch，channel，最终裁剪后的网络由 soft mask 决定。<br>　　从知识蒸馏的角度：Baseline 就是一个大容量的教师网络，Pruned Net 就是个小容量的学生网络，用大容量网络来监督小容量网络学习。从生成对抗学习的角度：Baseline 是原始网络，PrunedNet 是生成的对抗网络，用一个 Discriminator 网络来区分原始网络与生成的对抗网络的区别，使生成的对抗网络输出逼近于原始网络。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Liu, Zhuang, et al. &quot;Rethinking the Value of Network Pruning.&quot; International Conference on Learning Representations. 2018.<br><a id="2" href="#2ref">[2]</a> He, Yang, et al. &quot;Soft filter pruning for accelerating deep convolutional neural networks.&quot; arXiv preprint arXiv:1808.06866 (2018).<br><a id="3" href="#3ref">[3]</a> Lin, Mingbao, et al. &quot;Filter Sketch for Network Pruning.&quot; arXiv preprint arXiv:2001.08514 (2020).<br><a id="4" href="#4ref">[4]</a> He, Yang, et al. &quot;Filter pruning via geometric median for deep convolutional neural networks acceleration.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br><a id="5" href="#5ref">[5]</a> Ye, Jianbo, et al. &quot;Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers.&quot; arXiv preprint arXiv:1802.00124 (2018).<br><a id="6" href="#6ref">[6]</a> Lin, Mingbao, et al. &quot;Channel Pruning via Automatic Structure Search.&quot; arXiv preprint arXiv:2001.08565 (2020).<br><a id="7" href="#7ref">[7]</a> Liu, Zechun, et al. &quot;Metapruning: Meta learning for automatic neural network channel pruning.&quot; Proceedings of the IEEE International Conference on Computer Vision. 2019.<br><a id="8" href="#8ref">[8]</a> Lin, Shaohui, et al. &quot;Towards optimal structured cnn pruning via generative adversarial learning.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br><a id="9" href="#9ref">[9]</a> Singh, Pravendra, et al. &quot;Play and prune: Adaptive filter pruning for deep model compression.&quot; arXiv preprint arXiv:1905.04446 (2019).<br><a id="11" href="#11ref">[11]</a> https://github.com/Eric-mingjie/rethinking-network-pruning<br><a id="12" href="#12ref">[12]</a> https://github.com/he-y/softfilter-pruning<br><a id="13" href="#13ref">[13]</a> https://github.com/lmbxmu/FilterSketch<br><a id="14" href="#14ref">[14]</a> https://github.com/he-y/filter-pruning-geometric-median<br><a id="16" href="#16ref">[16]</a> https://github.com/lmbxmu/ABCPruner<br><a id="17" href="#17ref">[17]</a> https://github.com/liuzechun/MetaPruning</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　文章 &lt;a href=&quot;/pruning/&quot; title=&quot;pruning&quot;&gt;pruning&lt;/a&gt; 中详细阐述了模型压缩中 Pruning 的基本方法与理论。Pruning 可分为 Structured Pruning 与 Unstructured Pruning 两
      
    
    </summary>
    
      <category term="Model Compression" scheme="https://leijiezhang001.github.io/categories/Model-Compression/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Model Compression" scheme="https://leijiezhang001.github.io/tags/Model-Compression/"/>
    
  </entry>
  
  <entry>
    <title>Ground Segmentation with Gaussian Process</title>
    <link href="https://leijiezhang001.github.io/Ground-Segmentation-with-Gaussian-Process/"/>
    <id>https://leijiezhang001.github.io/Ground-Segmentation-with-Gaussian-Process/</id>
    <published>2020-01-21T09:00:34.000Z</published>
    <updated>2020-02-01T08:53:34.072Z</updated>
    
    <content type="html"><![CDATA[<p>　　地面分割可作为自动驾驶系统的一个重要模块，本文介绍一种基于高斯过程的地面分割方法。</p><h2 id="算法概要">1. 算法概要</h2><p><img src="/Ground-Segmentation-with-Gaussian-Process/ground_seg.png" width="80%" height="80%" title="图 1. ground segmentation"> 　　为了加速，本方法<a href="#1" id="1ref"><sup>[1]</sup></a>将三维地面分割问题分解为多个一维高斯过程来求解，如图 1. 所示，其步骤为：</p><ol type="1"><li><strong>Polar Grid Map</strong><br>将点云用极坐标栅格地图表示，二维地面估计分解成射线方向的多个一维地面估计；</li><li><strong>Line Fitting</strong><br>在每个一维方向，根据梯度大小，作可变数量的线段拟合；</li><li><strong>Seed Estimation</strong><br>在半径 \(B\) 范围内，如果某个 Grid 绝对高度(Grid 高度定义为该 Grid 内所有点的最小高度，其绝对高度则是与本车传感器所在地面的比较)大于 \(T_s\)，那么就将其作为 Seed；</li><li><strong>Ground Model Estimation with Gaussian Process</strong><br>采用高斯过程生成每个一维方向 Grid 的地面估计量，这里为了进一步加速，可以删除冗余的 Seed；根据地面估计模型，将满足模型的 Grid 加入 Seed，更新模型，迭代直至收敛，满足模型的 Seed 条件为： <span class="math display">\[\begin{align}V[z]&amp;\leq  t_{model}\\\frac{|z_*-\bar{z}|}{\sqrt{\sigma^2_n+V[z]}} &amp;\leq t_{data}\end{align} \tag{0}\]</span></li><li><strong>Point-wise Segmentation</strong><br>得到地面估计模型后，就得到了每个 Grid 是否为地面的标签量，对于属于地面标签量的 Grid 内的点，与 Grid 高度的相对高度小于 \(T_r\)，则认为该点属于地面。</li></ol><h2 id="高斯过程">2. 高斯过程</h2><p>　　步骤四中用高斯过程来估计地面模型，对于每个极射线方向的 Grids，假设有 \(n\) 个已经确定是地面的训练集：\(D=\{(r _ i,z _ i)\} _ {i=1}^n\)。根据高斯过程定义，这些样本的联合概率分布为： <span class="math display">\[p(Z|R)\sim N(f(R)+\mu,K) \tag{1}\]</span> 其中 \(R=[r_1,...,r_n]^T\) 为每个 Grid 的距离量，\(Z=[z_1,...,z_n]^T\) 为该 Grid 地面高度，\(f(\cdot)\)为高斯过程要回归的函数。\(\mu\) 设计为零，协方差矩阵 \(K\) 表示变量之间的关系，由协方差方程与噪音项构成： <span class="math display">\[K(r_i,r_j)=k(r_i,r_j)+\sigma^2_n\delta_{ij}\tag{2}\]</span> 其中当且仅当 \(i==j\) 时 \(\delta _ {ij} =1\)。<br>　　一般的协方差方程是静态，同向的(stationary, isotropic): <span class="math display">\[k(r_i,r_j)=\sigma_f^2\mathrm{exp}\left(-\frac{(r_i-r_j)^2}{2l^2}\right) \tag{3}\]</span> 其中 \(\sigma_f^2\) 是信号协方差，\(l\) 是 length-scale。该方程假设了全空间内 length-scale 的一致性，然而实际上，<strong>越平坦的地面区域，我们需要越大的 length-scale，因为此时该区域对周围区域的概率输出能更大</strong>，所以可进一步设计协方差方程为: <span class="math display">\[k(r_i,r_j)=\sigma_f^2\left(l_i^2\right)^{\frac{1}{4}}\left(l_j^2\right)^{\frac{1}{4}}\left(\frac{l_i^2+l_j^2}{2}\right)^{-\frac{1}{2}}  \mathrm{exp}\left(-\frac{2(r_i-r_j)^2}{l_i^2+l_j^2}\right) \tag{4}\]</span> 其中 \(l_i\) 为位置 \(r_i\) 的 length-scale。\(l_i\) 由该位置距离最近的线段梯度决定(步骤二): <span class="math display">\[l_i=\left\{\begin{array}{l}a\cdot \mathrm{log}\left(\frac{1}{|g(r_i)|}\right) \,\, if\, |g(r_i)|&gt;g_{def}\\a\cdot \mathrm{log}\left(\frac{1}{|g_{def}|}\right) \,\, otherwise\end{array}\tag{5}\right.\]</span> 　　高斯回归预测的过程为，对于测试集 \(T=(r_\ast,z_\ast)\)，其与训练集的联合概率分布为： <span class="math display">\[\begin{bmatrix}Z\\z_\ast\\\end{bmatrix}\simN\left(0,\begin{bmatrix}K(R,R) &amp; K(R,r_\ast)\\K(r_\ast,R) &amp; K(r_\ast,r_\ast)\\\end{bmatrix}\right)\tag{6}\]</span> 那么，高斯过程回归预测为： <span class="math display">\[\begin{align}\bar{z}_\ast &amp;=K(r_\ast,R)K^{-1}Z\\V[z_\ast] &amp;= K(r_\ast,r_\ast)-K(r_\ast,R)K^{-1}K(R,r_\ast)\end{align} \tag{7}\]</span> 由此得到测试集的预测量，由式(0)可决定该测试量是否标记为地面，进一步迭代估计地面模型，直至收敛。<br>　　需要注意的是，以上我们假设高斯过程的超参数 \(\theta=\{\sigma_f,a,\sigma_n\}\) 是已知的，实际应用中，可以将超参数设定为经验量，也可以基于训练集用 SGD 学习出一个最优量，这里不做展开。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Chen, Tongtong, et al. &quot;Gaussian-process-based real-time ground segmentation for autonomous land vehicles.&quot; Journal of Intelligent &amp; Robotic Systems 76.3-4 (2014): 563-582.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　地面分割可作为自动驾驶系统的一个重要模块，本文介绍一种基于高斯过程的地面分割方法。&lt;/p&gt;
&lt;h2 id=&quot;算法概要&quot;&gt;1. 算法概要&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/Ground-Segmentation-with-Gaussian-Process/ground
      
    
    </summary>
    
      <category term="Semantic Segmentation" scheme="https://leijiezhang001.github.io/categories/Semantic-Segmentation/"/>
    
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/tags/Segmentation/"/>
    
  </entry>
  
</feed>
