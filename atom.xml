<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LeijieZhang</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://leijiezhang001.github.io/"/>
  <updated>2020-02-08T08:02:01.527Z</updated>
  <id>https://leijiezhang001.github.io/</id>
  
  <author>
    <name>Leijie</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>LOAM(Lidar Odometry and Mapping)</title>
    <link href="https://leijiezhang001.github.io/LOAM/"/>
    <id>https://leijiezhang001.github.io/LOAM/</id>
    <published>2020-02-06T07:11:08.000Z</published>
    <updated>2020-02-08T08:02:01.527Z</updated>
    
    <content type="html"><![CDATA[<p>　　SLAM 是机器人领域非常重要的一个功能模块，而基于激光雷达的 SLAM 算法，LOAM(Lidar Odometry and Mapping)，则应用也相当广泛。本文从经典的 LOAM 出发，详细描述下激光 SLAM<a href="#1" id="1ref"><sup>[1]</sup></a><a href="#2" id="2ref"><sup>[2]</sup></a> 中的一些模块细节。</p><h2 id="问题描述">1. 问题描述</h2><h3 id="scan-定义">1.1. Scan 定义</h3><p>　　针对旋转式机械雷达，Scan 为单个激光头旋转一周获得的点云，类似 VLP-16 旋转一周则是“几乎”同时获得了 16 个 Scan。针对棱镜旋转而激光头不旋转的雷达(Solid State LiDARs)，如大疆 Livox 系列，Scan 则可定义为一定时间下累积获得的点云。</p><h3 id="sweep-定义">1.2. Sweep 定义</h3><p>　　Sweep 定义为静止的机器人平台上激光雷达能覆盖到所有空间的点云。<br>　　针对旋转式机械雷达，Sweep 即为旋转一周获得的由一个或多个 Scan 组成的点云。针对棱镜旋转而激光头不旋转的雷达，由于其属于非重复性扫描(Non-repetitive Scanning)结构，所以 Sweep 理论上为时间趋于无穷大时获得的点云，但是狭义上，可以认为一段较长时间下(相对于 Scan 时间)，获得的点云。<br><img src="/LOAM/motor_lidar.png" width="60%" height="60%" title="图 1. 3D Lidar Updated from 2D Lidar with a Motor"> 　　那么，如果给激光雷达加上一个马达呢？如图 1. 所示，<a href="#1" id="1ref">[1]</a> 中设计了一种 3D Lidar 装置，由一个只有一个激光头的 2D Lidar 和一个马达组成，激光扫描频率为 40Hz，马达转速为 180°/s。这种装置下，Scan 意义不变，Sweep 则为 1s 内该装置获得的点云(因为 1s 的时间内，该装置获得的点云可覆盖所有能覆盖的空间)。</p><h3 id="非重复性扫描激光雷达">1.2. 非重复性扫描激光雷达</h3><p><img src="/LOAM/livox.png" width="60%" height="60%" title="图 2. Livox Scanning Pattern"> 　　其实，大疆的 Livox 非重复性扫描雷达相当于把这马达移到了内部的棱镜中，而且加上非对称，所以随着时间的累积，可获得相当稠密的点云。<br>　　Livox 这种非重复式扫描的激光雷达价格低廉，相对于传统的多线激光雷达有很多优点，但是有个致命的缺点：<strong>只能准确捕捉静态物体，无法准确捕捉动态物体；对应的，只能作 Mapping，很难作动态障碍物的估计。</strong>因为在一帧点云的扫描周期 \(T\) 内，如果目标速度为 \(v\)，那么 Livox 式雷达在扫描周期内都会扫到目标，目标的尺寸会被放大 \(Tv\)，而传统旋转的线束雷达真正扫到目标的时间为 \(t\ll T\)。当 \(T=0.1s\)，\(v=20m/s\) 时，尺寸放大为 2m，而一般小汽车车长也就几米。<strong>所以尺寸是估不准的，但是其它属性，如位置，速度，在目标加速度不是很大的情况下，可能还是有技巧可以估准的，具体就得看实验效果。另一种思路：直接对其进行物理建模，先假设已知目标速度，那么所有点即可恢复出目标的真实尺寸，然后可进一步估计速度，由此迭代至最优值</strong>。<br>　　由于本车的状态可以通过其它方式(如 IMU)获得，所以本车运动所引起的点云畸变(即 Motion Blur，基本所有雷达都会有这个问题，详见 2.3，4.1 章节)可以很容易得到补偿，所以对于静态目标，点云是能准确捕捉到其物理属性的。</p><h3 id="符号定义">1.3. 符号定义</h3><p>　　本文首先基于图 1. 的装置进行 LOAM 算法的描述，一般的多线激光雷达或是 Livox 雷达则可以认为是图 1. 的特殊形式，算法过程很容易由此导出。<br>　　设第 \(k\) 次 Sweep 的点云为 \(\mathcal{P} _ k\)，Lidar 坐标系定义为此次 Sweep 初始扫描(也可定义为结束扫描)时刻 \(t_k\) 时， Lidar 位置下的坐标系 \(L\)，Sweep 由 \(S\) 个 Scan 组成，或由 \(I\) 个点组成，归纳为： <span class="math display">\[\mathcal{P} _ k = \{\mathcal{P}_{(k,s)}\}_{s=1}^S = \{\mathit{X}_{(k,i)}^L\}_{i=1}^I  \tag{1}\]</span> 定义 \(\mathit{T} _ k^L(t)\) 为 Lidar 从时间 \(t_k\to t\) 的位姿变换；定义 \(\mathit{T} _ {k}^L(t_{(k,i)})\)(简写为 \(\mathit{T} _ {(k,i)}^L\)) 为 \(t_{(k,i)}\) 时刻接收到的点 \(\mathit{X} _ {(k,i)}\) 变换到坐标系 \(L\)，即 Sweep 初始时刻 Lidar 位置，的变换矩阵。<br>　　<strong>运动补偿问题</strong>： <span class="math display">\[\{\mathit{T} _ {(k,i)}^L\} _ {i=1}^I \tag{2}\]</span> 　　<strong>里程计问题</strong>： <span class="math display">\[\mathit{T} _ K^L(t) \prod _ {k=1}^K\mathit{T} _ {k-1}^L(t _ {k}) \tag{3}\]</span></p><h2 id="loam-for-2d-lidar-with-motor12">2. LOAM for 2D Lidar with Motor<a href="#1" id="1ref"><sup>[1]</sup></a><a href="#2" id="2ref"><sup>[2]</sup></a></h2><p><img src="/LOAM/loam.png" width="70%" height="70%" title="图 3. LOAM Software System"> 　　硬件装置如图 1. 所示，这里不再赘述，软件算法流程如图 3. 所示，\(\mathcal{\hat{P}} _ k=\{\mathcal{P} _ {(k,s)}\}\) 为累积的 Scan 点云，其都会注册到 \(L\) 坐标系，得到 \(\mathcal{P} _ k\)。Lidar Odometry 由 \(\mathcal{\hat{P}} _ k\) 注册到 \(\mathcal{P} _ {k-1}\) 生成高频低精度的位姿，并且生成运动补偿后的 Sweep 点云(这里也可以用其它的里程计实现，如 IMU 等)；Lidar Mapping 则由 \(\mathcal{P}_k\) 注册到世界坐标系 \(W\) 下的地图 \(\mathcal{P}_m\) 中，生成低频高精度的位姿和地图；Transform Integration 则插值出高精度高频的位姿。</p><h3 id="feature-extraction">2.1. Feature Extraction</h3><p>　　这里提取的特征并没有描述子，更确切的说是找出有代表性的点。定义一种描述局部平面曲率的的变量： <span class="math display">\[c = \frac{1}{\vert \mathcal{S}\vert\cdot \Vert\mathit{X} _ {(k,i)}^L\Vert} \left\Vert\sum _ {j\in\mathcal{S},j\ne i}\left(\mathit{X} _ {(k,i)}^L-\mathit{X} _ {(k,j)}^L\right)\right\Vert \tag{3}\]</span> 其中 \(\mathcal{S}\) 为点 \(\mathit{X} _ {(k,i)}^L\) 相邻的同一 Scan 的点，其前后时序上各一半。根据 \(c\) 的值，由大到小选出 Edge Points 集，由小到大选出 Planar Points 集。最终选出的点需满足以下条件：</p><ol type="1"><li>为了特征点的均匀分布，将空间进行栅格化，每个栅格最多容纳特定的点数；</li><li>被选择的点的周围点不会被选择；</li><li>对于 Planar Points 集中的点，如果其平面与雷达射线接近平行，那么则不予采用；</li><li>对于 Edge Points 集中的点，如果其处于被遮挡的区域边缘，那么也不予采用；</li></ol><h3 id="feature-registration">2.2. Feature Registration</h3><p><img src="/LOAM/icp.png" width="50%" height="50%" title="图 4. Registration"> 　　如图 4. 所示，Lidar Odometry 模块的作用是将累积的 Scan 注册到上一时刻的 Sweep 中。设 \(\mathcal{\bar{P}} _ {k-1}\) 为点云 \(\mathcal{P} _ {k-1}\) 投影到 \(t _ {k}\) 的 Lidar 坐标系 \(L _ k\) 后的表示。\(\mathcal{\tilde{E}} _ k, \mathcal{\tilde{H}} _ k\) 为 \(\mathcal{\hat{P}} _ k\) 中提取的 Edge Points 与 Planar Points 集，并转换到了 \(L _ k\) 坐标系。 <img src="/LOAM/loss.png" width="50%" height="50%" title="图 4. Edge & Planar Points Correspondence"></p><ol type="1"><li><strong>Point to Edge</strong><br>对于点 \(i\in\mathcal{\tilde{E}} _ k\)，如图 4. 所示，找到其最近的点 \(j\in\mathcal{\bar{P}} _ {k-1}\)，并在点 \(j\) 前后相邻的两个 Scan 中找到与点 \(i\) 最近的点，记为 \(l\)（同一 Scan 不会打到同一 Edge 处）。通过式 (3) 进一步确认 \(j,l\) 是否满足 Edge Points 的条件，如果满足，那么直线 \((j,l)\) 则就是点 \(i\) 的对应直线，误差函数为： <span class="math display">\[d _ {\mathcal{E}} = \frac{\left\vert \left(\mathit{\tilde{X}} _ {(k,i)}^L-\mathit{\bar{X}} _ {(k-1,j)}^L\right)\times\left(\mathit{\tilde{X}} _ {(k,i)}^L-\mathit{\bar{X}} _ {(k-1,l)}^L\right) \right\vert}{\left\vert\left(\mathit{\bar{X}} _ {(k-1,j)}^L-\mathit{\bar{X}} _ {(k-1,l)}^L\right)\right\vert} \tag{4}\]</span></li><li><strong>Point to Plane</strong><br>对于点 \(i\in\mathcal{\tilde{H}} _ k\)，如图 4. 所示，找到其最近的点 \(j\in\mathcal{\bar{P}} _ {k-1}\)，并在点 \(j\) 同一 Scan 中找到与点 \(i\) 第二近的点 \(l\)，在其前后相邻的两个 Scan 中找到与点 \(i\) 最近的点，记为 \(m\)。通过式 (3) 进一步确认 \(j,l,m\) 是否满足 Planar Points 的条件，如果满足，那么平面 \((j,l,m)\) 则就是点 \(i\) 的对应面，误差函数为： <span class="math display">\[d _ {\mathcal{H}} = \frac{\left\vert \left(\mathit{\tilde{X}} _ {(k,i)}^L-\mathit{\bar{X}} _ {(k-1,j)}^L\right)^T\cdot\left(\left(\mathit{\bar{X}} _ {(k-1,j)}^L-\mathit{\bar{X}} _ {(k-1,l)}^L\right)\times\left(\mathit{\bar{X}} _ {(k-1,j)}^L-\mathit{\bar{X}} _ {(k-1,m)}^L\right)\right) \right\vert}{\left\vert\left(\mathit{\bar{X}} _ {(k-1,j)}^L-\mathit{\bar{X}} _ {(k-1,l)}^L\right)\times\left(\mathit{\bar{X}} _ {(k-1,j)}^L-\mathit{\bar{X}} _ {(k-1,m)}^L\right)\right\vert} \tag{5}\]</span></li></ol><h3 id="motion-estimation">2.3. Motion Estimation</h3><p>　　首先进行运动补偿，即求式(2)。记 \(\mathit{T} _ k^L(t) = [\mathit{R} _ k^L(t)\; \mathit{\tau} _ k^L(t)]\)。假设 \(t_k\to t\) 雷达为匀速运动，那么根据每个点的时间戳进行运动插值: <span class="math display">\[\mathit{T} _ {(k,i)}^L = \begin{bmatrix}\mathit{R} _ {(k,i)}^L &amp; \mathit{\tau} _ {(k,i)}^L\end{bmatrix} = \begin{bmatrix}e^{\hat{\omega}\theta s} &amp; s\mathit{\tau} _ k^L(t)\end{bmatrix} = \begin{bmatrix}e^{\hat{\omega}\theta \frac{t _ {(k,i)}-t _ k}{t-t _ k}} &amp; \frac{t _ {(k,i)}-t _ k}{t-t _ k}\mathit{\tau} _ k^L(t)\end{bmatrix} =\begin{bmatrix}\mathbf{I} + \hat{\omega} \mathrm{sin}\left(s\theta\right) + \hat{\omega}^2\left(1-\mathrm{cos}\left(s\theta\right)\right) &amp; s\mathit{\tau} _ k^L(t)\end{bmatrix}\tag{6}\]</span> 其中 \(\theta, \omega\) 分别是 \(\mathit{R} _ k^L(t)\) 的幅度与旋转角，\(\hat{\omega}\) 是 \(\omega\) 的 Skew Symmetric Matrix。<br>　　由此，对于特征点集，有如下关系： <span class="math display">\[\begin{align}\mathit{\tilde{X}} _ {(k,i)}^L &amp;= \mathit{T} _ {(k,i)}^L\mathit{X} _ {(k,i)} \\\tag{7}\end{align}\]</span> 带入式(4)(5)，可简化为以下非线性最小二乘优化函数： <span class="math display">\[f(\mathit{T} _ {k}^L(t)) = \mathbf{d} \tag{8}\]</span> 其中每一行表示一个特征点及对应的误差，用非线性优化使 \(\mathbf{d}\to \mathbf{0}\)： <span class="math display">\[\mathit{T} _ {k}^L(t)\gets \mathit{T} _ {k}^L(t) - (\mathbf{J}^T\mathbf{J}+\lambda\mathrm{diag(\mathbf{J}^T\mathbf{J})})^{-1}\mathbf{J}^T\mathbf{d} \tag{9}\]</span> 其中雅克比矩阵 \(\mathbf{J}=\frac{\partial f}{\partial \mathit{T} _ {k}^L(t)}\)；\(\lambda\) 由优化方法决定，如 LM，Gaussian-Newton 等。</p><h3 id="lidar-odometry">2.4. Lidar Odometry</h3><p><img src="/LOAM/loam_alg.png" width="40%" height="40%" title="图 5. Lidar Odometry Algorithm"> 　　Lidar Odometry 模块生成 10Hz 的高频低精度雷达位姿(雷达 Scan 频率为 40Hz)，1Hz 的去畸变的点云帧，算法过程如图 5. 所示，优化时对每个特征点根据匹配距离作了权重处理。这里求取雷达位姿 \(\mathit{T} _ k^L(t)\) 是通过点云注册实现的，<strong>也完全可以采用其它里程计，如 IMU 等</strong>。</p><h3 id="lidar-mapping">2.5. Lidar Mapping</h3><p>　　Lidar Mapping 模块生成 1Hz 的低频高精度雷达位姿以及地图。式(3)后半部分表示的就是本模块要求的第 \(t_k\) 时刻在世界坐标系下的低频高精度位姿 \(\mathit{T} _ {k-1}^W(t _ k)\)。设累积到第 \(k-1\) 个 Sweep 的地图为 \(\mathcal{Q} _ {k-1}\)，第 \(k\) 次 Sweep 点云 \(\mathcal{\bar{P}} _ k\) 在世界坐标系下的表示为 \(\mathcal{\bar{Q}} _ k \)，将 \(\mathcal{\bar{Q}} _ k \) 注册到世界地图 \(\mathcal{Q} _ {k-1}\) 中，就求解出了位姿 \(\mathit{T} _ {k}^W(t _ {k+1})\)。<br>　　算法过程与 Lidar Odometry 类似，不同的是：</p><ol type="1"><li>为了提升精度，特征点数量增加了好几倍(点云量也增多了，Sweep VS. Map)；</li><li>由于 Map 中无法区分相邻的 Scan，所以找 Map 中对应的 Edge 或 Planar 时，采用以下方法：找到该特征点在对应 Map 中最近的点集 \(\mathcal{S'}\)，计算该点集的协方差矩阵 \(\mathbf{M}\)，其特征值与特征向量为 \(\mathbf{V,E}\)。如果该点集分布属于 Edge Line，那么有一个显著较大的特征值，对应的特征向量代表该直线的方向；如果该点集分布属于 Planar Patch，那么有两个显著较大的特征值，最小特征值对应的特征向量表示了该平面的方向。由此找到 Point-to-Edge，Point-to-Plane 匹配。</li></ol><p>　　建图时需要对 Map 进行采样，通过 Voxel-Grid Filter 保持栅格内点的密度，由此减少内存及运算量，Edge Points 的栅格应该要比 Planar Points 的小。<br>　　得到低频高精度雷达位姿后，结合 Lidar Odometry(式(3))，即可输出高频高精度(精度相对世界坐标系而言)的雷达位姿。</p><h2 id="loam-for-livox3">3. LOAM for Livox<a href="#3" id="3ref"><sup>[3]</sup></a></h2><p>　　1.2 小节中已经阐述了 Livox 雷达的特性，这里整理如下：</p><ol type="a"><li><strong>Small FoV</strong><br>包括 MEMS 这种 Solid State LiDARs，一般都有较小的视场角，不像旋转式机械雷达可达 360°；</li><li><strong>Irregular Scanning Pattern</strong><br>如图 2. 所示，雷达扫描出的 Pattern 是无规则的，这就导致有效特征提取的难度提升；</li><li><strong>Non-repetitive Scanning</strong><br>非重复性扫描，有利有弊；</li><li><strong>Motion Blur</strong><br>包括自身运动及目标运动所产生的点云畸变。自身运动所导致的点云畸变可以通过估计自身运动后，对点云进行运动补偿来矫正；而由于帧内周期均会扫描到目标，所以目标运动所产生的点云畸变影响较大，且基本无法消除。</li></ol><h3 id="workflow">3.1. Workflow</h3><p><img src="/LOAM/livox_loam.png" width="90%" height="90%" title="图 6. Livox Loam"> 　　Livox LOAM 可以认为是 LOAM 的简化版，直接从每帧的点云中提取出 Edge Points 和 Planar Points，经过线性插值的运动补偿后，在 Map 中找到对应的 Edge Line 与 Planar Patch，由此建立优化函数。相比于 LOAM，本文干掉了高频低精度的 Lidar Odometry(因为 Livox 没有前后 Scan 概念，很难做 Scan-to-Sweep 的点云注册)，直接出 20Hz 高频高精度的 Odometry 与 Map(计算平台强+软件多线程)。<br>　　此外本文针对雷达特性还作了更细致的工程改进，包括：</p><ol type="1"><li>更严格的特征点选取<br>去除视场边缘处的特征点；去除较大或较小反射强度的点；</li><li>改进的特征提取<br>为了增多提取的特征点，将周围反射率变化较大的点也列入 Edge Points；</li><li>Outlier Rejection<br>在优化迭代时，先迭代两步，然后去除掉有较大误差的点，最后作进一步迭代；</li><li>Dynamic Objects Filtering<br>扣除掉动态障碍物的点云，这需要动态障碍物检测模块的支持；</li></ol><h2 id="loam-for-vlp-164">4. LOAM for VLP-16<a href="#4" id="4ref"><sup>[4]</sup></a></h2><h3 id="motion-blur">4.1. Motion Blur</h3><p>　　运动导致的点云畸变主要有两种：自身运动与目标运动。对于旋转式线束雷达来说，目标运动所导致的畸变基本可考虑不计(只有目标正好处于初始扫描与结束扫描的交界处时会有影响；Mapping 时则已扣掉动态障碍物，所以不影响)，这里主要讨论自身运动所导致的点云畸变影响。<br>　　每帧激光雷达数据(即一次 Sweep)都会标记到同一时间戳，假设标记到初始扫描的时刻。假设激光雷达旋转一周的扫描周期为 \(T\)，考虑一次 Sweep：\(t\in [0,T]\)。假设在扫描周期内自身为匀速运动，速度为 \(v\)，那么场景中点云的最大偏移畸变为 \(vT\)。考虑两次 Sweep: \(t _ 1,t _ 2\)，对应的速度为 \(v _ 1, v _ 2\)，那么两个时刻对同一物体的点云偏差量为 \(v _ 1T,v _ 2T\)。在世界坐标系下，该物体观测的点云最坏的不一致量可达到 \(|v _ 1T+v _ 2T|\)(自身运动有旋转的时候)，当然大多数情况可能是 \(|v _ 1T-v _ 2T|\)。</p><ol type="a"><li><strong>单帧情况</strong><br>当 \(T=0.1s,v=20m/s\) 时，畸变量为 2m，对于目标检测算法，虽然目标整体漂移了约 2m，不影响检测(尺寸未变)，但是直接导致观测的目标位置漂了约 2m！如果目标正好处于初始扫描和结束扫描的位置，那么目标的尺寸也会失真。</li><li><strong>多帧情况</strong><br>这种情况指 Mapping 的过程。如果 \(t _ 1, t _ 2\) 时间跨度大，那么世界坐标系下同一物体的不一致性会相当高。如果是相邻 \(n\) 帧，假设自身加速度为 \(a = 5m/s^2\)，那么不一致量为 \(|v _ 1T-v _ 2T|=nTaT=0.05n\)，相邻帧可达 5cm ！</li></ol><p>由此可见，不管是单帧任务还是多帧任务，点云的运动补偿不可不做。</p><h3 id="other">4.2. Other</h3><p>　　<a href="#4" id="4ref">[4]</a> 根据代码详细描述了 LOAM 应用到旋转式多线激光雷达的诸多细节，代码中采用了 IMU 里程计作为高频低精度的位姿估计。其它内容在以上章节中都有描述，这里就不再展开了。</p><h2 id="reference">5. Reference</h2><p><a id="1" href="#1ref">[1]</a> Zhang, Ji, and Sanjiv Singh. &quot;LOAM: Lidar Odometry and Mapping in Real-time.&quot; Robotics: Science and Systems. Vol. 2. No. 9. 2014.<br><a id="2" href="#2ref">[2]</a> Zhang, Ji, and Sanjiv Singh. &quot;Low-drift and real-time lidar odometry and mapping.&quot; Autonomous Robots 41.2 (2017): 401-416.<br><a id="3" href="#3ref">[3]</a> Lin, Jiarong, and Fu Zhang. &quot;Loam_livox: A fast, robust, high-precision LiDAR odometry and mapping package for LiDARs of small FoV.&quot; arXiv preprint arXiv:1909.06700 (2019).<br><a id="4" href="#4ref">[4]</a> https://zhuanlan.zhihu.com/p/57351961</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　SLAM 是机器人领域非常重要的一个功能模块，而基于激光雷达的 SLAM 算法，LOAM(Lidar Odometry and Mapping)，则应用也相当广泛。本文从经典的 LOAM 出发，详细描述下激光 SLAM&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;su
      
    
    </summary>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/categories/SLAM/"/>
    
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/tags/SLAM/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
  </entry>
  
  <entry>
    <title>Filter Pruning</title>
    <link href="https://leijiezhang001.github.io/Filter-Pruning/"/>
    <id>https://leijiezhang001.github.io/Filter-Pruning/</id>
    <published>2020-02-03T06:56:14.000Z</published>
    <updated>2020-02-04T12:08:14.148Z</updated>
    
    <content type="html"><![CDATA[<p>　　文章 <a href="/pruning/" title="pruning">pruning</a> 中详细阐述了模型压缩中 Pruning 的基本方法与理论。Pruning 可分为 Structured Pruning 与 Unstructured Pruning 两种，由于 Structured Pruning 不需要特定的芯片支持，可直接在现有 CPU/GPU 架构下进行加速，所以值得作研究及应用。而 Structured Pruning 主要指 Filter Pruning，以及伴随的 Channel Pruning。本文对近期 Filter Pruning 的进展作一个阐述及思考。<br>　　<a href="#1" id="1ref">[1]</a> 得出结论：<strong>Pruning 的本质并不应该是选择重要的 filter/channel，而应该是确定 filter/channel 的数量，在此基础上，从零开始训练也能达到原来的性能</strong>。所以 Pruning 其实只是 AutoML/NAS 领域的一个子任务，即用 AutoML/NAS 是能解决 Pruning 问题的，但是 AutoML/NAS 方法又相对复杂且耗时，所以短期内可能传统的预定义剪枝方法更容易得到应用。本文从预定义剪枝方法和自动学习剪枝方法两大块来作归纳思考。</p><h2 id="问题描述">1. 问题描述</h2><p>　　假设预训练好的网络 \(F\)，其有 \(L\) 层卷积，所有卷积层的 Filter 表示为： <span class="math display">\[ W=\{W^i\} _ {i=1}^L= \left\{\{W^i_j\} _ {j=1}^{c_i}\in\mathbb{R}^{d_i\times c_i}\right\} _ {i=1}^L \tag{1} \]</span> 其中 \(d_i=c_{i-1}\times h_i\times w_i\)；\(c_i,h_i,w_i\) 分别是第 \(i\) 层卷积的 filter 数量，高，宽；\(W_j^i\) 是第 \(i\) 层卷积第 \(j\) 个 filter。<br>　　目标是搜索被剪枝的网络 \(\mathcal{F}\)，剪枝后的 Filter 表示为： <span class="math display">\[ \mathcal{W}=\{\mathcal{W}^i\} _ {i=1}^L= \left\{\{\mathcal{W}^i_j\} _ {j=1}^{\tilde{c}_i}\in\mathbb{R}^{d_i\times \tilde{c} _ i}\right\} _ {i=1}^L \tag{2} \]</span> 其中 \(\tilde{c} _ i=\lfloor p_i\cdot c_i\rceil\)，\(p_i\) 为 Pruning Rate。<br>　　Filter Pruning 会导致输出的特征 Channel 数减少，对应的下一层的每个 Filter 参数需要相应的裁剪，如 <a href="/pruning/" title="pruning">pruning</a> 中提到的三种结构下的 Pruning，尤其需要注意后两种有交点的结构，剪枝时需要作一定的约束(为了简单，交点对应的 Filter 可以选择不剪枝)。</p><h2 id="预定义剪枝方法">2. 预定义剪枝方法</h2><p>　　预定义剪枝网络方法通常预定义的是 \(P=\{p_i\} _ {i=1}^L\)，其剪枝步骤为：</p><ol type="1"><li>Training<br>根据任务训练网络；</li><li>Pruning<br>设计 Filter 重要性度量准则，然后根据预定义的剪枝率，进行 Filter 剪枝；</li><li>Fine-tuning<br>对剪枝好的网络，进行再训练；</li></ol><h3 id="soft-filter-pruning212">2.1. Soft Filter Pruning<a href="#2" id="2ref"><sup>[2]</sup></a><a href="#12" id="12ref"><sup>[12]</sup></a></h3><p><img src="/Filter-Pruning/soft_filter_pruning.png" width="50%" height="50%" title="图 1. Soft Filter Pruning"> 　　如图 1. 所示，其核心思想就是剪枝后的 Filter 在 Fine-tuning 阶段还是保持更新，由此 Pruning，Fine-tuning 迭代获得较优剪枝结果。Filter 重要性度量准则为： <span class="math display">\[\left\Vert W_j^i\right\Vert _ p = \sqrt[p]{\sum_{cc=0}^{c_{i-1}-1}\sum_{k_1=0}^{h_i-1}\sum_{k_2=0}^{w_i-1}\left\vert W_j^i(cc,k_1,k_2)\right\vert ^p} \tag{3}\]</span></p><h3 id="filter-sketch313">2.2. Filter Sketch<a href="#3" id="3ref"><sup>[3]</sup></a><a href="#13" id="13ref"><sup>[13]</sup></a></h3><p>　　选择 Filter 进行剪枝，另一种思路是，如何选择一部分 Filter，使得该 Filter 集合的信息量与原 Filter 集合信息量近似: <span class="math display">\[\Sigma_{W^i}\approx \Sigma_{\mathcal{W}^i} \tag{4}\]</span> 这里的信息量表达方式采用了协方差矩阵: <span class="math display">\[\begin{align}\Sigma_{W^i} &amp;= \left(W^i-\bar{W}^i \right)\left(W^i-\bar{W}^i \right)^T \\\Sigma_{\mathcal{W}^i} &amp;= \left(\mathcal{W}^i-\mathcal{\bar{W}}^i \right)\left(\mathcal{W}^i-\mathcal{\bar{W}}^i \right)^T \\\end{align} \tag{5}\]</span> 其中 Filter 权重符合高斯分布，即 \(\bar{W}^i=\frac{1}{c_i}\sum _ {j=1}^{c _ i}W _ j ^ i\approx 0\)，\(\mathcal{\bar{W}} ^ i=\frac{1}{\tilde{c} _ i}\sum _ {j=1}^{\tilde{c} _ i}\mathcal{W} _ j^i\approx 0\)。由式(4)(5)，构建最小化目标函数： <span class="math display">\[\mathop{\arg\min}\limits_{\mathcal{W}^i}\left\Vert W^i(W^i)^T-\mathcal{W}^i(\mathcal{W}^i)^T \right\Vert \tag{6}\]</span> 将该问题转换为求取 \(W^i\) 矩阵的 Sketch 问题，则： <span class="math display">\[\left\Vert W^i(W^i)^T-\mathcal{W}^i(\mathcal{W}^i)^T \right\Vert _F \leq \epsilon\left\Vert W^i\right\Vert^2_F \tag{7}\]</span> <img src="/Filter-Pruning/sketch.png" width="50%" height="50%" title="图 2. Frequent Direction"> <img src="/Filter-Pruning/filter_sketch.png" width="50%" height="50%" title="图 3. FilterSketch"> 　　式(7)可用图 2. 所示的算法求解，最终的 Pruning 算法过程如图 3. 所示，改进的地方主要是 Filter 选择的部分，采用了 Matrix Sketch 算法。 <img src="/Filter-Pruning/pruning.png" width="60%" height="60%" title="图 4. 网络裁剪示意图"> 　　<a href="/pruning/" title="pruning">pruning</a> 中提到有分支结构的裁剪会比较麻烦，所以如图 4. 所示，本方法对分支节点的 Filter 不做裁剪处理，简化了问题。</p><h3 id="filter-pruning-via-geometric-median414">2.3. Filter Pruning via Geometric Median<a href="#4" id="4ref"><sup>[4]</sup></a><a href="#14" id="14ref"><sup>[14]</sup></a></h3><p>　　在预定义剪枝网络方法的三个步骤中，大家普遍研究步骤二中 Filter 的重要性度量设计。Filter 重要性度量基本是 Smaller-norm-less-informative 思想，<a href="#5" id="5ref">[5]</a> 中则验证了该思想并不一定正确。<strong>Smaller-norm-less-informative 假设成立的条件是</strong>：</p><ol type="1"><li>Filter 权重的规范偏差(norm deviation)要大；</li><li>Filter 权重的最小规范要小；</li></ol><p>只有满足这两个条件，该假设才成立，即可以裁剪掉规范数较小的 Filter。 <img src="/Filter-Pruning/norm_dist.png" width="60%" height="60%" title="图 5. Filter Norm Distribution"> 　　但是，如图 5. 所示，实际 Filter 的权重分布和理想的并不一致，当 Filter 分布是绿色区域时，采用 Smaller-norm-less-informative 就不合理了，而这种情况还比较多。一般性的，前几层网络的权重规范数偏差会比较大，后几层则比较小。<br><img src="/Filter-Pruning/criterion.png" width="50%" height="50%" title="图 6. Criterion for Filter Pruning"> 　　由此，本方法提出一种基于 Geometric Median 的 Filter 选择方法，如图 6. 所示，基于 Smaller-norm-less-informative 的裁剪后留下的均是规范数较大的 Filter，这还存在一定的冗余性，本方法则通过物理距离测算，剪掉冗余的 Filter。<strong>另一个角度可理解为最大程度的保留 Filter 集合的大概及具体信息，其思想与 FilterSketch 类似</strong>。<br>　　根据 Geometric Median 思想，第 \(i\) 层卷积要裁剪掉的 Filter 为： <span class="math display">\[W^i_{j^\ast}=\mathop{\arg\min}\limits_{W^i_{j^\ast}\,|\,j^\ast\in[0,c_i-1]}\sum_{j&#39;=0}^{c_i-1}\left\Vert W^i_{j^\ast}-W^i_{j&#39;}\right\Vert_2 \tag{8}\]</span> 由此裁剪掉满足条件的 \(W _ {j^*}^i\)，直至符合裁剪比率。<strong>本方法的思想非常类似于 Farthest Point Sampling 采样，留下的 Filter 即为原 Filter 集合采样的结果，且最大程度的保留了集合的信息</strong>。</p><h2 id="自动学习剪枝方法">3. 自动学习剪枝方法</h2><h3 id="abcpruner616">3.1. ABCPruner<a href="#6" id="6ref"><sup>[6]</sup></a><a href="#16" id="16ref"><sup>[16]</sup></a></h3><p><img src="/Filter-Pruning/ABCPruner.png" width="60%" height="60%" title="图 7. ABCPruner"> 　　出于<a href="#1" id="1ref">[1]</a>的结论：<strong>剪枝的本质应该是直接找到每层卷积最优的 Filter 数量，在此基础上从零开始训练也能达到原来的性能</strong>。ABCPruner 的目标就是搜索每层最优的 Filter 数量，如图 7. 所示，ABCPruner 步骤为：</p><ol type="1"><li>初始化一系列不同 Filter 数量的网络结构；</li><li>每个网络结构从 pre-trained 网络中继承权重值，fine-tune 获得每个网络的 fitness(即 accuracy)；</li><li>用 ABC 算法更新网络结构；</li><li>重复迭代 2,3 步骤，获取最高的 fitness 网络作为最终网络结构；</li></ol><h3 id="metapruning717">3.2. MetaPruning<a href="#7" id="7ref"><sup>[7]</sup></a><a href="#17" id="17ref"><sup>[17]</sup></a></h3><p><img src="/Filter-Pruning/metapruning.png" width="50%" height="50%" title="图 8. MetaPruning"> 　　同样，本方法也是基于<a href="#1" id="1ref">[1]</a>的结论。这里设计 PruningNet 来控制裁剪，步骤为：</p><ol type="1"><li>Training PruningNet<br>PruningNet 输入为网络编码向量，即每层卷积的 Filter 数量，输出为产生网络权重的编码量，如 size reshape，crop。每次训练时随机生成网络编码量，网络编码量与 PruningNet 输出共同决定了 PrunedNet 权重，两个网络联合训练；</li><li>Searching for the Best Pruned Net<br>即 Inference 过程，寻找最优的网络编码量，使得 PrunedNet 精度最高；得到最优网络后，不需要 fine-tuning。</li></ol><h3 id="generative-adversarial-learning8">3.3. Generative Adversarial Learning<a href="#8" id="8ref"><sup>[8]</sup></a></h3><p><img src="/Filter-Pruning/GAL.png" width="90%" height="90%" title="图 9. Generative Adversarial Learning"> 　　本方法主要思想来自知识蒸馏(Knowledge Distillation)和生成对抗网络(Generative Adversarial Network)，如图 9. 所示，Baseline 为完整的原始网络，PrunedNet 是为了学习一个 soft mask 来动态选择 block，branch，channel，最终裁剪后的网络由 soft mask 决定。<br>　　从知识蒸馏的角度：Baseline 就是一个大容量的教师网络，Pruned Net 就是个小容量的学生网络，用大容量网络来监督小容量网络学习。从生成对抗学习的角度：Baseline 是原始网络，PrunedNet 是生成的对抗网络，用一个 Discriminator 网络来区分原始网络与生成的对抗网络的区别，使生成的对抗网络输出逼近于原始网络。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Liu, Zhuang, et al. &quot;Rethinking the Value of Network Pruning.&quot; International Conference on Learning Representations. 2018.<br><a id="2" href="#2ref">[2]</a> He, Yang, et al. &quot;Soft filter pruning for accelerating deep convolutional neural networks.&quot; arXiv preprint arXiv:1808.06866 (2018).<br><a id="3" href="#3ref">[3]</a> Lin, Mingbao, et al. &quot;Filter Sketch for Network Pruning.&quot; arXiv preprint arXiv:2001.08514 (2020).<br><a id="4" href="#4ref">[4]</a> He, Yang, et al. &quot;Filter pruning via geometric median for deep convolutional neural networks acceleration.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br><a id="5" href="#5ref">[5]</a> Ye, Jianbo, et al. &quot;Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers.&quot; arXiv preprint arXiv:1802.00124 (2018).<br><a id="6" href="#6ref">[6]</a> Lin, Mingbao, et al. &quot;Channel Pruning via Automatic Structure Search.&quot; arXiv preprint arXiv:2001.08565 (2020).<br><a id="7" href="#7ref">[7]</a> Liu, Zechun, et al. &quot;Metapruning: Meta learning for automatic neural network channel pruning.&quot; Proceedings of the IEEE International Conference on Computer Vision. 2019.<br><a id="8" href="#8ref">[8]</a> Lin, Shaohui, et al. &quot;Towards optimal structured cnn pruning via generative adversarial learning.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br><a id="9" href="#9ref">[9]</a> Singh, Pravendra, et al. &quot;Play and prune: Adaptive filter pruning for deep model compression.&quot; arXiv preprint arXiv:1905.04446 (2019).<br><a id="11" href="#11ref">[11]</a> https://github.com/Eric-mingjie/rethinking-network-pruning<br><a id="12" href="#12ref">[12]</a> https://github.com/he-y/softfilter-pruning<br><a id="13" href="#13ref">[13]</a> https://github.com/lmbxmu/FilterSketch<br><a id="14" href="#14ref">[14]</a> https://github.com/he-y/filter-pruning-geometric-median<br><a id="16" href="#16ref">[16]</a> https://github.com/lmbxmu/ABCPruner<br><a id="17" href="#17ref">[17]</a> https://github.com/liuzechun/MetaPruning</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　文章 &lt;a href=&quot;/pruning/&quot; title=&quot;pruning&quot;&gt;pruning&lt;/a&gt; 中详细阐述了模型压缩中 Pruning 的基本方法与理论。Pruning 可分为 Structured Pruning 与 Unstructured Pruning 两
      
    
    </summary>
    
      <category term="Model Compression" scheme="https://leijiezhang001.github.io/categories/Model-Compression/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Model Compression" scheme="https://leijiezhang001.github.io/tags/Model-Compression/"/>
    
  </entry>
  
  <entry>
    <title>Ground Segmentation with Gaussian Process</title>
    <link href="https://leijiezhang001.github.io/Ground-Segmentation-with-Gaussian-Process/"/>
    <id>https://leijiezhang001.github.io/Ground-Segmentation-with-Gaussian-Process/</id>
    <published>2020-01-21T09:00:34.000Z</published>
    <updated>2020-02-01T08:53:34.072Z</updated>
    
    <content type="html"><![CDATA[<p>　　地面分割可作为自动驾驶系统的一个重要模块，本文介绍一种基于高斯过程的地面分割方法。</p><h2 id="算法概要">1. 算法概要</h2><p><img src="/Ground-Segmentation-with-Gaussian-Process/ground_seg.png" width="80%" height="80%" title="图 1. ground segmentation"> 　　为了加速，本方法<a href="#1" id="1ref"><sup>[1]</sup></a>将三维地面分割问题分解为多个一维高斯过程来求解，如图 1. 所示，其步骤为：</p><ol type="1"><li><strong>Polar Grid Map</strong><br>将点云用极坐标栅格地图表示，二维地面估计分解成射线方向的多个一维地面估计；</li><li><strong>Line Fitting</strong><br>在每个一维方向，根据梯度大小，作可变数量的线段拟合；</li><li><strong>Seed Estimation</strong><br>在半径 \(B\) 范围内，如果某个 Grid 绝对高度(Grid 高度定义为该 Grid 内所有点的最小高度，其绝对高度则是与本车传感器所在地面的比较)大于 \(T_s\)，那么就将其作为 Seed；</li><li><strong>Ground Model Estimation with Gaussian Process</strong><br>采用高斯过程生成每个一维方向 Grid 的地面估计量，这里为了进一步加速，可以删除冗余的 Seed；根据地面估计模型，将满足模型的 Grid 加入 Seed，更新模型，迭代直至收敛，满足模型的 Seed 条件为： <span class="math display">\[\begin{align}V[z]&amp;\leq  t_{model}\\\frac{|z_*-\bar{z}|}{\sqrt{\sigma^2_n+V[z]}} &amp;\leq t_{data}\end{align} \tag{0}\]</span></li><li><strong>Point-wise Segmentation</strong><br>得到地面估计模型后，就得到了每个 Grid 是否为地面的标签量，对于属于地面标签量的 Grid 内的点，与 Grid 高度的相对高度小于 \(T_r\)，则认为该点属于地面。</li></ol><h2 id="高斯过程">2. 高斯过程</h2><p>　　步骤四中用高斯过程来估计地面模型，对于每个极射线方向的 Grids，假设有 \(n\) 个已经确定是地面的训练集：\(D=\{(r _ i,z _ i)\} _ {i=1}^n\)。根据高斯过程定义，这些样本的联合概率分布为： <span class="math display">\[p(Z|R)\sim N(f(R)+\mu,K) \tag{1}\]</span> 其中 \(R=[r_1,...,r_n]^T\) 为每个 Grid 的距离量，\(Z=[z_1,...,z_n]^T\) 为该 Grid 地面高度，\(f(\cdot)\)为高斯过程要回归的函数。\(\mu\) 设计为零，协方差矩阵 \(K\) 表示变量之间的关系，由协方差方程与噪音项构成： <span class="math display">\[K(r_i,r_j)=k(r_i,r_j)+\sigma^2_n\delta_{ij}\tag{2}\]</span> 其中当且仅当 \(i==j\) 时 \(\delta _ {ij} =1\)。<br>　　一般的协方差方程是静态，同向的(stationary, isotropic): <span class="math display">\[k(r_i,r_j)=\sigma_f^2\mathrm{exp}\left(-\frac{(r_i-r_j)^2}{2l^2}\right) \tag{3}\]</span> 其中 \(\sigma_f^2\) 是信号协方差，\(l\) 是 length-scale。该方程假设了全空间内 length-scale 的一致性，然而实际上，<strong>越平坦的地面区域，我们需要越大的 length-scale，因为此时该区域对周围区域的概率输出能更大</strong>，所以可进一步设计协方差方程为: <span class="math display">\[k(r_i,r_j)=\sigma_f^2\left(l_i^2\right)^{\frac{1}{4}}\left(l_j^2\right)^{\frac{1}{4}}\left(\frac{l_i^2+l_j^2}{2}\right)^{-\frac{1}{2}}  \mathrm{exp}\left(-\frac{2(r_i-r_j)^2}{l_i^2+l_j^2}\right) \tag{4}\]</span> 其中 \(l_i\) 为位置 \(r_i\) 的 length-scale。\(l_i\) 由该位置距离最近的线段梯度决定(步骤二): <span class="math display">\[l_i=\left\{\begin{array}{l}a\cdot \mathrm{log}\left(\frac{1}{|g(r_i)|}\right) \,\, if\, |g(r_i)|&gt;g_{def}\\a\cdot \mathrm{log}\left(\frac{1}{|g_{def}|}\right) \,\, otherwise\end{array}\tag{5}\right.\]</span> 　　高斯回归预测的过程为，对于测试集 \(T=(r_\ast,z_\ast)\)，其与训练集的联合概率分布为： <span class="math display">\[\begin{bmatrix}Z\\z_\ast\\\end{bmatrix}\simN\left(0,\begin{bmatrix}K(R,R) &amp; K(R,r_\ast)\\K(r_\ast,R) &amp; K(r_\ast,r_\ast)\\\end{bmatrix}\right)\tag{6}\]</span> 那么，高斯过程回归预测为： <span class="math display">\[\begin{align}\bar{z}_\ast &amp;=K(r_\ast,R)K^{-1}Z\\V[z_\ast] &amp;= K(r_\ast,r_\ast)-K(r_\ast,R)K^{-1}K(R,r_\ast)\end{align} \tag{7}\]</span> 由此得到测试集的预测量，由式(0)可决定该测试量是否标记为地面，进一步迭代估计地面模型，直至收敛。<br>　　需要注意的是，以上我们假设高斯过程的超参数 \(\theta=\{\sigma_f,a,\sigma_n\}\) 是已知的，实际应用中，可以将超参数设定为经验量，也可以基于训练集用 SGD 学习出一个最优量，这里不做展开。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Chen, Tongtong, et al. &quot;Gaussian-process-based real-time ground segmentation for autonomous land vehicles.&quot; Journal of Intelligent &amp; Robotic Systems 76.3-4 (2014): 563-582.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　地面分割可作为自动驾驶系统的一个重要模块，本文介绍一种基于高斯过程的地面分割方法。&lt;/p&gt;
&lt;h2 id=&quot;算法概要&quot;&gt;1. 算法概要&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/Ground-Segmentation-with-Gaussian-Process/ground
      
    
    </summary>
    
      <category term="Semantic Segmentation" scheme="https://leijiezhang001.github.io/categories/Semantic-Segmentation/"/>
    
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/tags/Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>Grid Mapping</title>
    <link href="https://leijiezhang001.github.io/Grid-Mapping/"/>
    <id>https://leijiezhang001.github.io/Grid-Mapping/</id>
    <published>2020-01-20T02:19:15.000Z</published>
    <updated>2020-02-01T09:58:01.032Z</updated>
    
    <content type="html"><![CDATA[<p>　　占据栅格地图(Occupied Grid Map)是机器人领域一种地图表示方式。可以作为 SLAM 的一个模块，但是这里讨论：<strong>在本体位姿已知的情况下，如何构建 2D Grid Map</strong>。本文介绍两种方法，贝叶斯概率模型以及高斯过程。</p><h2 id="贝叶斯概率模型1">1. 贝叶斯概率模型<a href="#1" id="1ref"><sup>[1]</sup></a></h2><p>　　设机器人位姿序列为 \(x_{1:t}\)，观测序列为 \(z_{1:t}\)，那么 Grid Map 的构建就是求解地图的后验概率：\(p(m|x_{1:t},z_{1:t})\)，其中地图由栅格构成：\(m=\{m_1,m_2,...,m_n\}\)。<strong>假设每个栅格独立同分布</strong>，那么： <span class="math display">\[p(m|x_{1:t},z_{1:t})=p(m_1,m_2,...,m_n|x_{1:n},z_{1:t}) = \prod_{i=1}^n p(m_i|x_{1:t},z_{1:t}) \tag{1}\]</span> 　　每个栅格有三种状态：被占有，空，未被观测。设被占有的概率为 \(occ(m_i) = p(m_i|x_{1:t},z_{1:t})\)，那么空的概率为 \(free(m_i)=1-occ(m_i)\)，对于未被观测的区域认为 \(occ(m_i) = free(m_i) =0.5\)。下面通过贝叶斯法则及马尔科夫性推理后验概率计算过程： <span class="math display">\[\begin{align}occ_t(m_i) &amp;= p(m_i|x_{1:t},z_{1:t}) \\&amp;= \frac{p(z_t|m_i,x_{1:t},z_{1:t-1})\,p(m_i|x_{1:t},z_{1:t-1})}{p(z_t|x_{1:t},z_{1:t-1})} \\&amp;= \frac{p(z_t|m_i,x_{t})\,p(m_i|x_{1:t-1},z_{1:t-1})}{p(z_t|x_{1:t},z_{1:t-1})} \\&amp;= \frac{p(m_i|z_t,x_{t})\,p(z_t|x_t)\,p(m_i|x_{1:t-1},z_{1:t-1})}{p(m_i|x_t)\,p(z_t|x_{1:t},z_{1:t-1})} \\&amp;= \frac{p(m_i|z_t,x_{t})\,p(z_t|x_t)\,occ_{t-1}(m_{i})}{p(m_i)\,p(z_t|x_{1:t},z_{1:t-1})} \tag{2}\end{align}\]</span> 对应的栅格为空的概率为： <span class="math display">\[\begin{align}free_t(\hat{m}_i) &amp;=\frac{p(\hat{m}_i|z_t,x_{t})\,p(z_t|x_t)\,free_{t-1}(\hat{m}_{i})}{p(\hat{m}_i)\,p(z_t|x_{1:t},z_{1:t-1})} \\&amp;= \frac{(1-p(m_i|z_t,x_{t}))\,p(z_t|x_t)\,(1-occ_{t-1}(m_{i}))}{(1-p(m_i))\,p(z_t|x_{1:t},z_{1:t-1})} \tag{3}\end{align}\]</span> 由(2),(3)可得： <span class="math display">\[\frac{occ_t(m_i)}{1-occ_t(m_i)} = \frac{1-p(m_i)}{p(m_i)}\cdot\frac{occ_{t-1}(m_i)}{1-occ_{t-1}(m_i)}\cdot\frac{p(m_i|z_t,x_t)}{1-p(m_i|z_t,x_t)}   \tag{4}\]</span> 将上式进行对数化： <span class="math display">\[lm_i^{t} = lm_i^{t-1} + \mathrm{log}\left(\frac{p(m_i|z_t,x_t)}{1-p(m_i|z_t,x_t)}\right) - \mathrm{log}\left(\frac{p(m_i)}{1-p(m_i)}\right) \tag{5}\]</span> 其中 \(p(m_i)\) 表示未观测下其被占有的概率，\(p(m_i|z_t,x_t)\) 表示当前观测下其被占有的概率。比如，考虑到激光点云的测量噪声，我们可以假设如果该栅格有点云，那么 \(p(m_i|z_t,x_t) = 0.9\)；对于激光点光路经过的栅格区域 \(p(m_i|z_t,x_t) = 0.02\)，即 \(p(\hat{m}_i|z_t,x_t) = 0.98\)。<br>　　该模型下，每个栅格被占有的概率可以转换为前后相加测量量的过程，实际每个栅格被占有的概率为： <span class="math display">\[occ_t(m_i) = \frac{\mathrm{exp}(lm_i^t)}{1+\mathrm{exp}(lm_i^t)} \tag{6}\]</span></p><h2 id="高斯过程2">2. 高斯过程<a href="#2" id="2ref"><sup>[2]</sup></a></h2><p>　　以上概率模型有个缺陷，其假设栅格独立。实际上栅格并不是独立的，相邻的栅格有很强的相关性。高斯过程则可以处理时域及空域的概率估计与融合问题。<br>　　高斯过程基本理论在 <a href="/Ground-Segmentation-with-Gaussian-Process/" title="Ground Segmentation with Gaussian Process">Ground Segmentation with Gaussian Process</a> 中已经有较详细阐述，这里作简要概述。假设有训练集 \(\{X_n,y_n\}_{n=1}^N\)，那么高斯过程下其符合分布： <span class="math display">\[y_n=f(X_n)+\epsilon, \epsilon\sim \mathcal{N}(0,\sigma^2) \tag{7}\]</span> 对于测试集，则有： <span class="math display">\[f(X^\ast) = \mathcal{N}(\mu,\sigma) \tag{8}\]</span> 高斯过程对测试集的预测结果为： <span class="math display">\[\begin{align}\mu^\ast &amp;=K(X^\ast,X)(K(X,X)+\sigma_n^2I)^{-1}y\\\sigma^\ast &amp;=K(X^\ast,X^\ast) - K(X^\ast,X)(K(X,X)+\sigma_n^2I)^{-1}K(X,X^\ast)\end{align} \tag{9}\]</span> <img src="/Grid-Mapping/GPOM.png" width="60%" height="60%" title="图 1. GPOM"> 　　高斯过程占据栅格地图(Gaussian Process Occupancy Maps, GPOM)算法过程如图 1. 所示。\(\mathrm{p,r}\) 分别为机器人位姿以及观测量。基本思想就是根据当前时刻的观测数据，提取出正负样本训练集，然后构建高斯模型，对于未观测到的区域，用高斯模型进行预测；每个栅格的信息通过 BCM<a href="#3" id="3ref"><sup>[3]</sup></a> 进行时序的融合，最终采用 logistic 回归得到每个栅格被占据的概率(贝叶斯概率模型中，代替 BCM 及 logistic 的是 log 函数累加融合并求取概率，这里应该也可以用这种方式实现)。<br>　　可见，高斯过程来求解占据栅格地图，<strong>能融合时序及空间信息</strong>，但是效率会比较低，不过除了高斯过程中的矩阵求逆操作，其它操作基本可以并行化处理。代码可参考<a href="#4" id="4ref">[4]</a>。</p><h2 id="reference">3. reference</h2><p><a id="1" href="#1ref">[1]</a> Thrun, Sebastian. &quot;Probabilistic robotics.&quot; Communications of the ACM 45.3 (2002): 52-57.<br><a id="2" href="#2ref">[2]</a> Yuan, Yijun, Haofei Kuang, and Sören Schwertfeger. &quot;Fast Gaussian Process Occupancy Maps.&quot; 2018 15th International Conference on Control, Automation, Robotics and Vision (ICARCV). IEEE, 2018.<br><a id="3" href="#3ref">[3]</a> Tresp, Volker. &quot;A Bayesian committee machine.&quot; Neural computation 12.11 (2000): 2719-2741.<br><a id="4" href="#4ref">[4]</a> https://github.com/STAR-Center/fastGPOM</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　占据栅格地图(Occupied Grid Map)是机器人领域一种地图表示方式。可以作为 SLAM 的一个模块，但是这里讨论：&lt;strong&gt;在本体位姿已知的情况下，如何构建 2D Grid Map&lt;/strong&gt;。本文介绍两种方法，贝叶斯概率模型以及高斯过程。&lt;/p
      
    
    </summary>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/categories/SLAM/"/>
    
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/tags/SLAM/"/>
    
      <category term="Mapping" scheme="https://leijiezhang001.github.io/tags/Mapping/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Grid-GCN for Fast and Scalable Point Cloud Learning&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-Grid-GCN-for-Fast-and-Scalable-Point-Cloud-Learning/"/>
    <id>https://leijiezhang001.github.io/paper-reading-Grid-GCN-for-Fast-and-Scalable-Point-Cloud-Learning/</id>
    <published>2020-01-10T01:26:53.000Z</published>
    <updated>2020-01-11T10:55:40.032Z</updated>
    
    <content type="html"><![CDATA[<p>　　目前点云特征学习在学术界还处于各种探索阶段，<a href="/PointCloud-Feature-Extraction/" title="PointCloud-Feature-Extraction">PointCloud-Feature-Extraction</a> 中将点云特征提取分为三维物理空间操作以及映射空间操作两大类，其中对直接在三维空间中提取特征的操作进行了较详细的分析。由于变换到映射空间的操作会相对比较复杂，目前为了实时应用，本人还是比较倾向于直接在三维空间进行操作。<br>　　类比图像特征提取，直接在三维空间进行点云特征提取的基本操作有：</p><ul><li><strong>局部点云特征提取</strong>：对目标点的周围点特征进行融合，从而得到该目标点特征；</li><li><strong>上采样/下采样</strong>：采样以扩大感受野，进一步提取局部/全局信息；</li></ul><p>　　<a href="/PointCloud-Feature-Extraction/" title="PointCloud-Feature-Extraction">PointCloud-Feature-Extraction</a> 主要描述了已知周围点位置后，局部点云特征的提取方式，考虑的是特征提取的有效性，重写该问题为：针对待提取特征的坐标点 \(\mathcal{x} _ c\)，融合其周围 \(K\) 个点的操作： <span class="math display">\[ \tilde{f_c} = \mathcal{A}\left(\{e(\mathcal{x_i,x_c},f_c, f_i)\ast \mathcal{M}(f_i)\}, i\in1,...,K \right) \tag{1}\]</span> 其中 \(f_i\) 为点 \(\mathcal{x_i}\) 的特征，\(\mathcal{M}\) 为多层感知机；\(e,\mathcal{A}\) 分别为周围点特征权重函数以及特征聚合函数，大致对应 <a href="/PointCloud-Feature-Extraction/" title="PointCloud-Feature-Extraction">PointCloud-Feature-Extraction</a> 中的 \(h_\theta\) 以及 \(\Box\)。本文则思考这两个基本操作如何计算加速以能实时应用。具体来看，耗时操作主要是：</p><ul><li>Sampling</li><li>Points Querying</li></ul><p>　　<a href="#1" id="1ref">[1]</a> 提出了一种基于 Voxel 的快速采样方法，并依赖 Voxel 做近似而快速的 Points Querying，以下作详细分析。</p><h2 id="overview">1. Overview</h2><p><img src="/paper-reading-Grid-GCN-for-Fast-and-Scalable-Point-Cloud-Learning/grid-gcn.png" width="50%" height="50%" title="图 1. Grid-GCN Model"> 　　如图 1. 所示，Grid-GCN 模型目标是提取点级别的特征，从而可以作 semantic segmentation 等任务。基本模块为 GridConv，该模块又包括数据的构建-Coverage-aware Grid Query(CAGQ)，以及图卷积-Grid Context Aggregation(GCA)。 <img src="/paper-reading-Grid-GCN-for-Fast-and-Scalable-Point-Cloud-Learning/feature.png" width="80%" height="80%" title="图 2. Grid Context Aggregation"> 　　GCA 操作如图 2. 所示，与 <a href="/PointCloud-Feature-Extraction/" title="PointCloud-Feature-Extraction">PointCloud-Feature-Extraction</a> 中介绍的方法都大同小异，当信息量累加到一定程度后，基本只有一两个点的 mAP 差异，这里不作展开。<br>　　CAGQ 则包含 sampling 与 points querying 两个核心且又最耗时的操作，CAGQ 能极大提升这两个操作的速度。首先定义三维 voxel 大小 \((v_x,v_y,v_z)\)，那么对于点 \(x,y,z\)，其 voxel 索引为 \(Vid(u,v,w)=floor\left(\frac{x}{v_x},\frac{y}{v_y},\frac{z}{v_z}\right)\)，每个 voxel 限制点数量为 \(n_v\)。假设 \(O_v\) 为非空的 voxel 集合，采样 \(M\) 个 voxel \(O_c\subseteq O_v\)。对于每个 voxel \(v_i\)，定义其周围的 voxel 集合为 \(\pi(v_i)\)，该集合中的点则构成 context points。由此可知要解决的问题：</p><ul><li><strong>Sampling</strong>：采样 voxel 集合 \(O_c\subseteq O_v\)；</li><li><strong>Points Querying</strong>：从 Context Points 中选取 K 个点；</li></ul><h2 id="sampling">2. Sampling</h2><p>　　<a href="/paperreading-FlowNet3D/" title="FlowNet3D">FlowNet3D</a> 中大致阐述过几种采样方法，信息保留度较高的方法是 FPS，但是速度较慢。 <img src="/paper-reading-Grid-GCN-for-Fast-and-Scalable-Point-Cloud-Learning/sample2query.png" width="80%" height="80%" title="图 3. Sampling and Points Querying"> 　　如图 3. 所示，本文提出了两种基于 voxel 的采样方法:</p><ul><li><strong>Random Voxel Sampling(RVS)</strong><br>对每个 voxel 进行随机采样，相比对每个点进行随机采样(Random Point Sampling)，RVS 有更少的信息损失，更广的空间信息覆盖率。</li><li><strong>Coverage-Aware Sampling(CAS)</strong><br>在 RVS 基础上，CAS 有更广的信息覆盖率，其步骤为：<ol type="1"><li>随机采样 \(M\) 个 voxel，即执行 RVS；</li><li>对未被采样到的 voxel \(v_c\)，计算如果加入这个 voxel，空间覆盖率增益： <span class="math display">\[ H_{add} = \sum_{v\in \pi(v_c)}\delta(C_v) - \beta\frac{C_v}{\lambda} \tag{2}\]</span> 对采样集里面的 voxel \(v_i\)，计算如果去掉这个 voxel，空间覆盖率减少量： <span class="math display">\[ H_{rmv} = \sum_{v\in \pi(v_i)}\delta(C_v-1) \tag{3}\]</span></li><li>如果 \(H_{add} &gt; H_{rmv}\)，则进行替换；</li><li>迭代 2,3 步骤；</li></ol></li></ul><p>其中 \(\delta(x)=1,if x=0,else\,0\)。\(\lambda\) 为周围 voxel 个数，\(C_v\) 是采样集覆盖该 voxel 的个数。</p><h2 id="points-querying">3. Points Querying</h2><p>　　传统的 Points Querying 一般是在所有点中建立 KD-Tree 或 Ball Query 形式来找某点的邻近点。本文在 voxel 基础上来快速寻找邻近点，提供了两种方法：</p><ul><li><strong>Cube Query</strong><br>这是一种近似法，直接在 Context Points 中随机采样 \(K\) 个点作为最近邻点。从物理意义上将，最近邻的区域的点特征应该都是相似的，所以这种近似法应该会很有效。</li><li><strong>K-Nearest Neighbors</strong><br>在 Context Points 中寻找 K-NN，相比在全点云中找 K-NN，这种方法搜索速度会非常快。</li></ul><h2 id="experiments">4. Experiments</h2><p><img src="/paper-reading-Grid-GCN-for-Fast-and-Scalable-Point-Cloud-Learning/complexity.png" width="60%" height="60%" title="图 4. 时间复杂度"> <img src="/paper-reading-Grid-GCN-for-Fast-and-Scalable-Point-Cloud-Learning/time-eval.png" width="70%" height="70%" title="图 5. 空间覆盖率与耗时"> 　　如图 4. 与图 5. 所示，比较了 RPS，FPS，RVS，CAS 等采样算法的时间复杂度与空间覆盖率，以及 Ball Query，Cube Query，K-NN 等 Points Query 算法的时间复杂度。由此可见，本文提出的 Sample 及 Points Query 算法非常高效。</p><h2 id="reference">5. reference</h2><p><a id="1" href="#1ref">[1]</a> Xu, Qiangeng. &quot;Grid-GCN for Fast and Scalable Point Cloud Learning.&quot; arXiv preprint arXiv:1912.02984 (2019).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　目前点云特征学习在学术界还处于各种探索阶段，&lt;a href=&quot;/PointCloud-Feature-Extraction/&quot; title=&quot;PointCloud-Feature-Extraction&quot;&gt;PointCloud-Feature-Extraction&lt;/a&gt;
      
    
    </summary>
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>Epistemic Uncertainty for Active Learning</title>
    <link href="https://leijiezhang001.github.io/Epistemic-Uncertainty-for-Active-Learning/"/>
    <id>https://leijiezhang001.github.io/Epistemic-Uncertainty-for-Active-Learning/</id>
    <published>2020-01-04T01:44:00.000Z</published>
    <updated>2020-01-05T09:33:26.709Z</updated>
    
    <content type="html"><![CDATA[<p>　　<a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 中详细讨论了 Aleatoric Uncertainty 的建模以及应用。本文讨论 Epistemic Uncertainty 的建模，以及在 Active Learning 中的应用。Epistemic Uncertainty 描述了模型因为缺少训练数据而存在的不确定性，所以其可应用于 Active Learning。应用场景有：</p><ul><li><strong>减少训练时间</strong>：在大数据集下，训练时挑选当前模型认知困难的样本，减少训练数据从而减少训练时间；</li><li><strong>减少无效标注</strong>：只挑选当前模型认知困难的样本进行标注、迭代模型；</li></ul><p><img src="/Epistemic-Uncertainty-for-Active-Learning/active_learning.png" width="50%" height="50%" title="图 1. active learning 工作流"> 　　<a href="#1" id="1ref">[1]</a>中提到的一种 Active Learning 工作流如图 1. 所示，重要环节有 Estimating Uncertainty 以及 Querying Data。该工作流假设了<strong>一个完美的图像检测器(至少有个完美的召回率)</strong>，图像检测器提供目标 proposal，3D 检测对 proposal 作 uncertainty 估计，从而确定是否标注。 Estimating Uncertainty 指的是 Epistemic Uncertainty 的建模；Querying Data 则设计一种策略，其能通过估计的 Uncertainty 来选择模型认知困难的样本。<br>　　由于 Epistemic Uncertainty 只能通过 Monte-Carlo 等方法近似得到，这些方法都是基于模型预测的目标进行 Uncertainty 估计的，所以对于漏检的目标，其 Uncertainty 是无法有效获取的。换句话说，本文讨论的 Epistemic Uncertainty 只能抓取预测的正样本(TP)置信度不高，以及误检(FP)的 Uncertainty 信息，无法获得TP置信度非常低的样本 Uncertainty，即完全没见过的目标。<strong>所以基于 Epistemic Uncertainty 的 Active Learning，理论上只能使正样本置信度提高，以及消除误检；对于漏检，需要加入一定的随机性，让模型先“见到”这种类型的目标。</strong></p><h2 id="estimating-epistemic-uncertainty">1. Estimating Epistemic Uncertainty</h2><p>　　针对一批训练数据集\(\{\mathbf{X,Y}\}\)，训练模型 \(\mathbf{y=f^W(x)}\)，在贝叶斯框架下，预测量的后验分布为<a href="#3" id="3ref"><sup>[3]</sup></a>： <span class="math display">\[p\left(\mathbf{y\vert x,X,Y}\right) = \int p\left(\mathbf{y\,|\,f^W(x)}\right) p\left(\mathbf{W\,|\,X,Y}\right)d\mathbf{W} \tag{1}\]</span> 其中 \(p(\mathbf{W\,|\,X,Y})\) 为模型参数的后验分布，描述了模型的不确定性，即 Epistemic Uncertainty；\(p\left(\mathbf{y\,|\,f^W(x)}\right)\) 为观测似然，描述了观测不确定性，即Aleatoric Uncertainty。接下来讨论如何计算 Epistemic Uncertainty。</p><h3 id="分类问题">1.1. 分类问题</h3><p><img src="/Epistemic-Uncertainty-for-Active-Learning/softmax.png" width="60%" height="60%" title="图 2. softmax for unseen data"> 　　如图 2. 所示<a href="#2" id="2ref"><sup>[2]</sup></a>，softmax 可能会对没见过的目标产生较高的概率输出(如误检)。所以不能直接使用分类的概率输出作为 Uncertainty 估计。</p><ul><li><strong>Monte-Carlo Dropout</strong><br><a href="#2" id="2ref">[2]</a>中提出了 Monte-Carlo 近似求解 Epistemic Uncertainty 的方法，其指出：在训练阶段，Dropout 等价于优化网络权重 \(W\) 的 Bernoulli 分布；在测试阶段，使用 Dropout 对样本进行多次测试，能得到模型权重的后验分布，即 Epistemic Uncertainty。由此得到： <span class="math display">\[p(\mathbf{y|x}) \approx \frac{1}{T}\sum^T_{t=1} p(\mathbf{y|x,W}_t) = \frac{1}{T}\sum^T_{t=1}softmax_{(\mathbf{W}_t)}(\mathbf{x}) \tag{2}\]</span> 其中 \(\mathbf{W}_t\) 为第 \(t\) 次 Inference 网络权重。</li><li><strong>Deep Ensembles</strong><br>Deep Ensemble 则是一种非贝叶斯的方法，该方法用不同的初始化方法训练一系列网络 \(\{\mathbf{M} _ e\} _ {e=1}^E\)。那么： <span class="math display">\[p(\mathbf{y|x}) \approx \frac{1}{E}\sum^E_{e=1} p(\mathbf{y|x,M}_e) = \frac{1}{E}\sum^E_{e=1}softmax_{(\mathbf{M}_e)}(\mathbf{x}) \tag{3}\]</span></li></ul><p>　　有了预测的概率后，可用 Shannon Entropy 或者 Mutual Information 来计算目标的信息量，即 Uncertainty。</p><ul><li><strong>Shannon Entropy(SE)</strong><br>SE 计算公式为: <span class="math display">\[\mathcal{H}[\mathbf{y|x}] = -\sum^C_{c=1}p(y=c|\mathbf{x})\,\mathrm{log}\,p(y=c|\mathbf{x}) \tag{4}\]</span></li><li><strong>Mutual Information(MI)</strong><br>由于 Monte-Carlo 以及 Deep Ensembles 获取的是概率分布，以 Monte-Carlo 为例，由此可计算 MI： <span class="math display">\[\mathcal{I}[\mathbf{y;W}] = \mathcal{H}[\mathbf{y|x}] - \mathbb{E}\mathcal{H}[\mathbf{y|x,W}] \approx \mathcal{H}[\mathbf{y|x}] + \frac{1}{T}\sum_{t=1}^T\sum_{c=1}^Cp(y=c|\mathbf{x,W}_t)\,\mathrm{log}\,p(y=c|\mathbf{x,W}_t) \tag{5}\]</span></li></ul><p>　　SE 测量的是预测 Uncertainty，MI 测量的是模型对该数据的 Uncertainty。根据该 Uncertainty，即可挑选样本进行标注。Uncertainty 越高，代表该样本对模型的信息量更大，所以需要进一步标注来训练模型。</p><h3 id="回归问题">1.2. 回归问题</h3><p>　　Monte-Carlo 采样下，假设获得的回归量为 \(\{\mathbf{v}\}_{t=1}^T\)。那么其均值和方差为： <span class="math display">\[\left\{\begin{array}{l}\mathcal{M}_{\mathbf{v}} \approx \frac{1}{T}\sum_{t=1}^T\mathbf{v}_t \\\mathcal{C}_{\mathbf{v}} = \frac{1}{T}\sum_{t=1}^T\mathbf{v}_t\mathbf{v}_t^T-\mathcal{M}_{\mathbf{v}}\mathcal{M}_{\mathbf{v}}^T\end{array}\tag{6}\right.\]</span> 由此得到回归量的 Uncertainty： <span class="math display">\[TV_{\mathbf{v}} = trace\left(\mathcal{C}_{\mathbf{v}} \right) \tag{7}\]</span> 该 Uncertainty 越大，说明该数据对模型的信息也越多，所以可进一步标注训练。</p><h2 id="metrics">2. Metrics</h2><p>TODO</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Feng, Di, et al. &quot;Deep active learning for efficient training of a lidar 3d object detector.&quot; arXiv preprint arXiv:1901.10609 (2019).<br><a id="2" href="#2ref">[2]</a> Gal, Yarin. Uncertainty in deep learning. Diss. PhD thesis, University of Cambridge, 2016.<br><a id="3" href="#1ref">[3]</a> Feng, Di, Lars Rosenbaum, and Klaus Dietmayer. &quot;Towards safe autonomous driving: Capture uncertainty in the deep neural network for lidar 3d vehicle detection.&quot; 2018 21st International Conference on Intelligent Transportation Systems (ITSC). IEEE, 2018.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　&lt;a href=&quot;/Heteroscedastic-Aleatoric-Uncertainty/&quot; title=&quot;Heteroscedastic Aleatoric Uncertainty&quot;&gt;Heteroscedastic Aleatoric Uncertainty&lt;/
      
    
    </summary>
    
      <category term="Uncertainty" scheme="https://leijiezhang001.github.io/categories/Uncertainty/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Uncertainty" scheme="https://leijiezhang001.github.io/tags/Uncertainty/"/>
    
  </entry>
  
  <entry>
    <title>Heteroscedastic Aleatoric Uncertainty</title>
    <link href="https://leijiezhang001.github.io/Heteroscedastic-Aleatoric-Uncertainty/"/>
    <id>https://leijiezhang001.github.io/Heteroscedastic-Aleatoric-Uncertainty/</id>
    <published>2020-01-03T01:13:17.000Z</published>
    <updated>2020-01-05T09:46:50.875Z</updated>
    
    <content type="html"><![CDATA[<p>　　<a href="/paperreading-MT-Learning-Using-Uncertainty-to-Weight-Losses/" title="Multi-task Learning Using Uncertainty to Weigh Losses">Multi-task Learning Using Uncertainty to Weigh Losses</a> 已经详细描述了贝叶斯模型中几种可建模的不确定性(uncertainty)，并应用了<strong>任务依赖/同方差不确定性(Task-dependent or Homoscedastic Aleatoric Uncertainty)</strong>来自动学习多任务中的 Loss 权重。本文讨论同为偶然不确定性(Aleatoric Uncertainty)的<strong>数据依赖/异方差不确定性(Data-dependent or Heteroscedastic Aleatoric Uncertainty)</strong>。需要注意的是，偶然不确定性(Aleatoric Uncertainty)描述的是数据不能解释的信息，只能通过提高数据的精度来消除；而认知不确定性(Epistemic Uncertainty)描述的是模型因为缺少训练数据而存在的未知，可通过增加训练数据解决。<br>　　为什么要建模 Heteroscedastic Aleatoric Uncertainty？Learning 算法一个比较致命的问题是，网络能输出预测量，但是网络不知道其预测的不确定性，如目标状态估计中，需要获得观测的协方差矩阵<strong>(检测作为观测模块，理论上需要出检测的 Uncertainty，包括 Aleatoric 与 Epistemic Uncertainty，但是 Epistemic Uncertainty 只能通过多次采样近似得到，不能实时应用，所以一般只考虑 Aleatoric Uncertainty 作为观测的不确定性)</strong>。尤其在自动驾驶领域，<strong>我们不仅关注模型知道什么，更要关注模型不知道什么</strong>。<br>　　本文通过贝叶斯神经网络来建模 Aleatoric Uncertainty，并分析其应用效果。</p><h2 id="aleatoric-uncertainty-建模">1. Aleatoric Uncertainty 建模</h2><p>　　针对一批训练数据集\(\{\mathbf{X,Y}\}\)，训练模型 \(\mathbf{y=f^W(x)}\)，在贝叶斯框架下，预测量的后验分布为： <span class="math display">\[p\left(\mathbf{y\vert x,X,Y}\right) = \int p\left(\mathbf{y\,|\,f^W(x)}\right) p\left(\mathbf{W\,|\,X,Y}\right)d\mathbf{W} \tag{0}\]</span> 其中 \(p(\mathbf{W\,|\,X,Y})\) 为模型参数的后验分布，描述了模型的不确定性，即 Epistemic Uncertainty；\(p\left(\mathbf{y\,|\,f^W(x)}\right)\) 为观测似然，描述了观测不确定性，即Aleatoric Uncertainty。Epistemic Uncertainty 只能通过近似推断获得，本文不作讨论。<br>　　<a href="/paperreading-MT-Learning-Using-Uncertainty-to-Weight-Losses/" title="Multi-task Learning Using Uncertainty to Weigh Losses">Multi-task Learning Using Uncertainty to Weigh Losses</a> 已经详细推导了 Aleatoric Uncertainty 的建模过程，这里摘抄如下：</p><p><span class="math display">\[\mathcal{L}(\mathbf{W}, s_1, s_2) = \frac{1}{2}\mathrm{exp}(-s_1)\mathcal{L}_1(\mathbf{W}) + \mathrm{exp}(-s_2)\mathcal{L}_2(\mathbf{W}) + \mathrm{exp}(\frac{1}{2}s_1) + \mathrm{exp}(\frac{1}{2}s_2) \tag{1}\]</span> 其中 \(\mathcal{L}(\mathbf{W},s_1)\) 为回归项，\(\mathcal{L}(\mathbf{W},s_2)\) 为分类项。<br>　　<a href="#1" id="1ref">[1]</a><a href="#2" id="2ref">[2]</a><a href="#3" id="3ref">[3]</a> 中建模的回归项 loss uncertainty 与式(1)有细微出入(可以认为是 Uncertainty 的正则项不同，但是效果类似)，其负log似然为： <span class="math display">\[-\mathrm{log}p\left(\mathbf{y}\vert\mathbf{f^W(x)}\right) \propto \frac{1}{2\sigma ^2} \Vert \mathbf{y-f^W(x)} \Vert ^2 + \frac{1}{2}\mathrm{log}\sigma^2 \tag{2}\]</span> 所以其回归项 loss 为： <span class="math display">\[\mathcal{L}(\mathbf{W}, s_1) = \frac{1}{2}\mathrm{exp}(-s_1)\mathcal{L}_1(\mathbf{W}) + \frac{1}{2}s_1 \tag{3}\]</span></p><h3 id="d-object-detection-by-regressing-corners2">1.1. 3D Object Detection by regressing corners<a href="#2" id="2ref"><sup>[2]</sup></a></h3><p>　　该方案是在俯视图下回归 3D 框的 8 个角点，总共 24 个参数。假设观测为多变量的高斯分布，即： <span class="math display">\[\left\{\begin{array}{l}p\left(\mathbf{y}\vert\mathbf{f^W(x)}\right) = \mathcal{N}\left(\mathbf{f^W(x)}, \Sigma(\mathbf{x}) \right) \\\Sigma(\mathbf{x}) = diag(\sigma _ {\mathbf{x}}^2)\end{array}\tag{4}\right.\]</span> 其中 \(\mathbf{y}\) 是预测的目标框参数，\(\sigma _ {\mathbf{x}}^2\) 是 24 维的向量，表示了观测数据的噪声水平，由式(3)可知，噪声越大，其对 Loss 的作用越小。</p><p><img src="/Heteroscedastic-Aleatoric-Uncertainty/Aleatoric.png" width="60%" height="60%" title="图 1. Aleatoric Uncertainty 与 3D corner 关系"> 　　如图 1. 所示，同一目标，靠近本车的 corner 点，其 Aleatoric Uncertainty 越小；距离越远，目标被遮挡的越严重，其 Aleatoric Uncertainty 越高。</p><h3 id="d-object-detection-by-regressing-location-and-orientation-3">1.2. 3D Object Detection by regressing location and orientation <a href="#3" id="3ref"><sup>[3]</sup></a></h3><p><img src="/Heteroscedastic-Aleatoric-Uncertainty/regression_uncert.png" width="80%" height="80%" title="图 2. network arch"> 　　如图 2. 所示，网络结构比较简单，这里建模了三种 uncertainty: RPN bbox regression \(\sigma^2_{\mathbf{t_r}}\)；Head 中的 location \(\sigma^2_{\mathbf{t_v}}\)；Head 中的 orientation \(\sigma^2_{\mathbf{r_v}}\)。最终的 Loss 由三项式(3) 以及两项分类 loss 构成。<br><img src="/Heteroscedastic-Aleatoric-Uncertainty/Aleatoric_Uncert.png" width="80%" height="80%" title="图 3. Aleatoric Uncertainty 与目标状态关系"> 　　如图 3. 所示，TV(Total Variance) 与目标状态的关系。对于距离越远，遮挡越严重的目标，其 Aleatoric Uncertainty 会越高，因为其观测到的点云会比较少。</p><h3 id="semantic-segmentation-1">1.3. Semantic Segmentation <a href="#1" id="1ref"><sup>[1]</sup></a></h3><p><img src="/Heteroscedastic-Aleatoric-Uncertainty/Aleatoric_Epistemic.png" width="60%" height="60%" title="图 4. Aleatoric Uncertainty 在语义分割中的关系"> 　　如图 4. 所示，Aleatoric Uncertainty 在远处，边缘处较大；而 Epistemic Uncertainty 对没见过的数据/区域较大。</p><h2 id="aleatoric-uncertainty-预测">2. Aleatoric Uncertainty 预测</h2><p>　　<a href="/paperreading-MT-Learning-Using-Uncertainty-to-Weight-Losses/" title="Multi-task Learning Using Uncertainty to Weigh Losses">Multi-task Learning Using Uncertainty to Weigh Losses</a> 中 Uncertainty 不需要作为预测输出，可将其设计为网络的 weights，且每个任务都设计为单变量高斯分布的形式。<a href="#2" id="2ref">[2]</a><a href="#3" id="3ref">[3]</a> 中则将 Uncertainty 设计为网络的输出，且是多变量高斯分布。更一般的，假设模型输出为混合高斯分布： <span class="math display">\[\left\{\begin{array}{l}p\left(\mathbf{y}\vert\mathbf{f^W(x)}\right) = \sum_k \alpha_k \mathcal{N}\left(\mathbf{f^W(x)}_{(k)}, \Sigma(\mathbf{x})_{(k)} \right)\\\sum_k \alpha_k = 1\end{array}\tag{5}\right.\]</span> 　　对于 3D Detection 问题，网络输出的 3D 框参数为 \(\mathbf{y}=(x,y,z,l,h,w,\theta)\)，当输出满足 \(K\) 个混合高斯分布时，网络的输出量有：</p><ul><li>\(K\) 组目标框参数预测量 \(\{\mathbf{y}_k\}\)；</li><li>\(K\) 个对数方差 \(\{s_k\}\)；</li><li>\(K\) 个混合高斯模型权重参数 \(\{\alpha_k\}\)；</li></ul><p>　　训练时，找出与真值分布最近的一组预测量，混合高斯模型权重用 softmax 回归并用 cross-entropy loss，找到最相似的分布后，将该分布的方差用式(3)作用于回归的 Loss 项；测试时，找到混合高斯模型最大的权重项，对应的高斯分布，即作为最终的输出分布。这里只考虑了输出 3D 框的一个整体的方差，也可以输出定位方差+尺寸方差+角度方差，只要将该方差作用于对应的 Loss 项即可。当 \(K=1\) 时，就是多变量单高斯模型，一般也够用。</p><h2 id="metrics">3. Metrics</h2><p>TODO</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Kendall, Alex, and Yarin Gal. &quot;What uncertainties do we need in bayesian deep learning for computer vision?.&quot; Advances in neural information processing systems. 2017.<br><a id="2" href="#2ref">[2]</a> Feng, Di, Lars Rosenbaum, and Klaus Dietmayer. &quot;Towards safe autonomous driving: Capture uncertainty in the deep neural network for lidar 3d vehicle detection.&quot; 2018 21st International Conference on Intelligent Transportation Systems (ITSC). IEEE, 2018.<br><a id="3" href="#3ref">[3]</a> Feng, Di, et al. &quot;Leveraging heteroscedastic aleatoric uncertainties for robust real-time lidar 3d object detection.&quot; 2019 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2019.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　&lt;a href=&quot;/paperreading-MT-Learning-Using-Uncertainty-to-Weight-Losses/&quot; title=&quot;Multi-task Learning Using Uncertainty to Weigh Losses&quot;&gt;M
      
    
    </summary>
    
      <category term="Uncertainty" scheme="https://leijiezhang001.github.io/categories/Uncertainty/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Uncertainty" scheme="https://leijiezhang001.github.io/tags/Uncertainty/"/>
    
  </entry>
  
  <entry>
    <title>Object Registration with Point Cloud</title>
    <link href="https://leijiezhang001.github.io/Object-Registration-with-Point-Cloud/"/>
    <id>https://leijiezhang001.github.io/Object-Registration-with-Point-Cloud/</id>
    <published>2019-12-25T01:13:19.000Z</published>
    <updated>2019-12-26T09:37:09.878Z</updated>
    
    <content type="html"><![CDATA[<p>　　<a href="/ADH-Tracker/" title="ADH Tracker">ADH Tracker</a> 通过 ADH 方法有效得在两目标点云的 T 变换的解空间中搜索出高概率解集，并用简单的运动模型，在贝叶斯概率框架下进行目标状态(位置，速度)的估计。这其中关键的环节还是两目标点云之间变换关系 \((R,T)\) 的求解，即 Object Registration。<br>　　求解两点云之间的位姿关系，传统的做法是 ICP。以 ICP 为代表的方法大多数都是迭代法，本文介绍两种 learning-based 点云注册方法。</p><h2 id="deep-closet-point1">1. Deep Closet Point<a href="#1" id="1ref"><sup>[1]</sup></a></h2><h3 id="icp-描述">1.1. ICP 描述</h3><p>　　假设两个点云集：\(\mathcal{X}=\{x _ 1,...,x _ i,...,x _ N\}\in\mathbb{R}^3\)，\(\mathcal{Y}=\{y _ 1,...,y _ j,...y _ M\}\in\mathbb{R}^3\)。两个点集之间的变换为 \(R,t\)，定义点集匹配的误差函数： <span class="math display">\[ E(R,t) = \frac{1}{N}\sum_i^N\Vert Rx_i+t-y _ {m(x_i)}\Vert \tag{1}\]</span> 其中 \(y_{m(x_i)}\) 为 \(x_i\) 经过变换后匹配上的最近点，即： <span class="math display">\[ m(x_i,\mathcal{Y}) = \mathop{\arg\min}_j\Vert Rx_i+t-y_j\Vert \tag{2}\]</span> 定义点云重心：\(\bar{x}=\frac{1}{N}\sum _ {i=1}^Nx _ i\)，\(\bar{y}=\frac{1}{M}\sum _ {j=1}^Ny _ j\)。计算 Cross-covariance 矩阵： <span class="math display">\[ H = \sum_{i=1}^N(x_i-\bar{x})(y_i-\bar{y}) \tag{3}\]</span> \(R,t\) 变换可通过 \(H=USV^T\) 最小化误差函数 \(E(R,t)\) 实现： <span class="math display">\[\left\{\begin{array}{l}R= VU^T\\t= -R\bar{x}+\bar{y}\end{array}\tag{4}\right.\]</span> ICP 算法就是迭代得求解式(2)与式(1)的过程。</p><h3 id="网络结构">1.2. 网络结构</h3><p><img src="/Object-Registration-with-Point-Cloud/DCP.png" width="80%" height="80%" title="图 1. DCP"> 　　如图 1. 所示，DCP 网络结构由三部分组成：</p><ul><li><strong>Embedding Module</strong><br>特征提取层，可以用 PointNet，也可以用 DGCNN 网络(<a href="/PointCloud-Feature-Extraction/" title="PointCloud Feature Extraction">PointCloud Feature Extraction</a>)，DGCNN 能更有效的提取局部特征。</li><li><strong>Transformer</strong><br>该模块基于 Attention 机制，详情可参考<a href="#3" id="3ref">[3]</a><a href="#4" id="4ref">[4]</a>。</li><li><strong>Head</strong><br>该模块用于预测 \((R,t)\)，可以简单的用 MLP 回归，也可以用 SVD 层来预测，因为 Transformer 会输出 \(x_i\) 在 \(\mathcal{Y}\) 中的匹配点。</li></ul><h3 id="loss">1.3. Loss</h3><p>　　Loss 比较简单，也是基于有监督的学习： <span class="math display">\[ Loss = \Vert R^TR_g-I\Vert ^2 + \Vert t-t_g\Vert ^2 + \lambda \Vert\theta\Vert ^2\]</span></p><h2 id="alignnet-3d2">2. AlignNet-3D<a href="#2" id="2ref"><sup>[2]</sup></a></h2><h3 id="网络结构-1">2.1. 网络结构</h3><p><img src="/Object-Registration-with-Point-Cloud/AlignNet.png" width="60%" height="60%" title="图 2. AlignNet"> 　　如图 2. 所示，AlignNet 由两个网络组成：</p><ul><li><strong>CanonicalNet</strong><br>CanonicalNet 作用是预测点集目标3D框的中心点坐标系，从而将点集坐标转换到中心点坐标系。预测点集目标3D框的中心点坐标系通过 coarse-to-fine 方式实现，stage1(T-CoarseNet) 只粗略预测中心点的位置信息，stage2(T-FineNet) 预测中心点位置相对 Stage1 的残差，以及中心点坐标系的旋转量。参考以前的方法，旋转量通过角度区域分类＋残差实现。通过该网络，每个点集的坐标均在各自目标框中心点坐标系下，能直观的反应目标的形状。</li><li><strong>Head</strong><br>Head(stage3) 则将两个点集特征聚合，预测各中心点坐标系下两个点集的相对位姿。<br>设点集 \(s_1\) 经过 CanonicalNet 预测的变换为 \(T_1\)，\(s_2\) 对应的变换为 \(T_2\)，stage3 预测的两者的变换为 \(T_f\)，那么最终得到的两个点集的变换为 \(T_1T_fT_2^{-1}\)。</li></ul><h3 id="loss-1">2.2. Loss</h3><p>　　stage1 预测了 translation，stage2/stage3 预测了 translation 和 rotation，总的 Loss 为： <span class="math display">\[\begin{align}L &amp;= L_{trans,overall}+\lambda_2\cdot L_{angle,overall}\\  &amp;= \lambda_1(L_{trans,s1}+L_{trans,s2}) + L_{trans,s3} + \lambda_2(\lambda_1L_{angle,s2}+L_{angle,s3})\end{align}\]</span> stage1/stage2 预测的目标框中心点坐标系(包括中心点坐标及目标框的朝向)真值由点云所构成的目标框提供。</p><h3 id="不足点">2.3. 不足点</h3><p>　　这种级联式的方法，思想是非常好的，将两个点集的相对位姿分解为两大部来求解，即先将点集转换到中心点坐标系，然后再求解点集剩下位姿残差，coarse-to-fine，能较好回归且收敛。<br>　　但是存在一些问题。我们假设两个点集作为同一刚性目标，其3D框没有偏差(标注非常准)，那么 CanonicalNet 出来结果，已经可以作为相对位姿结果。但是标注肯定会有抖动(除非是生成的数据)，可以认为是高斯分布，以及获取点云的传感器的测量噪音，这样的话，看起来 stage3 就是只用来拟合这种均值为 0 的高斯分布了。<br>　　所以本方法对生成的数据与真实的数据，存在一定的偏差，因为目标框真值的抖动分布不一致。这样的话在生成的数据上训练的网络，直接迁移到真实数据中，可能性能会下降比较明显，反之可能还好。</p><h2 id="参考文献">3. 参考文献</h2><p><a id="1" href="#1ref">[1]</a> Wang, Yue, and Justin M. Solomon. &quot;Deep Closest Point: Learning Representations for Point Cloud Registration.&quot; arXiv preprint arXiv:1905.03304 (2019).<br><a id="2" href="#2ref">[2]</a> Groß, Johannes, Aljoša Ošep, and Bastian Leibe. &quot;AlignNet-3D: Fast Point Cloud Registration of Partially Observed Objects.&quot; 2019 International Conference on 3D Vision (3DV). IEEE, 2019.<br><a id="3" href="#3ref">[3]</a> Vaswani, Ashish, et al. &quot;Attention is all you need.&quot; Advances in neural information processing systems. 2017.<br><a id="4" href="#4ref">[4]</a> https://zhuanlan.zhihu.com/p/48508221</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　&lt;a href=&quot;/ADH-Tracker/&quot; title=&quot;ADH Tracker&quot;&gt;ADH Tracker&lt;/a&gt; 通过 ADH 方法有效得在两目标点云的 T 变换的解空间中搜索出高概率解集，并用简单的运动模型，在贝叶斯概率框架下进行目标状态(位置，速度)的估计。这
      
    
    </summary>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/categories/MOT/"/>
    
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="ICP" scheme="https://leijiezhang001.github.io/tags/ICP/"/>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/tags/MOT/"/>
    
      <category term="tracking" scheme="https://leijiezhang001.github.io/tags/tracking/"/>
    
  </entry>
  
  <entry>
    <title>ADH(Annealed Dynamic Histograms) Tracker</title>
    <link href="https://leijiezhang001.github.io/ADH-Tracker/"/>
    <id>https://leijiezhang001.github.io/ADH-Tracker/</id>
    <published>2019-12-24T00:57:49.000Z</published>
    <updated>2019-12-28T04:11:46.213Z</updated>
    
    <content type="html"><![CDATA[<p>　　<a href="/卡尔曼滤波详解/" title="卡尔曼滤波详解">卡尔曼滤波详解</a>中详细推导了卡尔曼滤波及其扩展卡尔曼滤波基于贝叶斯的推导过程。由贝叶斯法则式(7)，<strong>状态估计问题可定义为：已知似然及先验概率，最大化后验概率的过程</strong>。其中先验即为“运动学模型(motion model)”，似然即为“观测”，后验概率即为待估计的状态量。对于卡尔曼滤波，对应了式(1)的运动方程及测量方程。<br>　　用扩展卡尔曼滤波来估计目标状态的原理可见<a href="/卡尔曼滤波器在三维目标状态估计中的应用/" title="卡尔曼滤波器在三维目标状态估计中的应用">卡尔曼滤波器在三维目标状态估计中的应用</a>。该文重点讨论基于质点的一系列运动学模型，以及基于刚体的前转向车模型；测量模型则没做深入研究，默认是目标重心级别的测量量。比如，观测量如果是三维框，那么自然可得到目标的位置，相减就得到速度的观测量。<br>　　但是基于点云的目标检测中，目标的观测量更准确的应该是点集(cluster)。<strong>如何在贝叶斯框架下，定义点集的运动学模型及观测模型</strong>，对提高目标状态的估计显得尤其重要。ADH Tracker<a href="#1" id="1ref"><sup>[1]</sup></a> 就是一种点集状态估计方法，其描述了一种可跟踪目标表面形状特性的概率模型，本文主要阐述 ADH Tracker 的原理及实现细节。</p><h2 id="点集状态估计的概率模型">1. 点集状态估计的概率模型</h2><h3 id="贝叶斯框架">1.1. 贝叶斯框架</h3><p><img src="/ADH-Tracker/bayesian.png" width="50%" height="50%" title="图 1. 点集状态估计的贝叶斯概率模型"> 　　如图 1. 所示，状态量为 \(x_t\)，点集状态为 \(s_t\)，测量/观测量为 \(z_t\)，\(s_t\) 表示为从目标点集中采样的点集。 <img src="/ADH-Tracker/gaussian.png" width="50%" height="50%" title="图 2. 传感器噪声"> 　　如图 2. 所示，由于传感器的噪声 \(\Sigma_e\)，实际的目标上的点集 \(s_t\) 需要加上传感器噪声，以及目标的当前位置，才是最终的观测量点集 \(z_t\): <span class="math display">\[z_{t,j} \sim \mathcal{N}(s_{t,j},\Sigma_e) + x_{t,p}  \tag{1}\]</span> 注意坐标系是在前一时刻目标的中心，状态量中的位置是相对位置，所以前一时刻目标点服从分布： <span class="math display">\[z_{t-1,i} \sim \mathcal{N}(s_{t-1,i},\Sigma_e)  \tag{2}\]</span> 图 1. 的贝叶斯模型下： <span class="math display">\[p(z_{t-1}|x_t,s_{t-1}) = p(z_{t-1}|s_{t-1}) \tag{3}\]</span> 由于目标的遮挡等位置变换，目标上的点集 \(s_t\) 又是随时间变化的，假设 \(p(V)\) 表示当前时刻点集从前一时刻点集采样的先验概率，那么当前时刻每个点从前一时刻采样的概率为： <span class="math display">\[p(s_{t,j}|s_{t-1}) = p(V)p(s_{t,j}|s_{t-1},V) + p(\neg V)p(s_{t,j}|s_{t-1},\neg V) \tag{4}\]</span> 假设当前点在前一时刻不可见的均为被遮挡的情况，那么： <span class="math display">\[p(s_{t,j}|s_{t-1},\neg V) = k_1(k_2-(s_{t,j}|s_{t-1},V))\]</span> 合并可得： <span class="math display">\[p(s_{t,j}|s_{t-1}) = \eta(p(s_{t,j}|s_{t-1},V) +k) \tag{5}\]</span></p><h3 id="状态估计问题">1.2. 状态估计问题</h3><p>　　式(1)~(5)描述了该贝叶斯网络下各变量之间的关系，状态估计求解的目标是：在所有观测量的基础上估计当前状态，即\(p(x_t|z_1...z_t)\)。根据贝叶斯法则： <span class="math display">\[p(x_t|z_1...z_t)=\eta\; p(z_t|x_t,z_1...z_{t-1}) p(x_t|z_1...z_{t-1}) \tag{6}\]</span> 其中 \(\eta\) 为归一化常数，<strong>第一项是观测模型，第二项是运动模型</strong>。如果依据条件独立，观测模型则可简化为： <span class="math display">\[p(z_t|x_t,z_1...z_{t-1}) = p(z_t|x_t)\]</span> 但是这里考虑到 \(s_t\) 均是从同一目标采样的，所以条件独立性不成立，将观测模型简化近似为： <span class="math display">\[p(z_t|x_t,z_1...z_{t-1}) \approx p(z_t|x_t,z_{t-1}) \tag{7}\]</span> 直观上理解为，当前观测不仅依赖当前状态，还依赖上一时刻的观测量。</p><h2 id="adh-tracker-观测模型">2. ADH Tracker 观测模型</h2><p>　　观测模型式(7)可重写为： <span class="math display">\[\begin{align}p(z_t|x_t,z_{t-1}) &amp;= \int p(z_t,s_t|x_t,z_{t-1})ds_t \\&amp;= \int p(z_t|s_t,x_t)p(s_t|x_t,z_{t-1})ds_t \\&amp;= \int p(z_t|s_t,x_t)\left(\int p(s_t,s_{t-1}|x_t,z_{t-1})ds_{t-1}\right)ds_t \\&amp;= \int p(z_t|s_t,x_t)\left(\int p(s_t|s_{t-1})p(s_{t-1}|x_t,z_{t-1})ds_{t-1}\right)ds_t \\&amp;= \int p(z_t|s_t,x_t)\left(\int \eta\;p(s_t|s_{t-1})p(z_{t-1}|x_t,s_{t-1})p(s_{t-1})ds_{t-1}\right)ds_t \\&amp;= \int p(z_t|s_t,x_t)\left(\int \eta\;p(s_t|s_{t-1})p(z_{t-1}|s_{t-1})p(s_{t-1})ds_{t-1}\right)ds_t\tag{8}\end{align}\]</span> 式(1)(2)(5)可得高斯模型: <span class="math display">\[\left\{\begin{array}{l}p(z_t|s_t,x_t) = \mathcal{N}(z_t;s_t+x_{t,p},\Sigma_e) \\p(z_{t-1}|s_{t-1}) = \mathcal{N}(z_{t-1};s_{t-1},\Sigma_e) \\ p(s_t|s_{t-1}) = \eta\left(\mathcal{N}(s_{t};s_{t-1},\Sigma_r)+k \right) \\ \end{array}\tag{9}\right.\]</span> 其中 \(\Sigma_e \) 为传感器噪声方差，\(\Sigma_r\) 为传感器不同距离的分辨率。因为两个高斯分布相乘还是高斯分布，所以由式(8)(9-2)(9-3)，可得： <span class="math display">\[ p(s_t|x_t,z_{t-1}) = \eta (\mathcal{N}(s_t;z_{t-1},\Sigma_r+\Sigma_e)+k) \tag{10}\]</span> 进一步由式(8)(9-1)(10)可得： <span class="math display">\[p(z_t|x_t,z_{t-1}) = \eta \left(\mathcal{N}(z_t;z_{t-1}+x_{t,p},\Sigma_r+2\Sigma_e)+k \right) \tag{11}\]</span> 　　观测模型实际计算中，令 \(\bar{z} _ {t-1}\) 为点集 \(z_{t-1}\) 经过状态量变换后的点集，即 \(\bar{z} _ {t-1}=z _ {t-1}+x _ {t,p}\)；对于 \(z _ j\in z _ t\)，令 \(\bar{z} _ i \) 为 \(z _ j\) 在点集 \(\bar{z}_ { t-1}\) 中的最近点。那么: <span class="math display">\[ p(z_t|x_t,z_{t-1}) = \eta \left(\prod_{z_j\in z_t} \mathrm{exp}\left(-\frac{1}{2}(z_j-\bar{z_i})^T\Sigma^{-1}(z_j-\bar{z}_i)\right)+k\right) \tag{12}\]</span> 其中 \(\Sigma=2\Sigma_e+\Sigma_r\)。</p><h2 id="adh-tracker-运动模型">3. ADH Tracker 运动模型</h2><p>　　这里使用的是质点匀速模型，因为在 \((R,t)\) 搜索空间中得到了一组不同概率的解，所以可用多变量高斯分布去拟合这组解： <span class="math display">\[\left\{\begin{array}{l}\mu_t=\sum_i p(x_{t,i}|z_i...z_t)x_{t,i}\\\Sigma_t = \sum_i p(x_{t,i}|z_1...z_t)(x_{t,i}-\mu_t)(x_{t,i}-\mu_t)^T\end{array}\tag{13}\right.\]</span> 其中 \(x_{t,i}\) 为第 \(i\) 组解对应的状态量。得到该状态量的高斯分布后，就可以用匀速运动模型预测下一时刻的状态。<br>　　同时针对每一组解空间中的候选解，还可计算其匀速模型下的速度概率项，叠加到观测概率中。</p><h2 id="adh-算法">4. ADH 算法</h2><p><img src="/ADH-Tracker/adh.png" width="60%" height="60%" title="图 3. ADH 原理"> 　　对 \((R,t)\) 解空间进行有效搜索直接决定求解速度，如图 3. 所示，将解空间(state space)分割成一系列搜索区域，每个区域基于后验概率 \(p(x_t|z_1...z_t)\) 计算区域离散概率： <span class="math display">\[\begin{align}p(c_i) &amp;= p(c_i\cap R) \\&amp;= p(c_i|R)p(R) \\&amp;= \frac{p(x_i|z_1...z_t)\vert c_i\vert}{\sum_{j\in R}p(x_j|z_1...z_t)\vert c_i\vert} p(R) \\&amp;= \eta p(x_i|z_1...z_t)p(R)\tag{14}\end{align}\]</span> 其中 \(R\) 为待细分的区域集合(cells)，其被划分为子区域 \(c_i\in R\)，所以区域概率满足 \(\sum_{i\in R}p(c_i) = p(R)\)。对拥有较大离散概率的区域，进一步细分搜索区域，进行迭代搜索。初始化时，\(p(R)=1\)。<br>　　这里需要制定区域细分的策略，考虑最大化划分前后区域概率分布的 KL-divergence，即 KL-divergence 能描述划分后，后验概率与真实分布的相似性，越接近真实分布，前后区域离散概率分布的 KL-divergence 会越小。而为了提高搜索效率，要求前后离散概率分布的 KL-divergence 要最大，最终收敛到真实分布。<br>　　假设 \(R\) 区域的离散概率分布为 \(P_i\)，需要划分 \(k\) 个区域。那么划分前，可以认为其概率分布为每个 cell 概率为 \(P_i/k\)；划分后，其概率分布为：\(\sum_{j=1}^kp_j=P_i\)。这两个分布的 KL-divergence 为： <span class="math display">\[ D_{KL}(A\Vert B)=\sum_{j=1}^k p_j \mathrm{In}\left(\frac{p_j}{P_i/k}\right) \tag{15} \]</span> 当某个细分区域 \(p_{j'} = P_i\) 时： <span class="math display">\[ D_{KL}(A\Vert B)=P_i \mathrm{In}k  \tag{16}\]</span> 如果每个 cell 后验概率计算需要时间 \(t\) 秒，那么每秒能获得最大的 DL-divergence 为 \(P_i\mathrm{In}k/(kt)\)，由此可以选择策略：</p><ul><li>对 \(P_i\) 大于一定阈值的区域进行划分；</li><li>每个搜索维度划分的区域个数设定为 \(k=3\)。因为该函数在 \(k=e\) 时取得最大值。</li></ul><p><img src="/ADH-Tracker/adh_alg2.png" width="80%" height="80%" title="图 4. ADH Tracker"> 　　图 4. 为 ADH Tracker 算法的伪代码。</p><h2 id="adh-tracker-实现细节2">5. ADH Tracker 实现细节<a href="#2" id="2ref"><sup>[2]</sup></a></h2><h3 id="kalman-部分">5.1. Kalman 部分</h3><p>　　ADH 代码中 centroid-based kalman 的运动模型为质点匀速模型，较为简单。 其设置为：状态量 \(x=[v_x,v_y,v_z]\)，测量量 \(z=\frac{1}{\delta t}[p_x,p_y,p_z]\)。状态转移矩阵 \(A\) 以及观测矩阵 \(C\) 均为单位阵。过程噪声为高斯分布，其协方差矩阵为 \(Q_k = diag(\sigma_a,\sigma_a,\sigma_{a_z})\cdot \delta ^2 t\)，测量噪声的协方差矩阵为 \(R_k = diag(\sigma_m,\sigma_m,\sigma_m)\)。由此可方便的计算 kalman 预测及更新两个过程。</p><h3 id="adh-部分">5.2. ADH 部分</h3><p>　　ADH 算法中，每个采样分辨率下需要多次计算解空间中各 \((R,t)\) 下的观测模型，而观测模型计算中，每次需要通过 KD-Tree 寻找两个点集的匹配点，再通过式(12)计算观测概率模型。这样会非常耗时，因为观测模型本质上就是求解两个点集相似度，所以代码实现中，作者采用的策略为：首先将被匹配的点集进行栅格化，然后将点集中每个点以稠密度(density)高斯概率分布的形式拓展一定栅格范围，每个栅格取拓展到该栅格的点的高斯概率值的最大值。之后任何一个点集需要与之计算观测模型(相似度)，只要直接统计索引这个点集在该栅格下的概率值即可。</p><h2 id="参考文献">6. 参考文献</h2><p><a id="1" href="#1ref">[1]</a> Held, David, et al. &quot;Robust real-time tracking combining 3D shape, color, and motion.&quot; The International Journal of Robotics Research 35.1-3 (2016): 30-49.<br><a id="2" href="#2ref">[2]</a> https://github.com/davheld/precision-tracking</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　&lt;a href=&quot;/卡尔曼滤波详解/&quot; title=&quot;卡尔曼滤波详解&quot;&gt;卡尔曼滤波详解&lt;/a&gt;中详细推导了卡尔曼滤波及其扩展卡尔曼滤波基于贝叶斯的推导过程。由贝叶斯法则式(7)，&lt;strong&gt;状态估计问题可定义为：已知似然及先验概率，最大化后验概率的过程&lt;/stron
      
    
    </summary>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/categories/MOT/"/>
    
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/tags/MOT/"/>
    
      <category term="tracking" scheme="https://leijiezhang001.github.io/tags/tracking/"/>
    
  </entry>
  
  <entry>
    <title>PointCloud Feature Extraction</title>
    <link href="https://leijiezhang001.github.io/PointCloud-Feature-Extraction/"/>
    <id>https://leijiezhang001.github.io/PointCloud-Feature-Extraction/</id>
    <published>2019-12-07T06:52:30.000Z</published>
    <updated>2019-12-25T02:25:36.418Z</updated>
    
    <content type="html"><![CDATA[<p>　　机器学习中，特征提取是非常重要的一个环节（认为是最重要的一环也不为过）。对图像数据的特征提取操作已经较为成熟，如卷积；而点云数据由于无序性，所以对其进行高效的特征提取则比较困难。 一个好的点云特征提取操作需要具备以下特征：</p><ul><li>能提取点云的<strong>局部以及全局特征</strong>；</li><li>计算高效；</li></ul><p>　　目前已知的点云特征提取方法可分为两大类：Voxel-level，以及 Point-level。Voxel-Level 的特征提取也已经相当成熟，基本思路是将点云空间网格化，每个网格进行手工特征填充或者 Point-level 的特征提取，然后就可以应用标准的 2D/3D 卷积操作进行局部及全局特征提取。这种方法提取的特征细粒度取决于空间栅格化的分辨率，针对点级别的任务（如 semantic segmentation，Scene flow等），其特征的细粒度还是不够的。<br>　　本文主要介绍 Point-level 的方法，这种方法能提取点级别的局部、全局特征信息，是处理点云的有效手段。这种方法首先要将无序的点云进行一定的结构化组织，由此可分为若干方法，如下阐述。</p><h2 id="基于原始三维空间操作">1. 基于原始三维空间操作</h2><p>　　在三维空间下进行点的局部特征提取，需要快速找到每个点周围的点，所以需要对点云构建 Kd-tree(或 Ball-tree)，来加速邻近点的快速查询。Kd-tree 的算法复杂度为：</p><ul><li>构建：\(\mathcal{O}(\mathrm{log}^2n)\)</li><li>插入：\(\mathcal{O}(\mathrm{log}n)\)</li><li>删除：\(\mathcal{O}(\mathrm{log}n)\)</li><li>查询：\(\mathcal{O}(n^{1-\frac{1}{k}}+m)\)，其中 \(m\) 为要查询的最近点个数</li></ul><h3 id="问题描述">1.1. 问题描述</h3><p>　　设点云集合：\(P=\{p_1,...,p_n\}\in R^{F}\)，每个点有 \(F\) 维的特征，以及每个点的三维坐标为：\(p_i=(x_i,y_i,z_i)\)（注意，坐标也可作为特征包含于 \(F\) 维中）。因为点云的无序性，定义点云集合的最近邻图(k-nearest neighbor graph) \(\mathcal{G=(V,E)}\)，其中 \(\mathcal{V}\) 表示点云中的点，\(\mathcal{E}\) 表示点 \(p_i\) 与最近的 \(k\) 个点 \(P_i^k=\{p_j ^ {i1},...,p_j ^ {ik}\}\) 所构成的有向边集合 \(\{(i,j_{i1}),...,(i,j_{ik})\}\)。由此定义<strong>点级别特征提取操作</strong>： <span class="math display">\[ p_i&#39; = \displaystyle\Box_{j:(i,j)\in\mathcal{E}} h_\Theta(p_i,p_j) \tag{1}\]</span> 其中 \(h _ {\Theta}\) 表示非线性映射函数，将特征空间：\(\mathbb{R} ^ F \times \mathbb{R} ^ F \to \mathbb{R} ^ {F'}\)；\(\Box\) 为用于特征聚合的对称函数。该操作类似图像二维卷积操作，将输入的点云集合：\(P=\{p_1,...,p_n\}\in R^{F}\) 映射到相同点数的：\(P'=\{p_1',...,p_n'\}\in R^{F'}\)。</p><h3 id="hbox-的选择">1.2. \(h,\Box\) 的选择</h3><h4 id="euclidean-conv">1.2.1. Euclidean Conv</h4><p>　　设计 \(h_{\Theta}(p_i,p_j)=\theta_jp_j\)，\(\Box=\sum\)，得到传统的 Euclidean convolution： <span class="math display">\[ p_i&#39; = \displaystyle\sum_{j:(i,j)\in\mathcal{E}}(\theta_jp_j) \tag{2}\]</span> 其中 \(\Theta=(\theta_i,...,\theta_k)\) 为滤波器的权重。</p><h4 id="pointnet1">1.2.2. PointNet<a href="#1" id="1ref"><sup>[1]</sup></a></h4><p>　　设计 \(h_{\Theta}(p_i,p_j)=h_{\Theta}(p_i) = \mathrm{MLP}(p_i)\)，\(\Box=\mathrm{MAX} 或 \sum\)，得到 PointNet 中的操作： <span class="math display">\[ p_i&#39; = \displaystyle\left\{ \sum|\mathrm{MAX}\right\}(\theta_ip_i) = \displaystyle\left\{\sum|\mathrm{MAX}\right\}\, \mathrm{MLP}(p_i) \tag{3}\]</span> 感知机的权重可以共享。</p><h4 id="deep-parametric-continuous-convoluion2">1.2.3. Deep Parametric Continuous Convoluion<a href="#2" id="2ref"><sup>[2]</sup></a></h4><p>　　设计 \(h_{\Theta}(p_i,p_j)=\mathrm{MLP}(p_j^{xyz}-p _ i^{xyz})\cdot p _ j^{\mathrm{exclude}\,xyz}\)，\(\Box=\mathrm{\sum}\)，得到 Deep Parametric Continuous Convolution 操作： <span class="math display">\[ p_i&#39; = \displaystyle\sum_{j:(i,j)\in\mathcal{E}}\left(\mathrm{MLP}(p_j^{xyz}-p _ i^{xyz})\cdot p _ j^{\mathrm{exclude}\,xyz}\right) \tag{4}\]</span> 根据邻近点的距离，显示的来学习其对中心点的特征贡献。Continuous Fusion Layer 中证明没必要显示的学习，直接将相对距离 Concate 到特征上，隐式的学习同样有效。</p><h4 id="pointnet3flownet3d4continuous-fusion-layer5">1.2.4. PointNet++<a href="#3" id="3ref"><sup>[3]</sup></a>/FlowNet3D<a href="#4" id="4ref"><sup>[4]</sup></a>/Continuous Fusion Layer<a href="#5" id="5ref"><sup>[5]</sup></a></h4><p>　　设计 \(h _ {\Theta}(p_i,p_j)=\mathrm{MLP}\,\left(p _ j^{\mathrm{exclude}\,xyz}\oplus (p _ j ^ {xyz}-p _ i^{xyz})\right)\)，\(\Box=\left\{\mathrm{MAX}|\sum\right\}\)，得到 PointNet++/FlowNet3D/Continuous Fusion Layer(前两者是 \(\mathrm{MAX}\)，后者是 \(\sum\) 操作) 中的操作： <span class="math display">\[ p_i&#39; = \displaystyle\left\{\mathrm{MAX}|\sum\right\}_{j:(i,j)\in\mathcal{E}}\mathrm{MLP}\,\left(p _ j^{\mathrm{exclude}\,xyz}\oplus (p _ j ^ {xyz}-p _ i^{xyz})\right) \tag{5}\]</span> 将点 \(p_j\) 中的坐标都转换到以中心点 \(p_i\) 为参考的局部坐标。这样能更好的提取局部信息，但是丢失了点的绝对坐标信息。</p><h4 id="edgeconvdgcnn6">1.2.5. EdgeConv(DGCNN)<a href="#6" id="6ref"><sup>[6]</sup></a></h4><p>　　设计 \(h _ {\Theta}(p_i,p_j)=\mathrm{MLP}\,\left(p _ j^{\mathrm{exclude}\,xyz}\oplus (p _ j ^ {xyz}-p _ i^{xyz})\oplus p _ i^{xyz}\right)\)(这里只是猜测是这么做的，EdgeConv paper 中没有具体说怎么做的)，\(\Box=\mathrm{MAX}\)，得到 EdgeConv 中的操作： <span class="math display">\[ p_i&#39; = \displaystyle\mathrm{MAX}_{j:(i,j)\in\mathcal{E}}\mathrm{MLP}\,\left(p _ j^{\mathrm{exclude}\,xyz}\oplus( p _ j ^ {xyz}-p _ i^{xyz})\oplus p _ i^{xyz}\right) \tag{6}\]</span> 额外加上点　\(p_i\) 的世界坐标，保留点的全局信息。 <img src="/PointCloud-Feature-Extraction/DGCNN.png" width="80%" height="80%" title="图 1. DGCNN"> 　　如图 1. 所示，DGCNN 网络结构与 PointNet 网络差不多，区别就在核心的点特征提取操作。<br>　　代码实现可参考<a href="#14" id="14ref">[14]</a>, <a href="#15" id="15ref">[15]</a>，其中 <a href="#15" id="15ref">[15]</a>是完整的 DGCNN，每次卷积操作都是要在该点的新特征下取寻找 \(k\) 个最近邻，而 <a href="#14" id="14ref">[14]</a> 是简化版，最近邻点是固定的，分析代码可知其步骤：</p><ol type="1"><li>针对每个点 \(p_i\)，首先找到该点最近的 \(k\) 个点及对应的特征，得到 tensor 维度：\(B\times N\times k\times F\);</li><li>然后将本点 \(p_i\) 的特征 concate 到对应的 \(k\) 个点特征，得到 tensor 维度： \(B\times N\times k\times 2F\)；</li><li>不同层 conv，bn，relu 的作用，得到多个 tensor，其维度：\(B\times N\times k\times \{F'|F'_1,...,F'_s\}\)；</li><li>对 \(k\) 个点作最大化聚合，得到各 tensor 维度：\(B\times N\times \{F'|F_1',...,F_s'\}\)</li><li>每个点的特征进行 concate，然后作 conv，bn，relu 操作，最终得到点的特征 tensor，维度为 \(B\times N\times F^{final}\)；</li></ol><p>该实现与式 (6) 有点出入，该实现没有显示计算本点坐标与对应的 \(k\) 个点坐标的差值。但是总体思想一致。</p><h4 id="randla-net7">1.2.6. RandLA-Net<a href="#7" id="7ref"><sup>[7]</sup></a></h4><p>　　设计 \(h _ {\Theta}(p_i,p_j)=\mathrm{MLP}\,\left(p _ j^{\mathrm{exclude}\,xyz}\oplus\left\Vert p _ j^{xyz}-p _ i^{xyz}\right\Vert\oplus (p _ j ^ {xyz}-p _ i^{xyz})\oplus p _ j^{xyz}\oplus p _ i^{xyz}\right)\)，\(\Box=\sum \mathrm{softmax\,MLP}(h_{\Theta}(p_i,p_j))\)，得到 RandLA-Net 中的操作(详见 <a href="/paper-reading-RandLA-Net/" title="RandLA-Net">RandLA-Net</a>)： <span class="math display">\[ p_i&#39; = \displaystyle\sum_{j:(i,j)\in\mathcal{E}}\left(\mathrm{softmax\,MLP&#39;}\,\left(p _ j^{\mathrm{exclude}\,xyz}\oplus\left\Vert p _ j^{xyz}-p _ i^{xyz}\right\Vert\oplus (p _ j ^ {xyz}-p _ i^{xyz})\oplus p _ j^{xyz}\oplus p _ i^{xyz}\right)\right)\cdot \left(\mathrm{MLP}\,\left(p _ j^{\mathrm{exclude}\,xyz}\oplus\left\Vert p _ j^{xyz}-p _ i^{xyz}\right\Vert\oplus (p _ j ^ {xyz}-p _ i^{xyz})\oplus p _ j^{xyz}\oplus p _ i^{xyz}\right)\right) \tag{7}\]</span> 这里的 \(\Box\) 函数称为 Attention Pooling，即将特征维度进行加权求和。</p><h4 id="tanet12">1.2.7. TANet<a href="#12" id="12ref"><sup>[12]</sup></a></h4><p><img src="/PointCloud-Feature-Extraction/TANet.png" width="40%" height="40%" title="图 2. TANet"> 　　如图 2. 所示，TANet 中提出了 TA Module，该模块包含三种注意力机制：point-wise，channel-wise，voxel-wise。其中前两种注意力可用于任意点的特征提取。对应的前两种注意力构成了 \(h _ {\Theta}(p_i,p_j)\) 函数： <span class="math display">\[h_{\Theta} = \left(\mathrm{MLP_1}(\mathrm{MaxPool_{feats}}\,P_i^k) \times \mathrm{MLP_2}(\mathrm{MaxPool_{points}}\, P_i^k)\right) \cdot P_i^k \tag{8}\]</span> 其中 point-wise attention 为 \(\mathrm{MLP_1}(\mathrm{MaxPool_{feats}}\,P_i^k) = S \in \mathbb{R}^{K\times 1}\)；channel-wise attention 为 \(\mathrm{MLP_2}(\mathrm{MaxPool_{points}}\,P_i^k) = T \in \mathbb{R}^{F\times 1}\)；由此构成 \(M=S\times T\in\mathbb{R}^{K\times F}\)，作为权重作用于 \(P_i^k\)，最后用 \(\sum |\mathrm{MAX}\) 操作对点维度进行特征聚合。注意，这里的 point-wise attention 是与点的顺序有关的，看起来这里经过训练，可以消除点顺序的影响。</p><h4 id="pointconv13">1.2.8. PointConv<a href="#13" id="13ref"><sup>[13]</sup></a></h4><p><img src="/PointCloud-Feature-Extraction/PointConv.png" width="60%" height="60%" title="图 3. PointConv"> <img src="/PointCloud-Feature-Extraction/PointConv2.png" width="60%" height="60%" title="图 4. Efficient PointConv"> 　　如图 3. 以及 4. 所示，PointConv 设计的 \(h_{\Theta}\) 有两部分组成。一是根据 \(P_i^k\) 点集计算权重矩阵 \(W\)；二是用核密度函数(Kernel Density Estimation)计算点的密度，然后根据密度计算权重。这里加入基于点密度的权重，是因为，点密度高的区域，需要显式地降低其特征权重，避免最终特征学不到稀疏点的特征。图 4. 是高效版本。</p><h2 id="基于映射空间操作">2. 基于映射空间操作</h2><p>　　基于原始三维空间的点特征提取操作，<strong>其算法复杂度直接依赖点数</strong>；而如果将其映射到高维空间，则点数只会影响映射与反映射的过程，核心特征提取操作将不受点的个数影响。<br>　　三维空间下点云无法有序组织，将点云映射到更高维空间，在高维空间下进行结构化组织后，即可应用传统的卷积操作进行特征提取。</p><h3 id="bilateral-convolutional-layerbcl8splatnet9hplflownet10">2.1. Bilateral Convolutional Layer(BCL<a href="#8" id="8ref"><sup>[8]</sup></a>)(SPLATNet<a href="#9" id="9ref"><sup>[9]</sup></a>/HPLFlowNet<a href="#10" id="10ref"><sup>[10]</sup></a>)</h3><p><img src="/PointCloud-Feature-Extraction/BCL.png" width="60%" height="60%" title="图 3. BCL"> 　　如图 3. 所示，BCL 操作有三部分组成：</p><ul><li><strong>Splat</strong><br>将三维空间的点 \(p_i^{xyz}\) 投影到高维空间，实际操作中直接乘以一个预定义的 \(4\times 3\) 矩阵。4 维空间的晶格顶点聚合晶格内映射点的信息，聚合过程中以映射点与格点的距离作为权重；</li><li><strong>Convolve</strong><br>因为晶格空间内空间是栅格化的，所以直接进行传统的 2D 卷积操作；</li><li><strong>Slice</strong><br>卷积得到的是晶格空间的特征图，反映射到三维空间，即得到点级别的包含周围信息的特征向量；</li></ul><p>　　映射与反映射的操作实现上需要建立哈希表作点的快速查询，需要记录的辅助信息也比较多。后期有时间再对着代码分析。</p><h2 id="参考文献">3. 参考文献</h2><p><a id="1" href="#1ref">[1]</a> Qi, Charles R., et al. &quot;Pointnet: Deep learning on point sets for 3d classification and segmentation.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.<br><a id="2" href="#2ref">[2]</a> Wang, S., Suo, S., Ma, W.C., Urtasun, R.: Deep parameteric convolutional neural networks. In: CVPR (2018)<br><a id="3" href="#3ref">[3]</a> Qi, Charles Ruizhongtai, et al. &quot;Pointnet++: Deep hierarchical feature learning on point sets in a metric space.&quot; Advances in neural information processing systems. 2017.<br><a id="4" href="#4ref">[4]</a> Liu, Xingyu, Charles R. Qi, and Leonidas J. Guibas. &quot;Flownet3d: Learning scene flow in 3d point clouds.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br><a id="5" href="#5ref">[5]</a> Liang, Ming, et al. &quot;Deep continuous fusion for multi-sensor 3d object detection.&quot; Proceedings of the European Conference on Computer Vision (ECCV). 2018.<br><a id="6" href="#6ref">[6]</a> Wang, Yue, et al. &quot;Dynamic graph cnn for learning on point clouds.&quot; ACM Transactions on Graphics (TOG) 38.5 (2019): 146.<br><a id="7" href="#7ref">[7]</a> Hu, Qingyong, et al. &quot;RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds.&quot; arXiv preprint arXiv:1911.11236 (2019).<br><a id="8" href="#8ref">[8]</a> Kiefel, Martin, Varun Jampani, and Peter V. Gehler. &quot;Permutohedral lattice cnns.&quot; arXiv preprint arXiv:1412.6618 (2014).<br><a id="9" href="#9ref">[9]</a> Su, Hang, et al. &quot;Splatnet: Sparse lattice networks for point cloud processing.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.<br><a id="10" href="#10ref">[10]</a> Gu, Xiuye, et al. &quot;Hplflownet: Hierarchical permutohedral lattice flownet for scene flow estimation on large-scale point clouds.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br><a id="11" href="#11ref">[11]</a> Xie, Liang, et al. &quot;PI-RCNN: An Efficient Multi-sensor 3D Object Detector with Point-based Attentive Cont-conv Fusion Module.&quot; arXiv preprint arXiv:1911.06084 (2019).<br><a id="12" href="#12ref">[12]</a> Liu, Zhe, et al. &quot;TANet: Robust 3D Object Detection from Point Clouds with Triple Attention.&quot; arXiv preprint arXiv:1912.05163 (2019).<br><a id="13" href="#13ref">[13]</a> Wu, Wenxuan, Zhongang Qi, and Li Fuxin. &quot;Pointconv: Deep convolutional networks on 3d point clouds.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br><a id="14" href="#14ref">[14]</a> https://github.com/WangYueFt/dcp/blob/master/model.py<br><a id="15" href="#15ref">[15]</a> https://github.com/WangYueFt/dgcnn/blob/master/pytorch/model.py</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　机器学习中，特征提取是非常重要的一个环节（认为是最重要的一环也不为过）。对图像数据的特征提取操作已经较为成熟，如卷积；而点云数据由于无序性，所以对其进行高效的特征提取则比较困难。 一个好的点云特征提取操作需要具备以下特征：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;能提取点云的&lt;str
      
    
    </summary>
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;RandLA-Net&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-RandLA-Net/"/>
    <id>https://leijiezhang001.github.io/paper-reading-RandLA-Net/</id>
    <published>2019-12-04T01:08:09.000Z</published>
    <updated>2019-12-04T06:44:02.741Z</updated>
    
    <content type="html"><![CDATA[<p>　　不同与点云 3D 检测，可以 Voxel 化牺牲一定的分辨率，点云语义分割则要求点级别的分辨率，所以栅格化做点云分割信息会有一定的损失。但是直接对所有点进行特征提取，计算量又相当巨大，为了平衡效率与性能，一般也不得不对点云进行采样处理。这种点云级别的处理方式有 <a href="/PointNet-系列论文详读/" title="PointNet++">PointNet++</a>， <a href="/paperreading-FlowNet3D/" title="FlowNet3D">FlowNet3D</a> 等。 <img src="/paper-reading-RandLA-Net/arch2.png" width="90%" height="90%" title="图 1. RandLA-Net"> 　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出的方法主要为了解决大尺度点云集下，如何高效提取点云局部特征的问题。针对大尺度点云集，作者对比了不同采样算法，得出随机采样最简单高效的结论；针对随机采样丟失信息的问题，以及为了提高局部特征提取能力，本文提出了局部特征聚合(Local Feature Aggregation)模块，该模块包含 Local Spatial Encoding，Attentive Pooling，以及 Dilated Residual Block。<br>　　如图 1. 所示，LFA 作为基本模块用于特征提取，下采样采用随机采用，上采样过程类似图像中的 dconv，包含向上插值以及 MLP 过程。</p><h2 id="sampling">1. Sampling</h2><p>　　关于点云采样，在 <a href="/paperreading-FlowNet3D/" title="FlowNet3D">FlowNet3D</a> 中有简单介绍。本文将采样算法分为两大类：</p><ul><li>Heuristic Sampling<ol type="1"><li>Farthest Point Sampling(FPS)， <a href="/paperreading-FlowNet3D/" title="FlowNet3D">FlowNet3D</a> 中有介绍，是一种均匀采样方法。其算法复杂度为 \(\mathcal{O}(N^2)\)。</li><li>Inverse Density Importance Sampling(IDIS)，计算每个点的密度属性，根据属性选取 K 个点，其复杂度为 \(\mathcal{O}(N)\)。</li><li>Random Sampling(RS)，随机采样，复杂度为 \(\mathcal{O}(1)\)。</li></ol></li><li>Learning-based Sampling<br>...</li></ul><p>　　本文作者认为随机采样复杂度最低，其它采样复杂度太高。我认为也不能这么说，在一定策略及加速下，其它采样算法效率也可以很高。比如栅格化后在采样，可以高效的并行加速，并且使得稀疏区域保留更多信息。</p><h2 id="local-feature-aggregation">2. Local Feature Aggregation</h2><p><img src="/paper-reading-RandLA-Net/arch1.png" width="90%" height="90%" title="图 2. RandLA-Net"> 　　特征提取非常关键，尤其在本文采用随机采样后，稀疏区域信息丢失比较严重的情况下。如图 2. 所示，本文提出了局部特征聚合(Local Feature Aggregation)模块，包含 Local Spatial Encoding，Attentive Pooling，以及 Dilated Residual Block。</p><h3 id="local-spatial-encoding">2.1. Local Spatial Encoding</h3><p>　　在原始点云中提取每个点的局部特征，类似 <a href="/paperreading-FlowNet3D/" title="FlowNet3D">FlowNet3D</a>(PointNet++) 中的 set conv 层，这里多了手工特征信息，其步骤为：</p><ol type="1"><li>针对每个点 \(p_i\)，用 KNN 找到与其最近的 K 个点: \(\{p _ i^1,...p _ i^k,...p _ i^K\}\)；</li><li>针对最近邻的每个点 \(p_i^k\)，设计其相对位置的特征： <span class="math display">\[ \mathrm{r}_i^k = \mathrm{MLP}\left(p_i\oplus p_i^k\oplus (p_i-p_i^k)\oplus ||p_i-p_i^k||\right) \tag{1}\]</span></li><li>针对最近领的每个点 \(p_i^k\)，其本来的特征为 \(\mathrm{f}_i^k\)，叠加相对位置特征 \(\mathrm{r}_i^k\) 后得到每个点的特征为 \(\mathrm{\hat{f}}_i^k\)。由此最近领点集的特征为： \(\mathrm{\hat{F}}_i=\{\hat{\mathrm{f}}_i^1,...\hat{\mathrm{f}}_i^k,...\hat{\mathrm{f}}_i^K\}\)。</li></ol><h3 id="attentive-pooling">2.2. Attentive Pooling</h3><p>　　该模块的作用是聚合 \(p_i\) 的最近邻点集特征 \(\hat{\mathrm{F}}_i\)。PointNet 的 SA 层(FlowNet3D 中的 set conv 层)直接用 Max/Mean 这种对称函数聚合，本文采用一种更有效的基于注意力机制的 pooling 方式，其步骤为：</p><ol type="1"><li>计算注意力分数，对每个特征设计分数计算方式为： <span class="math display">\[ \mathrm{s}_i^k = \mathrm{g}\left(\hat{\mathrm{f}}_i^k, W\right) \tag{2}\]</span> 其中 \(\mathrm{g}\) 表示一个感知机 MLP(W 为其权重) 以及一个 softmax 函数。</li><li>聚合，根据注意力分数，权重求和，得到 \(p_i\) 点的特征： <span class="math display">\[ \bar{\mathrm{f}}_i = \sum_{k=1}^K \left(\hat{\mathrm{f}}_i^k \cdot \mathrm{s}_i^k \right) \tag{3}\]</span></li></ol><h3 id="dilated-residual-block">2.3.  Dilated Residual Block</h3><p><img src="/paper-reading-RandLA-Net/LA.png" width="60%" height="60%" title="图 3. LA Module"> 　　如图 2. 及 3. 所示，连续堆叠多个 LA 模块，能起到增加感受野的效果，然后引入 residual 思想，图 2. 下图就构成了一个 LFA 的基础模块。</p><p><a id="1" href="#1ref">[1]</a> Hu, Qingyong, et al. &quot;RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds.&quot; arXiv preprint arXiv:1911.11236 (2019).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　不同与点云 3D 检测，可以 Voxel 化牺牲一定的分辨率，点云语义分割则要求点级别的分辨率，所以栅格化做点云分割信息会有一定的损失。但是直接对所有点进行特征提取，计算量又相当巨大，为了平衡效率与性能，一般也不得不对点云进行采样处理。这种点云级别的处理方式有 &lt;a h
      
    
    </summary>
    
      <category term="Semantic Segmentation" scheme="https://leijiezhang001.github.io/categories/Semantic-Segmentation/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/tags/Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>Model Compression - &#39;Pruning&#39;</title>
    <link href="https://leijiezhang001.github.io/pruning/"/>
    <id>https://leijiezhang001.github.io/pruning/</id>
    <published>2019-11-15T03:56:23.000Z</published>
    <updated>2019-11-18T11:25:07.739Z</updated>
    
    <content type="html"><![CDATA[<p>　　模型压缩技术主要有：Pruning，Regularization，Quantization，KnowLedge Distillation，Comditional Computation等。本文主要讨论剪枝技术(Pruning)。复杂模型存在存储空间大，计算量大等问题，对其进行剪枝使网络中的权重及特征层稀疏化(Regularization 也是稀疏化的过程)，能获得以下效益：</p><ul><li><strong>模型更小</strong><br>稀疏化的模型含有大量的零值，称为稀疏表达(Sparse Representation)，通过稀疏矩阵压缩技术进行编码压缩后得到压缩表达(Compressed Representation)。片内内存(On-chip Mem)与片外内存(Off-chip Mem)数据的传输可用压缩表达，使实际传输中的模型内存更小，而计算时，可通过反编码算法得到稀疏表达，从而进行正常的矩阵运算；也可以直接用压缩表达进行矩阵运算，这需要特殊的硬件支持，并且稀疏化的过程一般是结构化剪枝(Structured Pruning)或是正则。</li><li><strong>速度更快</strong><br>目前大部分矩阵运算芯片，性能瓶颈都在片内片外内存的带宽，稀疏化后能有效压缩矩阵单元，降低模型传输内存；另一方面，通过结构化的剪枝，在特定硬件下，能直接减少零值运算量。</li><li><strong>能效更高</strong><br>片外内存访问所花费的能量大概比片内内存多两个数量级，所以降低模型的传输内存，甚至将模型及中间计算量(如特征层)直接塞到片内内存，减少与片外内存的交互，能有效提高能效。</li></ul><p>　　剪枝的过程主要是：根据剪枝类型选用对应的稀疏性定义方式；剪枝前模型的敏感度分析；应用剪枝算法及策略。以下根据 Distiller<a href="#1" id="1ref"><sup>[1]</sup></a> 库分别对这三部分进行详细阐述。</p><h2 id="稀疏性定义">1. 稀疏性定义</h2><p>　　剪枝大致可分为 element-wise 剪枝以及 Structured 剪枝，element-wise 剪枝只需要定义每个张量的稀疏性，即 Element-wise Sparsity，而 Structured 剪枝需要定义不同结构的稀疏性，有 Filter-wise Sparsity，Channel-wise Sparsity，Kernel-wise Sparsity，Block-wise Sparsity，Column-wise Sparsity，Row-wise Sparsity。<br>　　设输入特征层 IFM(Input Feature Map)\(\in\mathbb{R}^{N\times C_1\times H_1\times W_1}\)，卷积核 Filter\(\in\mathbb{R}^{C_2\times C_1\times K\times K}\)，则输出特征层 OFM(Output Feature Map)\(\in\mathbb{R}^{N\times C_2\times H_2\times W_2}\)。</p><h3 id="element-wise-sparsity">1.1. Element-wise Sparsity</h3><p>　　张量元素的稀疏性，设 \(X\in\mathbb{R}^{N\times C\times H\times W}\)： <span class="math display">\[\Vert X\Vert_{element-wise} = \frac{l_0(X)}{N\times C\times H\times W} = \frac{\sum_{n=1}^{N}\sum_{c=1}^{C}\sum_{h=1}^{H}\sum_{w=1}^{W}\left\vert X_{n,c,h,w} \right\vert ^0}{N\times C\times H\times W} \tag{1}\]</span> 其中 \(l_0\) 正则根据元素是否为 0，确定输出 0/1。</p><h3 id="filter-wise-sparsity">1.2. Filter-wise Sparsity</h3><p>　　对于有 \(C_2\) 个卷积核的 Filter\(\in\mathbb{R}^{C_2\times C_1\times K\times K}\)，其 Filter-wise 的稀疏性可表示为： <span class="math display">\[\Vert X\Vert_{filter-wise} = \frac{\sum_{c_2=1}^{C_2}\left\vert\sum_{c_1=1}^{C_1}\sum_{k_1=1}^{K}\sum_{k_2=1}^{K}\vert X_{c_2,c_1,k_1,k_2}\vert \right\vert ^0}{C_2} \tag{2}\]</span></p><h3 id="kernel-wise-sparsity">1.3. Kernel-wise Sparsity</h3><p>　　卷积核 Filter\(\in\mathbb{R}^{C_2\times C_1\times K\times K}\) 拥有 \(C_2\times C_1\) 个 \(K\times K\) 大小的 Kernel，其 Kernel-wise 的稀疏性可表示为： <span class="math display">\[\Vert X\Vert_{kernel-wise} = \frac{\sum_{c_2=1}^{C_2}\sum_{c_1=1}^{C_1}\left\vert\sum_{k_1=1}^{K}\sum_{k_2=1}^{K}\vert X_{c_2,c_1,k_1,k_2}\vert \right\vert ^0}{C_2\times C_1} \tag{3}\]</span></p><h3 id="channel-wise-sparsity">1.4. Channel-wise Sparsity</h3><p>　　对于张量单元 \(X\in\mathbb{R}^{N\times C\times H\times W}\)： <span class="math display">\[\Vert X\Vert_{channel-wise} = \frac{\sum_{c=1}^{C}\left\vert\sum_{n=1}^{N}\sum_{h=1}^{H}\sum_{w=1}^{W}\vert X_{n,c,h,w}\vert \right\vert ^0}{C} \tag{4}\]</span></p><h3 id="column-wise-sparsity">1.5. Column-wise Sparsity</h3><p>　　对于张量单元 \(X\in\mathbb{R}^{H\times W}\)： <span class="math display">\[\Vert X\Vert_{column-wise} = \frac{\sum_{h=1}^{H}\left\vert\sum_{w=1}^{W}\vert X_{h,w}\vert \right\vert ^0}{H} \tag{5}\]</span></p><h3 id="row-wise-sparsity">1.6. Row-wise Sparsity。</h3><p>　　对于张量单元 \(X\in\mathbb{R}^{H\times W}\)： <span class="math display">\[\Vert X\Vert_{row-wise} = \frac{\sum_{w=1}^{W}\left\vert\sum_{h=1}^{H}\vert X_{h,w}\vert \right\vert ^0}{W} \tag{6}\]</span></p><h3 id="block-wise-sparsity">1.7. Block-wise Sparsity</h3><p>　　对于张量单元 \(X\in\mathbb{R}^{N\times C\times H\times W}\)，设定 block\(\in\mathbb{R}^{repetitions\times depth\times 1\times1}\)，由此将 \(X\) 划分为 \(\frac{N\times C}{repetitions\times depth}\times (repetitions\times depth)\times (H\times W)=N'\times B\times K\)。block-sparsity 定义为： <span class="math display">\[\Vert X\Vert_{block-wise} = \frac{\sum_{n=1}^{N&#39;}\sum_{k=1}^K\left\vert\sum_{b=1}^{B}\vert X_{n,b,k}\vert \right\vert ^0}{N&#39;\times K} \tag{7}\]</span></p><h2 id="模型敏感度分析sensitivity-analysis">2. 模型敏感度分析(Sensitivity Analysis)</h2><p>　　在剪枝前，我们首先要确定减哪几层，每层减多少(即剪枝阈值或剪枝程度)。这就涉及到模型中每层网络对模型输出的敏感度分析(Sensitivity Analysis)。<a href="#2" id="2ref">[2]</a> 提出了一种有效的方法来确定每层的敏感度。在一个已训练模型下，分别对每一层进行不同程度的剪枝，得到对应的网络输出精度，绘制敏感度曲线。<br><img src="/pruning/sensitivity.png" width="70%" height="70%" title="图 1. 敏感度分析"> 　　如图 1. 所示，AlexNet 网络各层对 element-wise 剪枝的敏感度曲线显示，越深的网络层对输出越不敏感，尤其是全连接层，所以剪枝程度可以更高。而对于非常敏感的浅层网络，则需要降低剪枝程度，甚至不剪枝。</p><h2 id="剪枝算法">3. 剪枝算法</h2><h3 id="magnitude-pruner">3.1. Magnitude Pruner</h3><p>　　这是最基本的剪枝方法，对于要剪枝的对象，判断其绝对值是否大于阈值 \(\lambda\)，如果小于阈值，则将该对象置为零。该对象可以是 element-wise，也可以是其它结构化的对象，如 filter，Kernel 等。<br>　　该方法需要直接设定阈值，而阈值的设定是比较困难的。</p><h3 id="sensitivity-pruner">3.2. Sensitivity Pruner</h3><p>　　卷积网络每层的权重值为高斯分布，由高斯分布的性质可知，在标准差 \(\sigma\) 内，有 68% 的元素，所以阈值可设定为 \(\lambda=s\times \sigma\)，其表示了 \(s\times 68\%\) 的元素被剪枝掉。</p><h3 id="level-pruner">3.3. Level Pruner</h3><p>　　Level Pruner 直接设定需要剪枝的比例，即直接设定剪枝后的稀疏性，这比前两种方法更加稳定。具体做法就是对每个对象进行排序，然后以此裁剪，直到裁剪到设定的比例。</p><h3 id="automated-gradual-pruneragp">3.4. Automated Gradual Pruner(AGP)</h3><p>　　<a href="#3" id="3ref">[3]</a>提出了一种训练剪枝的方法，在 Level Pruner 基础上，随着训练的过程，设计剪枝的稀疏性从初始的 \(s_i\) 增加到目标 \(s_f\)，其数学表示为： <span class="math display">\[ s_t = s_f+(s_i-s_f)\left(1-\frac{t-t_0}{n\Delta t}\right)^3 \; \mathrm{for} \, t\in \{t_0, t_0+\Delta t,...,t_0+n\Delta t\} \tag{8}\]</span> 实现的效果是，初始阶段，剪枝比较厉害，越到最后，剪枝的量越少，直到达到目标剪枝值。</p><h3 id="structure-pruners">3.5. Structure Pruners</h3><p>　　这里讨论结构化剪枝中 Filter 以及 Channel 的剪枝<a href="#4" id="4ref"><sup>[4]</sup></a>，对应的需要用到前面提到的 Filter-wise 以及 Channel-wise 的稀疏性。不同于 element-wise 剪枝，结构化剪枝由于网络的连接性会更复杂，这里考虑三种链接情况。</p><h4 id="连接结构1">3.5.1. 连接结构1</h4><p><img src="/pruning/filter1.png" width="70%" height="70%" title="图 2. 连接结构1"> 　　如图 2. 所示，设第\(i\)层特征 \(X_i\in\mathbb{R}^{C_i\times H_i\times W_i}\)，经过卷积核 \(\mathcal{F}\in\mathbb{R}^{C_{i+1}\,\times\, C_i\,\times\,K\,\times\,K}\)后得到第 \(i+1\)层特征层 \(X_{i+1}\in\mathbb{R}^{C_{i+1}\,\times\, H_{i+1}\,\times\, W_{i+1}}\)。图中绿色及黄色代表剪枝掉的 Filter，对应的输出少了这两个卷积计算得到的 channel 维度的两个特征图，再往后就是去除 BN 里面对应 channel 层的 scale 以及 shift 信息(Distiller 中自动删除)，最后再次应用的卷积核需要去除对应的 channel，即类似做 channel-wise 剪枝。由此可见，结构化剪枝会影响后面的网络结构，需要根据网络信息流作网络调整。<br>　　第 \(i\) 卷积层运算量 MAC 为 \(C_{i+1}C_iK^2H_{i+1}W_{i+1}\)，如果剪枝掉 \(m\) 个卷积核，那么第 i 层卷积减少的运算量为 \(mC_iK^2H_{i+1}W_{i+1}\)，下一层 \(i+1\) 卷积层减少的运算量为 \(C_{i+2}mK^2H_{i+2}W_{i+2}\)。所以在第 \(i\) 层剪枝掉 \(m\) 个卷积核，会使第 \(i,i+1\) 层的运算量各减少 \(m/C_{i+1}\)。</p><h4 id="连接结构2">3.5.2. 连接结构2</h4><p><img src="/pruning/filter2.png" width="60%" height="60%" title="图 3. 连接结构2"> 　　如图 3. 所示，与被剪枝的特征图直连的卷积核均需要作 channel 维度的裁剪，这一步在 Distiller 中自动进行。</p><h4 id="连接结构3">3.5.3. 连接结构3</h4><p><img src="/pruning/filter3.png" width="60%" height="60%" title="图 4. 连接结构3"> 　　如图 4. 所示，如果两个卷积层的输出要做 element-wise 相加操作，那么就要求两个卷积层的输出大小要一样。这就要求对这两个卷积层的卷积核裁剪过程要一样，包括裁剪的卷积数量以及卷积位置。这需要在 Distiller 中显示的指定。</p><h2 id="参考文献">4. 参考文献</h2><p><a id="1" href="#1ref">[1]</a> https://nervanasystems.github.io/distiller/index.html<br><a id="2" href="#2ref">[2]</a> Han, Song, et al. &quot;Learning both weights and connections for efficient neural network.&quot; Advances in neural information processing systems. 2015.<br><a id="3" href="#3ref">[3]</a> Zhu, Michael, and Suyog Gupta. &quot;To prune, or not to prune: exploring the efficacy of pruning for model compression.&quot; arXiv preprint arXiv:1710.01878 (2017).<br><a id="4" href="#4ref">[4]</a> Li, Hao, et al. &quot;Pruning filters for efficient convnets.&quot; arXiv preprint arXiv:1608.08710 (2016).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　模型压缩技术主要有：Pruning，Regularization，Quantization，KnowLedge Distillation，Comditional Computation等。本文主要讨论剪枝技术(Pruning)。复杂模型存在存储空间大，计算量大等问题，对
      
    
    </summary>
    
      <category term="Model Compression" scheme="https://leijiezhang001.github.io/categories/Model-Compression/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Model Compression" scheme="https://leijiezhang001.github.io/tags/Model-Compression/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;3D-LaneNet End-to-End 3D Multiple Lane Detection&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-3D-LaneNet-End-to-End-3D-Multiple-Lane-Detection/"/>
    <id>https://leijiezhang001.github.io/paper-reading-3D-LaneNet-End-to-End-3D-Multiple-Lane-Detection/</id>
    <published>2019-11-09T09:48:38.000Z</published>
    <updated>2019-11-12T04:22:48.736Z</updated>
    
    <content type="html"><![CDATA[<p>　　在上一篇 paper reading <a href="/paper-reading-Deep-Multi-Sensor-Lane-Detection/" title="Deep Multi-Sensor Lane Detection">Deep Multi-Sensor Lane Detection</a> 中，最后我提到一个思考点：借鉴 STN 的思路，用前视图直接去回归 IPM 变换需要的矩阵参数。本文<a href="#1" id="1ref"><sup>[1]</sup></a>就是采用了这种思路！ <img src="/paper-reading-3D-LaneNet-End-to-End-3D-Multiple-Lane-Detection/res.png" width="60%" height="60%" title="图 1. 方法概图"> 如图 1. 所示，车道线检测还是在俯视图下来做的，车道线输出是三维曲线，一定程度上估计出了地面高度。</p><h2 id="网络结构">1. 网络结构</h2><p><img src="/paper-reading-3D-LaneNet-End-to-End-3D-Multiple-Lane-Detection/arch.png" width="90%" height="90%" title="图 2. 网络结构"> 　　如图 2. 所示，网络有两部分组成：</p><ul><li>Image-view 通路<br>输入为前视图图像，输出相机 pitch 角度 \(\theta\) 以及相机高度 \(H\)，这里假设相机坐标系相对地面坐标系没有 roll，yaw 偏转，由此可得到相机外参矩阵，用于 IPM 变换；</li><li>Top-view 通路<br>输入为前视图某个特征层经过 Projective Transformation Layer 变换后的特征，之后的特征层叠加来自经过变换的前视图特征层，最后输出车道线检测；</li></ul><h3 id="projective-transformation-layer">1.1. Projective Transformation Layer</h3><p>　　<a href="/lane-det-from-BEV/" title="Apply IPM in Lane Detection from BEV">Apply IPM in Lane Detection from BEV</a> 中较详细得阐述了 IPM 原理，<a href="/paper-reading-Deep-Multi-Sensor-Lane-Detection/" title="Deep Multi-Sensor Lane Detection">Deep Multi-Sensor Lane Detection</a> 则阐述了 STN 的原理。Projective Transformation Layer 类似 STN 的结构，输入相机内外参后，沿用 STN 中的 Grid Generator 以及 Sampler 模块，Grid Generator 就是 IPM 的过程。此外，Projective Transformation Layer 还增加一个卷积层，将前视图的 C 维特征卷积为 C/2 维特征与俯视图的特征层进行叠加。<br>　　该层不仅从前视图特征层上产生了俯视图特征，还融合了前视图与俯视图特征层，融合前视图特征有两大好处：</p><ul><li>瘦高型物体，如栅栏，行人，在俯视图下信息量很小，而前视图能有效提取丰富特征；</li><li>远距离时，俯视图下的信息会比较稀疏(类似点云)，而前视图信息会比较密集，能有效提取远距离下的信息特征；</li></ul><h3 id="anchor-based-lane-prediction">1.2. Anchor-Based Lane Prediction</h3><p><img src="/paper-reading-3D-LaneNet-End-to-End-3D-Multiple-Lane-Detection/anchor.png" width="60%" height="60%" title="图 3. Anchor-Based Lane Prediction"> 　　如图 3. 所示，作者提出了一种 Anchor-Based 车道线检测方法，其实这和目标检测中的 Anchor-Based 还是不太一样，这里的 Anchor 指的是几条线。设定 \(y\) 方向的 anchor 线段：\(\{X_A^i\} _ {i=1}^N\)，\(y\) 坐标上的预定义位置：\(\{y_j\} _ {j=1}^K\)。对于每个 anchor 线段，分类上以 \(Y_{ref}\) 为基准，输出三种类别(距离 \(Y_{ref}\) 最近的线的类型)，两种车道中心线，一种车道线，即 \(\{c_1,c_2,d\}\)；回归上每种类别都输出 2K 个 Offsets：\(\{(x_j ^ i,z_j ^ i)\} _ {j=1}^K\)，对应的第 \(i\) 个 anchor，在第 \(j\) 位置上的 3D 点表示为 \((x_j ^ i+X_A ^ i,y_j,z_j ^ i)\in\mathbb{R}^3\)。综上网络输出 \(N\times(3(2K+1))\) 维的向量，最后经过 1D NMS 处理后，每个 anchor 上的 3D 点通过样条插值出 3D 线条。</p><h2 id="loss">2. Loss</h2><p>　　训练阶段，真值如何匹配 anchor 很重要，过程如下：</p><ol type="1"><li>将所有车道线以及车道中心线通过 IPM 投影到俯视图下；</li><li>在 \(Y_{ref}\) 位置上将每条线匹配给 \(x\) 方向距离最近的 anchor 线段；</li><li>对于每个 anchor 上匹配到的线，将最左边的车道线与中心线赋为 \(d,c_1\)，如果还有其它中心线，则赋为 \(c_2\)；</li></ol><p>对于没有穿过 \(Y_{ref}\) 的车道线，则予以忽略，中心线理论上都会穿过 \(Y_{ref}\)。所以理论上，本文预测的中心线是全的，而车道线会不全，前方的岔路口，一部分车道线不会被预测出来。<br>　　Loss 项有四部分组成，分别为车道线分类，车道线锚点 Offsets 回归，相机外参 pitch 角 \(\theta\) 以及高度 \(h_{cam}\) 的回归，如下： <span class="math display">\[\begin{align}\mathcal{L} =&amp; - \sum_{t\in\{c_1,c_2,d\}} \sum_{i=1}^N\left(\hat{p}_t^i\mathrm{log}p_t^i + \left(1-\hat{p}_t^i\right)\mathrm{log}\left(1-p_t^i\right)\right) \\&amp;+ \sum _ {t\in\{c_1,c_2,d\}}\sum_{i=1}^N \hat{p}_t^i\left(\left\Vert x_t^i-\hat{x}_t^i\right\Vert+\left\Vert z_t^i-\hat{z}_t^i\right\Vert\right) \\&amp;+ \left|\theta-\hat{\theta}\right| + \left|h_{cam}-\hat{h}_{cam}\right| \tag{1}\end{align}\]</span></p><h2 id="参考文献">3. 参考文献</h2><p><a id="1" href="#1ref">[1]</a> Garnett, Noa, et al. &quot;3D-LaneNet: end-to-end 3D multiple lane detection.&quot; Proceedings of the IEEE International Conference on Computer Vision. 2019.<br><a id="2" href="#2ref">[2]</a> <a href="/lane-det-from-BEV/" title="Apply IPM in Lane Detection from BEV">Apply IPM in Lane Detection from BEV</a><br><a id="3" href="#3ref">[3]</a> <a href="/paper-reading-Deep-Multi-Sensor-Lane-Detection/" title="Deep Multi-Sensor Lane Detection">Deep Multi-Sensor Lane Detection</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　在上一篇 paper reading &lt;a href=&quot;/paper-reading-Deep-Multi-Sensor-Lane-Detection/&quot; title=&quot;Deep Multi-Sensor Lane Detection&quot;&gt;Deep Multi-Senso
      
    
    </summary>
    
      <category term="Lane Detection" scheme="https://leijiezhang001.github.io/categories/Lane-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Lane Detection" scheme="https://leijiezhang001.github.io/tags/Lane-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Deep Multi-Sensor Lane Detection&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-Deep-Multi-Sensor-Lane-Detection/"/>
    <id>https://leijiezhang001.github.io/paper-reading-Deep-Multi-Sensor-Lane-Detection/</id>
    <published>2019-11-09T03:54:34.000Z</published>
    <updated>2019-11-09T13:54:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　前文 <a href="/lane-det-from-BEV/" title="Apply IPM in Lane Detection from BEV">Apply IPM in Lane Detection from BEV</a> 已经较详细得阐述了俯视图下作车道线检测的逆透视原理，提到传统 IPM 有个较强的假设：地面是平坦的。对于 L4 自动驾驶，在这个假设下车道线检测不管是精度还是可靠性，都远远不够。如果有高精度地图，那么这些问题都有方法来消除。当然，如果有高精度地图，且自定位准确，也就不需要车道线检测了，所以这里讨论，在无高精度地图下，本文<a href="#1" id="1ref"><sup>[1]</sup></a>如何通过激光点云数据学习的方法解决上述问题。</p><h2 id="网络结构">1. 网络结构</h2><p><img src="/paper-reading-Deep-Multi-Sensor-Lane-Detection/lane_det.png" width="90%" height="90%" title="图 1. Multi-Sensor Lane Detection"> 　　如图 1. 所示，整个算法有两个网络组成：</p><ul><li><strong>地面估计(Ground Height Estimation)网络</strong><br>输入是俯视图下历史 N 帧的栅格点云，输出的是俯视图下地面高度；</li><li><strong>车道线检测(Lane Prediction)网络</strong><br>输入是俯视图下历史 N 帧的栅格点云，并且叠加前视图图像逆透视变换到俯视图后的图像，输出为像素级别的车道线检测结果；</li></ul><p>历史 N 帧点云需要经过 ego-motion 补偿到当前本车位置，补偿后的点云只对运动物体会存在变形，而网络正好需要忽视运动物体。通过地面估计得到了俯视图下稠密的地面估计后，就可以将前视图的图像投影到俯视图下了。具体的过程为：取地面估计的三维点(高度+像素坐标经过分辨率变换后的物理坐标)，投影到图像上，然后双线性插值取得图像像素值，填充至俯视图上。这种透视变换是借助 3D 点信息完成的，原理可详见 <a href="/lane-det-from-BEV/" title="Apply IPM in Lane Detection from BEV">Apply IPM in Lane Detection from BEV</a>。</p><h2 id="differentiable-warping-function">2. Differentiable Warping Function</h2><p>　　其实这里估计出来的地面高度就是个简陋的高精度地图，所以这种方案理论上就能消除上述问题。并且，投影的过程采用了可求导的映射方程(differentiable warping function)，所以整个算法可以端到端的训练。 <img src="/paper-reading-Deep-Multi-Sensor-Lane-Detection/STN.png" width="90%" height="90%" title="图 2. Spatial Transformer Networks"> 　　关于可求导的映射方程，这里借鉴了 DeepMind 的 Spatial Transformer Networks<a href="#2" id="2ref"><sup>[2]</sup></a> 的思想。传统卷积网络只对较小的位移有位移不变性，而 STN 引入 2D/3D 仿射/透视变换，显示得将特征层变换到有利于分类的形态，这样整个网络就具有了仿射甚至透视(位移，旋转，裁剪，尺度，歪斜)不变性。如图 2. 所示，STN 有三部分构成：</p><ol type="1"><li><strong>Localisation Net</strong>，对于 2D 仿射，回归预测出仿射变换矩阵 \(\theta \in \mathbb{R}_{2\times 3}\);</li><li><strong>Grid Generator</strong>，根据仿射变换矩阵及仿射变换前后特征图的大小，建立仿射前后坐标映射关系；</li><li><strong>Sampler</strong>，根据坐标映射关系设计可求导的插值采样方法(如双线性)，从输入特征中采样出特征值填入仿射后的特征图中；</li></ol><p>　　本文则是一个透视变换矩阵 \(P\)，但是 \(P\) 不需要网络预测，其完全由激光雷达与相机的内外参决定，这个需要提前标定好。预测的地面高度通过 <a href="/lane-det-from-BEV/" title="Apply IPM in Lane Detection from BEV">Apply IPM in Lane Detection from BEV</a> 中的式 (3) 即可与图像坐标系建立联系，作为 Grid Generator。最后采用可求导的 Sampler，这个模块就可以嵌入到网络中，进行端到端的训练。</p><h2 id="loss">3. Loss</h2><p>　　Loss 采用 SmoothL1 Loss，其有两种构成：</p><ul><li>地面估计项<br><span class="math display">\[ L_{gnd} = \sum_{p\in Output Image} \Vert z_{p,gt}-z_{p,pred}\Vert \tag{1}\]</span></li><li>车道线检测项<br><span class="math display">\[ L_{lane} = \sum_{p\in Output Image} \left\Vert \left(\tau-\mathrm{min}\{d_{p,gt}, \tau\}\right)-d_{p,pred}\right\Vert \tag{2}\]</span> 其中 \(\tau\) 是车道线真值标签的衰减像素区域，高速场景设为 30，城市道路设为 20。</li></ul><h2 id="其它思考">4. 其它思考</h2><p>　　既然 STN 专门是用来作仿射/透视变换的，那么是否可以在不借助激光点云的情况下，用前视图图像直接回归出透视变换到俯视图的透视矩阵 \(P\) ？理论上是可行的，但是训练过程不一定能收敛，需要精心设计训练过程，以及针对斜坡还会有一定的距离误差。</p><h2 id="参考文献">5. 参考文献</h2><p><a id="1" href="#1ref">[1]</a> Bai, Min, et al. &quot;Deep Multi-Sensor Lane Detection.&quot; 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018.<br><a id="2" href="#2ref">[2]</a> Jaderberg, Max, Karen Simonyan, and Andrew Zisserman. &quot;Spatial transformer networks.&quot; Advances in neural information processing systems. 2015.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　前文 &lt;a href=&quot;/lane-det-from-BEV/&quot; title=&quot;Apply IPM in Lane Detection from BEV&quot;&gt;Apply IPM in Lane Detection from BEV&lt;/a&gt; 已经较详细得阐述了俯视图下作车道
      
    
    </summary>
    
      <category term="Lane Detection" scheme="https://leijiezhang001.github.io/categories/Lane-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Lane Detection" scheme="https://leijiezhang001.github.io/tags/Lane-Detection/"/>
    
  </entry>
  
  <entry>
    <title>Anchor-Free Detection</title>
    <link href="https://leijiezhang001.github.io/Anchor-Free-Detection/"/>
    <id>https://leijiezhang001.github.io/Anchor-Free-Detection/</id>
    <published>2019-11-04T04:38:25.000Z</published>
    <updated>2019-11-07T14:00:50.232Z</updated>
    
    <content type="html"><![CDATA[<p>　　3D 目标检测的技术思路大多数源自 2D 目标检测，所以图像 2D 检测的技术更迭极有可能在将来影响 3D 检测的发展。目前 3D 检测基本还是 Anchor-Based 方法(也称为 Top-Down 方法)，而今年以来，Anchor-Free(也称为 bottom-Up 法) 的 2D 检测已经达到了 SOTA，所以本文来探讨下 Anchor-Free 的目标检测方法发展历程。<br><img src="/Anchor-Free-Detection/history.jpg" width="90%" height="90%" title="图 1. 目标检测发展历程"> 　　如图 1. 所示(图片出自<a href="https://zhuanlan.zhihu.com/p/82491218" class="uri" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/82491218</a>)，每种技术思路的发展都是为了解决目标检测中的一些痛点，这些技术思路又交相互用，才推动目标检测往更简单、更高性能方向发展。列举一些主要的痛点：</p><ul><li><strong>正负样本不均衡及无法区分困难样本导致网络学习困难</strong>，two-stage;</li><li><strong>网络及后处理复杂</strong>，one-stage，包含 Anchor-Free 方法；</li><li><strong>尺度问题很难同时检测大小目标</strong>，pyramid-scale；</li><li><strong>框与特征的对齐问题导致提取出的目标特征有偏差</strong>，deformable；</li></ul><p>　　本文包含两大块，一块是 Anchor-Free 方法的概括总结，另一块是代表算法的详细分析。</p><h2 id="归纳总结">1. 归纳总结</h2><p>　　首先推荐下乃爷写的文章——<a href="https://zhuanlan.zhihu.com/p/68291859" target="_blank" rel="noopener">聊聊 Anchor 的“前世今生”</a>，高屋建瓴。本节也是打算聊聊 Anchor-Free 方法的来龙去脉，以及归纳总结下各算法的思路。<br>　　由之前讨论的，其中一个比较大的问题是，目标检测中正负样本严重不平衡。这会导致网络学习时很难针对性的学习困难样本，而 two-stage 相比 one-stage 多了一级正样本的删选，所以在没有额外困难样本选择策略的情况下，two-stage 普遍比 one-stage 效果好。可以想象的是，更多 stage 这种级联结构效果会更好，但是网络会变得相当复杂。这个痛点极大地阻碍了 one-stage 以及 Anchor-Free(负样本更多) 方法的发展，OHEM 困难样本学习当然是种有效的方法，但是还不够，直到 RetinaNet 中 Focal Loss 的提出，有效解决了正负样本严重不均衡所导致的学习困难问题。由此不仅 Anchor-based one-stage 方法性能达到了 two-stage 高度，甚至 Anchor-Free 方法性能也达到了 SOTA。<br>　　回顾 Anchor-Free 检测，最早的应该是 YOLO-v1<a href="#1" id="1ref"><sup>[1]</sup></a>，DenseBox<a href="#2" id="2ref"><sup>[2]</sup></a>，而 RetinaNet<a href="#3" id="3ref"><sup>[3]</sup></a> 中 Focal Loss 的提出，使得 Anchor-Free 方法引来爆发式发展。大体上可分为两种：</p><ol type="1"><li><strong>回归目标角点，后处理需要匹配角点以生成目标框</strong>，以 CornerNet<a href="#4" id="4ref"><sup>[4]</sup></a> 为代表的一系列改进方法 CornerNet-Lite<a href="#5" id="5ref"><sup>[5]</sup></a>，CenterNet(KeyPoint Triplets)<a href="#6" id="6ref"><sup>[6]</sup></a>，ExtremeNet<a href="#7" id="7ref"><sup>[7]</sup></a>等；</li><li><strong>像素级别预测目标框的不同编码量</strong>，后处理很容易生成目标框，有 CenterNet(Objects as Points)<a href="#8" id="8ref"><sup>[8]</sup></a>，FCOS<a href="#9" id="9ref"><sup>[9]</sup></a>，FoveaBox<a href="#10" id="10ref"><sup>[10]</sup></a>，FSAF<a href="#11" id="11ref"><sup>[11]</sup></a>等；</li></ol><p>回归角点的方法继承了人体姿态估计的很多策略，backbone 都使用 Hourglass<a href="#14" id="14ref"><sup>[14]</sup></a> 网络，在单尺度上能提取有效的特征；而像素级别预测目标框的不同编码量，引入了 FPN<a href="#15" id="15ref"><sup>[15]</sup></a> 网络进行多尺度检测，解决大小框在同一中心点或有相同角点的情况(CenterNet 还是使用了 Hourglass 网络，因为单尺度能很容易融合 3D 检测，人体姿态估计等任务)。此外，RepPoints<a href="#12" id="12ref"><sup>[12]</sup></a> 延续了 Deformable Conv 的工作，去掉了角点的框约束，使得角点一定贴合目标的边缘，本质上基本解决了以上所列的问题，其思想很值得借鉴。</p><h2 id="cornernet4-cornernet-lite5">2. CornerNet<a href="#4" id="4ref"><sup>[4]</sup></a>, CornerNet-Lite<a href="#5" id="5ref"><sup>[5]</sup></a></h2><h3 id="网络结构">2.1. 网络结构</h3><p><img src="/Anchor-Free-Detection/CornerNet-arch.png" width="80%" height="80%" title="图 2.1. CornerNet 框架"> <img src="/Anchor-Free-Detection/CornerNet-arch2.png" width="80%" height="80%" title="图 2.2. CornerNet 网络结构"> 　　如图 2.1 与 2.2 所示，CornerNet 的 backbone 采用了人体关键点检测中常用的 Hourglass 网络，这种沙漏网络类似多层 FPN，能有效提取细节信息；网络最终输出的是 Top-Left Corners Heatmaps，Bottom-Right Corners Heatmaps，以及对应的 Embeddings，Offsets。这里以 Top-Left Corners 分支为例，说明其网络计算过程。 <img src="/Anchor-Free-Detection/CornerNet-block.png" width="80%" height="80%" title="图 2.3. CornerNet"> 　　如图 2.3. 所示，这里引入 Corner Pooling Module，该模块能提取角点的上下文信息，其计算过程是行最大值与列最大值的叠加。网络输出的:</p><ul><li>Score Heatmaps \(\in\mathbb{R}^{C\times H\times W}\)，每个 Channel 的监督项是个二值图，代表了是否是该类别下的角点；</li><li>Embeddings \(\in\mathbb{R}^{C\times H\times W}\)，每个角点都会预测一个 Embedding 值(度量空间下的值)，用来对 top-left 与 bottom-right 角点的配对；</li><li>Offsets \(\in\mathbb{R}^{2\times H\times W}\)，由于 \(H\times W\) 可能是原图的下采样，所以变换到原图的角点坐标会有离散偏差，需要预测 Offsets 修正，类别无关或者类别有关都可以；</li></ul><p>Inference 阶段，得到这三个输出后，还需要进行后处理才能得到目标检测框。后处理过程为：</p><ol type="1"><li>对 Heatmaps 采用点 NMS 处理(可通过 \(3\times 3\) max-pooling 实现)得到分数最高的前 100 个 top-left 角点以及前 100 个 bottom-right 角点；</li><li>类内计算 top-left 角点与 bottom-right 角点的 Embedding L1 距离，删除大于 0.5 的配对；</li><li>通过 Offsets 调整配对的角点值；</li><li>计算配对的角点值的平均分数，作为该目标框的分数；</li></ol><p>相比 Anchor-based 方法，整个后处理还是相对较为简单，没有框之间的 IoU 计算。</p><h3 id="loss">2.2. Loss</h3><p>　　网络训练的 Loss 表示为： <span class="math display">\[ L= L_{det} + \alpha L_{pull} + \beta L_{push} + \gamma L_{off} \tag{2.1}\]</span> 其中 \(L_{det}\) 是角点检测的 Loss 项，\(L_{pull}, L_{push}\)是 Embedding 距离监督项，\(L_{off}\)是 Offsets 的 Loss 项；\(\alpha,\beta,\gamma\)是权重。</p><ul><li>\(L_{det}\)<br>角点检测是 pixel-level 的检测，每个角点虽然只有一个真值，但是靠近角点的像素点作为角点而构成的目标框与真值框重合度也会较高，所以在真值角点处设计高斯函数 \(e^ {-\frac{x^ 2+y^ 2}{2(r/3) ^ 2}}\) 作为标签衰减函数，\(r\) 值等于真值角点周围定义的圆的半径。圆半径由以下准则确定：四个角点为中心构成四个圆，在这区域内构成的目标框与真值框的 IoU 要小于 \(t\)(文中设为 0.3)。所以这里引入超参数 \(t\)。由此得到检测的 Loss 项： <span class="math display">\[ L_{det} = -\frac{1}{N}\sum_{c=1}^C\sum_{i=1}^H\sum_{j=1}^W\left\{\begin{array}{l}(1-p_{cij})^{\alpha} \mathrm{log}(p_{cij}) &amp; \mathrm{if} \; y_{cij}=1\\(1-y_{cij})^{\beta} (p_{cij})^{\alpha} \mathrm{log}(1-p_{cij}) &amp; \mathrm{otherwise}\end{array}\tag{2.2}\right.\]</span> 其中 \(p_{cij}\) 代表 Heatmaps 中 \(c\) 类别的 \((i,j)\) 位置预测的角点分数，\(y_{cij}\) 表示经过高斯衰减后的真值标签值。可以看出这是 Focal Loss 的变种，对平衡正负样本及学习困难样本有重要作用。</li><li>\(L_{off}\)<br>原图点 \((x,y)\) 经过网络下采样后变换到 \((\lfloor\frac{x}{n}\rfloor,\lfloor\frac{y}{n}\rfloor)\)，与真值的 Offset 可表示为 \(\mathbf{o}_k=(\frac{x_k}{n}-\lfloor\frac{x_k}{n}\rfloor\, \frac{y_k}{n}-\lfloor\frac{y_k}{n}\rfloor)\)，由此可得 Offsets 的 Loss 项： <span class="math display">\[ L_{off}=\frac{1}{N}\sum_{k=1}^N\mathrm{SmoothL1Loss}(\mathbf{o}_k,\mathbf{\hat{o}}_k) \tag{2.3}\]</span></li><li>\(L_{pull}, L_{push}\)<br>每个角点都会预测一个 Embedding 值，期望的是，同一个目标框的 top-left 角点与 bottom-right 角点的 Embedding 值要相近，不同框的角点的 Embedding 值差异要大，由此设计： <span class="math display">\[\left\{\begin{array}{l}L_{pull} = \frac{1}{N}\sum_{k=1}^N\left[(e_{t_k}-e_k)^2+(e_{b_k}-e_k)^2\right] \\L_{push} = \frac{1}{N(N-1)}\sum_{k=1}^N\sum_{j=1, j\not=k}^N\mathrm{max}(0,\Delta-|e_k-e_j|)\end{array}\tag{2.4}\right.\]</span> 其中 \(e_{t_k}, e_{b_k}\) 分别表示 top-left 角点与 bottom-right 角点的 Embedding 值，\(e_k\) 是二者的平均值，\(\Delta\) 设定为 1。与 Offsets 一样，该 Loss 项也只作用于真值角点。</li></ul><h2 id="centernet-keypoint-triplets6">3. CenterNet: KeyPoint Triplets<a href="#6" id="6ref"><sup>[6]</sup></a></h2><h3 id="网络结构-1">3.1. 网络结构</h3><p><img src="/Anchor-Free-Detection/CenterNetKey-arch.png" width="80%" height="80%" title="图 3.1. CenterNet 框架"> 　　CenterNet 的 Motivation是：<strong>CornerNet 的 corner pooling 对目标框内的特征提取能力有限，以及角点匹配得到的目标框在没有其它约束下有时候检测结果会出错。</strong>由此，如图 3.1 所示，CenterNet 在 CornerNet 基础上增加了 Center 的预测分支，并引入 center pooling 以及 cascade corner pooling 模块。<br>　　Inference 处理时，预测的 Center 点用于删除不合理的框。具体的，取 top-left 角点与 bottom-right 角点匹配后得到的目标框中心点，在中心点附近检测是否有 Center 点，如没有，则删除该匹配；否则，保留该目标框，并用这三个点的平均分数代表该目标框的分数。 <img src="/Anchor-Free-Detection/CenterNetKey-pool2.png" width="60%" height="60%" title="图 3.2. CenterNet pooling Module"> 　　如图 3.2 所示，CenterNet 引入 center pooling 并升级了 cascade corner pooling，这两个模块极大的提升了目标框内的特征提取融合能力，类似 ROI-pooling 的作用。具体的：</p><ul><li><strong>Center Pooling</strong>，叠加了水平和垂直方向上的最大值；</li><li><strong>Cascade Corner Pooling</strong>，不同于 Corner Pooling 只在角点所在的目标框边缘处取最大值，它还在目标框的内部取得最大值；</li></ul><p><img src="/Anchor-Free-Detection/CenterNetKey-pool.png" width="60%" height="60%" title="图 3.3. CenterNet pooling Module"> 　　如图 3.3 所示，这两个模块可通过不同方向的 corner pooling 组合而成，实现也较为简单。</p><h3 id="loss-1">3.2. Loss</h3><p>　　相比 CornerNet，增加了 center Heatmaps 的 Loss 项，其它都一样： <span class="math display">\[ L= L_{det}^{co} + L_{det}^{ce} + \alpha L_{pull}^{co} + \beta L_{push}^{co} + \gamma \left(L_{off}^{co}+L_{off}^{ce}\right) \tag{3.1}\]</span></p><h2 id="extremenet7">4. ExtremeNet<a href="#7" id="7ref"><sup>[7]</sup></a></h2><p><img src="/Anchor-Free-Detection/ExtremeNet-arch.png" width="80%" height="80%" title="图 4.1. ExtremeNet 框架"> 　　如图 4.1 所示，ExtremeNet 继承了 CornerNet(CenterNet) 主干，所不同的是，ExtremeNet 预测了目标的上下左右四个点，这四个点都是在目标上的，而传统的目标框上的左上及右下点则离目标有一定距离。所以输出上，角点的 Heatmaps \(\in\mathbb{I}^{4\times C\times H\times W}\)，Center 点 Heatmaps \(\in\mathbb{I}^{C\times H\times W}\)，只对角点预测 Offsets \(\in\mathbb{R}^{4\times 2\times H\times W}\)，去掉了 Embedding 的预测。</p><p><img src="/Anchor-Free-Detection/ExtremeNet-post.png" width="40%" height="40%" title="图 4.2. ExtremeNet 后处理"> 　　CornerNet 与 CenterNet 因为预测的角点是目标框的左上及右下点，所以 Embedding 能较好的用于角点配对，而 ExtremeNet 预测的角点可能在目标框的任意位置，所以作者采用暴力穷举匹配的方法，实验表面效果也更好。如图 4.2 所示，最后判断是否是一个匹配到的角点，与 CenterNet 类似，也是判断待匹配角点的中心角点上是否有较强的 Center 响应。</p><h2 id="centernet-objects-as-points8">5. CenterNet: Objects as Points<a href="#8" id="8ref"><sup>[8]</sup></a></h2><p><img src="/Anchor-Free-Detection/CenterNetObj-arch.png" width="80%" height="80%" title="图 5.1. CenterNet 网络结构"> 　　如图 5.1 所示，CenterNet 网络大体上还是继承了 CornerNet，在 2D 检测上，CenterNet 预测目标框的中心点 Center \(\in\mathbb{I}^{C\times H\times W}\)，中心点 Offsets \(\in\mathbb{R}^{2\times H\times W}\)，以及目标框的尺寸 size \(\in\mathbb{R}^{2\times C\times H\times W}\)。其 Loss 为： <span class="math display">\[ L_{det}=L_k + \lambda_{size}L_{size}+\lambda_{off}L_{off} \tag{5.1} \]</span> 　　Inference 的后处理只需要对 Center Heatmaps 作 3x3 的 max-pooling，<strong>不需要对目标框作 NMS</strong>！</p><p><img src="/Anchor-Free-Detection/CenterNetObj-tasks.png" width="40%" height="40%" title="图 5.2. CenterNet 多任务输出"> 　　此外，这种 pixel-level 的预测容易将其它任务也包含进来，如图 5.2 所示，作者还融入了 3D 检测，人体姿态估计。<br>　　3D 检测任务中，预测项为:</p><ul><li>目标距离编码量 \(\sigma(\hat{d}_k)\in\mathrm{(0,1)}^{3\times C\times H\times W}\)，由于直接回归距离比较困难，实际距离的回归量为 \(\frac{1}{\sigma(\hat{d}_k)}-1\);</li><li>三围尺寸 \(\hat{\gamma}_k\in\mathbb{R}^{3\times C\times H\times W}\)，包括长，宽，高；</li><li>角度 \(\hat{\theta}_k\in\mathrm{[-\pi/2,\pi/2]}^{C\times H\times W}\)，直接回归比较困难，借鉴目前用的比较多的分类+回归的思想，设计编码量 \(\hat{\alpha}_k\in\mathbb{R}^{8\times C\times H\times W}\)，将角度划分为两个 bin，\(B_1=\left[-\frac{7\pi}{6},\frac{\pi}{6}\right]\)，\(B_2=\left[-\frac{\pi}{6},\frac{7\pi}{6}\right]\)，每个 bin 有四个预测量，其中两个预测量用来作 softmax 分类，另外两个预测量作相对于 bin 中心点 \(m_i\) 的 sin，cos 的 Offsets 量；</li></ul><p>综上，3D 检测的 Loss 为： <span class="math display">\[\left\{\begin{array}{l}L_{dep} = \frac{1}{N}\sum_{k=1}^N\left\vert\frac{1}{\sigma(\hat{d}_k)}-1-d_k\right\vert \\L_{dim} = \frac{1}{N}\sum_{k=1}^N\left\vert\hat{\gamma}_k-\gamma_k\right\vert \\L_{ori} = \frac{1}{N}\sum_{k=1}^N\sum_{i=1}^2\left(softmax\left(\hat{b}_i,c_i\right)+c_i\left\vert \hat{a}_i-a_i\right\vert\right)\end{array}\tag{5.2}\right.\]</span> 其中 \(c_i=\mathbb{1}(\theta\in B_i)\)，\(a_i=\left(\mathrm{sin}(\theta-m_i),\mathrm{cos}(\theta-m_i)\right)\)，预测的角度可解码为 \(\hat{\theta}=arctan2\left(\hat{a}_{i1},\hat{a}_{i2}\right)+m_i\)。</p><h2 id="fcos9">6. FCOS<a href="#9" id="9ref"><sup>[9]</sup></a></h2><h3 id="网络结构-2">6.1. 网络结构</h3><p><img src="/Anchor-Free-Detection/FCOS-res.png" width="40%" height="40%" title="图 6.1. FCOS 目标框定义方式"> 　　如图 6.1 所示，FCOS 提出了另一种目标框的表示方式，“参考点”+\((l,t,r,b)\)，当“参考点”是中心点时，就退化为中心点+尺寸的方式了。这种方式弱化了中心点的重要性，一定程度上“更有可能”回归出准确的目标框。 <img src="/Anchor-Free-Detection/FCOS-arch.png" width="80%" height="80%" title="图 6.2. FCOS 网络结构"> 　　如图 6.2 所示，FCOS 继承了 RetinaNet 主体网络，采用 FPN 形式，在不同尺度的特征层上进行目标检测。HourGlass 设计之初就是用于 pixel-level 的预测的，而 FPN 多尺度检测一定程度上更有利于框检测，<strong>不同尺度上检测不同大小的框能有效解决两个大小框中心点重合的情况</strong>，HourGlass 则无法解决，虽然这种情况很少。网络预测量有：</p><ul><li>Score Heatmaps \(\in\mathbb{R}^{C\times H\times W}\)，每个 Channel 的监督项是个二值图，代表了是否是该类别下的角点；</li><li>Regression \(\in\mathbb{R}^{4\times H\times W}\)，“参考点” 上的 \((l,t,r,b)\)；</li><li>Center-ness \(\in\mathbb{R}^{1\times H\times W}\)，监督“参考点”趋向于中心点，因为接近目标框边缘的“参考点”效果会比较差；</li></ul><h3 id="多尺度检测">6.2. 多尺度检测</h3><p>　　不同于 Hourglass 网络只在一个尺度上进行预测，FPN 在多尺度上对真值框的划分会比较复杂，基本准则是：<strong>不同尺度要检测不同尺寸的目标框，尺度越大(特征层越小)要检测的目标框尺寸越大</strong>。所以在真值框监督的划分上，具体的，如图 6.2 所示，多尺度特征表示为 \(\{P_i|i=3,4,5,6,7\}\)，对应每个特征层能回归的最大像素距离设定为 \(\{m_i|i=2,3,4,5,6,7\} = \{0,64,128,256,512,\infty\}\)。监督第 \(i\) 特征层学习的正样本真值框需满足： <span class="math display">\[ m_{i-1}&lt;\mathrm{max}(l^{gt},t^{gt},r^{gt},b^{gt})\le m_i \tag{6.1}\]</span></p><h3 id="loss-2">6.3. Loss</h3><p>　　Loss 由三部分组成：</p><ul><li><p>类别分类<br>目标框内的所有点都作为正样本，所以直接采用 Focal Loss 中的 Loss 定义方式： <span class="math display">\[ L_{det} = -\alpha(1-p_k)^\gamma\mathrm{log}(p_k) \tag{6.2}\]</span></p></li><li><p>目标框回归<br>传统的 L2 Loss 用于目标框的直接回归有两个问题：</p><ol type="1"><li>目标框参数只是作独立的优化；</li><li>较大的目标框有较大的 Loss；</li></ol></li></ul><p>这里采用 UnitBox 中提出的 IoU Loss<a href="#13" id="13ref"><sup>[13]</sup></a>： <span class="math display">\[ L_{box} = -\mathrm{ln}(IoU_k) \tag{6.3} \]</span></p><ul><li>参考点中心化监督<br>不像 CornerNet 之流，这里的参考点全作为正样本，并没有向负样本方向的权重衰减，所以为了参考点趋向于中心点，作者提出了 Center-ness，其真值监督项为： <span class="math display">\[ centerness^{gt} = \sqrt{\frac{\mathrm{min}(l^{gt},r^{gt})}{\mathrm{max}(l^{gt},r^{gt})} \times \frac{\mathrm{min}(t^{gt},b^{gt})}{\mathrm{max}(t^{gt},b^{gt})}} \tag{6.4} \]</span> 从而可用 L1 Loss 来计算该项的 Loss。</li></ul><h2 id="foveabox10">7. FoveaBox<a href="#10" id="10ref"><sup>[10]</sup></a></h2><p><img src="/Anchor-Free-Detection/Fovea-arch.png" width="60%" height="60%" title="图 7.1. FoveaBox 框架"> 　　如图所示，FoveaBox 完全继承了 RetinaNet 的主体网络，采用 FPN 形式。多尺度检测中的真值分配方式基本与 FCOS 一致，这里不做展开。 <img src="/Anchor-Free-Detection/Fovea-assign.png" width="60%" height="60%" title="图 7.2. FoveaBox 正负样本区域"> 　　正负样本的分配上，作者提出了 Fovea 区域，如图 7.2 所示，目标框收缩一定比例后的区域定义为正样本，收缩一定比例后的区域外定义为负样本。<br>　　目标框的回归上，作者提出了另一种回归量，在 \((x,y)\) 像素点上，回归量定义为： <span class="math display">\[\left\{\begin{array}{l}t_{x_1^{gt}} = \mathrm{log}\frac{2^l(x+0.5)-x_1^{gt}}{\sqrt{S_l}} \\t_{y_1^{gt}} = \mathrm{log}\frac{2^l(y+0.5)-y_1^{gt}}{\sqrt{S_l}} \\t_{x_2^{gt}} = \mathrm{log}\frac{x_2^{gt}-2^l(x+0.5)}{\sqrt{S_l}} \\t_{y_2^{gt}} = \mathrm{log}\frac{y_1^{gt}-2^l(y+0.5)}{\sqrt{S_l}} \\\end{array}\tag{7.1}\right.\]</span> 其中 \(S_l\) 为第 \(l\) 特征层设计的最大检测像素长度的平方。</p><h2 id="fsaf11">8. FSAF<a href="#11" id="11ref"><sup>[11]</sup></a></h2><p>　　网络结构及多尺度检测设置上与 FCOS，FoveaBox 并无新意。FSAF 新的东西是提出了多尺度特征层自动选择对应大小的真值目标框，用作本特征层的训练，具体选择的过程就是看每层特征层对该目标框输出的 Loss 大小，思想与 OHEM 或是 Focal Loss 差不多。该模块可与 Anchor-Based 方法一起嵌入到网络中。</p><h2 id="参考文献">9.参考文献</h2><p><a id="1" href="#1ref">[1]</a> Redmon, Joseph, et al. &quot;You only look once: Unified, real-time object detection.&quot; Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.<br><a id="2" href="#2ref">[2]</a> Huang, Lichao, et al. &quot;Densebox: Unifying landmark localization with end to end object detection.&quot; arXiv preprint arXiv:1509.04874 (2015).<br><a id="3" href="#3ref">[3]</a> Lin, Tsung-Yi, et al. &quot;Focal loss for dense object detection.&quot; Proceedings of the IEEE international conference on computer vision. 2017.<br><a id="4" href="#4ref">[4]</a> Law, Hei, and Jia Deng. &quot;Cornernet: Detecting objects as paired keypoints.&quot; Proceedings of the European Conference on Computer Vision (ECCV). 2018.<br><a id="5" href="#5ref">[5]</a> Law, Hei, et al. &quot;CornerNet-Lite: Efficient Keypoint Based Object Detection.&quot; arXiv preprint arXiv:1904.08900 (2019).<br><a id="6" href="#6ref">[6]</a> Duan, Kaiwen, et al. &quot;Centernet: Keypoint triplets for object detection.&quot; Proceedings of the IEEE International Conference on Computer Vision. 2019.<br><a id="7" href="#7ref">[7]</a> Zhou, Xingyi, Jiacheng Zhuo, and Philipp Krahenbuhl. &quot;Bottom-up object detection by grouping extreme and center points.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br><a id="8" href="#8ref">[8]</a> Zhou, X., Wang, D., &amp; Krähenbühl, P. (2019). Objects as Points arXiv preprint arXiv:1904.07850<br><a id="9" href="#9ref">[9]</a> Tian, Zhi, et al. &quot;FCOS: Fully Convolutional One-Stage Object Detection.&quot; arXiv preprint arXiv:1904.01355 (2019).<br><a id="10" href="#10ref">[10]</a> Kong, Tao, et al. &quot;FoveaBox: Beyond Anchor-based Object Detector.&quot; arXiv preprint arXiv:1904.03797 (2019).<br><a id="11" href="#11ref">[11]</a> Zhu, Chenchen, Yihui He, and Marios Savvides. &quot;Feature selective anchor-free module for single-shot object detection.&quot; arXiv preprint arXiv:1903.00621 (2019).<br><a id="12" href="#12ref">[12]</a> Yang, Ze, et al. &quot;RepPoints: Point Set Representation for Object Detection.&quot; arXiv preprint arXiv:1904.11490 (2019).<br><a id="13" href="#13ref">[13]</a> Yu, Jiahui, et al. &quot;Unitbox: An advanced object detection network.&quot; Proceedings of the 24th ACM international conference on Multimedia. ACM, 2016.<br><a id="14" href="#14ref">[14]</a> Newell, Alejandro, Kaiyu Yang, and Jia Deng. &quot;Stacked hourglass networks for human pose estimation.&quot; European conference on computer vision. Springer, Cham, 2016.<br><a id="15" href="#15ref">[15]</a> Lin, Tsung-Yi, et al. &quot;Feature pyramid networks for object detection.&quot; Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　3D 目标检测的技术思路大多数源自 2D 目标检测，所以图像 2D 检测的技术更迭极有可能在将来影响 3D 检测的发展。目前 3D 检测基本还是 Anchor-Based 方法(也称为 Top-Down 方法)，而今年以来，Anchor-Free(也称为 bottom-
      
    
    </summary>
    
      <category term="2D Detection" scheme="https://leijiezhang001.github.io/categories/2D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="2D Detection" scheme="https://leijiezhang001.github.io/tags/2D-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;FlowNet3D&quot;</title>
    <link href="https://leijiezhang001.github.io/paperreading-FlowNet3D/"/>
    <id>https://leijiezhang001.github.io/paperreading-FlowNet3D/</id>
    <published>2019-10-22T11:51:11.000Z</published>
    <updated>2019-11-07T14:07:28.032Z</updated>
    
    <content type="html"><![CDATA[<p>　　本来以为这篇文章是 FlowNet<a href="#1" id="1ref"><sup>[1]</sup></a>，FlowNet2.0<a href="#2" id="2ref"><sup>[2]</sup></a> 的续作，其实不是，大概只是借鉴了其网络框架。从网络细节上来说，应该算是 PointNet<a href="#3" id="3ref"><sup>[3]</sup></a>，PointNet++<a href="#4" id="4ref"><sup>[4]</sup></a> 系列的续作，本文<a href="#5" id="5ref"><sup>[5]</sup></a>二作也是 PointNet 系列的作者。<br>　　光流(Optical Flow)是指图像坐标系下像素点的运动(详细可见 <a href="/KLT/" title="KLT">KLT</a>)，而 Scene Flow 是三维坐标下，物理点的运动。Scene Flow 是较底层的一种信息，可进一步提取高层的语义信息，如运动分割等。</p><h2 id="背景">1. 背景</h2><h3 id="flownet-系列">1.1. FlowNet 系列</h3><p><img src="/paperreading-FlowNet3D/flownet.png" width="90%" height="90%" title="图 1. FlowNet"> <img src="/paperreading-FlowNet3D/refine.png" width="80%" height="80%" title="图 2. FlowNet Refinement"> 　　如图 1. 与 2. 所示，FlowNet 在特征提取编码阶段提出了两种网络结构：FlowNetSimple 以及 FlowNetCorr。FlowNetSimple 是将前后帧图像按通道维拼接作为输入，FlowNetCorr 则设计了互相关层，描述前后帧特征的相关性，从而得到像素级偏置。refinement 解码阶段则采用 FPN 形式进行上采样，这样每一层反卷积层在细化时，不仅可以获得深层的抽象信息，同时还能获得浅层的具体信息。 <img src="/paperreading-FlowNet3D/flownet2.png" width="90%" height="90%" title="图 3. FlowNet2.0"> 　　FlowNet 虽然验证了用深度学习预测光流的可行性，但是性能比不上传统方法。FlowNet2.0 在此基础上进行了三大改进：</p><ul><li><strong>增加训练数据，改进训练策略</strong>；<br>在数据足够的情况下，证明了 FlowNetCorr 比 FlowNetSimple 较好。</li><li><strong>利用堆叠结构使性能得到多级提升</strong>；<br>如图 3. 所示，采用 FlowNet2-CSS 形式堆叠一个 FlowNetCorr 以及两个 FlowNetSimple 模块，FlowNetSimple 的输入为前一模块预测的光流，原图像经过光流变换后的图像，以及与另一图像的误差，这样可以使得该模块专注去学习前序模块未预测准确的误差项。训练时，由前往后单独训练每个模块。</li><li><strong>针对小位移的情况引入特定的子网络进行处理</strong>；<br>如图 3. 所示，FlowNet2-SD 网络卷积核均改为 3x3 形式，以增加对小位移的分辨率。最后再利用一个小网络将 FlowNet2-CSS 与 FlowNet2-SD 的结果进行融合。</li></ul><h3 id="pointnet-系列">1.2. PointNet 系列</h3><p>　　这部分详见 <a href="/PointNet-系列论文详读/" title="PointNet-系列论文详读">PointNet-系列论文详读</a>。<br>　　这里介绍下 PointNet++ 中点云采样的过程。点云采样有集中形式：</p><ul><li>格点采样<br>空间栅格化，然后按照栅格进行点云采样；</li><li>随机采样<br></li><li>几何采样<br>根据点云所在平面的曲率，将点云分成不同集合，在每一集合里面进行均匀采样，获得曲率大的地方采样点多的效果，即获得更多“细节”；</li><li>均匀采样</li></ul><p>PointNet++ 中采用的 Farthest Point Sample 属于均匀采样，其可以采样出特定个数的点，且比较均匀。大致过程为：</p><ol type="1"><li>点云总集合为 \(\mathcal{C}\)，随机取一点，形成采样目标集合 \(\mathcal{S}\)；</li><li>在剩余点集 \(\mathcal{C}-\mathcal{S}\) 中取与集合 \(\mathcal{S}\) 距离最远的一点，加入目标集合 \(\mathcal{S}\)；</li><li>如果目标集合 \(\mathcal{S}\) 个数达到预定值，则终止，否则重复步骤 2.；</li></ol><h2 id="flownet3d-网络结构">2. FlowNet3D 网络结构</h2><p><img src="/paperreading-FlowNet3D/flownet3d.png" width="90%" height="90%" title="图 4. FlowNet3D"> 　　如图 4. 所示，FlowNet3D 整体思路与 FlowNetCorr 非常像，其 set conv，flow embedding，set upconv 三个层相当于 FlowNetCorr 中的 conv，correlation，upconv 层。网络结构的连接方式也比较相像，上采样的过程都有接入前面浅层的具体特征。下面重点分析下这三个层的细节。 <img src="/paperreading-FlowNet3D/flownet3d-layers.png" width="90%" height="90%" title="图 5. FlowNet3D Layers"> 　　假设两个连续帧的两堆点：\(\mathcal{P} = \{x_i\vert i = 1,...,n_1\}\) 以及 \(\mathcal{Q} = \{y_j\vert j = 1,...,n_2\}\)，其中 \(x_i, y_j \in \mathbb{R}^3\) 是每个点的物理空间坐标。Scene Flow 的目标是求解 \(\mathcal{D}=\{x_i'-x_i \vert i = 1,...,n_1\} = \{d_i\vert i=1,...,n_1\}\)，其中 \(x_i'\) 是 \(x_i\) 在下一帧的位置。图 5. 较清晰地阐述了这三个层对点云的作用：</p><h3 id="set-conv-layer">2.1. set conv layer</h3><p>　　set conv layer 就是 PointNet++ 中的 set abstraction layer，其作用相当于图像中的卷积操作，能提取环境上下文特征。假设输入 \(n\) 个点，每个点 \(p_i = \{x_i, f_i\}\)，其中 \(x_i\in \mathbb{R}^3\) 是物理坐标空间，\(f_i\in\mathbb{R}^c\) 是特征空间；输出 \(n'\) 个点，对应每个点为 \(p_j'=\{x_j',f_j'\}\)，其中 \(f_j'\in\mathbb{R}^{c'}\) 为特征空间。那么 set conv layer 可以描述为： <span class="math display">\[f_j&#39; = \max_{\left\{i\vert\Vert x_i-x_j&#39;\Vert \leq r\right\}}\left\{\mathbf{h}\left(\mathrm{concat}(f_i,x_i-x_j&#39;)\right)\right\}\]</span> 其中 \(x_j'\) 是输入的 \(n\) 个点经过 Farthest Point Sample 后的点集，感知机 \(\mathbf{h}\) 将空间 \(\mathbb{R}^{c+3}\) 映射到空间 \(\mathbb{R}^{c'}\)，然后进行 max 操作。</p><h3 id="flow-embedding-layer">2.2. flow embedding layer</h3><p>　　有了 PointNet 思想后，其实比较容易想到如何进行两个点云的特征融合提取(看论文之前，自己有想过，和论文一样⊙o⊙)。对于两个点集：\(\left\{p_i = \{x_i, f_i\}\right\}_{i=1}^{n_1}\) 以及 \(\left\{q_j = \{y_j, g_j\}\right\}_{j=1}^{n_2}\)，其中 \(x_i,y_j\in\mathbb{R}^3\)，特征量 \(f_i,g_j\in\mathbb{R}^c\)， 那么输出为：\(\left\{o_i=\{x_i,e_i\}\right\}_{i=1}^{n_1}\)，其中 \(e_i\in\mathbb{R}^{c'}\)。由此 flow embedding layer 可描述为： <span class="math display">\[e_i = \max_{\left\{j\vert\Vert y_j-x_i\Vert \leq r\right\}}\left\{\mathbf{h}\left(\mathrm{concat}(f_i,g_j,y_j-x_i)\right)\right\}\]</span> 可见，其数学形式与 set conv layer 基本一致，但是物理意义是完全不一样的， flow embedding layer 是以 \(x_i\) 为锚点，在另一堆点云中找到距离 \(r\) 范围内的点，从何提取特征，用来描述该点与另一堆点云的相关性。这里的感知机作用可以有其它形式，作者试验后发现这种方式最简单有效。</p><h3 id="set-upconv-layer">2.3. set upconv layer</h3><p>　　PointNet++ 中 refinement 过程是 3D 插值上采样与 unit pointnet 过程，这里作者参考图像中 conv2D 与 upconv2D 的关系，提出了 set upconv layer。图像中 upconv2D 可以认为是特征扩大+填0+conv的结合(插值上采样则等价于扩大+插值的过程)，那么类似的，set upconv layer 就是点云扩大后，再对每个目标点进行 set conv layer 的操作。<br>　　作者称这种方法比纯插值上采样好(这当然了)，也有可能是称比插值上采样+unit pointnet 好？但是这种方法本质上还是插值上采样+pointnet。</p><h2 id="其它细节">3. 其它细节</h2><h3 id="training-loss">3.1. Training Loss</h3><p>　　输入两堆点云： \(\mathcal{P}=\{x_i\}_{i=1}^{n_1}\), \(\mathcal{Q}=\{y_j\}_{j=1}^{n_2}\)，网络预测的 Scene Flow 为 \(\mathcal{D}=F(\mathcal{P,Q;\theta})=\{d_i\}_{i=1}^{n_1}\)， 真值为 \(\mathcal{D}^*=\{d_i^*\}_{i=1}^{n_1}\)。经过 Scene Flow 变换后的点云为：\(\mathcal{P'}=\{x_i+d_i\}_{i=1}^{n_1}\)，那么经过网络预测的反向的 Scene Flow 为 \(\{d_i'\}_{i=1}^{n_1}=F(\mathcal{P',P;\theta})\)，由此定义 cycle-consistency 项 \(\Vert d_i'+d_i\Vert\)，最终的 Loss 函数为： <span class="math display">\[L(\mathcal{P,Q,D^*,\theta})=\frac{1}{n_1}\sum_{i=1}^{n_1}\left(\Vert d_i-d_i^*\vert+\lambda\Vert d_i&#39;+d_i\Vert\right)\]</span></p><h3 id="three-meta-architectures">3.2. Three Meta-architectures</h3><p><img src="/paperreading-FlowNet3D/mixture.png" width="60%" height="60%" title="图 6. 三种特征融合方式对比"> 　　如图 6. 所示，两个点云集合特征融合方式有三种，作者的 baseline 模型也是基于这三种，flow embedding layer 属于 Deep Mixture 类型。</p><h3 id="runtime">3.3.  Runtime</h3><p><img src="/paperreading-FlowNet3D/runtime.png" width="70%" height="70%" title="图 7. NIVIDA 1080 GPU with TensorFlow"> 　　速度嘛，还是比较慢的，要应用得做优化。</p><h3 id="applications-scan-registration-motion-segmentation">3.4.  Applications: Scan Registration &amp; Motion Segmentation</h3><p>　　待补充。</p><h2 id="实验结果">4. 实验结果</h2><p><img src="/paperreading-FlowNet3D/ablation.png" width="60%" height="60%" title="图 8. ablation study"> 　　如图 8. 所示，可得结论：</p><ul><li>PointNet 中 max 操作比 avg 操作效果好；</li><li>上采样中 upconv 比 interpolation 效果好；</li><li>cycle-consistency loss 项有助于提升性能；</li></ul><h2 id="参考文献">5. 参考文献</h2><p><a id="1" href="#1ref">[1]</a> Dosovitskiy, Alexey, et al. &quot;Flownet: Learning optical flow with convolutional networks.&quot; Proceedings of the IEEE international conference on computer vision. 2015.<br><a id="2" href="#2ref">[2]</a> Ilg, Eddy, et al. &quot;Flownet 2.0: Evolution of optical flow estimation with deep networks.&quot; Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.<br><a id="3" href="#3ref">[3]</a> Qi, Charles R., et al. &quot;Pointnet: Deep learning on point sets for 3d classification and segmentation.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.<br><a id="4" href="#4ref">[4]</a> Qi, Charles Ruizhongtai, et al. &quot;Pointnet++: Deep hierarchical feature learning on point sets in a metric space.&quot; Advances in neural information processing systems. 2017.<br><a id="5" href="#5ref">[5]</a> Liu, Xingyu, Charles R. Qi, and Leonidas J. Guibas. &quot;Flownet3d: Learning scene flow in 3d point clouds.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　本来以为这篇文章是 FlowNet&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt;，FlowNet2.0&lt;a href=&quot;#2&quot; id=&quot;2ref&quot;&gt;&lt;sup&gt;[2]&lt;/sup&gt;&lt;/a&gt; 的续作，其实不是，大概只是借鉴了其网络框架。从网
      
    
    </summary>
    
      <category term="Scene Flow" scheme="https://leijiezhang001.github.io/categories/Scene-Flow/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Scene Flow" scheme="https://leijiezhang001.github.io/tags/Scene-Flow/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds&quot;</title>
    <link href="https://leijiezhang001.github.io/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/"/>
    <id>https://leijiezhang001.github.io/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/</id>
    <published>2019-10-21T03:30:27.000Z</published>
    <updated>2019-11-07T14:06:05.130Z</updated>
    
    <content type="html"><![CDATA[<p>　　在多视角融合 3D 检测上，研究比较多的是俯视图下的激光点云以及前视图下的图像做多传感器融合，而融合点云俯视图(Bird's Eye View)与前视图(Perspective View)的特征则比较少，新鲜出炉的本文<a href="#1" id="1ref"><sup>[1]</sup></a>提供了一种较好的点云前视图与俯视图特征前融合(early fusion)方法。</p><h2 id="为什么要融合点云前视图特征">1. 为什么要融合点云前视图特征</h2><p>　　目前主流的点云检测算法，都是将点云在俯视图下以一定分辨率体素化(Voxelization)，然后用网络提取特征做 3D 检测。单纯在俯视图下提取特征虽然比单纯在前视图下做有优势，但还是存在几个问题：</p><ol type="1"><li>激光点云在远处，会变得很稀疏，从而空像素会比较多；</li><li>行人等狭长型小目标特征所占像素会很小；</li></ol><p>将点云投影到前视图，这两个问题则能有效减弱，所以本文提出融合点云前视图特征。</p><h2 id="贡献点">2. 贡献点</h2><p>　　本文是在 <a href="/paperreading-PointPillars/" title="PointPillars">PointPillars</a> 基础上做的工作，PointPillars 主要由三个模块构成：</p><ul><li>Voxelization；</li><li>Point Feature Encoding；</li><li>CNN Backbone；</li></ul><p>本文改进了前两个模块，但是本质思想还是 PointNet 形式。其余包括 Loss 形式等与 PointPillars 一致。<br>　　针对这两个模块，本文有两个贡献点，Dynamic Voxelization 以及 Point-level Feature Fusion，接下来作详细介绍。</p><h3 id="动态体素化dynamic-voxelization">2.1. 动态体素化(Dynamic Voxelization)</h3><p><img src="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/voxelization.png" width="90%" height="90%" title="图 1. 体素化过程对比"> 　　如图 1. 所示，PointPillars (包括之前的 VoxelNet 等工作)体素化的过程都是 Hard Voxelization，即 Voxel 数目要采样，每个 Voxel 里面的点数也会采样，比如 PointPillars 将每个 Voxel 的点数定义为 100 个，少于 100 个点，则作补零处理。这样会存在问题：</p><ul><li>内存消耗大，很多稀疏的区域导致体素中要补零的内存很多；</li><li>采样导致信息丢失；</li><li>采样导致检测输出有一定的不一致性；</li><li>不能作点级别的特征融合；</li></ul><p>　　由此提出动态体素化(Dynamic Voxelization)，取消所有的采样过程，为什么可以这么做呢？其实这么做也比较自然，PointPillars 中 PointNet 网络将 \((P, N, D)\) 特征映射为 \((P, N, C)\)，这里就是多层感知机将输入的 channel 维度从 \(D\) 变换到 \(C\)，与其它两个维度没有关系，而接下来做的 max-pooling 操作则将 \(N\) 维(N 个点)压缩到 1，PointPillars 中每个柱子的 N 是采样成一样的。但是可以不一样！这就是本文的动态体素化思想了。</p><h3 id="点级别特征融合point-level-feature-fusion">2.2. 点级别特征融合(Point-level Feature Fusion)</h3><p>　　<a href="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/" title="MMF">MMF</a> 以 Voxel-level 将前视图的图像特征融合到俯视图的点云特征中，并以 ROI-level 融合图像前视图特征及点云俯视图特征做检测分类，本文则提出了更加前序的特征融合-Point-level 融合。 <img src="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/MVF.png" width="90%" height="90%" title="图 2. 点级别特征融合框架"> 　　如图 2. 所示，首先将每个点的特征(x,y,z,intensity...)映射到高维度，然后经过 FC+Maxpool(PointNet 形式) 得到标准卷积网络需要的输入数据形式，再经过 Convolution Tower 模块进行环境上下文特征提取，最终每个体素的特征作为体素内每个点的特征，由此拼接成总的点特征。<br><img src="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/encoding.png" width="40%" height="40%" title="图 3. Convolution Tower"> 　　其中 Convolution Tower 网络结构如图 3. 所示，输入输出的尺寸保持不变，类似于 FPN 结构。<br>　　最终每个点的特征由三部分构成：</p><ul><li>自身特征维度映射；</li><li>俯视图下抽取的 Voxel 级别特征，有一定的感受野；</li><li>前视图下抽取的 Voxel 级别特征，有一定的感受野；</li></ul><p>　　俯视图下点云特征提取过程我们比较熟悉了，这里再详细介绍下点云在前视图下提取特征的过程(还没看懂，论文中好像没有详细信息，看懂了再补充)。</p><h2 id="实验结果">3. 实验结果</h2><p><img src="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/eval.png" width="90%" height="90%" title="图 4. 实验结果"> 　　网络参数配置可详见论文，图 4. 是在 Waymo 公开数据集上的实验结果。可知：</p><ol type="1"><li>动态体素化在全距离范围内对检测都有一定的提升；</li><li>融合前视图特征能有效提升提升检测性能，尤其是远距离情况，距离越远，提升越明显；</li><li>融合前视图特征对小目标提升更加明显，如行人；</li></ol><h2 id="参考文献">4. 参考文献</h2><p><a id="1" href="#1ref">[1]</a> Zhou, Yin, et al. &quot;End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds.&quot; arXiv preprint arXiv:1910.06528 (2019).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　在多视角融合 3D 检测上，研究比较多的是俯视图下的激光点云以及前视图下的图像做多传感器融合，而融合点云俯视图(Bird&#39;s Eye View)与前视图(Perspective View)的特征则比较少，新鲜出炉的本文&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;su
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
  </entry>
  
  <entry>
    <title>Apply IPM in Lane Detection from BEV</title>
    <link href="https://leijiezhang001.github.io/lane-det-from-BEV/"/>
    <id>https://leijiezhang001.github.io/lane-det-from-BEV/</id>
    <published>2019-10-17T09:53:12.000Z</published>
    <updated>2019-11-09T09:19:54.325Z</updated>
    
    <content type="html"><![CDATA[<p>　　车道线检测(Lane Detection)是 ADAS 系统中重要的功能模块，而对于 L4 自动驾驶系统来说，在不完全依赖高精度地图的情况下，车道线检测结果也是车辆运动规划的重要输入信息。由于俯视图(BEV, Bird's Eye View)下做车道线检测相比于前视图，有天然的优势，所以本文根据几篇论文(就看了两三篇)及项目经验，探讨总结俯视图下做车道线检测的流程方案，并主要介绍 IPM 逆透视变换原理，<a href="#0" id="0ref">[0]</a>为车道线检测资源集。</p><h2 id="流程框架">1. 流程框架</h2><p>　　由于激光点云的稀疏性，目前车道线检测主要还是依靠图像，激光点云数据当然可作为辅助输入。由此归纳一种可能的粒度较粗的俯视图下车道线检测的流程：</p><ol type="1"><li>IPM 逆透视变换，将图像前视图变换为俯视图；</li><li>网络，提取特征，进行像素级别的分类或回归；</li><li>后处理，根据网络输出作相应后处理，网络输出可能是像素级别预测；</li></ol><p>网络相对比较成熟，后处理则在不同网络方法下复杂度差异很大，这里不做讨论。接下来主要讨论如何进行逆透视变换。</p><h2 id="ipm-逆透视变换">2. IPM 逆透视变换</h2><p>　　设变换前后图像坐标为 \((u,v)\), \((u',v')\), 对于仿射变换(Affine transformation)，变换前后保持了线的平行性，其变换矩阵 A： <span class="math display">\[\begin{bmatrix}u&#39; \\v&#39; \\\end{bmatrix} = A\begin{bmatrix}u \\v \\1\end{bmatrix} =\begin{bmatrix}a_{11} &amp;a_{12} &amp;a_{13} \\a_{21} &amp;a_{22} &amp;a_{23}\end{bmatrix}\begin{bmatrix}u \\v \\1\end{bmatrix} \tag{1}\]</span> 对透视变换，可表示为： <span class="math display">\[\begin{bmatrix}u&#39; \\v&#39; \\1 \\\end{bmatrix} = s\cdot P\begin{bmatrix}u \\v \\1\end{bmatrix} = \frac{1}{p_{31}u+p_{32}v+p_{33}}\begin{bmatrix}p_{11} &amp;p_{12} &amp;p_{13} \\p_{21} &amp;p_{22} &amp;p_{23} \\p_{31} &amp;p_{32} &amp;p_{33} \\\end{bmatrix}\begin{bmatrix}u \\v \\1\end{bmatrix} \tag{2}\]</span></p><p>用于前视图到俯视图的 IPM 逆透视变换本质上还是透视变换，变换矩阵 \(P\in \mathbb{R}^{3\times3}\) 有 8 个自由度。</p><h3 id="ipminverse-perspective-mapping">2.1. IPM(Inverse Perspective Mapping)</h3><p><img src="/lane-det-from-BEV/coords.png" width="60%" height="60%" title="图 1. 坐标关系"> 　　世界(road)坐标系与相机坐标系如图 1. 所示，设 \((u',v')\) 表示图像像素坐标系下的点，\((X_w,Y_w,0)\) 表示世界坐标系下地面上的点坐标，\((u,v)\)表示俯视图像素坐标点，<strong>IPM 假设地面是平坦的</strong>。那么根据相机透视变换原理，可得： <span class="math display">\[\begin{align}\begin{bmatrix}u&#39; \\v&#39; \\1\end{bmatrix} &amp;= K_{cam}\frac{1}{Z_{cam}}T_{world}^{cam}\begin{bmatrix}X \\Y \\Z \\1\end{bmatrix}_{world}\\ &amp;=\begin{bmatrix}f_x &amp;0 &amp;u_0 \\0 &amp;f_y &amp;v_0\\0 &amp;0 &amp;1\end{bmatrix}\frac{1}{r_{31}X+r_{32}Y+t_z}\begin{bmatrix}R &amp;t\\0 &amp;1\end{bmatrix}\begin{bmatrix}X \\Y \\0 \\1\end{bmatrix}_{world}\\&amp;= \frac{1}{r_{31}X+r_{32}Y+t_z}\begin{bmatrix}m_{11} &amp;m_{12} &amp;m_{13}\\m_{21} &amp;m_{22} &amp;m_{23}\\r_{31} &amp;r_{32} &amp;t_{z}\\\end{bmatrix}\begin{bmatrix}X \\Y \\1 \end{bmatrix}_{world} = \frac{Res}{Z_{cam}} P \begin{bmatrix}u \\v \\1 \end{bmatrix} \tag{3}\end{align}\]</span></p><p>式 (3) 与式 (2) 形式一致，其中 \(P\) 为相机内参及外参，\(Res\) 为俯视图像素对物理空间尺寸的分辨率，单位为\((meter/pixel)\)。<strong>IPM 需要预先标定相机的内外参</strong>，尤其是外参 \(R\)，表示与地面平行的世界坐标系与相机成像平面的相机坐标系之间的旋转关系，一般情况下不考虑相机的横滚角以及偏航角，只考虑俯仰角。</p><h3 id="俯视图求解过程">2.2. 俯视图求解过程</h3><p>　　已知前视图，相机内外参，求解俯视图有两种思路。一种是在世界坐标系下划定感兴趣区域，另一种是在前视图图像上划定感兴趣区域。</p><h4 id="世界坐标系下划定感兴趣区域">2.2.1 世界坐标系下划定感兴趣区域</h4><p>　　这种方式很直接，假设世界坐标系下感兴趣区域是 \(x\in [X_{min},X_{max}],y\in [Y_{min},Y_{max}], z\in [Z_{min},Z_{max}]\)，设定 \(Res\)，即可生成俯视图要生成的像素图，然后通过公式 (2) 投影到前视图的亚像素上，用双线性插值获得采样值填入俯视图中即可。</p><h4 id="前视图图像上划定感兴趣区域">2.2.2 前视图图像上划定感兴趣区域</h4><p>　　基于(3)，可以求出世界坐标系下两条平行 \(z\) 轴的平行直线在图像坐标系下的交点，即<strong>消失点(Vanishing Point)</strong>。假设世界坐标系下平行 \(z\) 轴的直线表示为，点 \((x_a,x_b,x_c)\) 及方向向量 \(k(0,0,1)\)，那么可得该直线上任意一点投影到图像坐标系下表示，当 \(k\) 趋向于无穷大时，即可得到消失点坐标： <span class="math display">\[\begin{bmatrix}u \\v \\1\end{bmatrix} = \frac{1}{Z_{cam}} M \begin{bmatrix}x_a \\x_b \\x_c + k\end{bmatrix}_{world} = \begin{bmatrix}\frac{m_{11}x_a+m_{12}x_b+m_{13}(x_c+k)}{m_{31}x_a+m_{32}x_b+m_{33}(x_c+k)} \\\frac{m_{21}x_a+m_{22}x_b+m_{23}(x_c+k)}{m_{31}x_a+m_{32}x_b+m_{33}(x_c+k)} \\1\end{bmatrix} \overset{k\to\infty}{\simeq}\begin{bmatrix}\frac{m_{13}}{m_{33}} \\\frac{m_{23}}{m_{33}} \\1\end{bmatrix}\tag{3}\]</span> 有了图像坐标系下的消失点坐标以后，我们就可以选定需要作透视变换的 ROI 梯形区域(逆透视变换到俯视图后，梯形变矩形)。选定梯形四个角点后，根据像素距离关系，定义俯视图下其对应的矩形框四个像素坐标点，这样能得到四组(2)方程组，足可求解自由度 8 的透视矩阵 \(P\)。OpenCV 有较成熟的函数，更详细的代码原理可见<a href="#1" id="1ref">[1]</a>。</p><h2 id="其它思考">3. 其它思考</h2><p>　　如果在俯视图下作车道线检测，IPM 是必不可少的。以上 IPM 的缺陷是有一个较强的假设：路面是平坦的。并且时间一长标定参数，尤其是外参会失效，而且距离越远，路面的不平坦导致的逆透视变换误差也会增大。但对于 ADAS 系统来说，车道偏离预警(LDW，Lane Departure Warnings) 中车道线的检测距离在 50m 已经能满足要求。如果要消除更远距离下路面不平坦所带来的影响，也是有方法可以消除的，留到日后再讨论。<br>　　按照之前的项目经验，LDW 系统完成度可以很高，基本思路就是 IPM，parsing(segmentation)，clustering，hough，optimization 等几个步骤(这里就不能说得太细了)，更多的精力可能在指标设计及 cornercase 优化上。唯一对用户不太友好的地方就是安装时要进行相机外参(尤其是 pitch 角)的标定，当然标定方法比较简单，我们假设相机坐标系与路面平行，所以透视变换矩阵是固定的，用户只要看路面经过逆透射后，两条 \(z\) 方向的直线是否平行即可。相对于 Mobileye 这种标定巨麻烦的产品，这种标定方式算是非常友好了。此外还可以用自动外参标定方法，脑洞也可以开出很多，效果嘛看具体环境了，需要作谨慎的收敛判断。</p><h2 id="参考文献">4. 参考文献</h2><p><a id="0" href="#0ref">[0]</a> <a href="https://github.com/amusi/awesome-lane-detection" target="_blank" rel="noopener">awesome-lane-detection</a><br><a id="1" href="#1ref">[1]</a> <a href="https://blog.csdn.net/qq_32864683/article/details/85471800" target="_blank" rel="noopener">LDW 原理及代码</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　车道线检测(Lane Detection)是 ADAS 系统中重要的功能模块，而对于 L4 自动驾驶系统来说，在不完全依赖高精度地图的情况下，车道线检测结果也是车辆运动规划的重要输入信息。由于俯视图(BEV, Bird&#39;s Eye View)下做车道线检测相比于前视图，
      
    
    </summary>
    
      <category term="Lane Detection" scheme="https://leijiezhang001.github.io/categories/Lane-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Lane Detection" scheme="https://leijiezhang001.github.io/tags/Lane-Detection/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Multi-Task Learning Using Uncertainty to Weigh Losses&quot;</title>
    <link href="https://leijiezhang001.github.io/paperreading-MT-Learning-Using-Uncertainty-to-Weight-Losses/"/>
    <id>https://leijiezhang001.github.io/paperreading-MT-Learning-Using-Uncertainty-to-Weight-Losses/</id>
    <published>2019-10-15T07:19:09.000Z</published>
    <updated>2019-11-07T14:08:04.715Z</updated>
    
    <content type="html"><![CDATA[<p>　　深度学习网络中的不确定性(Uncertainty)是一个比较重要的问题，本文<a href="#1" id="1ref"><sup>[1]</sup></a>讨论了其中一种不确定性在多任务训练中的应用。目前关于深度学习不确定性的研究基本出自本文作者及其团队，后续我会较系统得整理其研究成果，这篇文章先只讨论一个较为实用的应用。</p><h2 id="不确定性概述">1. 不确定性概述</h2><p>　　在贝叶斯模型中，可以建模两类不确定性<a href="#2" id="2ref"><sup>[2]</sup></a>：</p><ul><li><strong>认知不确定性(Epistemic Uncertainty)</strong>，描述模型因为缺少训练数据而存在的未知，可通过增加训练数据解决；</li><li><p><strong>偶然不确定性(Aleatoric Uncertainty)</strong>，描述了数据不能解释的信息，可通过提高数据的精度来消除；</p><ul><li><strong>数据依赖地或异方差不确定性(Data-dependent or Heteroscedastic Uncertainty)</strong>，与模型输入数据有关，可作为模型预测输出；</li><li><strong>任务依赖地或同方差不确定性(Task-dependent or Homoscedastic Uncertainty)</strong>，与模型输入数据无关，且不是模型的预测输出，不同任务有不同的值；</li></ul></li></ul><p>本文讨论同方差不确定性，其描述了不同任务间的相关置信度，所以可用同方差不确定性来设计不同任务的 \(Loss\) 权重项。</p><h2 id="为什么需要设计不同任务的-loss-权重项">2. 为什么需要设计不同任务的 \(Loss\) 权重项</h2><p><img src="/paperreading-MT-Learning-Using-Uncertainty-to-Weight-Losses/mt_weight.png" width="90%" height="90%" title="图 1. Multi-task loss weightings"> 　　如图 1. 所示，多任务学习能提高单任务的性能，但是要充分发挥多任务的性能，那么得精心调节各任务的 \(Loss\) 权重。当任务多的时候，人工搜索最优的权重项则显得费时费力，依靠模型的同方差不确定性，我们可以自动学习权重项。</p><h2 id="多任务似然建模">3. 多任务似然建模</h2><p>　　下面推倒基于同方差不确定性的最大化高斯似然过程。设模型权重 \(\mathbf{W}\)，输入 \(\mathbf{x}\)，输出为 \(\mathbf{f^W(x)}\)。对于回归任务，定义模型输出为高斯似然形式： <span class="math display">\[p\left(\mathbf{y}\vert\mathbf{f^W(x)}\right) = \mathcal{N}\left(\mathbf{f^W(x)}, \sigma ^2\right) \tag{1}\]</span> 其中 \(\sigma\) 为观测噪声方差，描述了模型输出中含有多大的噪声。对于分类任务，玻尔兹曼分布下的模型输出概率分布为： <span class="math display">\[p\left(\mathbf{y}\vert\mathbf{f^W(x)},\sigma\right) = \mathrm{Softmax}\left(\frac{1}{\sigma ^2}\mathbf{f^W(x)}\right) \tag{2}\]</span> 由此对于多任务，模型输出的联合概率分布为： <span class="math display">\[p\left(\mathbf{y}_1,\dots,\mathbf{y}_K\vert\mathbf{f^W(x)}\right) = p\left(\mathbf{y}_1\vert\mathbf{f^W(x)}\right) \dots p\left(\mathbf{y}_K\vert\mathbf{f^W(x)}\right) \tag{3}\]</span></p><p>　　对于回归任务，\(log\)似然函数： <span class="math display">\[\mathrm{log}p\left(\mathbf{y}\vert\mathbf{f^W(x)}\right) \propto -\frac{1}{2\sigma ^2} \Vert \mathbf{y-f^W(x)} \Vert ^2 - \mathrm{log}\sigma \tag{4}\]</span> 对于分类任务，\(log\)似然函数： <span class="math display">\[\mathrm{log}p\left(\mathbf{y}=c\vert\mathbf{f^W(x)}, \sigma\right) = \frac{1}{2\sigma ^2}f_c^{\mathbf{W}}(\mathbf{x})- \mathrm{log}\sum_{c&#39;} \mathrm{exp}\left(\frac{1}{\sigma^2}f^{\mathbf{W}}_{c&#39;}(\mathbf{x}) \right) \tag{5}\]</span></p><p>　　现同时考虑回归与分类任务，则多任务的联合 \(Loss\)： <span class="math display">\[\begin{align}\mathcal{L}(\mathbf{W}, \sigma _1, \sigma _2) &amp;= -\mathrm{log}p\left(\mathrm{y_1,y_2}=c\vert\mathbf{f^W(x)} \right) \\&amp;= -\mathrm{log}\mathcal{N}\left(\mathbf{y_1};\mathbf{f^W(x)}, \sigma_1^2\right) \cdot \mathrm{Softmax}\left(\mathbf{y_2}=c;\mathbf{f^W(x)},\sigma_2\right) \\&amp;= \frac{1}{2\sigma_1^2}\Vert \mathbf{y}_1-\mathbf{f^W(x)}\Vert ^2 + \mathrm{log}\sigma_1 - \mathrm{log}p\left(\mathbf{y}_2=c\vert\mathbf{f^W(x)},\sigma_2\right) \\&amp;= \frac{1}{2\sigma_1^2}\mathcal{L}_1(\mathbf{W}) +\frac{1}{\sigma_2^2}\mathcal{L}_2(\mathbf{W}) + \mathrm{log}\sigma_1 + \mathrm{log}\frac{\sum_{c&#39;}\mathrm{exp}\left(\frac{1}{\sigma_2^2}f_{c&#39;}^{\mathbf{W}}(x)\right)}{\left(\sum_{c&#39;}\mathrm{exp}\left(f_{c&#39;}^{\mathbf{W}}(x) \right) \right)^{\frac{1}{\sigma_2^2}}} \\&amp;\approx \frac{1}{2\sigma_1^2}\mathcal{L}_1(\mathbf{W}) +\frac{1}{\sigma_2^2}\mathcal{L}_2(\mathbf{W}) + \mathrm{log}\sigma_1 + \mathrm{log}\sigma_2 \tag{6}\end{align}\]</span></p><p>由此得到两个权重项，任务噪声 \(\sigma\) 越大，则该任务的误差权重越小。实际应用中，为了数值稳定，令 \(s:=\mathrm{log}\sigma^2\): <span class="math display">\[\mathcal{L}(\mathbf{W}, s_1, s_2) = \frac{1}{2}\mathrm{exp}(-s_1)\mathcal{L}_1(\mathbf{W}) + \mathrm{exp}(-s_2)\mathcal{L}_2(\mathbf{W}) + \mathrm{exp}(\frac{1}{2}s_1) + \mathrm{exp}(\frac{1}{2}s_2) \tag{7}\]</span> 对于更多任务的模型，根据任务类型也很容易扩展，网络自动学习权重项 \((s_1,s_2,...,s_n)\)。</p><h2 id="实验结果">4. 实验结果</h2><p><img src="/paperreading-MT-Learning-Using-Uncertainty-to-Weight-Losses/mt.png" width="90%" height="90%" title="图 2. Multi-task"> <img src="/paperreading-MT-Learning-Using-Uncertainty-to-Weight-Losses/ablation.png" width="90%" height="90%" title="图 3. 实验结果"> 　　如图 2. 所示，作者设计了同时作语义分割、实例分割、深度估计的网络，由图 3. 可知，用任务的不确定性来加权任务的 \(Loss\)，效果显著。</p><h2 id="参考文献">5. 参考文献</h2><p><a id="1" href="#1ref">[1]</a> Kendall, Alex, Yarin Gal, and Roberto Cipolla. &quot;Multi-task learning using uncertainty to weigh losses for scene geometry and semantics.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.<br><a id="2" href="#2ref">[2]</a> Kendall, Alex, and Yarin Gal. &quot;What uncertainties do we need in bayesian deep learning for computer vision?.&quot; Advances in neural information processing systems. 2017.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　深度学习网络中的不确定性(Uncertainty)是一个比较重要的问题，本文&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt;讨论了其中一种不确定性在多任务训练中的应用。目前关于深度学习不确定性的研究基本出自本文作者及其团队，后续我会较系
      
    
    </summary>
    
      <category term="Uncertainty" scheme="https://leijiezhang001.github.io/categories/Uncertainty/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Uncertainty" scheme="https://leijiezhang001.github.io/tags/Uncertainty/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Multi-Task Multi-Sensor Fusion for 3D Object Detection&quot;&quot;</title>
    <link href="https://leijiezhang001.github.io/paperreading-MT-MS-Fusion-for-3D-Object-Detection/"/>
    <id>https://leijiezhang001.github.io/paperreading-MT-MS-Fusion-for-3D-Object-Detection/</id>
    <published>2019-10-14T02:42:54.000Z</published>
    <updated>2019-11-07T14:08:24.622Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种 3D 检测的多任务多传感器融合方法。输入数据为图像以及点云，输出为地面估计，2D/3D检测，稠密深度图。为了让其它任务来帮助提升 3D 检测效果，作者设计了很多方法，工作还是比较细致且系统。<br><img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/算法框架.png" width="90%" height="90%" title="图 1. 算法框架"> 　　整个算法框架如图 1. 所示。点云数据还是在俯视图(BEV)下进行栅格化处理，高度切割是在地面估计归一化后的基础上来做，因为要 3D 定位的目标都是在地面上的；另一方面，图像与投影到前视图的点云数据进行合并，作为网络的输入数据。 网络结构上作者提出了两种俯视图与前视图特征融合策略：1. Point-wise feature fusion; 2. ROI-wise feature fusion. 这也是文章比较重要的一个贡献点。<br>　　文章所提的 3D 检测方法大多数细节技巧并无新意，这里主要讨论分析文章中与传统方法不太一样的两大贡献点： 1. 俯视图与前视图特征融合策略； 2. 其它两个任务对检测任务提升的作用。</p><h2 id="俯视图与前视图特征融合策略">1. 俯视图与前视图特征融合策略</h2><p>　　由于网络输入有俯视图与前视图两个数据流，所以如何将这两个数据流进行特征级别的融合就显得尤为重要，文章提出了两种方式，backbone 网络级别的 point-wise feature fusion 以及第二阶段 ROI-wise feature fusion。</p><h3 id="point-wise-feature-fusion">1.1. Point-wise Feature Fusion</h3><p>　　3D 检测主体还是在俯视图下来做的，相比前视图对 3D 检测的处理，俯视图 3D 检测有天然的优势。因此，如何有效地将前视图的特征融合到俯视图的特征中，就显得尤为重要（俯视图特征融合到前视图相对比较简单）。<br><img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/point-wise.png" width="55%" height="55%" title="图 2. Point-wise Feature Fusion"> 　　如图 2. 所示，像素点级别的特征融合方式有两个模块，Multi-scale Fusion 以及 Continuous Fusion。Multi-scale Fusion 我们比较熟悉，可以采用类似 FPN 的结构实现。这里主要讨论 Continuous Fusion 模块。<br><img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/算法框架2.png" width="90%" height="90%" title="图 3. Deep Continuous Fusion 检测框架"> 　　Continuous Fusion 源自作者的另一篇文章<a href="#2" id="2ref"><sup>[2]</sup></a>。如图 3. 所示，该文检测框架基本就是本文的主干，其中 Fusion Layers 就是 Continuous Fusion 模块。而 continuous fusion 前身是作者团队提出的 Deep Parametric Continuous Convolution<a href="#3" id="3ref"><sup>[3]</sup></a>。</p><ul><li><p><strong>Deep Parametric Continuous Convolution</strong><br>传统的卷积只能作用于网格结构(gird-structured)的数据上，为了能处理点云这种非网格结构的数据，<a href="#3" id="3ref">[3]</a>提出了带参数的卷积(Parametric Continuous Convolution)。对于第 \(i\) 个需要计算的特征位置，其特征值 \(\mathrm{h}_i \in \mathbb{R}^N\) 数学形式为： <span class="math display">\[ \mathrm{h}_i=\sum_j \mathbf{MLP}(x_i-x_j)\cdot \mathrm{f}_j \]</span> 其中 \(j\) 表示第 \(i\) 个点周围的点，\(\mathrm{f}_j \in \mathbb{R}^N\) 为输入特征，\(x_j\in \mathbb{R}^3\) 是点的坐标值。多层感知机 \(\mathbf{MLP}\) 则起到了参数核函数的作用，将 \(\mathbb{R}^{J\times 3}\) 映射为 \(\mathbb{R}^{J\times N}\) 空间，用作特征计算的权重值。</p></li><li><p><strong>Continuous Fusion Layer</strong><br>Continuous Fusion 则没有显示得计算卷积权重的过程，这样使得特征提取能力更强，而且计算效率更高，不用存储权重值。其数学描述为： <span class="math display">\[ \mathrm{h}_i=\sum_j \mathbf{MLP}(\mathrm{concat}[\mathrm{f}_j,x_i-x_j]) \]</span> 多层感知机 \(\mathbf{MLP}\) 直接将 \(\mathbb{R}^{J\times (N+3)}\) 映射到 \(\mathbb{R}^{J\times M}\) 空间，最后再做一个 element-wise 的相加即得空间为 \(\mathbb{R}^{M}\) 的特征输出(<strong>这个和 PointNet 几乎一模一样，本质就是将每个点的特征空间升维，然后用对称函数(pooling, sum)消除无序点的影响, 只是这里输入的点的特征空间 \(N\) 可能已经很大了</strong>)。 <img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/continuous_fusion.png" width="70%" height="70%" title="图 4. Continuous Fusion"> 具体步骤如图 4. 所示：</p><ol type="1"><li>将点云投影到图像坐标系，在图像特征图上用双线性插值求取每个点对应的图像特征向量；</li><li>俯视图下对于每个需要求取特征的像素点，采样邻近的 \(K\) 个物理点，然后应用 Continuous Fusion，得到该像素点的特征向量；</li></ol></li></ul><h3 id="roi-wise-feature-fusion">1.2. ROI-wise Feature Fusion</h3><p>　　在俯视图上获得 3D 检测框后(见图 1.)，将其分别投影到图像特征图以及点云特征图上，图像特征图上用 ROIAlign 提取出目标框内的图像特征；点云特征图上用类似方法提取出带方向的目标框内的点云特征，两种特征合并到一起，再用网络进行 2D/3D 目标框的优化回归。 <img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/roi-wise.png" width="60%" height="60%" title="图 5. ROI-wise Fusion"> 　　如图 5. 所示，点云特征图上的目标框是带有一定方向的，准确提取特征时会有一些问题。由于旋转框有周期性，所以将目标框分成两种情况来考虑，这样提取的特征就没有奇异性了，如图 5.2 所示。此外 3D 优化回归是在目标框旋转后的坐标系下进行的。</p><h2 id="多任务对检测任务的提升作用">2. 多任务对检测任务的提升作用</h2><p><img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/ablation.png" width="100%" height="100%" title="图 6. Ablation on Kitti"> <img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/ablation2.png" width="50%" height="50%" title="图 7. Ablation on TOR4D"></p><h3 id="地面估计">2.1. 地面估计</h3><p>　　俯视图下点云进行栅格化手工提取特征之前，作者作了一个地面归一化的操作。地面估计是在栅格分辨率下进行的，所以自然能对点云的每个栅格进行地面归一化。作者认为自动驾驶 3D 检测的目标都是在地面上的，所以地面的先验知识应该有助于 3D 定位，与 HDNET<a href="#4" id="4ref"><sup>[4]</sup></a> 思想类似。而在线地面估计(地面估计是建图的其中一个任务)不依赖离线地图，能提高系统鲁棒性。<br><img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/ground_est.png" width="70%" height="70%" title="图 8. 目标定位误差"> 　　如图 6.,8 所示，地面估计的加入，确实使得 3D 检测性能有所提升。</p><h3 id="深度估计">2.2. 深度估计</h3><p>　　由于前视图输入的是图像以及点云的投影图，所以可进一步通过网络预测稠密的前视深度图。作者对点云的投影图作了精心的设计，这里不做展开，有可能直接投影的 \((x,y,z)\) 3 通道的投影图也够用。<br>　　获得了前视稠密深度图后，可将其反投影到点云俯视图下，这样稀疏的点云会变得更加稠密，更有利于图像到点云的 Point-wise Feature Fusion。这里作者只在邻近取不到点云的时候用这反投影的伪雷达点(pseudo LiDARP)。如图 7. 所示，在该数据集上效果提升还是比较明显，而 Kitti 上不太明显，因为两者的相机与雷达配置不太一样。在 TOR4D 数据集上，远距离的车上点云数量更小，所以该技术效果较好。</p><h2 id="其它细节">3. 其它细节</h2><p>　　Loss 设计为： <span class="math display">\[ Loss = L_{cls} + \lambda(L_{box}+L_{r2d}+L_{r3d}) + \gamma L_{depth} \]</span> 其中 \(\lambda\) 与 \(\gamma\) 为权重项，\(L_{box}\) 为俯视图下预测的 3D 框，\(L_{r2d},L_{r3d}\) 为优化回归的 2D/3D 框。每一项的 Loss 计算方式与传统无异。 <img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/eval.png" width="90%" height="90%" title="图 9. 算法对比"> 　　本文方法与其它方法对比如图 9. 所示。</p><h2 id="参考文献">4. 参考文献</h2><p><a id="1" href="#1ref">[1]</a> Liang, Ming, et al. &quot;Multi-Task Multi-Sensor Fusion for 3D Object Detection.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br><a id="2" href="#2ref">[2]</a> Liang, Ming, et al. &quot;Deep continuous fusion for multi-sensor 3d object detection.&quot; Proceedings of the European Conference on Computer Vision (ECCV). 2018.<br><a id="3" href="#3ref">[3]</a> Wang, Shenlong, et al. &quot;Deep parametric continuous convolutional neural networks.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.<br><a id="4" href="#4ref">[4]</a> Yang, Bin, Ming Liang, and Raquel Urtasun. &quot;Hdnet: Exploiting hd maps for 3d object detection.&quot; Conference on Robot Learning. 2018.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　本文&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt;提出了一种 3D 检测的多任务多传感器融合方法。输入数据为图像以及点云，输出为地面估计，2D/3D检测，稠密深度图。为了让其它任务来帮助提升 3D 检测效果，作者设计了很多方法，工作还
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
  </entry>
  
</feed>
