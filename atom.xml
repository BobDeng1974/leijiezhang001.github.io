<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LeijieZhang</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://leijiezhang001.github.io/"/>
  <updated>2020-07-28T03:11:12.000Z</updated>
  <id>https://leijiezhang001.github.io/</id>
  
  <author>
    <name>Leijie</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SuMa(Surfel-based Mapping)</title>
    <link href="https://leijiezhang001.github.io/SuMa/"/>
    <id>https://leijiezhang001.github.io/SuMa/</id>
    <published>2020-07-20T01:31:34.000Z</published>
    <updated>2020-07-28T03:11:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　目前业界比较流行的基于激光雷达的 SLAM 是 <a href="/LOAM/" title="LOAM">LOAM</a>，其中 Mapping 又是非常重要的一环，LOAM 提取 Edge 点与 Surf 点然后建立以 Voxel 约束点个数的点云地图，该地图用于 Lidar Odometry 时的匹配定位。实际应用于工业界时，Mapping 的数据结构设计及存取管理对整体系统的效率至关重要，具体可优化的细节以后再写文阐述。<br>　　本系列文章<a href="#1" id="1ref"><sup>[1]</sup></a><a href="#2" id="2ref"><sup>[2]</sup></a> 提出了一种基于 Surfel 和语义信息的建图及定位方法。整体框架与 LOAM 类似，只是这里只用了面区域的特征点，其它模块，如优化方式，也有很大的差异。</p><h2 id="suma">1. SuMa</h2><p>　　设 \(A\) 坐标系下的点 \(p _ A\)，\(B\) 坐标系下的点 \(p _ B\)，其变换矩阵 \(T _ {BA}\in\mathbb{R}^{4\times 4}\)，使得 \(p _ B = T _ {BA} p _ A\)。变换矩阵 \(T _ {BA}\) 又由 \(R _ {BA}\in\mathbf{SO}(3)\) 和 \(t _ {BA}\in\mathbb{R}^3\) 构成。设每帧点云的雷达坐标系为 \(C _ k,k\in\{0,...,t\}\)，那么 Lidar Odometry 要求解的问题就是当前雷达坐标系在世界坐标系下的表示： <span class="math display">\[T _ {WC _ t} = T _ {WC _ 0}T _ {C _ 0C _ 1}\cdots T _ {C _ {t-1}C _ t} \tag{1}\]</span> 其中 \(T _ {WC _ 0}\) 为已标定的变换矩阵。 <img src="/SuMa/suma.png" width="65%" height="65%" title="图 1. SuMa Framework"> 　　如图 1. 所示，SuMa 根据点云 \(\mathcal{P} = \{p\in\mathbb{R}^3\}\) 估计 \(T _ {WC _ t}\) 的步骤为：</p><ol type="1"><li>当前帧地图计算。将当前帧的三维点云投影到二维，得到顶点图 \(\mathcal{V} _ D\)，以及计算对应的法向量图 \(\mathcal{N} _ D\)；</li><li>当前地图计算。对上一帧优化出的 Surfel Map \(\mathcal{M} _ {active}\) 作顶点图和法向量图的渲染 \(\mathcal{V} _ M,\mathcal{N} _ M\)；</li><li>位姿计算。根据 \(\mathcal{V} _ D, \mathcal{N} _ D\) 以及 \(\mathcal{V} _ M,\mathcal{N} _ M\) 作 frame-to-model 的 ICP 匹配，得到相对位姿 \(T _ {C _ {t-1}C _ t}\)，最后用式(1)计算当前帧在世界坐标系下的位姿态 \(T _ {WC _ t}\)；</li><li>地图更新。根据 \(T _ {WC _ t}\)，更新 Surfel Map \(\mathcal{M} _ {active}\)：初始化首次观测的区域，优化更新再次观测的区域；</li><li>闭环检测。在未激活的 Surfel Map \(\mathcal{M} _ {inactive}\) 中搜索当前帧地图的匹配；</li><li>闭环检测验证。在接下来 \(\Delta _ {verification}\) 时间内，验证闭环检测的有效性，如果有效，那么加入之后的位姿图优化；</li><li>位姿图优化。另一个线程作位姿图优化，输入信息是前后帧的相对位姿里程计以及闭环检测的相对位姿结果，类似 <a href="/AVP-SLAM/" title="AVP-SLAM">AVP-SLAM</a> 中的位姿图优化。优化后的位姿用来更新 Surfel Map。</li></ol><h3 id="preprocessing">1.1. Preprocessing</h3><p>　　与 RangeNet++<a href="#3" id="3ref"><sup>[3]</sup></a> 中对点云的表示一样，顶点图 \(\mathcal{V} _ D\) 的计算方法为： <span class="math display">\[\left(\begin{matrix}u\\v\\\end{matrix}\right)=\left(\begin{matrix}\frac{1}{2}[1-\mathrm{arctan}(y,x)\cdot \pi ^ {-1}]\cdot w\\[1-(\mathrm{arcsin}(z\cdot r ^ {-1})+f _ {up})f ^ {-1}]\cdot h\end{matrix}\right)\tag{2}\]</span> 其中 \(r = \Vert p\Vert _ 2\) 为点的距离，\(f = f _ {up} + f _ {down}\) 是雷达的上下视野角，\(w,h\) 为顶点图的宽和高。然后基于 \(\mathcal{V} _ D\) 计算每个顶点的法向量，得到法向量图 \(\mathcal{N} _ D\): <span class="math display">\[\mathcal{N} _ D((u,v)) = \left(\mathcal{V} _ D((u+1,v))-\mathcal{V} _ D((u,v))\right)\times \left(\mathcal{V} _ D((u,v+1))-\mathcal{V} _ D((u,v))\right) \tag{3}\]</span> 其中只计算坐标点 \((u,v)\) 有顶点的法向量。因为 \(u\) 方向物理世界是环状的，所以对边界作环向处理。这种法向量计算的 \(\mathcal{N} _ D\) 由较大噪声，但是实验发现对 Frame-to-Model 的 ICP 匹配不会产生精度影响。 <img src="/SuMa/preprocess_suma.png" width="55%" height="55%" title="图 2. SuMa Preprocessing"> 　　顶点图 \(\mathcal{V} _ D\) 与法向量图 \(\mathcal{V} _ N\) 的可视化结果如图 2. 所示。</p><h3 id="map-representation">1.2. Map Representation</h3><p>　　不同于 <a href="/LOAM/" title="LOAM">LOAM</a> 中采用了 Edge 和 Surf 两种特征来表示地图，本文只用 Surfel 来表示地图 \(\mathcal{M}\)。<a href="/LOAM/" title="LOAM">LOAM</a> 中计算了每个点的曲率，然后将其归为 Edge 或是 Surf，实际工程应用中，为了存储的高效性，首先将点云地图体素化，然后将体素内的特征点用 Mean，Normal，协方差矩阵的 EigenVector 等信息来存储，Normal 可用来表征 Surf 特征点，EigenVector 则可用来表征 Edge 特征点，这块具体的细节以后开文再详细阐述。<br>　　本文的 Surfel Map 自然就提取了点云的 Surf 特征，每个Surfel 可以用位置 \(v _ s\in\mathbb{R} ^ 3\)，法向量 \(n _ s\in\mathbb{R} ^ 3\)，半径 \(r _ s\in\mathbb{R}\) 来表示。此外每个 Surf 包含两个时间戳：首次建立的时间 \(t _ c\)，以及最新更新的时间 \(t _ u\)。然后采用贝叶斯滤波方法(详见 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a>)，定义及计算 Surfel 特征的稳定概率： <span class="math display">\[\begin{align}l _ s ^ {(t)} &amp;= l _ s ^ {t-1} + \mathrm{log}(p\cdot (1-p) ^ {-1}) - \mathrm{log}(p _ {prior}\cdot (1-p _ {prior}) ^ {-1})\\&amp;= l _ s ^ {t-1} + \mathrm{odds}(p) - \mathrm{odds}(p _ {prior})\\&amp;= l _ s ^ {t-1} + \mathrm{odds}\left(p _ {stable}\cdot \mathrm{exp}\left(-\frac{\alpha ^ 2}{\sigma _ {\alpha} ^ 2}\right)\mathrm{exp}\left(-\frac{d ^ 2}{\sigma _ d ^ 2}\right)\right) - \mathrm{odds}(p _ {prior})\end{align} \tag{4}\]</span> 其中 \(p _ {stable}, p _ {prior}\) 分别为测量为 surfel 是 stable 的概率，以及先验概率。\(\sigma ^ 2\) 为测量噪声方差。\(\alpha\) 为测量的 Surfel 法向量与对应的地图中 Surfel 法向量的夹角，\(d\) 则为测量的 Surfel 与对应的地图中 Surfel 的距离。<br>　　每个 Surfel 的位置及法向量都是以建立时的位置作为参考系，即 \(C _ {t _ c}\)。这样经过全局位姿优化后，就不需要重新建图，只需要通过 \(T _ {WC _ {t _ c}}\) 将 Surfel 地图更新到世界坐标系即可。<br>　　\(\mathcal{M} _ {active}\) 与 \(\mathcal{M} _ {inactive}\) 的区分也比较简单：\(\mathcal{M} _ {active}\) 定义为最近更新的 Surfels，即 \(t _ u\geq t - \Delta _ {active}\)；\(\mathcal{M} _ {inactive}\) 则定义为不是最近建立的 Surfels，即 \(t _ c&lt; t - \Delta _ {active}\)。Odometry 只在 \(\mathcal{M} _ {active}\) 中作匹配计算，Loop Closure 则只在 \(\mathcal{M} _ {inactive}\) 中搜索。</p><h3 id="odometry-estimation">1.3. Odometry Estimation</h3><p>　　里程计是将当前帧点云与地图点云匹配的过程。将上一时刻的地图 \(\mathcal{M} _ {active}\) 渲染成上一时刻局部坐标系下的顶点图 \(\mathcal{V} _ M\) 与法向量图 \(\mathcal{N} _ M\) 形式。然后采用 point-to-plane 的 ICP 匹配方法，其最小化误差为： <span class="math display">\[ E(\mathcal{V} _ D,\mathcal{V} _ M, \mathcal{N} _ M) = \sum _ {u\in\mathcal{V} _ D}n _ u ^ T\cdot\left(T _ {C _ {t-1}\;C _ t}^{(k)}\;u-v _ u\right) ^ 2 \tag{5}\]</span> 其中 \(u\in\mathcal{V} _ D\)，\(v _ u\in\mathcal{V} _ M,n _ u\in\mathcal{N} _ M\) 是地图上对应关联上的点，关联过程为： <span class="math display">\[\begin{align}v _ u &amp;= \mathcal{V} _ M\left(\Pi\left(T _ {C _ {t-1}\;C _ t}^{(k)}\;u\right)\right)\\n _ u &amp;= \mathcal{N} _ M\left(\Pi\left(T _ {C _ {t-1}\;C _ t}^{(k)}\;u\right)\right)\end{align} \tag{6}\]</span> 其中 \(T _ {C _ {t-1}\;C _ t} ^ {(t)}\) 为 frame-to-model ICP 得到的里程计估计的相对位姿。\(\Pi(u)\) 是特征点的关联方式，<a href="/LOAM/" title="LOAM">LOAM</a> 中根据前后线束的关系来寻找关联方式，本方案则采用直接坐标映射的方式。<strong>因为点云均投影到了前视图，所以可根据坐标直接搜索关联，这也是本方案最重要的优势之一</strong>。具体的，如图对应的地图顶点图中没有顶点，或者地图法向量点没有定义，那么忽略该待关联的特征点；对于关联的特征点对距离大于 \(\sigma _ {ICP}\) 或是法向量夹角大于 \(\theta _ {ICP}\) 的情况，则认为是离群点，不计入误差项。ICP 初始化为上一帧的相对位姿结果。<br>　　该问题是典型的非线性最小二乘问题，可在李空间下对位姿进行线性化并用 Gaussian-Newton 求解，这里不做展开。</p><h3 id="map-update">1.4. Map Update</h3><p>　　得到里程计估计的相对位姿后，要将当前帧的特征点更新到地图中，即要确定哪些 Surfel 要更新，哪些要重新构建新的 Surfel。对于 \(v _ s\in\mathcal{V} _ D\)，首先计算其面元的半径： <span class="math display">\[ r _ s = \frac{\sqrt{2}\Vert v _ s\Vert _ 2\cdot p}{\mathrm{clam}(-v _ s ^ T n _ s\cdot\Vert v _ s\Vert _ 2 ^ {-1}, 0.5, 1.0)} \tag{7}\]</span> 其中 \(p=\mathrm{max}(w\cdot f _ {horiz} ^ {-1}, h\cdot f _ {vert} ^ {-1})\)。<strong>根据式(2)，每个 \(v _ s\) 均能找到地图中对应的 Surfel \(s '\)。</strong>然后通过 \(\vert n _ {s'} ^ T(v _ s-v _ {s'})\vert &lt; \sigma _ M \;\mathrm{and}\; \Vert n _ s\times n _ {s'}\Vert &lt; \mathrm{sin}(\theta _ M)\) 判定当前帧的 Surfel 与地图中的 \(s'\) 是否一致：</p><ul><li>如果一致。那么更新地图中的 Surfel，如果估计的半径更准，那么也更新： <span class="math display">\[\begin{align}v _ {s&#39;} ^ {(t)} &amp;= (1-\gamma)\cdot v _ s + \gamma\cdot v _ {s&#39;} ^ {(t-1)}\\n _ {s&#39;} ^ {(t)} &amp;= (1-\gamma)\cdot n _ s + \gamma\cdot n _ {s&#39;} ^ {(t-1)}\\r _ {s&#39;} ^ {(t)} &amp;= r _ s, \; \mathrm{if} \; r _ s &lt; r _ {s&#39;}\end{align} \tag{8}\]</span></li><li>如果不一致。那么将地图中匹配上的 Surfel 作 Stable 概率衰减，然后创建新的 Surfel。如果地图中没有匹配的 Surfel，那么也创建新的 Surfel。</li></ul><p>最后将 Stable 概率较小的 Surfel 以及时间较早的 Surfel 删除，以此删除动态障碍物特征点以及较老的无关的特征点。</p><h3 id="loop-closures">1.5. Loop Closures</h3><p>　　检测到闭环后就可以作 Pose Graph 优化。闭环检测由检测与验证两部分组成，检测的过程为在未激活的地图 \(\mathcal{M} _ {inactive}\) 中找到一个最相近的位姿： <span class="math display">\[ j ^ * = \mathop{\arg\min}\limits _ {j\in 0,...,t-\Delta _ {active}} \Vert t _ {WC _ t}-t _ {WC _ j}\Vert \tag{9}\]</span> 然后类似 Odometry 的过程，将当前帧的点云特征注册到 \(T _ {WC _ j ^ * }\) 的地图特征中。为了用 ICP 求解两者的相对位姿 \(T _ {C _ {j ^ * }C _ t}\)，初始化 \(T ^ {(0) } _ {C _ {j ^ * }C _ t}\) 为： <span class="math display">\[\begin{align}R _ {C _ {j^ * }C _ t} &amp;= R ^ {-1} _ {WC _ {j ^ * }}R _ {WC _ t}\\t _ {C _ {j^ * }C _ t} &amp;= R ^ {-1} _ {WC _ {j ^ * }}(t _ {WC _ t}-t _ {WC _ {j ^ * }})\\\end{align} \tag{10}\]</span> 本文将 \(T ^ {(0) } _ {C _ {j ^ * }C _ t}\) 中的位移用 \(\lambda t _ {C _ {j ^ * }C _ t}\) 代替，其中 \(\lambda = \{0.0,0.5,1.0\}\)。由此可得到三种初始化后 ICP 迭代的结果，选择最合理的结果即可。<br>　　验证阶段，在 \(t + 1,...,t+ \Delta _ {verification}\) 时间段内，在 \(\mathcal{M} _ {active}\) 与 \(\mathcal{M} _ {inactive}\) 地图中分别作 Odometry 累加，查看两者的一致性，如果一致则认为该闭环检测是有效的。</p><h2 id="suma-1">2. SuMa++</h2><p><img src="/SuMa/suma++.png" width="95%" height="95%" title="图 3. SuMa++ Framework"> 　　SuMa++ 相比 SuMa，只增加了语义信息，算法框架没有改变。如图 3. 所示，SuMa++ 也有当前帧地图计算，当前地图计算，位姿计算，地图更新，闭环检测，闭环检测验证，位姿图优化等七个部分组成，其中，在地图计算中加入了有 RangeNet++ 产生的语义信息，在 \(\mathcal{V} _ D,\mathcal{N} _ D\) 的基础上，增加 \(\mathcal{S} _ D\) 特征；在地图更新中，根据语义信息加入了动态障碍物过滤的策略；在位姿计算中，用语义信息来权重化特征的 ICP 匹配迭代。</p><h3 id="semantic-map">2.1. Semantic Map</h3><p>　　RangeNet++ 也是基于式(2)投影试图下的分割模型，由此可得到 Surfel 特征图 \(\mathcal{V} _ D\) 中每个像素点的语义类别以及对应的类别概率。由于语义分割预测的噪声，本文用 Flood-fill 算法对网络输出的语义分割图 \(\mathcal{S} _ {raw}\) 作优化，得到顶点图对应的语义信息 \(\mathcal{S} _ D\)。 <img src="/SuMa/preprocess.png" width="65%" height="65%" title="图 4. SuMa++ Preprocessing"> 　　考虑到语义分割在物体中心区域确定性较高，而在边缘处不确定性较高，所以 Flood-fill 算法采用两个步骤：</p><ol type="1"><li>用腐蚀算法将与周围语义类别不一致的像素点移除，得到腐蚀后的语义图 \(\mathcal{S} _ {raw} ^ {eroded}\)；</li><li>结合有深度信息的顶点图 \(\mathcal{V} _ D\)，对腐蚀的边缘像素点填充为周围相近距离的顶点对应的语义类别，得到 \(\mathcal{S} _ D\)；</li></ol><p>如图 4. 所示，该方法能修正边缘类别错误的情况。由此，\(\mathcal{V} _ D, \mathcal{N} _ D,\mathcal{S} _ D\)组成每一帧的特征点信息。</p><h3 id="filtering-dynamics">2.2. Filtering Dynamics</h3><p><img src="/SuMa/res.png" width="65%" height="65%" title="图 5. Filterring Dynamics"> 　　有了语义类别信息后，在更新地图时，可计算当前帧每个 Surfel 与地图中对应 Surfel 的类别一致性，由此作为地图贝叶斯更新的惩罚项，如果类别不一致，地图中的 Surfel 稳定性概率会降低，直到去除。如图 5. 所示，这种方法能去除大部分动态障碍物区域所构成的 Surfel。地图具体的贝叶斯更新为： <span class="math display">\[\begin{align}l _ s ^ {(t)} = l _ s ^ {t-1} + \mathrm{odds}\left(p _ {stable}\cdot \mathrm{exp}\left(-\frac{\alpha ^ 2}{\sigma _ {\alpha} ^ 2}\right)\mathrm{exp}\left(-\frac{d ^ 2}{\sigma _ d ^ 2}\right)\right) - \mathrm{odds}(p _ {prior}) - \mathrm{odds}(p _ {penalty})\end{align} \tag{11}\]</span></p><h3 id="semantic-icp">2.3. Semantic ICP</h3><p>　　在式(5)的 ICP 误差项基础上，可加入语义约束，对误差项作权重化： <span class="math display">\[ E(\mathcal{V} _ D,\mathcal{V} _ M, \mathcal{N} _ M) = \sum _ {u\in\mathcal{V} _ D}w _ u n _ u ^ T\cdot\left(T _ {C _ {t-1}\;C _ t}^{(k)}\;u-v _ u\right) ^ 2 \tag{12}\]</span> 其中权重项结合了语义约束与几何约束，以此来减少离群特征点对优化的影响： <span class="math display">\[w _ u ^{(k)} = \rho _ {Huper}\left(r _ u ^ {(k)}C _ {semantic}(\mathcal{S} _ D(u),\mathcal{S} _ M(u))\right)\mathbb{1}\left\{l _ s ^ {(k)}\geq l _ {stable}\right\} \tag{13}\]</span> 其中 \(\rho _ {Huber}(r)\) 是 Huber 核函数： <span class="math display">\[\rho _ {Huber}(r)=\left\{\begin{array}{l}1 &amp;,\mathrm{if}\;\vert r\vert &lt; \sigma\\\sigma\vert r\vert ^ {-1} &amp;,\mathrm{otherwise}\end{array}\tag{14}\right.\]</span> 语义约束项为： <span class="math display">\[C _ {semantic}\left((y _ u,P _ u),(y _ {v _ u}, P _ {v _ u})\right)=\left\{\begin{array}{l}P(y _ u|u) &amp;,\mathrm{if}\;y _ u=y _ {v _ u}\\1-P(y _ u|u) &amp;,\mathrm{otherwise}\end{array}\tag{15}\right.\]</span></p><p><img src="/SuMa/icp.png" width="65%" height="65%" title="图 6. Weights of ICP"> 　　如图 6. 所示，在语义信息的约束下，如果当前帧某个 Surfel 的类别与地图中对应的 Surfel 类别不一致，那么就会减少该 Surfel 匹配对的误差项。</p><h2 id="thinkings">3. Thinkings</h2><p>　　利用检测或分割得到的语义信息去过滤当前帧以及地图中的动态障碍物，在 SLAM/Odometry 中已经非常常见，其实可以大概率相信语义信息，然后直接将对应的点云干掉。而本文以融合迭代的思路，想通过将信将疑的方式来完成有效的 ICP 匹配（既要滤掉大多数动态障碍物的影响，也期望一堆车停在场景中时然后保留足够匹配的特征点）。但是一般工程上，直接干掉也够用，毕竟场景够大，不太可能出现特征点不够匹配的情景。<strong>而本方法的高效性在于，寻找当前帧与地图中的 Surfel 匹配时，直接采用图像索引然后顶点图距离及法向量图角度判断有效性的形式，没有 KD-Tree，极大提高效率</strong>，类似 ICPCUDA<a href="#4" id="4ref"><sup>[4]</sup></a>。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Behley, Jens, and Cyrill Stachniss. &quot;Efficient Surfel-Based SLAM using 3D Laser Range Data in Urban Environments.&quot; Robotics: Science and Systems. 2018.<br><a id="2" href="#2ref">[2]</a> Chen, Xieyuanli, et al. &quot;Suma++: Efficient lidar-based semantic slam.&quot; 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2019.<br><a id="3" href="#3ref">[3]</a> Milioto, Andres, et al. &quot;RangeNet++: Fast and accurate LiDAR semantic segmentation.&quot; 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2019.<br><a id="4" href="#4ref">[4]</a> https://github.com/mp3guy/ICPCUDA</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　目前业界比较流行的基于激光雷达的 SLAM 是 &lt;a href=&quot;/LOAM/&quot; title=&quot;LOAM&quot;&gt;LOAM&lt;/a&gt;，其中 Mapping 又是非常重要的一环，LOAM 提取 Edge 点与 Surf 点然后建立以 Voxel 约束点个数的点云地图，该地图用于 
      
    
    </summary>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/categories/SLAM/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/tags/SLAM/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>AVP-SLAM</title>
    <link href="https://leijiezhang001.github.io/AVP-SLAM/"/>
    <id>https://leijiezhang001.github.io/AVP-SLAM/</id>
    <published>2020-07-15T01:17:56.000Z</published>
    <updated>2020-07-17T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　Visual-SLAM 一般采用特征点或像素直接法来建图定位，这种方式对光照较为敏感。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 提出了一种基于语义特征的 Visual Semantic SLAM，应用于光照条件较为复杂的室内停车场环境，相比于采用特征点的 ORB-SLAM，性能较为鲁棒。</p><h2 id="framework">1. Framework</h2><p><img src="/AVP-SLAM/framework.png" width="95%" height="95%" title="图 1. AVP-SLAM Framework"> 　　如图 1. 所示，AVP-SLAM 由 Mapping，Localization 两部分组成。Mapping 阶段，将车周围的四张图通过 IPM 拼接并变换到俯视图，然后作 Guide Signs，Parking Lines，Speed Bumps 等语义信息的提取，接着通过 Odometry 将每帧的特征累积成局部地图，最后通过回环检测，全局优化出全局地图。Localization 阶段，提取出每帧的语义信息后，用 Odometry 初始化位姿，然后用 ICP 匹配求解当前帧在全局地图中的位姿，得到基于地图的位姿观测量，最后用 EKF 融合该观测量与 Odometry 信息，得到本车的最终位姿。<br>　　有了本车在全局地图下的位姿后，然后通过语义信息识别停车位，即可达到本车自动泊车的目的。</p><h2 id="mapping">2. Mapping</h2><h3 id="ipm">2.1. IPM</h3><p>　　传感器为车身四周四个鱼眼相机，相机内外参已知。IPM(Inverse Perspective Mapping) 是将图像中的像素点投影到车身物理坐标系下的俯视图中，具体的： <span class="math display">\[\frac{1}{\lambda}\;\begin{bmatrix}x ^ v \\y ^ v \\1\end{bmatrix} =[\mathbf{R} _ c \;\mathbf{t} _ c] ^ {-1} _ {col:1,2,4} \;\pi _ c ^ {-1}\begin{bmatrix}u \\v \\1\end{bmatrix}\tag{1}\]</span> 其中 \(\pi _ c(\cdot)\) 为鱼眼相机的内参矩阵，\([\mathbf{R} _ c\;\mathbf{t} _ c]\) 为每个相机到车身坐标系的外参矩阵，\([x ^ v\; y ^ v]\) 为车身坐标系下语义特征的位置。关于 IPM 更多细节可参考 <a href="/lane-det-from-BEV/" title="Apply IPM in Lane Detection from BEV">Apply IPM in Lane Detection from BEV</a>。<br>　　进一步将 IPM 图拼接成一张全景图： <span class="math display">\[\begin{bmatrix}u _ {ipm}\\v _ {ipm}\\1\end{bmatrix}=\mathbf{K} _ {ipm}\begin{bmatrix}x ^ v \\y ^ v \\1\end{bmatrix}\tag{2}\]</span> 其中 \(\mathbf{K} _ {ipm}\) 是全景图的内参。</p><h3 id="feature-detection">2.2. Feature Detection</h3><p><img src="/AVP-SLAM/segment.png" width="65%" height="65%" title="图 2. Segmentation in IPM Image"> 　　将每张 IPM 图拼接成一张大全景图，然后用基于深度学习的语义分割方法，对全景图作像素级别作 lane，parking line，guide sign，speed bump，free space，obstacle，wall 等类别的语义分割。如图 4. 所示，parking line，guide sign，speed bump 是稳定的特征，用于定位；parking line 用于车位的识别；free space 与 obstacle 用于路径规划。</p><h3 id="local-mapping">2.3. Local Mapping</h3><p>　　全景图语义分割得到的用于定位的特征(parking line，guide sign，speed bump)需要反投影回车身物理坐标系： <span class="math display">\[\begin{bmatrix}x ^ v \\y ^ v \\1\end{bmatrix}=\mathbf{K} _ {ipm} ^ {-1}\begin{bmatrix}u _ {ipm}\\v _ {ipm}\\1\end{bmatrix}\tag{3}\]</span> 然后基于 Odometry 的相对位姿，将当前的语义特征点转换到世界坐标系下： <span class="math display">\[\begin{bmatrix}x ^ w \\y ^ w \\z ^ w\end{bmatrix}=\mathbf{R _ o}\begin{bmatrix}x ^ v \\y ^ v \\0\end{bmatrix} + \mathbf{t _ o}\tag{4}\]</span> 由此得到局部地图，本文保持车身周边 30m 内的局部地图。</p><h3 id="loop-detection">2.4. Loop Detection</h3><p><img src="/AVP-SLAM/loop_det.png" width="65%" height="65%" title="图 3. Loop Detection"> 　　因为 Odometry 有累计误差，所以这里对局部地图作一个闭环检测。如图 3. 所示，通过 ICP 对两个局部地图作匹配，一旦匹配成功，就说明检测到了闭环，ICP 匹配的相对位姿用于之后的全局位子图优化，以消除里程计累计误差。</p><h3 id="global-optimization">2.5. Global Optimization</h3><p>　　检测到闭环后，需进行全局位姿图优化。位姿图中，节点(node)为每个局部地图的位姿：\((\mathbf{r, t})\)；边(edge)有两种：odometry 相对位姿以及闭环检测中 ICP 匹配位姿。由此位姿图优化的损失函数为： <span class="math display">\[\chi ^ * = \mathop{\arg\min}\limits _ \chi \sum _ t\Vert f(\mathbf{r} _ {t+1},\mathbf{t} _ {t+1}, \mathbf{r} _ t, \mathbf{t} _ t) - \mathbf{z} ^ o _ {t,t+1}\Vert ^ 2 + \sum _ {i,j\in\mathcal{L}}\Vert f(\mathbf{r} _ i,\mathbf{t} _ i,\mathbf{r} _ j, \mathbf{t} _ j)-\mathbf{z} ^ l _ {i,j}\Vert ^ 2 \tag{5}\]</span> 其中 \(\chi = [\mathbf{r} _ 0,\mathbf{t} _ 0,...,\mathbf{r} _ t,\mathbf{t} _ t] ^ T\) 是所有局部地图的位姿，也是待优化的参数。\(\mathbf{z} ^ 0 _ {t,t+1}\) 为 Odometry 得到的位姿。\(\mathbf{z} ^ l _ {i,j}\) 为闭环检测 ICP 得到的位姿。\(f(\cdot)\) 为计算两个局部地图相对位姿的方程。该优化问题可通过 Gauss-Newton 法求解。<br>　　用优化后的位姿将局部地图叠加起来，就获得了整个场景的全局地图。</p><h2 id="localization">3. Localization</h2><p><img src="/AVP-SLAM/loc.png" width="65%" height="65%" title="图 4. Localization"> 　　有了全局地图后，基于全局地图的定位观测量可通过当前帧与全局地图的匹配得到。如图 4. 所示，绿色为当前帧检测到的语义特征，与全局地图匹配后即可得到当前的绝对位置。匹配通过 ICP 算法实现： <span class="math display">\[ \mathbf{r ^ * ,t ^ * } =  \mathop{\arg\min}\limits _ {\mathbf{r,t}}\sum _ {k\in\mathcal{S}}\Vert\mathbf{R(r)}\begin{bmatrix}x ^ v  _ k\\y ^ v  _ k\\0\end{bmatrix} + \mathbf{t} - \begin{bmatrix}x ^ w _ k \\y ^ w _ k\\z ^ w _ k\end{bmatrix}\Vert ^ 2 \tag{6}\]</span> 其中 \(\mathcal{S}\) 为当前帧语义特征点集，\([x _ k ^ w\; y _ k ^ w\; z _ k ^ w]\) 分别为对应的全局地图中最近的特征点集。<br>　　ICP 的初始化非常重要，本文提出了两种初始化方法：1. 直接在地图上标记车库入口作为全局坐标点；2. 室外 GPS 信号初始化，然后用 Odometry 累积到车库。</p><h2 id="extended-kalman-filter">4. Extended Kalman Filter</h2><p>　　Visual Localization 在语义特征较少的情况下，比如车辆停满了，定位会不稳定，所以这里采用 EKF 对 Visual Localization 与 Odometry 中的轮速计和 IMU 作扩展卡尔曼融合，这里不做展开。</p><h2 id="thinkings">5. Thinkings</h2><p>　　Semantic SLAM 相比基于几何特征点的 SLAM 更加鲁棒。但是在车库场景下，一旦车子停满后，停车线等语义信息会急剧减少，所以实际商业应用中，AVP-SLAM 能满足室内自动泊车的需求吗？<br>　　对此我持怀疑态度。我认为，对于车库自动泊车的商业落地，可能最有效且低成本的方法还是得基于室内 UWB 定位技术。至少 UWB 可作为辅助。当然要将 UWB 应用于车载装置，目前好像还没有，不过随着车载软硬件系统的完善，手机上能做的事，车载平台问题也不大。</p><h2 id="reference">6. Reference</h2><p><a id="1" href="#1ref">[1]</a> Qin, Tong, et al. &quot;AVP-SLAM: Semantic Visual Mapping and Localization for Autonomous Vehicles in the Parking Lot.&quot; arXiv preprint arXiv:2007.01813 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Visual-SLAM 一般采用特征点或像素直接法来建图定位，这种方式对光照较为敏感。本文&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt; 提出了一种基于语义特征的 Visual Semantic SLAM，应用于光照条件较为复杂的室内
      
    
    </summary>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/categories/SLAM/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/tags/SLAM/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-Point-GNN/"/>
    <id>https://leijiezhang001.github.io/paper-reading-Point-GNN/</id>
    <published>2020-07-10T01:22:07.000Z</published>
    <updated>2020-07-14T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种基于图网络来提取点云特征的方法，理论上可在不损失原始信息的情况下，高效的学习点云特征，其在点云 3D 检测任务中效果提升明显。</p><h2 id="different-point-cloud-representations">1. Different Point Cloud Representations</h2><p><img src="/paper-reading-Point-GNN/repr.png" width="65%" height="65%" title="图 1. Point Cloud Representations"> 　　如图 1. 所示，目前点云表示方式以及对应的特征学习方式有三种：Grids，栅格化后类似图像 2D/3D 卷积的形式；Sets，以 PointNet 为代表的最近邻查找周围点并学习的形式；Graph，将无序点集转换为图模型，特征信息通过点云顶点传递学习的形式。Grids 及 Sets 形式我们已经比较熟悉了，Graph 则查询效率比 Sets 高，特征提取能力又比 Grids 高。Graph 的建图时间复杂度为 \(\mathcal{O}(cN)\)，领域查询复杂度则为 \(\mathcal{O}(1)\)，Sets 中的 KNN 建树及查询复杂度可见 <a href="/PointCloud-Feature-Extraction/" title="PointCloud Feature Extraction">PointCloud Feature Extraction</a>。当然 KNN 式的领域查询方式可以用近似 \(\mathcal{O}(1)\) 方法实现，但是会影响特征学习的准确度。</p><h2 id="framework">2. Framework</h2><p><img src="/paper-reading-Point-GNN/framework.png" width="95%" height="95%" title="图 2. Framework of Point-GNN"> 　　如图 2. 所示，基于 Graph 的 3D 点云检测，首先对点云作 Graph Construction，然后用 GNN 来学习每个顶点的特征，接着对每个顶点预测目标框，最后作目标框的整合和 NMS。</p><h3 id="graph-construction">2.1. Graph Construction</h3><p>　　设点云集：\(P=\{p _ 1,...,p _ N\}\)，其中 \(p _ i=(x _ i, s _ i)\) 分别表示坐标 \(x _ i\in\mathbb{R} ^ 3\)，以及该点反射率，领域点相对位置等信息 \(s _ i\in\mathbb{R} ^ k\)。对该点集建图 \(G=(P,E)\)，将距离小于一定阈值的两个点进行连接，即： <span class="math display">\[E = \{(p _ i, p _ j)|\Vert x _ i-x _ j\Vert _ 2 &lt; r\} \tag{1}\]</span> 这种建图方式是 Fixed Radius Near-Neighbors 问题，可在 \(\mathcal{O}(cN)\) 时间复杂度下解决，其中 \(c\) 为最大连接数。<br>　　建图完成后，要对每个点信息状态 \(s _ i\) 作初始化。这里采用类似 Sets 的特征提取方式，即将该点的反射率，以及与领域内点的相对位置，串联成特征向量，然后用 MLP 作空间变换，最后在点维度上作 Max Pooling，即可得到初始化的该点特征状态量 \(s _ i\)。</p><h3 id="graph-neural-network-with-auto-registration">2.2. Graph Neural Network with Auto-Registration</h3><p>　　传统的图神经网络，通过边迭代每个顶点的特征。在 \((t+1) ^ {th}\) 迭代时： <span class="math display">\[\begin{align} v _ i ^ {t+1} &amp;= g ^ t\left(\rho\left(\{e _ {ij} ^ t|(i,j)\in E\}\right), v _ i ^ t\right) \\e _ {ij} ^ t &amp;= f ^ t(v _ i ^ t, v _ j ^ t) \tag{2}\end{align}\]</span> 其中 \(e ^ t,v ^ t\) 分别是边和顶点特征，\(f ^ t(\cdot)\) 计算两个顶点之间边的特征，\(\rho(\cdot)\) 将与该点连接的边特征整合，得到该点特征增量，\(g ^ t(\cdot)\) 将该点特征增量与原特征进行整合得到本次迭代后该点的最终特征。<br>　　对于边特征，一种设计方式为，描述领域特征对该点位置的作用力，重写式 (2)： <span class="math display">\[s _ i ^ {t+1} = g ^ t\left(\rho\left(\{f ^ t(x _ j-x _ i,s _ j^t)|(i,j)\in E\}\right), s _ i ^ t\right) \tag{3}\]</span> 这样就得到了图神经网络的迭代模型。此外，本文还指出，由于边特征对领域点的距离较为敏感，所以作者提出对相对位置作自动补偿，实验表明其实意义不大： <span class="math display">\[\begin{align}\Delta x _ i ^ t &amp;= h ^ t(s _ i^t) \\s _ i ^ {t+1} &amp;= g ^ t\left(\rho\left(\{f ^ t(x _ j-x _ i+\Delta x _ i ^ t,s _ j^t)|(i,j)\in E\}\right), s _ i ^ t\right) \tag{4}\end{align}\]</span> 　　具体的，\(f ^ t(\cdot),g ^ t(\cdot), h ^ t(\cdot)\) 可用 MLP 来建模，\(\rho(\cdot)\) 则采用 Max 操作： <span class="math display">\[\begin{align}\Delta x _ i ^ t &amp;= MLP _ h ^ t(s _ i^t) \\e _ {ij} ^ t &amp;= MLP _ f ^ t([x _ j - x _ i + \Delta x _ i ^ t, s _ j ^ t]) \\s _ i ^ {t+1} &amp;= MLP _ g ^ t\left(MAX(\{e _ {ij}|(i,j)\in E\})\right)+ s _ i ^ t \tag{5}\end{align}\]</span></p><h3 id="loss">2.3. Loss</h3><p>　　为了作 3D 检测的任务，网络头输出为每个顶点的类别，目标框中心的 offset，以及目标框的尺寸，朝向。这与传统的基于 Ancho-Free 的 3D 目标检测基本一致，这里不做展开。</p><h3 id="box-merging-and-scoring">2.4. Box Merging and Scoring</h3><p>　　本方法的 3D 检测需要作 NMS 后处理，由于分类的 Score 不能反应目标框的 Uncertainty，所以基于 Score 的 NMS 是不合理的。这个问题在 2D 检测中也有比较多的研究，比如采用预测 IoU 值的方式来作权重。本文则认为遮挡信息能作为 NMS 操作的指导，由此定义了遮挡值的计算方式。但是实验显示，其实提升并不明显，所以这里不做具体展开。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Shi, Weijing, and Raj Rajkumar. &quot;Point-gnn: Graph neural network for 3d object detection in a point cloud.&quot; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　本文&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt;提出了一种基于图网络来提取点云特征的方法，理论上可在不损失原始信息的情况下，高效的学习点云特征，其在点云 3D 检测任务中效果提升明显。&lt;/p&gt;
&lt;h2 id=&quot;different-p
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>CenterTrack</title>
    <link href="https://leijiezhang001.github.io/CenterTrack/"/>
    <id>https://leijiezhang001.github.io/CenterTrack/</id>
    <published>2020-07-02T01:16:36.000Z</published>
    <updated>2020-07-08T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　障碍物感知由目标检测，目标跟踪(MOT)，目标状态估计等三个模块构成。目标状态估计一般是指将位置，速度等观测量作卡尔曼滤波平滑；广义的目标跟踪也包含了状态估计过程，这里采用狭义的目标跟踪定义方式，主要指出目标 ID 的过程。传统的做法，目标检测与目标跟踪是分开进行的，检测模块分别对前后帧作目标检测，目标跟踪模块则接收前后帧检测结果，然后用 Motion Model 将上一帧的检测结果预测到这一帧，最后与这一帧的检测结果作数据关联(Data Association)出目标 ID。这里的 Motion Model 可以是 3D 下目标的物理运动模型，也可以是图像下的单目标跟踪结果，如 KCF 算法。详细介绍可参考 <a href="/MOT-综述-Multiple-Object-Tracking-A-Literature-Review/" title="Multiple Object Tracking: A Literature Review">Multiple Object Tracking: A Literature Review</a>。<br>　　随着检测技术的发展，检测与跟踪的整合成为了趋势。<a href="#1" id="1ref">[1]</a> 是较早将跟踪的 “Motion Model” 用 Anchor-based Two-stage 网络来预测的方法，其网络输入为前后帧图像，其中一个分支输出当前帧的检测框，另一个分支用上一帧的检测结果作为 proposal，输出这一帧的跟踪框，最后用传统的数据关联方法得到目标的 ID。随着检测技术往 Anchor-Free One-stage 方向发展，在此基础上整合目标检测与跟踪也就顺理成章。<br>　　<a href="/Anchor-Free-Detection/" title="Anchor-Free Detection">Anchor-Free Detection</a> 中详细描述了 Anchor-Free 的目标检测方法，相比于 Anchor-Based 的目标检测，其有很多优势，这里不做赘述。本文基于 CenterNet<a href="#2" id="2ref"><sup>[2]</sup></a>，总结了 CenterTrack<a href="#3" id="3ref"><sup>[3]</sup></a>，以及 CenterPoint(3D CenterTrack)<a href="#4" id="4ref"><sup>[4]</sup></a>方法。</p><h2 id="centernet">1. CenterNet</h2><p>　　CenterNet 在 <a href="/Anchor-Free-Detection/" title="Anchor-Free Detection">Anchor-Free Detection</a> 中已经较为详细得阐述了。需要补充的是，中心点的正负样本设计为：正样本只有中心点像素，负样本则为其它区域，并加入以中心点为中心的高斯权重，越靠近中心点，负样本权重越小。其 Loss 基于 Focal Loss，数学描述为： <span class="math display">\[L _ k = \frac{1}{N}\sum _ {xyc}\left\{\begin{array}{l}(1-\hat{Y} _ {xyc})^{\alpha}\mathrm{log}(\hat{Y} _ {xyc}) &amp; \mathrm{if}\; Y _ {xyc} = 1\\(1- Y _ {xyc})^{\beta}(\hat{Y} _ {xyc})^{\alpha}\mathrm{log}(1-\hat{Y} _ {xyc}) &amp; \mathrm{otherwise}\end{array}\tag{1}\right.\]</span> 其中 \(Y _ {xyc}\) 为高斯权重后的正负样本分布值。<br>　　具体的，设图像 \(I\in \mathbb{R}^{W\times H\times 3}\)，CenterNet 输出的每个类别 \(c\in\{0,...,C-1\}\) 的目标为 \(\{(\mathbf{p} _ i, \mathbf{s} _ i)\} _ {i=0} ^ {N-1}\)。其中 \(\mathbf{p}\in \mathbb{R} ^ 2\)，\(\mathbf{s}\in\mathbb{R} ^ 2\) 为目标框的尺寸。对应的，最终输出的 heatmap 位置和尺寸图为：\(\hat{Y}\in [0,1]^{\frac{W}{R}\times\frac{H}{R}\times C}\)，\(\hat{S}\in\mathbb{R}^{\frac{W}{R}\times\frac{H}{R}\times 2}\)。对 \(\hat{Y}\) 作 \(3\times 3\) 的 max pooling，即可获得目标中心点，\(\hat{S}\) 上对应的的点即为该目标的尺寸。此外还用额外的 heatmap 作位置 offset 的回归，因为 \(\hat{Y}\) 存在量化误差。最终由中心点位置 loss，位置 offset loss，尺寸 loss 三部分组成。</p><h2 id="centertrack">2. CenterTrack</h2><h3 id="framework">2.1. Framework</h3><p><img src="/CenterTrack/centertrack.png" width="85%" height="85%" title="图 1. CenterTrack"> 　　如图 1. 所示，CenterTrack 基于 CenterNet，框架也较为简单：输入前后帧图像，以及上一帧跟踪到的目标中心点所渲染的 heatmap，经过网络后输出为当前帧的检测 heatmap，size map，以及这一帧相对上一帧跟踪的 offset map。最后通过最近距离匹配即可作数据关联获得目标的 ID。算法得到的目标属性有 \(b = (\mathbf{p,s},w,id)\)，分别为目标的 location，size，confidence，identity。<br>　　相比于 CenterNet，CenterTrack 还预测了这一帧相对上一帧，目标的 2D displacement：\(\hat{D}\in\mathbb{R}^{\frac{W}{R}\times\frac{H}{R}\times 2}\)。这相当于 Tracking 中 Motion Model 的结果，分别计算上一帧目标经过该 displacement 变换到这一帧后的目标位置与当前帧检测的目标位置的距离误差，用最小距离的贪心法即可将目标作数据关联，得到目标的 ID。</p><h3 id="experiments">2.2. Experiments</h3><p>　　网络结构相比于 CenterNet 只是增加了输入的四个通道特征，输出的两个通道特征。网络可在视频流图像或者单帧图像上训练，对于单帧图像，可对图像中的目标作伸缩平移变换来模拟目标运动，实验表明，也非常有效。 <img src="/CenterTrack/motion_models2d.png" width="85%" height="85%" title="图 2. Motion Models"> 　　如图 2. 所示，本文比较了 displacement 与 kalman filter，optical flow 等 Motion Model，显示本文效果是最好的，我猜测是因为 displacement 回归的直接是物体级别的像素运动，抗噪性更强。</p><h2 id="center-based-3d-object-detection-and-tracking">3. Center-based 3D Object Detection and Tracking</h2><h3 id="framework-1">3.1. Framework</h3><p><img src="/CenterTrack/centertrack3d.png" width="85%" height="85%" title="图 3. 3D CenterTrack"> 　　如图 3. 所示，CenterPoint 将点云在俯视图下栅格化，然后采用 CenterTrack 一样的网络结构，只是输出为目标的 3D location，size，orientation，velocity。<br>　　点云俯视图下的栅格化，如果对栅格不做点云的精细化估计，那么会影响到目标位置及速度估计的精度，所以理论上 PointPillars 这种栅格点云特征学习方式能更有效的提取点云的信息，保留特征的连续化信息(但是论文的实验表明 VoxelNet 比 PointPillars 效果更好)。否则，虽然目标位置等信息的监督项是连续量，但是栅格化的特征是离散量，这会降低预测精度。<br>　　具体的，网络输出为：\(K\) 个类别的 \(K\)-channel heatmap 表示目标中心点，目标的尺寸 \(\mathbf{s}=(w,l,h)\) heatmap，目标的中心点 offset \(\mathbf{o}=(o _ x,o _ y,o _ z)\) heatmap，朝向角 \(\mathbf{e} = (\mathrm{sin}(\alpha),\mathrm{cos}(\alpha))\) heatmap，目标速度 \(\mathbf{v}=(v _ x,v _ y)\) heatmap。与 CenterTrack 非常相似，只不过这里的速度就是真实的物理速度。</p><h3 id="experiments-1">3.2. Experiments</h3><p><img src="/CenterTrack/detmap.png" width="85%" height="85%" title="图 4. 3D Detection Benchmark"> 　　如图 4. 所示，引入 Velocity 预测，能有效提升检测的性能，这应该是网络输入前一帧信息的结果，对半遮挡情况能有较好效果。 <img src="/CenterTrack/experiment3d.png" width="85%" height="85%" title="图 5. 3D MOT Benchmark"> 　　如图 5. 所示，跟踪性能也是有很大提升，而且数据关联等后处理相对比较简单。</p><h2 id="reference">5. Reference</h2><p><a id="1" href="#1ref">[1]</a> Feichtenhofer, Christoph, Axel Pinz, and Andrew Zisserman. &quot;Detect to track and track to detect.&quot; Proceedings of the IEEE International Conference on Computer Vision. 2017.<br><a id="2" href="#2ref">[2]</a> Zhou, Xingyi, Dequan Wang, and Philipp Krähenbühl. &quot;Objects as points.&quot; arXiv preprint arXiv:1904.07850 (2019).<br><a id="3" href="#3ref">[3]</a> Zhou, Xingyi, Vladlen Koltun, and Philipp Krähenbühl. &quot;Tracking Objects as Points.&quot; arXiv preprint arXiv:2004.01177 (2020).<br><a id="4" href="#4ref">[4]</a> Yin, Tianwei, Xingyi Zhou, and Philipp Krähenbühl. &quot;Center-based 3D Object Detection and Tracking.&quot; arXiv preprint arXiv:2006.11275 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　障碍物感知由目标检测，目标跟踪(MOT)，目标状态估计等三个模块构成。目标状态估计一般是指将位置，速度等观测量作卡尔曼滤波平滑；广义的目标跟踪也包含了状态估计过程，这里采用狭义的目标跟踪定义方式，主要指出目标 ID 的过程。传统的做法，目标检测与目标跟踪是分开进行的，检
      
    
    </summary>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/categories/MOT/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="Tracking" scheme="https://leijiezhang001.github.io/tags/Tracking/"/>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/tags/MOT/"/>
    
  </entry>
  
  <entry>
    <title>Rethinking of Sparse 3D Convolution</title>
    <link href="https://leijiezhang001.github.io/Rethinking-of-Sparse-3D-Convolution/"/>
    <id>https://leijiezhang001.github.io/Rethinking-of-Sparse-3D-Convolution/</id>
    <published>2020-06-23T09:37:12.000Z</published>
    <updated>2020-06-25T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　Sparse 3D Convolution 最早在<a href="#1" id="1ref">[1]</a>中提出，然后该作者又提出了 Submanifold Sparse Convolution<a href="#2" id="2ref"><sup>[2]</sup></a>，并将其应用于 3D 语义分割中<a href="#3" id="3ref"><sup>[3]</sup></a>。<a href="#4" id="4ref">[4]</a>则改进了 Sparse 3D Convolution 的实现方式，并应用于 3D 目标检测中。之前一直没仔细看 Sparse 3D Convolution 原理，以为只是基于稀疏矩阵的矩阵相乘加速，最近的一些实验发现 Sparse 3D Convolution 在点云相关的任务中不仅仅是加速，还能提升网络特征提取的性能，所以回过头来重新思考 Sparse 3D Convolution 原理及作用。</p><h2 id="sparse-convolution">1. Sparse Convolution</h2><p><img src="/Rethinking-of-Sparse-3D-Convolution/spconv.png" width="85%" height="85%" title="图 1. sparse VS. submanifold sparse"> 　　如图 1. 左图所示，对于稀疏的特征输入，传统的 Sparse Convolution 与 Convolution 一致，只是对于卷积核覆盖的输入特征为零的区域不做计算，直接置为零。这种方式下，随着卷积层的增加，特征层会变得不那么稀疏，这样不仅使得计算量上升，而且会使得提取的信息变得不那么准确。</p><h2 id="submanifold-sparse-convolution">2. Submanifold Sparse Convolution</h2><p>　　如图 1. 右图所示，Submanifold Sparse Convolution 解决了 Sparse Convolution 存在的问题。原理也很直观：只计算输出特征层映射到输入特征层不为零的位置区域。这种方式下，随着卷积层的增加，不仅能保持稀疏性，而且能保证原始信息的准确性。 <img src="/Rethinking-of-Sparse-3D-Convolution/flops.png" width="85%" height="85%" title="图 2. Flops"> 　　如图 2. 所示，Sparse Convolution 相比传统的 Convolution 已经能减少较多的计算量，而 Submanifold Sparse Convolution 则能减少更多的计算量。特征输入越稀疏，减少的计算量就越多，这对点云的三维特征提取，或者是俯视图下的二维特征提取有很大的帮助。</p><h2 id="implementation">3. Implementation</h2><p><img src="/Rethinking-of-Sparse-3D-Convolution/speed.png" width="85%" height="85%" title="图 3. Speed"> 　　<a href="#2" id="2ref">[2]</a> 中实现了 Submanifold Sparse Convolution，其中的卷积运算是手写的矩阵相乘，所以速度较慢；<a href="#4" id="4ref">[4]</a> 则基于 GEMM 实现了更高效的 Submanifold Sparse Convolution。如图 3. 所示，其有将近一倍的速度提升。 <img src="/Rethinking-of-Sparse-3D-Convolution/imple.png" width="90%" height="90%" title="图 4. Implementation"> 　　图 4. 描述了<a href="#4" id="4ref">[4]</a>实现的 Submanifold Sparse Convolution 原理。其首先通过 gather 操作将非零的元素进行矩阵相乘，然后通过 scatter 操作将结果映射回原位置。为了加速，前后元素的映射矩阵计算比较关键，这里实现了一种 GPU 计算方法，这里不做展开。</p><h2 id="application">4. Application</h2><p><img src="/Rethinking-of-Sparse-3D-Convolution/second.png" width="90%" height="90%" title="图 5. SECOND Framework"> 　　Submanifold Sparse Convolution 可应用于点云的分类，分割，检测等任务的特征提取中，SECOND<a href="#4" id="4ref"><sup>[4]</sup></a>是一种点云检测方法，如图 5. 所示，其检测框架与传统的一致，只是将体素化后的点云特征信息，进一步用 Sparse Convolution 来作特征提取。该方法不仅速度较快，而且性能也有不少提升。所以 Submanifold Sparse Convolution 是非常高效的，可作为点云特征提取的基本操作。但是传统的 Convolution，在 GPU 平台下，已经有较多的硬件级优化(cudnn)，在 CPU 平台下也有很多的指令集优化，所以最终在特定硬件下作 Inference 时，到底 Submanifold Sparse Convolution 速度能提升多少，还得看 Submanifold Sparse Convolution 实现的好不好。不过可以猜测，在目前的实现下，Submanifold Sparse Convolution 在 GPU 平台下应该能有不少的速度提升。<br>　　此外，传统的卷积量化操作也比较成熟，cudnn 已经有基本的操作引擎，而 Submanifold Sparse Convolution 的 INT8 引擎则目前还没有。所以 float32/float16 的 Submanifold Sparse Convolution 与 INT8 的 Convolution，孰快孰慢？这两条路大概就是部署的思路了，当然 INT8 的 Submanifold Sparse Convolution 会更好，但是开发成本会比较高。</p><h2 id="reference">5. Reference</h2><p><a id="1" href="#1ref">[1]</a> Graham, Ben. &quot;Sparse 3D convolutional neural networks.&quot; arXiv preprint arXiv:1505.02890 (2015).<br><a id="2" href="#2ref">[2]</a> Graham, Benjamin, and Laurens van der Maaten. &quot;Submanifold sparse convolutional networks.&quot; arXiv preprint arXiv:1706.01307 (2017).<br><a id="3" href="#3ref">[3]</a> Graham, Benjamin, Martin Engelcke, and Laurens Van Der Maaten. &quot;3d semantic segmentation with submanifold sparse convolutional networks.&quot; Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.<br><a id="4" href="#4ref">[4]</a> Yan, Yan, Yuxing Mao, and Bo Li. &quot;Second: Sparsely embedded convolutional detection.&quot; Sensors 18.10 (2018): 3337.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Sparse 3D Convolution 最早在&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;[1]&lt;/a&gt;中提出，然后该作者又提出了 Submanifold Sparse Convolution&lt;a href=&quot;#2&quot; id=&quot;2ref&quot;&gt;&lt;sup&gt;[2]&lt;/sup
      
    
    </summary>
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection&quot;</title>
    <link href="https://leijiezhang001.github.io/End-to-End-Pseudo-LiDAR-for-3D-Det/"/>
    <id>https://leijiezhang001.github.io/End-to-End-Pseudo-LiDAR-for-3D-Det/</id>
    <published>2020-06-22T01:19:12.000Z</published>
    <updated>2020-06-25T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　基于视觉的 3D 目标检测方法因为成本较低，所以在 ADAS 领域应用非常广泛。其基本思路有以下几种：</p><ul><li>单目\(\rightarrow\)3D 框，代表文章有<a href="#1" id="1ref">[1]</a>。</li><li>单目\(\rightarrow\)深度图\(\rightarrow\)3D 框</li><li>双目\(\rightarrow\)3D 框，代表文章有<a href="#2" id="2ref">[2]</a>。</li><li>双目\(\rightarrow\)深度图\(\rightarrow\)3D 框</li></ul><p>由单目或双目直接回归 3D 目标框的属性，这种方法优势是 latency 小，缺点则是，没有显式的预测深度图，导致目标 3D 位置回归较为困难。而在深度图基础上回归 3D 目标位置则相对容易些，这种方法由两个模块构成：深度图预测，3D 目标预测。得到深度图后，可以在前视图下将深度图直接 concate 到 rgb 图上来做，另一种方法是将深度图转换为 pseudo-LiDAR 点云，然后用基于点云的 3D 目标检测方法来做，目前学术界基本有结论：pseudo-LiDAR 效果更好。<br>　　本文<a href="#3" id="3ref"><sup>[3]</sup></a>即采用双目出深度图，然后基于 pseudo-LiDAR 来作 3D 目标检测的方案，并且解决了两个模块需要两个网络来优化的大 lantency 问题，实现了 End-to-End 联合优化的方式。</p><h2 id="framework">1. Framework</h2><p><img src="/End-to-End-Pseudo-LiDAR-for-3D-Det/framework.png" width="60%" height="60%" title="图 1. Framework"> 　　基于点云作 3D 目标检测大致可分为 point-based 与 voxel-based 两大类，详见 <a href="/Point-based-3D-Det/" title="Point-based 3D Detection">Point-based 3D Detection</a>，传统的基于双目的 pseudo-LiDAR 方案无法 End-to-End 作俯视图下 voxel-based 3D 检测，因为点云信息需要作俯视图离散化，离散的过程是无法作反向传播训练的，本文提出了 Change of Representation(CoR) 模块有效解决了这个问题。如图 1. 所示，本方案中 Depth Estimation 可由任何深度估计网络实现，然后经过 CoR 模块，将深度图变换成点云形式用于 point-based 3D detection，或者是 Voxel 形式用于 voxel-based 3D detection。这里的关键是可求导的 CoR 模块设计。</p><h2 id="cor">2. CoR</h2><h3 id="quantization">2.1. Quantization</h3><p>　　点云检测模块如果采用 voxel-based 方案，那么点云到俯视图栅格的离散化(quantization)是必不可少的。假设点云 \(P = \{p _ 1,...,p _ N\}\)，待生成的 3D 占据栅格(最简单的特征形式) \(T\) 包含 \(M\) 个 bins，即 \(m\in\{1,...,M\}\)，每个 bin 的中心点设为 \(\hat{p} _ m\)。那么生成的 \(T\) 可表示为： <span class="math display">\[ T(m) = \left\{\begin{array}{l}1, &amp; \mathrm{if}\;\exists p\in P \; \mathrm{s.t.}\; m = \mathop{\arg\min}\limits _ {m &#39;}\Vert p - \hat{p} _ {m&#39;}\Vert _ 2 \\0, &amp; \mathrm{otherwise}.\end{array}\tag{1}\right.\]</span> 即如果有点落在该 bin 里，那么该 bin 对应的值置为 1。这种离散方式是无法求导的。<br>　　本文提出了一种可导的软量化模块(soft quantization module)，即用 RBF 作权重计数，另一种角度来看，<strong>这其实类似于点的空间概率密度表示</strong>。设 \(P _ m\) 为落在 bin \(m\) 的点集： <span class="math display">\[ P _ m=\left\{p\in |, \mathrm{s.t.}\; m=\mathop{\arg\min}\limits _ {m &#39;}\Vert p - \hat{p} _ {m&#39;}\Vert _ 2\right\} \tag{2}\]</span> 那么，\(m'\) 作用于 \(m\) 的值为： <span class="math display">\[ T(m, m&#39;) = \left\{\begin{array}{l}0 &amp; \mathrm{if}\; \vert P _ {m&#39;}\vert = 0;\\\frac{1}{\vert P _ {m&#39;}\vert} \sum _ {p\in P _ {m&#39;}} e^{-\frac{\Vert p-\hat{p} _ m\Vert ^2}{\sigma ^ 2}} &amp; \mathrm{if}\; \vert P _ {m&#39;}\vert &gt; 0.\end{array}\tag{3}\right.\]</span> 最终的 bin 值为： <span class="math display">\[ T(m) = T(m,m)+\frac{1}{\vert \mathcal{N} _ m\vert}\sum _ {m&#39;\in\mathcal{N} _ m}T(m,m&#39;) \tag{4}\]</span> 当 \(\sigma ^2\gg 0\) 以及 \(\mathcal{N} _ m=\varnothing\) 时，回退到式 (1) 的离散化方式。本文实验中采用 \(\sigma ^2 = 0.01\)，\(\mathcal{N} _ m=3\times 3\times 3 -1 = 26\)。传统的点云栅格概率密度计算方式为：将点云中的每个点高斯化，然后统计每个栅格中心坐标上覆盖到的值。与上述方法的高斯原点不一样，但是计算结果是一致的。 <img src="/End-to-End-Pseudo-LiDAR-for-3D-Det/quantization.png" width="90%" height="90%" title="图 2. Quantization"> 　　这种方法可将导数反向传播到 \(m'\) 中的每个点：\(\frac{\partial\mathcal{L} _ {det}}{\partial T(m)}\times\frac{\partial T(m)}{\partial T(m,m')}\times\bigtriangledown _ pT(m,m')\)。如图 2. 所示，蓝色 voxel 表示梯度为正，即 \(\frac{\partial\mathcal{L} _ {det}}{\partial T(m)} &gt; 0\)，红色 voxel 表示梯度为负。那么蓝色 voxel 期望没有点，所以将点往外推，红色 voxel 则将点往里拉，最终使点云与 LiDAR 点云，即 GT 点云一致。</p><h3 id="subsampling">2.1. Subsampling</h3><p>　　点云检测模块如果采用 point-based 方案，那么就比较容易直接与深度图网络进行 End-to-End 整合。point-based 3D Detection 一般通过 sampling 来扩大感受野，提取局部信息，因为这种方法的计算量对点数比较敏感，所以 sampling 也是降低计算量的有效手段。一个 \(640\times 480\) 的深度图所包含的点云超过 30 万，远远超过一个 64 线的激光雷达，所以对其进行采样就非常关键。<br>　　本文对深度图点云进行模拟雷达式的采样，即定义球坐标系下栅格化参数：\((r,\theta,\phi)\)。其中 \(\theta\) 为水平分辨率，\(\phi\) 为垂直分辨率。对每个栅格内采样一个点，即可得到一个较为稀疏，且接近激光雷达扫描属性的点云。</p><h2 id="loss">3. Loss</h2><p>　　Loss 由 depth 估计与 3D Detection 两项构成： <span class="math display">\[\mathcal{L} = \lambda _ {det}\mathcal{L} _ {det} + \lambda _ {depth}\mathcal{L} _ {depth} \tag{5}\]</span></p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Mousavian, Arsalan, et al. &quot;3d bounding box estimation using deep learning and geometry.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.<br><a id="2" href="#2ref">[2]</a> Li, Peiliang, Xiaozhi Chen, and Shaojie Shen. &quot;Stereo r-cnn based 3d object detection for autonomous driving.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br><a id="3" href="#3ref">[3]</a> Qian, Rui, et al. &quot;End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection.&quot; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　基于视觉的 3D 目标检测方法因为成本较低，所以在 ADAS 领域应用非常广泛。其基本思路有以下几种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;单目\(\rightarrow\)3D 框，代表文章有&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;[1]&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;单目
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;PointPainting: Sequential Fusion for 3D Object Detection&quot;</title>
    <link href="https://leijiezhang001.github.io/PointPainting/"/>
    <id>https://leijiezhang001.github.io/PointPainting/</id>
    <published>2020-06-17T03:27:38.000Z</published>
    <updated>2020-06-22T09:00:45.632Z</updated>
    
    <content type="html"><![CDATA[<p>　　相机能很好的捕捉场景的语义信息，激光雷达则能很好的捕捉场景的三维信息，所以图像与点云的融合，对检测，分割等任务有非常大的帮助。融合可分为，<strong>数据级或特征级的前融合</strong>，以及<strong>任务级的后融合</strong>。本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种将图像分割结果的语义信息映射到点云，进而作 3D 检测的方法。这种串行方式的融合，既有点前融合的意思，也有点后融合的意思，暂且可归为前融合吧。本方法可认为是个框架，该框架下，基于图像的语义分割，以及基于点云的 3D 检测，均为独立模块。实验表明，融合了图像的语义信息后，点云针对行人等小目标的检测有较大的性能提升。</p><h2 id="framework">1. Framework</h2><p><img src="/PointPainting/framework.png" width="100%" height="100%" title="图 1. Framework"> 　　如图 1. 所示，算法框架非常简单，一句话能说明白：1). 首先经过图像语义分割获得语义图；2). 然后将点云投影到图像上，查询点云的语义信息，并连接到坐标信息中；3). 最后用点云 3D 检测的方法作 3D 检测。</p><h2 id="experiments">2. Experiments</h2><p><img src="/PointPainting/sota.png" width="90%" height="90%" title="图 2. PointPainting Applied to SOTA"> 　　采用 DeepLabv3+ 作为语义分割模块，应用到不同的点云 3D 检测后，结果如图 2. 所示，均有不同程度的提升，尤其是行人这种小目标。 <img src="/PointPainting/pointrcnn.png" width="90%" height="90%" title="图 3. Painted PointRCNN"> 　　图 3. 显示了 Painted PointRCNN 与各个方法的对比结果，mAP 是最高的。 <img src="/PointPainting/per-class.png" width="90%" height="90%" title="图 4. 不同类别的提升程度"> 　　由图 4. 可知，对行人，自行车，雪糕筒等小目标(俯视图下来说)，本方法提升非常显著。这也比较好理解，因为前视图下，这些目标所占的像素会比较多，所以更容易在前视相机图像下提取有效信息，辅助俯视图下作更准确的检测。</p><h2 id="rethinking-of-early-fusion">3. Rethinking of Early Fusion</h2><p>　　这里将本方法归为前融合，但是并不是真正意义上的前融合。如果是前融合，那么一般是 concate 语义分割网络的中低层特征到点云信息中，然而本文是直接取语义分割网络的最高层特征(即分类结果)。<strong>所以问题来了，所谓的前融合，一定比后融合更好吗？</strong>我想，这篇文章可能给了一些答案(不知道作者有没有做过取其它特征的实验，姑且认为做过，然后选择了本方法的策略)，虽然理论上前融合信息最完整，但是，如果这种完整的信息无法有效学出来或者对标定外参比较敏感，那么这种前融合也提升不了后续任务的性能，更有甚者，由于信息空间的变大或紊乱，导致后续任务性能下降。相反，对于不是那么“前”的后融合，我们能极大得保证各个任务学习结果的有效性，基于此，融合后学习的有效性也会比较确定。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Vora, Sourabh, et al. &quot;Pointpainting: Sequential fusion for 3d object detection.&quot; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　相机能很好的捕捉场景的语义信息，激光雷达则能很好的捕捉场景的三维信息，所以图像与点云的融合，对检测，分割等任务有非常大的帮助。融合可分为，&lt;strong&gt;数据级或特征级的前融合&lt;/strong&gt;，以及&lt;strong&gt;任务级的后融合&lt;/strong&gt;。本文&lt;a href=
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Boundary-Aware Dense Feature Indicator for Single-Stage 3D Object Detection from Point Clouds&quot;</title>
    <link href="https://leijiezhang001.github.io/DENFIDet/"/>
    <id>https://leijiezhang001.github.io/DENFIDet/</id>
    <published>2020-05-22T03:27:38.000Z</published>
    <updated>2020-06-12T02:22:23.113Z</updated>
    
    <content type="html"><![CDATA[<p>　　俯视图下 Voxel-based 点云 3D 目标检测一般会使用 2D 检测网络及相关策略。但是不同于图像的 2D 目标检测，俯视图下目标的点云信息基本在边缘处，所以如何准确得捕捉目标的边缘信息对特征提取的有效性非常关键。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 提出了一种能捕捉目标边缘信息的网络结构，从而作更准确的目标检测。</p><h2 id="framework">1. Framework</h2><p><img src="/DENFIDet/framework.png" width="80%" height="80%" title="图 1. Framework of DENFIDet"> 　　如图 1. 所示，在传统的 One-Stage 2D/3D 检测框架下，嵌入了 DENFI 模块，该模块首先通过 DBPM 生成稠密的目标边缘 proposals，然后指导 DENFIConv 去提取更准确的目标特征，输出到检测头作 3D 目标框属性的分类与回归。</p><h2 id="denfidense-feature-indicator">2. DENFI(Dense Feature Indicator)</h2><h3 id="dbpmdense-boundary-proposal-module">2.1. DBPM(Dense Boundary Proposal Module)</h3><p><img src="/DENFIDet/DBPM.png" width="60%" height="60%" title="图 2. DBPM"> 　　DBPM 的输入为 Backbone 输出的 \(H\times W\times C\) 特征图，其由分类和回归两个分支构成：</p><ul><li>分类分支<br>分类分支经过 \(1\times 1\) 卷积输出 \(H\times W\times K\) 大小的 pixel 级别的 Score Map，其中 \(K\) 为类别数；</li><li>回归分支<br>回归分支也经过 \(1\times 1\) 卷积，输出 \(H\times W\times (4+n\times 2)\) 大小的回归量。回归量包括 \((l,t,r,b)\) 以及角度 \((\theta ^{bin}, \theta ^{res})\)(角度回归分解成了 n 个 bin 分类与 bin 内残差回归两个问题)。最终解码为描述目标边缘的信息：\((l,t,r,b,\theta)\)。</li></ul><p>　　Loss 的计算首先得区分正负样本。正负样本的划分思想与传统的差不多，主要思想是正负样本过渡区域引入 Ignore。如图 4. 所示，设 3D 真值框属性表示为 \((x,y,w,l,\theta)\)，正样本区域设计为 \((x,y,\sigma _ 1w,\sigma _ 1l,\theta)\)，定义另一缩小框 \((x,y,\sigma _ 2w,\sigma _ 2l,\theta)\)，其中 \(\sigma _ 1 &lt; \sigma _ 2\)。由此可得，灰色为正样本区域，黄色为 Ignore 区域，其它为负样本区域。<br>　　对于分类的 Loss，直接对正负样本进行 Focal Loss 计算。对于回归分支，则采用正样本的平均 Loss。回归 Loss 由目标框的 IoU Loss 以及角度 Loss 组成，角度 Loss 又由 bin 分类 Loss 加 bin 残差回归 Loss 组成。这里不做展开。<br>　　需要注意的是，分类分支只在训练的时候计算，Inference 时候只作回归分支的计算，从而得到每个像素感知到的目标边缘的信息。</p><h3 id="denficonv">2.2. DENFIConv</h3><p><img src="/DENFIDet/dconv.png" width="80%" height="80%" title="图 3. dconv"> 　　如图 3. 所示，Deformable Convolution 的思想是自动去寻找感兴趣的卷积区域。DBPM 获得每个像素点的目标边缘信息以后，自然的，接下来对像素点的卷积运算，运用可变形卷积可以捕捉更准确的目标区域信息。 <img src="/DENFIDet/DSDC.png" width="60%" height="60%" title="图 4. DSDC"> 　　如图 4. 所示，结合 Deformable Convolution 与 depth-wise separable Convolution，本文提出了 Depth-wise Separable Deformable Convolution。即将 \(3\times 3\) 的可变形卷积拆解成 \(3\times 3\) 的 Depth-wise 卷积以及 \(1\times 1\) 的可变形卷积，极大减少参数量。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Xu, Guodong, et al. &quot;Boundary-Aware Dense Feature Indicator for Single-Stage 3D Object Detection from Point Clouds.&quot; arXiv (2020): arXiv-2004.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　俯视图下 Voxel-based 点云 3D 目标检测一般会使用 2D 检测网络及相关策略。但是不同于图像的 2D 目标检测，俯视图下目标的点云信息基本在边缘处，所以如何准确得捕捉目标的边缘信息对特征提取的有效性非常关键。本文&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;SA-SSD: Structure Aware Single-stage 3D Object Detection from Point Cloud&quot;</title>
    <link href="https://leijiezhang001.github.io/SA-SSD/"/>
    <id>https://leijiezhang001.github.io/SA-SSD/</id>
    <published>2020-05-22T03:27:38.000Z</published>
    <updated>2020-05-22T10:04:17.044Z</updated>
    
    <content type="html"><![CDATA[<p>　　Voxel-based 3D Detection 相比 <a href="/Point-based-3D-Det/" title="Point-based 3D Detection">Point-based 3D Detection</a> 的缺点是特征提取不仅在 Voxel 阶段损失了一定的点云信息，而且 Voxel 化后丢失了点云之间的拓扑关系。<a href="/Point-based-3D-Det/" title="Point-based 3D Detection">Point-based 3D Detection</a> 中详细描述了几种 Point-based 方法，这种方法目前比较棘手的地方是，即使作 Inference 时，也需要作 kd-tree 搜索与采样等运算量较大的操作。那么如何榨干 Voxel-based 的性能对工业界落地就显得比较重要了，本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种单阶段的 Voxel-based 3D 检测方法，并借助了 Point 级别特征提取的相关策略，使得检测性能有较大提升。</p><h2 id="framework">1. Framework</h2><p><img src="/SA-SSD/framework.png" width="100%" height="100%" title="图 1. Framework of SA-SSD"> 　　如图 1. 所示，SA-SSD 由三部分组成：Backbone，Detection Head，Auxiliary Network。<br>　　Backbone 的输入是栅格化后的点云表示方式，文中栅格大小设定为 \(0.05m,0.05m,0.1m\)。Backbone 由一系列的 3D convolution 组成，因为需要保留空间三维位置信息，作 Voxel-to-Point 的映射。这里如果用 2D convolution 代替，那么 Auxiliary Network 估计也只能作 BridView 的分割了。<br>　　Detection Head 主体就是传统 Anchor-Free 结构，一个分支用于预测每个特征层像素点的 Confidence，另一个分支用于预测基于每个特征层像素点的 BBox 属性，如，以该点为 &quot;Anchor&quot; 的四个顶点坐标。此外，为了消除 One-Stage 方法中目标框与置信度不对齐的问题，本文引入 Part-sensitive Warping 来实现与 PSRoiAlign 类似的作用，实现两者的对齐。<br>　　Auxiliary Network 只在训练的阶段起作用，Inference 阶段不需要计算。该模块的作用是训练时通过 Voxel-to-Point 特征映射来反向传播监督 Backbone 中的 Voxel 特征学习 Point 级别的特征，包括点云的空间拓扑关系。<strong>当然 Inference 时也可以保留该分割模块，那么还可以增加点级别的特征反映射到 Voxel 的模块(Point-to-Voxel)，进一步作特征增强。</strong></p><h2 id="detachable-auxiliary-network">2. Detachable Auxiliary Network</h2><p><img src="/SA-SSD/sa.png" width="60%" height="60%" title="图 2. Structured Aware Feature Learning"> 　　如图 2. 所示，随着 Backbone 特征提取的感受野增大(特征分辨率下降)，背景点会接近目标的边缘，使得目标框大小不容易预测准确。本文提出的 Auxiliary Network，通过增加点级别分割及目标中心坐标预测任务，来监督 Backbone 特征层捕捉这种结构信息，从而达到更准确的目标检测的目的。<br>　　Auxiliary Network 的输入来自 Backbone 各个分辨率的特征层。将特征层上不为零的特征点，通过 Voxel-to-Point 反栅格化映射到三维空间，设该特征点表示为 \(\{(f _ j,p _ j):j=1,...,M\}\)，其中 \(f\) 为特征向量，\(p\) 为坐标向量。有了栅格对应的伪三维坐标点下的特征表示后，即可插值出实际点云中每个点的特征向量。设点云中点的插值特征为：\(\{(\hat{f} _ i,p _ i):i=1,...,N\}\)，采用 Inverse Distance Weighted 方法进行插值： <span class="math display">\[ \hat{f} _ i = \frac{\sum _ {j=1}^Mw _ j(p _ i)f _ j}{\sum _ {j=1}^Mw _ j(p _ i)} \tag{1}\]</span> 其中： <span class="math display">\[w _ j(p _ i)=\left\{\begin{array}{l}\frac{1}{\Vert p _ i-p _ j\Vert _ 2} &amp; \mathrm{if} p _ j\in\mathcal{N}(p _ i)\\0 &amp; \mathrm{otherwise}\end{array}\tag{2}\right.\]</span> \(\mathcal{N}(p _ i)\) 为球状区域，本文在四个分辨率下分别设定为：0.05m，0.1m，0.2m，0.4m。然后通过 cross-stage link 对各个分辨率下的点特征进行 concatenate 融合。最后通过感知机进行点云分割及目标中心点预测任务的构建。<br>　　对于点级别前景分割的任务，经过 sigmoid 函数后，应用二分类的 Focal Loss： <span class="math display">\[ \mathcal{L} _ {seg} = \frac{1}{N _ {pos}}\sum _ i^N -\alpha(1-\hat{s} _ i)^{\gamma}\mathrm{log}(\hat{s} _ i) \tag{3}\]</span> 该分割任务使得目标检测的框更加准确，如图 2.c 所示。但是还得优化其尺度与形状。<br>　　中心点的预测任务则能有效约束目标框的尺度与形状，具体的，预测的是每个属于目标的点云与中心点的相对位置(残差)。可用 Smooth-l1 来构建预测的中心点与实际中心点的 Loss。</p><h2 id="part-sensitive-warping">3. Part-sensitive Warping</h2><p><img src="/SA-SSD/psw.png" width="60%" height="60%" title="图 3. Part-sensitive Warping"> 　　One-Stage 方法都会有 Confidence 和 BBox 错位的现象，本文提出一种类似 PSROIAlign 但更有高效的 PSW 方法，具体步骤为：</p><ol type="1"><li>对于分类分支，修改为 \(K\) 个 Part-sensitive 的 cls maps，每个 map 包含目标的部分信息，比如当 \(K=4\) 时，可以理解为将目标切分为 \(2\times 2\) 部分；</li><li>对于回归分支，将每个目标框的 Feature map 划分为 \(K\) 个子区域，每个区域的中心点作为采样点；</li><li>如图 3. 所示，通过采样得到最终 cls map 的平均值。</li></ol><h2 id="experiment">4. Experiment</h2><p><img src="/SA-SSD/ablation.png" width="60%" height="60%" title="图 4. Ablation Study"> 　　如图 4. 所示，Auxiliary Network 能有效提升网络的定位精度，PSWarp 也能有效消除 Confidence 与 BBox 的错位影响。</p><h2 id="reference">5. Reference</h2><p><a id="1" href="#1ref">[1]</a> henhang, et al. &quot;Structure Aware Single-stage 3D Object Detection from Point Cloud.&quot;</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Voxel-based 3D Detection 相比 &lt;a href=&quot;/Point-based-3D-Det/&quot; title=&quot;Point-based 3D Detection&quot;&gt;Point-based 3D Detection&lt;/a&gt; 的缺点是特征提取不仅在 Vo
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Joint 3D Instance Segmentation and Objection Detection for Autonomous Driving&quot;</title>
    <link href="https://leijiezhang001.github.io/Instance-Seg-and-Obj-Det/"/>
    <id>https://leijiezhang001.github.io/Instance-Seg-and-Obj-Det/</id>
    <published>2020-05-22T03:27:38.000Z</published>
    <updated>2020-06-17T02:16:44.008Z</updated>
    
    <content type="html"><![CDATA[<p>　　检测的发展基本上是从 Anchor-based 这种稀疏的方式到 Anchor-free 这种密集检测方案演进的。相比于 Anchor-free 这种特征层像素级别的回归与分类来检测，更密集的方式，是直接作 Instance Segmentation，然后经过聚类等后处理来得到目标框属性。越密集的检测方案，因为样本较多(一定程度增大了样本空间)，所以学习越困难，但是理论上有极高的召回率。随着一系列技术的发展，如 Focal-loss 等，密集检测性能得以超过二阶段的 Anchor-based 方案，具体描述可参考 <a href="/Anchor-Free-Detection/" title="Anchor-Free Detection">Anchor-Free Detection</a>。<br>　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>借鉴 2D Instance Segmentation 思路，提出了一种同时作 3D Instance Segmentation 与 Detection 的方法。百度 Apollo 中的点云分割方法就是俯视图下 Instance Segmentation 然后后处理得到目标 Polygon 与 BBox 的思路，这种方法虽然后处理较为复杂，但是有超参数较少且召回率高的特点。本文算是该方法的 3D 版本(想法很自然，被人捷足先登。。)。</p><h2 id="framework">1. Framework</h2><p><img src="/Instance-Seg-and-Obj-Det/framework.png" width="100%" height="100%" title="图 1. Framework"> 　　如图 1. 所示，本方法由三部分构成：点级别的分类及回归，候选目标聚类，目标框优化。</p><ul><li><strong>点级别的分类与回归</strong><br>原始点云经过 Backbone 网络提取局部及全局特征，这里的 Backbone 网络可以是任意能提取点级别特征的网络。基于 Backbone 网络提取的特征，可进行点级别的 Semantic Segmentation 以及 Instance-aware SE(Spatial Embedding)。SE 回归的是每个点距离目标中心点的 offset，该目标的 size，以及该目标的朝向。</li><li><strong>候选目标聚类</strong><br>基于预测的 SE，将每个点的位置加上距离目标中心点的 offset，然后可通过简单的聚类算法(如 K-means)即可得到各个目标的点云集合，取 top k 个该点云集合回归的目标框属性，作下一步的目标框进一步优化。</li><li><strong>目标框优化</strong><br>基于候选目标聚类得到的目标框，提取目标点集，将其转换到该目标 Local 坐标系下，作进一步的目标框优化。</li></ul><h2 id="instance-aware-se">2. Instance-aware SE</h2><p>　　该框架的关键是 Instance-aware SE 的回归，回归量有：距离目标中心点的 offset，目标 size，目标 orientation。传统的 Instance Segmentation 做法是 Feature Embedding，将相同 Instance 的特征拉近，不同的 Instance 的特征推远，这种方法很难构造有效的 Loss 函数，而且同为车的不同 Instance，其特征已经非常接近。而本文 Spatial Embedding 中 offset 的回归量，经过聚类后处理，可以很容易的得到 Instance Segmentation 结果。<br>　　Apollo 点云分割的方案中，是在俯视图的 2D 栅格下做的，主要回归量也是这三种，不同的是，2D 栅格是离散的，所以根据 offset 找某一点的中心点时，可以迭代的进行，然后投票出中心点位置，后处理可以做的更细致。这里不做展开，有机会以后写一篇详解。</p><h2 id="loss">3. Loss</h2><p>　　Loss 项由 Semantic Segmentation，SE，3D BBox regression 组成： <span class="math display">\[ L = L _ {seg-cls}+L _ {SE}+L _ {reg} \tag{1}\]</span> Semantic Segmentation Loss 为： <span class="math display">\[ L _ {seg-cls}=-\sum _ {i=1}^C (y _ i\mathrm{log}(p _ i)(1-p _ i)^{\gamma}\alpha _ i+(1- y _ i)\mathrm{log}(1-p _ i)(p _ i)^{\gamma}(1-\alpha _ i)) \tag{2}\]</span> 其中 \(C\) 表示类别数；如果某点属于某类，那么 \(y _ i=1\)；\(p _ i\) 表示预测为第 \(i\) 类的概率；\(\gamma,\alpha\) 为超参数。<br>SE Loss 为： <span class="math display">\[ L _ {SE} = \frac{1}{N}\sum _ {i=1}^N\frac{1}{N _ c}\sum _ {i\in ins _ c}^{N _ c}(\mathcal{l} _ {offset}^i+\mathcal{l} _ {size}^i+\mathcal{l} _ {\theta}^i) \tag{3}\]</span> 其中 \(N\) 为 Instance 个数，\(N _ c\) 为内部点数，\(\mathcal{l}\) 为 L1 Smooth Loss。<br>BBox regression Loss 为 rotated 3D IOU Loss： <span class="math display">\[ L _ {reg} = 1-\mathbf{IoU}(B _ g,B _ d)\tag{4}\]</span></p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Zhou, Dingfu, et al. &quot;Joint 3D Instance Segmentation and Object Detection for Autonomous Driving.&quot; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　检测的发展基本上是从 Anchor-based 这种稀疏的方式到 Anchor-free 这种密集检测方案演进的。相比于 Anchor-free 这种特征层像素级别的回归与分类来检测，更密集的方式，是直接作 Instance Segmentation，然后经过聚类等后处
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="Semantic Segmentation" scheme="https://leijiezhang001.github.io/tags/Semantic-Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>非线性最小二乘</title>
    <link href="https://leijiezhang001.github.io/Non-linear-Least-Squares/"/>
    <id>https://leijiezhang001.github.io/Non-linear-Least-Squares/</id>
    <published>2020-05-18T01:19:54.000Z</published>
    <updated>2020-05-24T04:47:24.293Z</updated>
    
    <content type="html"><![CDATA[<p>　　非线性最小二乘(Non-linear Least Squares)问题应用非常广泛，尤其是在 SLAM 领域。<a href="/LOAM/" title="LOAM">LOAM</a>，，<a href="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/" title="Stereo-RCNN">Stereo-RCNN</a>，<a href="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/" title="Stereo Vision-based Semantic and Ego-motion Tracking for Autonomous Driving">Stereo Vision-based Semantic and Ego-motion Tracking for Autonomous Driving</a> 等均需要求解非线性最小二乘问题。其中 <a href="/LOAM/" title="LOAM">LOAM</a> 作为非常流行的激光 SLAM 框架，其后端是一个典型的非线性最优化问题，本文会作为实践进行代码级讲解。</p><h2 id="问题描述">1. 问题描述</h2><p>　　在前端观测-后端优化框架下，设观测数据对集合为：\(\{y _ i,z _ i\} _ {i=1}^m\)，待求解的变量参数 \(x\in\mathbb{R}^n\) 定义了观测数据对的映射关系，即 \(z _ i=h(y _ i;x)\)，由此得到有 \(m\) 个参数方程 \(F(x)=[f _ 1(x),...,f _ m(x)]^T\)，其中 \(f _ i(x) = z _ i-h(y _ i;x)\)。我们要找到最优的参数 \(x\) 来描述观测数据对之间的关系，即求解的最优化问题为： <span class="math display">\[\begin{align}\mathop{\arg\min}\limits _ x \frac{1}{2}\Vert F(x)\Vert ^2 \iff \mathop{\arg\min}\limits _ x\frac{1}{2}\sum _ i \rho _ i\left(\Vert f _ i(x)\Vert ^ 2\right)\\L\leq x \leq U\end{align}\tag{1}\]</span> 其中 \(f _ i(\cdot)\) 为 Cost Function，\(\rho _ i(\cdot)\) 为 Loss Function，即核函数，用来减少离群点对非线性最小二乘优化的影响；\(L,U\) 分别为参数 \(x\) 的上下界。当核函数 \(\rho _ i(x)=x\) 时，就是常见的非线性最小二乘问题。<br>　　《视觉 SLAM 十四讲》<a href="#1" id="1ref"><sup>[1]</sup></a>在 SLAM 的状态估计问题中，从概率学角度导出了最大似然估计求解状态的方法，并进一步引出了最小二乘问题。回过头来看，本文很多内容在《视觉 SLAM 十四讲》中已经有非常清晰的描述，可作进一步参考。</p><h2 id="问题求解">2. 问题求解</h2><p>　　根据 \(F(x)\) 求得雅克比矩阵(Jacobian)：\(J(x) \in\mathbb{R}^{m\times n}\)，即 \(J _ {ij}(x)=\frac{\partial f _ i(x)}{\partial x _ j}\)。目标函数的梯度向量为 \(g(x) = \nabla\frac{1}{2}\Vert F(x)\Vert ^ 2=J(x)^TF(x)\)。在 \(x\) 处将目标函数线性化：\(F(x+\Delta x)\approx F(x)+J(x)\Delta x\)。由此非线性最小二乘问题可转换为线性最小二乘求解残差量 \(\Delta x\) 来近似求解： <span class="math display">\[\mathop{\arg\min}\limits _ {\Delta x}\frac{1}{2}\Vert J(x)\Delta x+F(x)\Vert ^ 2\tag{2}\]</span> 根据如何控制 \(\Delta x\) 的大小，非线性优化算法可分为两大类：</p><ul><li>Line Search<ul><li>Gradient Descent</li><li>Gaussian-Newton</li></ul></li><li>Trust Region<ul><li>Levenberg-Marquardt</li><li>Dogleg</li><li>Inner Iterations</li><li>Non-monotonic Steps</li></ul></li></ul><p>Line Search 首先确定迭代方向，然后最小化 \(\Vert f(x+\alpha \Delta x)\Vert ^2\) 确定迭代步长；Trust Region 则划分一个局部区域，在该区域内求解最优值，然后根据近似程度，扩大或缩减该局部区域范围。Trust Region 相比 Linear Search，数值迭代会更加稳定。这里介绍几种有代表性的方法：属于 Line Search 的梯度下降法，高斯牛顿法，以及属于 Trust Region 的 LM 法。</p><h3 id="梯度下降法">2.1. 梯度下降法</h3><p>　　将目标函数式(1)在 \(x\) 附近泰勒展开： <span class="math display">\[ \Vert F(x+\Delta x)\Vert ^2 \approx \Vert F(x)\Vert ^2 + J(x)\Delta x+\frac{1}{2}\Delta x^TH\Delta x \tag{3}\]</span> 其中 \(H\) 是二阶导数(Hessian 矩阵)。<br>　　如果保留一阶导数，那么增量的解就为： <span class="math display">\[\Delta x = -\lambda J^T(x) \tag{4}\]</span> 其中 \(\lambda\) 为步长，可预先由相关策略设定。<br>　　如果保留二阶导数，那么增量方程为： <span class="math display">\[\mathop{\arg\min}\limits _ {\Delta x} \Vert F(x)\Vert ^2+J(x)\Delta x+\frac{1}{2}\Delta x^TH\Delta x\tag{5}\]</span> 对 \(\Delta x\) 求导即可求解增量的解为： <span class="math display">\[\Delta x = -H^{-1}J^T \tag{6}\]</span> 　　一阶梯度法又称为最速下降法，二阶梯度法又称为牛顿法。一阶和二阶法都是将函数在当前值下泰勒展开，然后线性得求解增量值。最速下降法过于贪心，容易走出锯齿路线，反而增加迭代步骤。牛顿法需要计算 \(H\) 矩阵，计算量较大且困难。</p><h3 id="高斯牛顿法">2.2. 高斯牛顿法</h3><p>　　 将式(2)对 \(\Delta x\) 求导并令其为零，可得： <span class="math display">\[\begin{align}&amp;J(x)^TJ(x)\Delta x=-J(x)^TF(x)\\\iff &amp; H\Delta x=g\end{align}\tag{7}\]</span> 相比牛顿法，高斯牛顿法不用计算 \(H\) 矩阵，直接用 \(J^TJ\) 来近似，所以节省了计算量。但是高斯牛顿法要求 \(H\) 矩阵是可逆且正定的，而实际计算的 \(J^TJ\) 是半正定的，所以 \(J^TJ\) 会出现奇异或病态的情况，此时增量的稳定性就会变差，导致迭代发散。另一方面，增量较大时，目标近似函数式(2)就会产生较大的误差，也会导致迭代发散。这是高斯牛顿法的缺陷。高斯牛顿法的步骤为：</p><ol type="1"><li>根据式 (7) 求解迭代步长 \(\Delta x\)；</li><li>变量迭代：\(x ^ * \leftarrow x+\Delta x\)；</li><li>如果 \(\Vert F(x ^ * )-F(x)\Vert &lt; \epsilon\)，则收敛，退出迭代，否则重复步骤 1.；</li></ol><p>高斯牛顿法简单的将 \(\alpha\) 置为 1，而其它 Line Search 方法会最小化 \(\Vert f(x+\alpha \Delta x)\Vert ^2\) 来确定 \(\alpha\) 值。</p><h3 id="lm-法">2.3. LM 法</h3><p>　　Line Search 依赖线性化近似有较高的拟合度，但是有时候线性近似效果较差，导致迭代不稳定；Region Trust 就是解决了这种问题。高斯牛顿法中采用的近似二阶泰勒展开只在该点附近有较好的近似结果，对 \(\Delta x\) 添加一个信赖域区域，就变为 Trust Region 方法。其最优化问题转换为： <span class="math display">\[\begin{align}\mathop{\arg\min}\limits _ x \frac{1}{2}\Vert J(x)\Delta x+F(x)\Vert ^2 \\\Vert D(x)\Delta x\Vert ^2 \leq \mu\\L\leq x \leq U\\\end{align}\tag{8}\]</span> 用 Lagrange 乘子将其转换为无约束优化问题： <span class="math display">\[\mathop{\arg\min}\limits _ {\Delta x}\frac{1}{2}\Vert J(x)\Delta x+F(x)\Vert ^ 2+\frac{1}{\mu}\Vert D(x)\Delta x\Vert ^2 \tag{9}\]</span> 其中 Levenberg 提出的方法中 \(D=I\)，相当于把 \(\Delta x\) 约束在球中；Marquart 提出的方法中将 \(D\) 取为非负数对角阵，通常为 \(J(x)^TJ(x)\) 的对角元素平方根。<br>　　对于信赖域区域 \(\mu\) 的定义，一个比较好的方式是根据近似模型与实际函数之间的差异来确定这个范围：如果差异小，那么增大信赖域；反之减小信赖域。因此，考虑： <span class="math display">\[\rho = \frac{\Vert F(x+\Delta x)\Vert ^2-\Vert F(x)\Vert ^2}{\Vert J(x)\Delta x+F(x)\Vert ^2-\Vert F(x)\Vert ^2} \tag{10}\]</span> 　　 将式(9)对 \(\Delta x\) 求导并令其为零，可得： <span class="math display">\[\begin{align}&amp;\left(J(x)^TJ(x)+\frac{2}{\mu}D^T(x)D(x)\right)\Delta x=-J(x)^TF(x)\\\iff &amp; (H+\lambda D^TD)\Delta x=g\end{align}\tag{11}\]</span> 当 \(\lambda\) 较小时，接近于高斯牛顿法；当 \(\lambda\) 较大时，接近于最速下降法。LM 法的步骤为：</p><ol type="1"><li>根据式(11)求解迭代步长 \(\Delta x\);</li><li>根据式(10)求解 \(\rho\);</li><li>若 \(\rho &gt; \eta _ 1\)，则 \(\mu = 2\mu\);</li><li>若 \(\rho &lt; \eta _ 2\)，则 \(\mu = 0.5\mu\);</li><li>若 \(\rho &gt; \epsilon\)，则 \(x ^ * \leftarrow x+\Delta x\)；</li><li>如果满足收敛条件，则结束，否则继续步骤1.；</li></ol><h2 id="ceres-实践">3. Ceres 实践</h2><p>　　Ceres 是谷歌开发的一个用于非线性优化的库，使用 Ceres 库有以下几个步骤：</p><ul><li>构建 Cost Function，式(1)中的 \(\rho _ i\left(\Vert f _ i(x)\Vert ^ 2\right)\) 即为代码中需要增加的 ResidualBlock；</li><li>累加的 Cost Function 构成最终的 Loss Function 目标函数；</li><li>配置求解器参数并求解问题；</li></ul><h3 id="例子-曲线拟合">3.1. 例子-曲线拟合</h3><p>　　以下代码为拟合曲线参数的简单例子：</p><p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// copy from http://zhaoxuhui.top/blog/2018/04/04/ceres&amp;ls.html</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/core/core.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;ceres/ceres.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cv;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> ceres;</span><br><span class="line"></span><br><span class="line"><span class="comment">//vector,用于存放x、y的观测数据</span></span><br><span class="line"><span class="comment">//待估计函数为y=3.5x^3+1.6x^2+0.3x+7.8</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; xs;</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; ys;</span><br><span class="line"></span><br><span class="line"><span class="comment">//定义CostFunctor结构体用于描述代价函数</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">CostFunctor</span>&#123;</span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">double</span> x_guan,y_guan;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//构造函数，用已知的x、y数据对其赋值</span></span><br><span class="line">  CostFunctor(<span class="keyword">double</span> x,<span class="keyword">double</span> y)</span><br><span class="line">  &#123;</span><br><span class="line">    x_guan = x;</span><br><span class="line">    y_guan = y;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//重载括号运算符，两个参数分别是估计的参数和由该参数计算得到的残差</span></span><br><span class="line">  <span class="comment">//注意这里的const，一个都不能省略，否则就会报错</span></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">  <span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> T* <span class="keyword">const</span> params,T* residual)</span><span class="keyword">const</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    residual[<span class="number">0</span>]=y_guan-(params[<span class="number">0</span>]*x_guan*x_guan*x_guan+params[<span class="number">1</span>]*x_guan*x_guan+params[<span class="number">2</span>]*x_guan+params[<span class="number">3</span>]);</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;  </span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//生成实验数据</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">generateData</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  RNG rng;</span><br><span class="line">  <span class="keyword">double</span> w_sigma = <span class="number">1.0</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">100</span>;i++)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">double</span> x = i;</span><br><span class="line">    <span class="keyword">double</span> y = <span class="number">3.5</span>*x*x*x+<span class="number">1.6</span>*x*x+<span class="number">0.3</span>*x+<span class="number">7.8</span>;</span><br><span class="line">    xs.push_back(x);</span><br><span class="line">    ys.push_back(y+rng.gaussian(w_sigma));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;xs.size();i++)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"x:"</span>&lt;&lt;xs[i]&lt;&lt;<span class="string">" y:"</span>&lt;&lt;ys[i]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//简单描述我们优化的目的就是为了使我们估计参数算出的y'和实际观测的y的差值之和最小</span></span><br><span class="line"><span class="comment">//所以代价函数(CostFunction)就是y'-y，其对应每一组观测值与估计值的残差。</span></span><br><span class="line"><span class="comment">//由于我们优化的是残差之和，因此需要把代价函数全部加起来，使这个函数最小，而不是单独的使某一个残差最小</span></span><br><span class="line"><span class="comment">//默认情况下，我们认为各组的残差是等权的，也就是核函数系数为1。</span></span><br><span class="line"><span class="comment">//但有时可能会出现粗差等情况，有可能不等权，但这里不考虑。</span></span><br><span class="line"><span class="comment">//这个求和以后的函数便是我们优化的目标函数</span></span><br><span class="line"><span class="comment">//通过不断调整我们的参数值，使这个目标函数最终达到最小，即认为优化完成</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">  generateData();</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//创建一个长度为4的double数组用于存放参数</span></span><br><span class="line">  <span class="keyword">double</span> params[<span class="number">4</span>]=&#123;<span class="number">1.0</span>&#125;;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//第一步，创建Problem对象，并对每一组观测数据添加ResidualBlock</span></span><br><span class="line">  <span class="comment">//由于每一组观测点都会得到一个残差，而我们的目的是最小化所有残差的和</span></span><br><span class="line">  <span class="comment">//所以采用for循环依次把每个残差都添加进来</span></span><br><span class="line">  Problem problem;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;xs.size();i++)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="comment">//利用我们之前写的结构体、仿函数，创建代价函数对象，注意初始化的方式</span></span><br><span class="line">    <span class="comment">//尖括号中的参数分别为误差类型，输出维度(因变量个数)，输入维度(待估计参数的个数)</span></span><br><span class="line">    CostFunction* cost_function = <span class="keyword">new</span> AutoDiffCostFunction&lt;CostFunctor,<span class="number">1</span>,<span class="number">4</span>&gt;(<span class="keyword">new</span> CostFunctor(xs[i],ys[i]));</span><br><span class="line">    <span class="comment">//三个参数分别为代价函数、核函数和待估参数</span></span><br><span class="line">    problem.AddResidualBlock(cost_function,<span class="literal">NULL</span>,params);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//第二步，配置Solver</span></span><br><span class="line">  Solver::Options options;</span><br><span class="line">  <span class="comment">//配置增量方程的解法</span></span><br><span class="line">  options.linear_solver_type=ceres::DENSE_QR;</span><br><span class="line">  <span class="comment">//是否输出到cout</span></span><br><span class="line">  options.minimizer_progress_to_stdout=<span class="literal">true</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//第三步，创建Summary对象用于输出迭代结果</span></span><br><span class="line">  Solver::Summary summary;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//第四步，执行求解</span></span><br><span class="line">  Solve(options,&amp;problem,&amp;summary);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//第五步，输出求解结果</span></span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;summary.BriefReport()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;<span class="string">"p0:"</span>&lt;&lt;params[<span class="number">0</span>]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;<span class="string">"p1:"</span>&lt;&lt;params[<span class="number">1</span>]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;<span class="string">"p2:"</span>&lt;&lt;params[<span class="number">2</span>]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;<span class="string">"p3:"</span>&lt;&lt;params[<span class="number">3</span>]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="例子-loam">3.2. 例子-LOAM</h3><p>　　<a href="/LOAM/" title="LOAM">LOAM</a> 前端提取线和面特征，后端最小化线和面的匹配误差。其源码实现了整个最优化过程，ALOAM<a href="#2" id="2ref"><sup>[2]</sup></a> 将后端代码用 Ceres 实现，这里对其作理解与分析。</p><p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">LidarEdgeFactor</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">LidarEdgeFactor(Eigen::Vector3d curr_point_, Eigen::Vector3d last_point_a_,</span><br><span class="line">Eigen::Vector3d last_point_b_, <span class="keyword">double</span> s_)</span><br><span class="line">: curr_point(curr_point_), last_point_a(last_point_a_), last_point_b(last_point_b_), s(s_) &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> T *q, <span class="keyword">const</span> T *t, T *residual)</span> <span class="keyword">const</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; cp&#123;T(curr_point.x()), T(curr_point.y()), T(curr_point.z())&#125;;</span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; lpa&#123;T(last_point_a.x()), T(last_point_a.y()), T(last_point_a.z())&#125;;</span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; lpb&#123;T(last_point_b.x()), T(last_point_b.y()), T(last_point_b.z())&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Eigen::Quaternion&lt;T&gt; q_last_curr&#123;q[3], T(s) * q[0], T(s) * q[1], T(s) * q[2]&#125;;</span></span><br><span class="line">Eigen::Quaternion&lt;T&gt; q_last_curr&#123;q[<span class="number">3</span>], q[<span class="number">0</span>], q[<span class="number">1</span>], q[<span class="number">2</span>]&#125;;</span><br><span class="line">Eigen::Quaternion&lt;T&gt; q_identity&#123;T(<span class="number">1</span>), T(<span class="number">0</span>), T(<span class="number">0</span>), T(<span class="number">0</span>)&#125;;</span><br><span class="line">q_last_curr = q_identity.slerp(T(s), q_last_curr);</span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; t_last_curr&#123;T(s) * t[<span class="number">0</span>], T(s) * t[<span class="number">1</span>], T(s) * t[<span class="number">2</span>]&#125;;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; lp;</span><br><span class="line">lp = q_last_curr * cp + t_last_curr;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; nu = (lp - lpa).cross(lp - lpb);</span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; de = lpa - lpb;</span><br><span class="line"></span><br><span class="line">residual[<span class="number">0</span>] = nu.x() / de.norm();</span><br><span class="line">residual[<span class="number">1</span>] = nu.y() / de.norm();</span><br><span class="line">residual[<span class="number">2</span>] = nu.z() / de.norm();</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> ceres::<span class="function">CostFunction *<span class="title">Create</span><span class="params">(<span class="keyword">const</span> Eigen::Vector3d curr_point_, <span class="keyword">const</span> Eigen::Vector3d last_point_a_,</span></span></span><br><span class="line"><span class="function"><span class="params">   <span class="keyword">const</span> Eigen::Vector3d last_point_b_, <span class="keyword">const</span> <span class="keyword">double</span> s_)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> (<span class="keyword">new</span> ceres::AutoDiffCostFunction&lt;</span><br><span class="line">LidarEdgeFactor, <span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>&gt;(</span><br><span class="line"><span class="keyword">new</span> LidarEdgeFactor(curr_point_, last_point_a_, last_point_b_, s_)));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Eigen::Vector3d curr_point, last_point_a, last_point_b;</span><br><span class="line"><span class="keyword">double</span> s;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>　　对于 Point2Line 误差，为了衡量该线特征上的点是否在地图对应的线特征上，在地图线特征上采样两个点，加上该点，组成两个向量，向量叉乘即可描述匹配误差。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">LidarPlaneFactor</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">LidarPlaneFactor(Eigen::Vector3d curr_point_, Eigen::Vector3d last_point_j_,</span><br><span class="line"> Eigen::Vector3d last_point_l_, Eigen::Vector3d last_point_m_, <span class="keyword">double</span> s_)</span><br><span class="line">: curr_point(curr_point_), last_point_j(last_point_j_), last_point_l(last_point_l_),</span><br><span class="line">  last_point_m(last_point_m_), s(s_)</span><br><span class="line">&#123;</span><br><span class="line">ljm_norm = (last_point_j - last_point_l).cross(last_point_j - last_point_m);</span><br><span class="line">ljm_norm.normalize();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> T *q, <span class="keyword">const</span> T *t, T *residual)</span> <span class="keyword">const</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; cp&#123;T(curr_point.x()), T(curr_point.y()), T(curr_point.z())&#125;;</span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; lpj&#123;T(last_point_j.x()), T(last_point_j.y()), T(last_point_j.z())&#125;;</span><br><span class="line"><span class="comment">//Eigen::Matrix&lt;T, 3, 1&gt; lpl&#123;T(last_point_l.x()), T(last_point_l.y()), T(last_point_l.z())&#125;;</span></span><br><span class="line"><span class="comment">//Eigen::Matrix&lt;T, 3, 1&gt; lpm&#123;T(last_point_m.x()), T(last_point_m.y()), T(last_point_m.z())&#125;;</span></span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; ljm&#123;T(ljm_norm.x()), T(ljm_norm.y()), T(ljm_norm.z())&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Eigen::Quaternion&lt;T&gt; q_last_curr&#123;q[3], T(s) * q[0], T(s) * q[1], T(s) * q[2]&#125;;</span></span><br><span class="line">Eigen::Quaternion&lt;T&gt; q_last_curr&#123;q[<span class="number">3</span>], q[<span class="number">0</span>], q[<span class="number">1</span>], q[<span class="number">2</span>]&#125;;</span><br><span class="line">Eigen::Quaternion&lt;T&gt; q_identity&#123;T(<span class="number">1</span>), T(<span class="number">0</span>), T(<span class="number">0</span>), T(<span class="number">0</span>)&#125;;</span><br><span class="line">q_last_curr = q_identity.slerp(T(s), q_last_curr);</span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; t_last_curr&#123;T(s) * t[<span class="number">0</span>], T(s) * t[<span class="number">1</span>], T(s) * t[<span class="number">2</span>]&#125;;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; lp;</span><br><span class="line">lp = q_last_curr * cp + t_last_curr;</span><br><span class="line"></span><br><span class="line">residual[<span class="number">0</span>] = (lp - lpj).dot(ljm);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> ceres::<span class="function">CostFunction *<span class="title">Create</span><span class="params">(<span class="keyword">const</span> Eigen::Vector3d curr_point_, <span class="keyword">const</span> Eigen::Vector3d last_point_j_,</span></span></span><br><span class="line"><span class="function"><span class="params">   <span class="keyword">const</span> Eigen::Vector3d last_point_l_, <span class="keyword">const</span> Eigen::Vector3d last_point_m_,</span></span></span><br><span class="line"><span class="function"><span class="params">   <span class="keyword">const</span> <span class="keyword">double</span> s_)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> (<span class="keyword">new</span> ceres::AutoDiffCostFunction&lt;</span><br><span class="line">LidarPlaneFactor, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>&gt;(</span><br><span class="line"><span class="keyword">new</span> LidarPlaneFactor(curr_point_, last_point_j_, last_point_l_, last_point_m_, s_)));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Eigen::Vector3d curr_point, last_point_j, last_point_l, last_point_m;</span><br><span class="line">Eigen::Vector3d ljm_norm;</span><br><span class="line"><span class="keyword">double</span> s;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>　　对于 Point2Plane 误差，为了衡量该面特征上的点是否在地图对应的面特征上，在地图面特征上采样一个点，加上该点，组成向量，然后点乘面的法向量即可衡量匹配误差。</p><h3 id="例子-ba">3.3. 例子-BA</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// copy from https://www.jianshu.com/p/3df0c2e02b4c</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"ceres/ceres.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"ceres/rotation.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Read a Bundle Adjustment in the Large dataset.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BALProblem</span> &#123;</span></span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  ~BALProblem() &#123;</span><br><span class="line">    <span class="keyword">delete</span>[] point_index_;</span><br><span class="line">    <span class="keyword">delete</span>[] camera_index_;</span><br><span class="line">    <span class="keyword">delete</span>[] observations_;</span><br><span class="line">    <span class="keyword">delete</span>[] parameters_;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">int</span> <span class="title">num_observations</span><span class="params">()</span>       <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> num_observations_;               &#125;</span><br><span class="line">  <span class="function"><span class="keyword">const</span> <span class="keyword">double</span>* <span class="title">observations</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> observations_;                   &#125;</span><br><span class="line">  <span class="function"><span class="keyword">double</span>* <span class="title">mutable_cameras</span><span class="params">()</span>          </span>&#123; <span class="keyword">return</span> parameters_;                     &#125;</span><br><span class="line">  <span class="function"><span class="keyword">double</span>* <span class="title">mutable_points</span><span class="params">()</span>           </span>&#123; <span class="keyword">return</span> parameters_  + <span class="number">9</span> * num_cameras_; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">double</span>* <span class="title">mutable_camera_for_observation</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> mutable_cameras() + camera_index_[i] * <span class="number">9</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">double</span>* <span class="title">mutable_point_for_observation</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> mutable_points() + point_index_[i] * <span class="number">3</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">bool</span> <span class="title">LoadFile</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>* filename)</span> </span>&#123;</span><br><span class="line">    FILE* fptr = fopen(filename, <span class="string">"r"</span>);</span><br><span class="line">    <span class="keyword">if</span> (fptr == <span class="literal">NULL</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    FscanfOrDie(fptr, <span class="string">"%d"</span>, &amp;num_cameras_);</span><br><span class="line">    FscanfOrDie(fptr, <span class="string">"%d"</span>, &amp;num_points_);</span><br><span class="line">    FscanfOrDie(fptr, <span class="string">"%d"</span>, &amp;num_observations_);</span><br><span class="line"></span><br><span class="line">    point_index_ = <span class="keyword">new</span> <span class="keyword">int</span>[num_observations_];</span><br><span class="line">    camera_index_ = <span class="keyword">new</span> <span class="keyword">int</span>[num_observations_];</span><br><span class="line">    observations_ = <span class="keyword">new</span> <span class="keyword">double</span>[<span class="number">2</span> * num_observations_];</span><br><span class="line"></span><br><span class="line">    num_parameters_ = <span class="number">9</span> * num_cameras_ + <span class="number">3</span> * num_points_;</span><br><span class="line">    parameters_ = <span class="keyword">new</span> <span class="keyword">double</span>[num_parameters_];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_observations_; ++i) &#123;</span><br><span class="line">      FscanfOrDie(fptr, <span class="string">"%d"</span>, camera_index_ + i);</span><br><span class="line">      FscanfOrDie(fptr, <span class="string">"%d"</span>, point_index_ + i);</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">2</span>; ++j) &#123;</span><br><span class="line">        FscanfOrDie(fptr, <span class="string">"%lf"</span>, observations_ + <span class="number">2</span>*i + j);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_parameters_; ++i) &#123;</span><br><span class="line">      FscanfOrDie(fptr, <span class="string">"%lf"</span>, parameters_ + i);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">FscanfOrDie</span><span class="params">(FILE *fptr, <span class="keyword">const</span> <span class="keyword">char</span> *format, T *value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> num_scanned = <span class="built_in">fscanf</span>(fptr, format, value);</span><br><span class="line">    <span class="keyword">if</span> (num_scanned != <span class="number">1</span>) &#123;</span><br><span class="line">      LOG(FATAL) &lt;&lt; <span class="string">"Invalid UW data file."</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> num_cameras_;</span><br><span class="line">  <span class="keyword">int</span> num_points_;</span><br><span class="line">  <span class="keyword">int</span> num_observations_;</span><br><span class="line">  <span class="keyword">int</span> num_parameters_;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span>* point_index_;</span><br><span class="line">  <span class="keyword">int</span>* camera_index_;</span><br><span class="line">  <span class="keyword">double</span>* observations_;</span><br><span class="line">  <span class="keyword">double</span>* parameters_;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Templated pinhole camera model for used with Ceres.  The camera is</span></span><br><span class="line"><span class="comment">// parameterized using 9 parameters: 3 for rotation, 3 for translation, 1 for</span></span><br><span class="line"><span class="comment">// focal length and 2 for radial distortion. The principal point is not modeled</span></span><br><span class="line"><span class="comment">// (i.e. it is assumed be located at the image center).</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">SnavelyReprojectionError</span> &#123;</span></span><br><span class="line">  SnavelyReprojectionError(<span class="keyword">double</span> observed_x, <span class="keyword">double</span> observed_y)</span><br><span class="line">      : observed_x(observed_x), observed_y(observed_y) &#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">  <span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> T* <span class="keyword">const</span> camera,</span></span></span><br><span class="line"><span class="function"><span class="params">                  <span class="keyword">const</span> T* <span class="keyword">const</span> point,</span></span></span><br><span class="line"><span class="function"><span class="params">                  T* residuals)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">    <span class="comment">// camera[0,1,2] are the angle-axis rotation.</span></span><br><span class="line">    T p[<span class="number">3</span>];</span><br><span class="line">    ceres::AngleAxisRotatePoint(camera, point, p);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// camera[3,4,5] are the translation.</span></span><br><span class="line">    p[<span class="number">0</span>] += camera[<span class="number">3</span>];</span><br><span class="line">    p[<span class="number">1</span>] += camera[<span class="number">4</span>];</span><br><span class="line">    p[<span class="number">2</span>] += camera[<span class="number">5</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Compute the center of distortion. The sign change comes from</span></span><br><span class="line">    <span class="comment">// the camera model that Noah Snavely's Bundler assumes, whereby</span></span><br><span class="line">    <span class="comment">// the camera coordinate system has a negative z axis.</span></span><br><span class="line">    T xp = - p[<span class="number">0</span>] / p[<span class="number">2</span>];</span><br><span class="line">    T yp = - p[<span class="number">1</span>] / p[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Apply second and fourth order radial distortion.</span></span><br><span class="line">    <span class="keyword">const</span> T&amp; l1 = camera[<span class="number">7</span>];</span><br><span class="line">    <span class="keyword">const</span> T&amp; l2 = camera[<span class="number">8</span>];</span><br><span class="line">    T r2 = xp*xp + yp*yp;</span><br><span class="line">    T distortion = <span class="number">1.0</span> + r2  * (l1 + l2  * r2);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Compute final projected point position.</span></span><br><span class="line">    <span class="keyword">const</span> T&amp; focal = camera[<span class="number">6</span>];</span><br><span class="line">    T predicted_x = focal * distortion * xp;</span><br><span class="line">    T predicted_y = focal * distortion * yp;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// The error is the difference between the predicted and observed position.</span></span><br><span class="line">    residuals[<span class="number">0</span>] = predicted_x - observed_x;</span><br><span class="line">    residuals[<span class="number">1</span>] = predicted_y - observed_y;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Factory to hide the construction of the CostFunction object from</span></span><br><span class="line">  <span class="comment">// the client code.</span></span><br><span class="line">  <span class="keyword">static</span> ceres::<span class="function">CostFunction* <span class="title">Create</span><span class="params">(<span class="keyword">const</span> <span class="keyword">double</span> observed_x,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">const</span> <span class="keyword">double</span> observed_y)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (<span class="keyword">new</span> ceres::AutoDiffCostFunction&lt;SnavelyReprojectionError, <span class="number">2</span>, <span class="number">9</span>, <span class="number">3</span>&gt;(</span><br><span class="line">                <span class="keyword">new</span> SnavelyReprojectionError(observed_x, observed_y)));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">double</span> observed_x;</span><br><span class="line">  <span class="keyword">double</span> observed_y;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span> </span>&#123;</span><br><span class="line">  google::InitGoogleLogging(argv[<span class="number">0</span>]);</span><br><span class="line">  <span class="keyword">if</span> (argc != <span class="number">2</span>) &#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cerr</span> &lt;&lt; <span class="string">"usage: simple_bundle_adjuster &lt;bal_problem&gt;\n"</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  BALProblem bal_problem;</span><br><span class="line">  <span class="keyword">if</span> (!bal_problem.LoadFile(argv[<span class="number">1</span>])) &#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cerr</span> &lt;&lt; <span class="string">"ERROR: unable to open file "</span> &lt;&lt; argv[<span class="number">1</span>] &lt;&lt; <span class="string">"\n"</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">double</span>* observations = bal_problem.observations();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Create residuals for each observation in the bundle adjustment problem. The</span></span><br><span class="line">  <span class="comment">// parameters for cameras and points are added automatically.</span></span><br><span class="line">  ceres::Problem problem;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; bal_problem.num_observations(); ++i) &#123;</span><br><span class="line">    <span class="comment">// Each Residual block takes a point and a camera as input and outputs a 2</span></span><br><span class="line">    <span class="comment">// dimensional residual. Internally, the cost function stores the observed</span></span><br><span class="line">    <span class="comment">// image location and compares the reprojection against the observation.</span></span><br><span class="line"></span><br><span class="line">    ceres::CostFunction* cost_function =</span><br><span class="line">        SnavelyReprojectionError::Create(observations[<span class="number">2</span> * i + <span class="number">0</span>],</span><br><span class="line">                                         observations[<span class="number">2</span> * i + <span class="number">1</span>]);</span><br><span class="line">    problem.AddResidualBlock(cost_function,</span><br><span class="line">                             <span class="literal">NULL</span> <span class="comment">/* squared loss */</span>,</span><br><span class="line">                             bal_problem.mutable_camera_for_observation(i),</span><br><span class="line">                             bal_problem.mutable_point_for_observation(i));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Make Ceres automatically detect the bundle structure. Note that the</span></span><br><span class="line">  <span class="comment">// standard solver, SPARSE_NORMAL_CHOLESKY, also works fine but it is slower</span></span><br><span class="line">  <span class="comment">// for standard bundle adjustment problems.</span></span><br><span class="line">  ceres::Solver::Options options;</span><br><span class="line">  options.linear_solver_type = ceres::DENSE_SCHUR;</span><br><span class="line">  options.minimizer_progress_to_stdout = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">  ceres::Solver::Summary summary;</span><br><span class="line">  ceres::Solve(options, &amp;problem, &amp;summary);</span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; summary.FullReport() &lt;&lt; <span class="string">"\n"</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>　　这里使用了 Bundle Adjustment in the Large<a href="#3" id="3ref"><sup>[3]</sup></a> 数据集，观测量为图像坐标系下路标(特征)的像素坐标系，待优化的参数为各路标的 3D 坐标以及相机内外参，这里相机内外参有 9 个，其中位置及姿态 6 个，畸变系数 2 个，焦距 1 个。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> 高翔. 视觉 SLAM 十四讲: 从理论到实践. 电子工业出版社, 2017.<br><a id="2" href="#2ref">[2]</a> https://github.com/HKUST-Aerial-Robotics/A-LOAM<br><a id="3" href="#3ref">[3]</a> http://grail.cs.washington.edu/projects/bal/</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　非线性最小二乘(Non-linear Least Squares)问题应用非常广泛，尤其是在 SLAM 领域。&lt;a href=&quot;/LOAM/&quot; title=&quot;LOAM&quot;&gt;LOAM&lt;/a&gt;，，&lt;a href=&quot;/[paper_reading]-Stereo-RCNN-ba
      
    
    </summary>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/categories/SLAM/"/>
    
    
      <category term="Optimization" scheme="https://leijiezhang001.github.io/tags/Optimization/"/>
    
  </entry>
  
  <entry>
    <title>如何搭建一个 ADAS 产品</title>
    <link href="https://leijiezhang001.github.io/How-to-Build-An-ADAS/"/>
    <id>https://leijiezhang001.github.io/How-to-Build-An-ADAS/</id>
    <published>2020-05-07T01:36:23.000Z</published>
    <updated>2020-06-23T07:20:24.170Z</updated>
    
    <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="本文介绍了基于视觉的 ADAS 产品构建方法，全文 6K 字 12 图，请输入密码查看：" />    <label for="pass">本文介绍了基于视觉的 ADAS 产品构建方法，全文 6K 字 12 图，请输入密码查看：</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      从零到一构建视觉 ADAS 产品。
    
    </summary>
    
      <category term="Autonomous Driving System" scheme="https://leijiezhang001.github.io/categories/Autonomous-Driving-System/"/>
    
      <category term="ADAS" scheme="https://leijiezhang001.github.io/categories/Autonomous-Driving-System/ADAS/"/>
    
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="ADAS" scheme="https://leijiezhang001.github.io/tags/ADAS/"/>
    
  </entry>
  
  <entry>
    <title>OctoMap</title>
    <link href="https://leijiezhang001.github.io/OctoMap/"/>
    <id>https://leijiezhang001.github.io/OctoMap/</id>
    <published>2020-04-23T02:23:05.000Z</published>
    <updated>2020-04-27T02:31:31.376Z</updated>
    
    <content type="html"><![CDATA[<p>　　地图是机器人领域非常重要的模块，也可以认为是自动驾驶保障安全的基础模块。根据存储建模类型，地图可分为拓扑地图，栅格地图，点云地图等。<a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 就是一种能在线检测静态障碍物的栅格地图。自动驾驶领域，地图的用处有：</p><ul><li><strong>高精度定位</strong>，一般是 3D 栅格地图，但是栅格中近似存储点云原始信息；</li><li><strong>路径规划</strong>，不同规划算法依赖不同地图，自动驾驶中比较靠谱又简单的规划算法一般依赖拓扑地图，俗称高精度语义地图，描述一些车道线等路面拓扑关系；而在室内或低速无道路信息场景，则会用如 \(A ^ * \) 算法在栅格地图上进行路径规划；</li><li><strong>辅助感知检测未定义类别的障碍物</strong>，有人称之为静态地图，一般是 2.5D 栅格地图，图层可以自定义一些语义信息；</li></ul><p>下游不同模块对不同存储方式的利用效率是不同的，所以需要针对不同下游任务设计不同地图建模方式。本文<a href="#1" id="1ref"><sup>[1]</sup></a>介绍了一种基于八叉树的栅格地图建模方法。<br>　　对于机器人而言，类似 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 能建模 FREE，OCCUPIED，UNMAPPED AREAS 的地图是信息量比较丰富的，但是 Grid-Mapping 是 2D 的。这里对 3D 地图有以下要求：</p><ul><li><strong>Probabilistic Representation</strong><br>测量都会有不确定性，这种不确定性需要用概率表征出来；另外多传感器融合也需要基于概率的表示；</li><li><strong>Modeling of Unmapped Areas</strong><br>对机器人导航而言，显式得表示哪些区域是观测未知的也非常重要；</li><li><strong>Efficiency</strong><br>地图构建与存储需要非常高效，一般而言，地图的内存消耗会是瓶颈；</li></ul><p><img src="/OctoMap/maps.png" width="90%" height="90%" title="图 1. Different Representations of Maps"> 　　如图 1. 所示，原始点云地图信息量丰富，但是不能结构化存储；Elevation Maps 与 Multi-level Surface Maps 虽然高效，但是不能表征未观测的区域信息。OctoMap 可以认为是 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 的 3D 版本，信息量丰富且高效。</p><h2 id="octomap-mapping-framework">1. OctoMap Mapping Framework</h2><h3 id="octrees">1.1. Octrees</h3><p><img src="/OctoMap/OctoMap.png" width="40%" height="40%" title="图 2. 八叉树地图存储"> 　　如图 2. 所示，八叉树是将空间递归得等分成八份(QuadTree 四叉树则等分为四份)，每个节点可以存储 Occupied，Free，Unknown 信息(Occupied 概率即可)。此外，如果子节点的状态都一样，那么可以进行剪枝，只保留大节点低分辨率的 Voxel，达到紧凑存储的目的。<br>　　时间复杂度上，对于有 \(n\) 个节点，深度为 \(d\) 的八叉树，那么单次查询的时间复杂度为 \(\mathcal{O}(d)=\mathcal{O}(\mathrm{log}\,n)\)；遍历节点的时间复杂度为 \(\mathcal{O}(n)\)。\(d = 16, r = 1cm\)，可以覆盖 \((655.36m)^3\)的区域。</p><h3 id="probabilistic-sensor-fusion">1.2. Probabilistic Sensor Fusion</h3><p>　　时序概率融合也是基于贝叶斯滤波，详见 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a>，只不过这里是 3D Mapping，作 Raycasting 的时候采用 <a href="/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/" title="What You See is What You Get">What You See is What You Get</a> 中提到的 Fast Voxel Traversal 算法。实际应用中，一般都会采用上下界限制概率值，这种限制也能提高八叉树的剪枝率。</p><h3 id="multi-resolution-queries">1.3. Multi-Resolution Queries</h3><p>　　由于八叉树的特性，OctoMap 支持低于最高分辨率的 Voxel 概率查询，即父节点是子节点的平均概率，或是子节点的最大概率: <span class="math display">\[\bar{l}(n)=\frac{1}{8}\sum _ {i=1}^8 L (n _ i)\\\hat{l}(n)=\max\limits _ iL(n _ i)\tag{1}\]</span> 其中 \(l\) 是测量模型下概率的 log-odds 值。</p><h2 id="implementation-details-statics">2. Implementation Details &amp; Statics</h2><h3 id="memory-efficient-node-map-file-generation">2.1. Memory-Efficient Node &amp; Map File Generation</h3><p><img src="/OctoMap/save.png" width="60%" height="60%" title="图 3. Node Memory Consumption and Serialization"> 　　如图 3. 左图所示，每个节点只分配一个 float 型的数据存储以及指向子节点地址数组的地址指针(而不是直接包含子节点地址的指针)，只有存在子节点时，才会分配子节点的地址数组空间。由此在 32-bit 系统中(4 字节对齐)，每个父节点需要 40B，子节点需要 8B；在 64-bit 系统中(8 字节对齐)，每个父节点需要 80B(\(4+9\times 8\))，子节点需要 16B(\(4+8)\)。<br>　　地图存储需要在信息量损失最小的情况下进行压缩。如图 3. 右图所示，存储序列化时，每个叶子节点总共需要 4B 概率值，不需要状态量；每个父节点总共需要 2B，表示 8 个子节点的 2bit 状态量(貌似与论文有出入，其不是最优的压缩)。在这种压缩方式下，大范围地图的存储大小一般也能接受。根据存储的地图重建地图时，只需要知道坐标原点即可。</p><h3 id="accessing-data-memory-consumption">2.2. Accessing Data &amp; Memory Consumption</h3><p><img src="/OctoMap/memusage1.png" width="60%" height="60%" title="图 4. Memory Usage VS. Scan Num."> 　　Freiburg 建图大小为 \((202\times 167\times 28) m^3\)，如图 4. 所示，随着点云扫描区域扩大，OctoMap 表示方式能有效降低建图大小。 <img src="/OctoMap/memusage2.png" width="60%" height="60%" title="图 5. Memory Usage VS. Resolution"> 　　图 5. 则说明建图大小与分辨率的关系。 <img src="/OctoMap/inserttime.png" width="60%" height="60%" title="图 6. Insert Date Time VS. Resolution"> <img src="/OctoMap/traversetime.png" width="60%" height="60%" title="图 7. Traverse Data Time VS. Depth"> 　　图 6. 显示了往图中插入一个节点所需时间，1000 个节点在毫秒级；图 7. 显示了遍历所有节点所需的时间，基本也在毫秒级。 <img src="/OctoMap/compress.png" width="60%" height="60%" title="图 8. Compression Ratio"> 　　通过限制概率上下界，可以剪枝压缩图，用 KL-diverge 来评估压缩前后图的分布相似性，图 8. 显示了压缩比与网络大小及相似性的关系。</p><h3 id="some-strategies">2.3. Some Strategies</h3><p><img src="/OctoMap/case.png" width="60%" height="60%" title="图 9. Corner Case Handle"> 　　如图 9. 所示，前后帧位姿的抖动，会导致 Occupied 持续观测的不稳定，所以需要一些领域约束策略来保证 Occupied 的稳定观测。这种类似的策略在 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 工程实现中也需要采用，因为实际的 Pose 肯定会有噪声，导致同一目标的栅格前后有一定概率不能完全命中。</p><h2 id="rethinking">3. ReThinking</h2><p>　　对于自动驾驶来说，高度方向的范围不需要很大，甚至四叉树足矣，如果采用八叉树，那么需要将高度方向的分辨率降低，从而更加紧凑的构建地图。<br>　　此外自动驾驶肯定是需要大范围建图的，如平方千公里级别。所以切片式的地图存储与查询就显得尤为重要，换句话说，需要动态得载入局部地图，这就有两种思路：</p><ul><li>动态载入完全局部地图<br>要求前后局部地图有一定的重叠，通过索引式的存储可以不存储重叠区域的地图信息；</li><li>动态载入部分局部地图<br>随着机器人本体的运动，实时动态载入前方更远处的地图，丢掉后方远处的历史地图。这对在线地图结构的灵活性要求比较高，如果基于八叉树，那么需要作片区域剪枝及插入的操作，效率不一定高；</li></ul><p>　　在自动驾驶领域，目前用于高精度定位的栅格地图与用于 PNC 规划控制的拓扑地图(高精地图)已经比较成熟；而用于环境感知的静态语义地图还没形成大范围的共识。不管从工程实现效果及效率上，还是语义信息描述定义上，还需作很多探索与实践。比如，可以定义最底层的语义信息：地面高度，此外也可以把车道线信息打到栅格图层中去(但是可能加大对 PNC 的搜索计算量)，等等。所以可能最优的存储查询方式并不是八叉树，<strong>可能还是栅格化后并对每个栅格哈希化，牺牲一定的内存空间，然后作 \(O(1)\) 的快速插入与查询</strong>。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Hornung, Armin, et al. &quot;OctoMap: An efficient probabilistic 3D mapping framework based on octrees.&quot; Autonomous robots 34.3 (2013): 189-206.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　地图是机器人领域非常重要的模块，也可以认为是自动驾驶保障安全的基础模块。根据存储建模类型，地图可分为拓扑地图，栅格地图，点云地图等。&lt;a href=&quot;/Grid-Mapping/&quot; title=&quot;Grid-Mapping&quot;&gt;Grid-Mapping&lt;/a&gt; 就是一种能在
      
    
    </summary>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/categories/SLAM/"/>
    
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/tags/SLAM/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="Mapping" scheme="https://leijiezhang001.github.io/tags/Mapping/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;What You See is What You Get, Exploiting Visibility for 3D Object Detection&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/"/>
    <id>https://leijiezhang001.github.io/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/</id>
    <published>2020-04-22T01:19:59.000Z</published>
    <updated>2020-04-23T02:34:12.937Z</updated>
    
    <content type="html"><![CDATA[<p>　　Bird-View 3D Detection 都是将点云离散化到 Voxel，有点的 Voxel 提取区域特征，无点的 Voxel 则置为空。而 LiDAR 的测量特性其实还包含更多的信息，<a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 中较详细的阐述了 LiDAR 的测量模型，每个栅格可以标记为三个状态：UNKNOWN，FREE，OCCUPIED。传统的 Bird-View 3D Detection 没有显式得提取 UNKNOW 与 FREE 的信息(即没有提取 Visibility 信息)，而 UNKNOW 与 FREE 对数据增广及检测效果非常重要。 <img src="/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/visibility.png" width="90%" height="90%" title="图 1. Visibility or Freespace from LiDAR"> 　　如图 1. 所示，左图是传统的点云表示方式，无法区分红色区域是否有车，而右图则非常容易得区分哪个区域不可能有车，哪个区域可能有车。所以本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了显式提取点云 UNKNOWN 与 FREE 信息来辅助数据增广与提高目标检测精度的方法。</p><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/framework.png" width="90%" height="90%" title="图 2. Framework"> 　　如图 2. 所示，本文的 3D 检测框架与传统的差不多，是 Anchor-Based 方法，主要不同点是输入网络的特征，即点云栅格化后提取出的特征不一样以及融合时序信息。并且，训练过程中，对数据增广做了精心的设计。 <img src="/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/fusion.png" width="90%" height="90%" title="图 3. Frusion Strategy"> 　　如图 3. 所示，点云栅格化后提取的特征不一样是指增加了 Visibility 图层。有两种融合方式，前融合是与点云栅格化后提取的特征作 Concate，然后输入到主干网络；后融合则是二者分别通过主干网络，然后再作 Concate。实验表明前融合效果较好。</p><h3 id="object-augmentation">1.1. Object Augmentation</h3><p>　　传统的数据增广关注在全帧点云的平移，旋转，翻转变换。本文则采用目标级别的数据增广。首先生成目标的点云集合，可以用 CAD 模型，也可以直接扣实际的目标点云(扣出来的点云增广能力有限)；然后将目标点云集合随机得放到全帧点云中。在放置的过程中需要模拟 LiDAR 的测量模型，也就是 Visibility 计算过程，这在第 2. 节中详细描述。实验表面能提升 ~9 个点。</p><h3 id="temporal-aggregation">1.2. Temporal Aggregation</h3><p>　　时序点云信息的利用可以有以下几种方法：</p><ul><li>将每帧点云栅格化，然后直接在 Chanel 层作 Concate，之后作 3D 卷积，或者先在 Chanel 维度作 1D 卷积，然后作 2D 卷积；</li><li>将点云中的点增加相对时间戳属性，然后作整体的栅格化，之后直接作传统的 2D 卷积；</li></ul><p>本文采用第二种方法，实验表明能提升 ~8 个点。</p><h2 id="visibility-computing">2. Visibility Computing</h2><p>　　<a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 中已经应用了 Raycasting 来计算 Visibility/Free。对于点云中的每一个点，我们不仅能获得该点探测到障碍物的这个信息，还能知道，传感器与该点之间的连线上是 Free 的。这就要求能高效得计算该连线相交 Voxel 的集合。该计算模型也用来修正 Object Augmentation 时的点云。</p><h3 id="efficient-voxel-traversal">2.1. Efficient Voxel traversal</h3><p>　　对每个点，都需要遍历传感器原点到该点所经过的 Voxel，采用 Fast Voxel Traversal<a href="#2" id="2ref"><sup>[2]</sup></a>方法来进行高效的 Voxel 遍历。</p><h3 id="raycasting-with-augmented-objects">2.2. Raycasting with Augmented Objects</h3><p><img src="/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/augment.png" width="90%" height="90%" title="图 4. Rectify Object Augmentation"> 　　如图 4. 所示，本文设计了两种策略来修正物体增广：</p><ul><li>Culling，如果该物体是被遮挡的，那么直接去掉，这样会极大减少增广的物体；</li><li>Drilling，如果该物体是被遮挡的，那么将遮挡物去掉，即置为 Free；</li></ul><p>实验表明 Drilling 效果较好，在训练时采用该策略进行物体增广后的点云修正，作 Inference 时就直接计算 Freespace 即可。</p><h3 id="online-occupancy-mapping">2.3. Online Occupancy Mapping</h3><p>　　栅格内点云提取特征时融合了时序信息，Visibility 也需要融合时序信息，最直观的方式是将 3D Occupancy Map 进行时间维度的堆叠，获得 4D Map，这样对后续的计算量较大。本文采用 OctoMap<a href="#3" id="3ref"><sup>[3]</sup></a> 计算方式，作贝叶斯滤波，得到时序滤波的 3D Occupancy Map。原理与 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 一样，只不过这里是 3D 的。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Hu, Peiyun, et al. &quot;What You See is What You Get: Exploiting Visibility for 3D Object Detection.&quot; arXiv preprint arXiv:1912.04986 (2019).<br><a id="2" href="#2ref">[2]</a> Amanatides, John, and Andrew Woo. &quot;A fast voxel traversal algorithm for ray tracing.&quot; Eurographics. Vol. 87. No. 3. 1987.<br><a id="3" href="#3ref">[3]</a> Hornung, Armin, et al. &quot;OctoMap: An efficient probabilistic 3D mapping framework based on octrees.&quot; Autonomous robots 34.3 (2013): 189-206.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Bird-View 3D Detection 都是将点云离散化到 Voxel，有点的 Voxel 提取区域特征，无点的 Voxel 则置为空。而 LiDAR 的测量特性其实还包含更多的信息，&lt;a href=&quot;/Grid-Mapping/&quot; title=&quot;Grid-Map
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Reconfigurable Voxels, A New Representation for LiDAR-Based Point Clouds&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-Reconfigurable-Voxels/"/>
    <id>https://leijiezhang001.github.io/paper-reading-Reconfigurable-Voxels/</id>
    <published>2020-04-20T01:54:31.000Z</published>
    <updated>2020-04-21T03:38:54.663Z</updated>
    
    <content type="html"><![CDATA[<p>　　Voxel-based 点云特征提取虽然损失了一定的信息，但是计算高效。Voxel-based 方法一个比较大的问题是，由于<strong>点云分布的不均匀性</strong>，作卷积时会导致可能计算的区域没有点，从而不能有效提取局部信息。为了解决栅格化后栅格中点云分布的不均匀问题，目前看到的有以下几种方法：</p><ol type="1"><li>Deformable Convolution，采用可变形卷积方法，自动学习卷积核的连接范围，理论上应该能更有效得使卷积核连接到点密度较高的栅格；</li><li><a href="/paper-reading-PolarNet/" title="PolarNet">PolarNet</a>提出了一种极坐标栅格化方式，因为点云获取的特性，这种方法获得的栅格中点数较为均匀;</li><li>手动设计不同分辨率的栅格，作特征提取，然后融合。比如近处分辨率较高，远处较低的方式；</li><li>本文<a href="#1" id="1ref"><sup>[1]</sup></a> 提出了一种自动选择栅格领域及分辨率，从而最大化卷积区域点数的方法；</li></ol><p><img src="/paper-reading-Reconfigurable-Voxels/reconfig.png" width="80%" height="80%" title="图 1. Reconfig Voxels"> 　　如图 1. 所示，本文提出的 Reconfigurable Voxel 方法，能自动选择领域内点数较多的栅格特征提取，进而作卷积运算，避免点数较少，从而信息量较少的栅格作特征提取操作；此外还可根据点数自动调整分辨率以获得合适的栅格点数。通过这种方法，每个栅格输入到网络前都能有效提取周围点数较多区域的特征信息。</p><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-Reconfigurable-Voxels/pipeline.png" width="80%" height="80%" title="图 2. Framework"> 　　如图 2. 所示，本文以检测任务为例，分三部分：Voxel/Pillar Feature Extraction，Backbone，RPN/Detection Head。后两个采用传统的方法，本文主要是改进 Voxel/Pillar Feature Extraction，这是输入到网络前的特征提取阶段。</p><h2 id="voxelpillar-feature-extraction">2. Voxel/Pillar Feature Extraction</h2><p>　　传统的输入到 2D 卷积网络的特征要么是手工提取的，要么是用 <a href="/paperreading-PointPillars/" title="PointPillars">PointPillars</a> 网络去学习每个 Voxel 的特征。由此输入到网络的特征不是最优的，因为点云的稀疏性会导致后面的 2D 卷积网络作特征提取时遇到很多“空”的 Voxel。本文提出的方法就能显式得搜索每个 Voxel 周围有点的区域作特征提取，使得之后 2D 卷积特征提取更加有效。其步骤为：</p><ul><li>点云栅格化，并存储每个 Voxel 周围 Voxel 的索引；</li><li>每个 Voxel 周围 Voxel 作 Biased Random Walk，去搜索有更稠密点云的 Voxel；</li><li>将每个 Voxel 与新搜索到的周围 Voxel 作特征提取与融合，得到该 Voxel 特征；</li></ul><h3 id="biased-random-walking-neighbors">2.1. Biased Random Walking Neighbors</h3><p>　　邻域 Voxel 搜索目标是：<strong>在距离较近的情况下寻找较稠密的 Voxel</strong>。由此设计几种策略：</p><ul><li>点数越少的 Voxel，有更高概率作 Random Walk，以及更多 Step 去周围相邻的 Voxel；</li><li>点数越多的 Voxel，有更高概率被其它 Voxel Random Walk 到；</li></ul><p>　　将以上策略数学化。设第 \(j\) 个 Voxel 有 \(N(j)\) 个点，最大点数为 \(n\)，其作 Random Walk 的概率为 \(P _ w(j)\)，步数 Step 为 \(S(j)\)，第 \(i\) 步到达的 Voxel 为 \(w _ j(i)\)，其四领域 Voxel 集合为 \(V(w _ j(i))\)，从该 Voxel 走到下一个 Voxel 的概率为 \(P(w _ j(i+1)|w _ j(i))\)。由此得到以上策略的数学描述： <span class="math display">\[P _ w(j)=\frac{1}{N(j)} \tag{1}\]</span> <span class="math display">\[S(j)=n-N(j)\tag{2}\]</span> <span class="math display">\[P\left(w _ j(i+1)|w _ j(i)\right) = \frac{N\left(w _ j(i+1)\right)}{\sum _ {v\in V(w _ j(i))}N(v)}\tag{3}\]</span> 需要注意的是，\(S(j)\) 是在开始时计算的，此后每走一步就减1。 <img src="/paper-reading-Reconfigurable-Voxels/random_walk.png" width="90%" height="90%" title="图 3. Random walk"> 　　如图 3. 所示，左边为单分辨率下 Voxel 搜索过程。</p><h3 id="reconfigurable-voxels-encoder">2.2. Reconfigurable Voxels Encoder</h3><p>　　每个 Voxel \(v _ i\) 搜索到最优的 4 领域 Voxel 集 \(V(v _ i)\) 后，需要融合得到该 Voxel 的特征： <span class="math display">\[\begin{align}F(v _ i) &amp;= \psi\left(f _ {v _ i}, f _ {V(v _ i)}\right)\\&amp;= \varphi _ 1\left[\varphi _ 2(f _ {v _ i}), \varphi _ 2\left(\sum _ {j=1}^4 W _ j(f _ {v _ i})f _ {V _ {j(v _ i)}}\right)\right] _ f\tag{4}\end{align}\]</span> 其中 \(\varphi _ 1\) 为 low-level 操作，如 average pooling，\(\varphi _ 2\) 为 high-level 操作，如 MLP。</p><h3 id="multi-resolution-reconfigurable-voxels">2.3. Multi-resolution Reconfigurable Voxels</h3><p>　　图 3. 左边是单分辨率情况，Random Walking 可以拓展到多分辨率情形。当点云非常稀疏的时候，就很有必要降低栅格的分辨率。如图 3. 所示，\(P _ w\) 计算时除以 4，以维持与高分辨率的一致性；高分辨率到低分辨率搜索概率为 \(0.25P _ w\)，低分辨率到高分辨率搜索概率为 \(0.5P _ w\)。其余准则与单分辨率一致。实验结果表面多分辨率有一定提升，但是相比单分辨率提升不明显。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Wang, Tai, Xinge Zhu, and Dahua Lin. &quot;Reconfigurable Voxels: A New Representation for LiDAR-Based Point Clouds.&quot; arXiv preprint arXiv:2004.02724 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Voxel-based 点云特征提取虽然损失了一定的信息，但是计算高效。Voxel-based 方法一个比较大的问题是，由于&lt;strong&gt;点云分布的不均匀性&lt;/strong&gt;，作卷积时会导致可能计算的区域没有点，从而不能有效提取局部信息。为了解决栅格化后栅格中点云分布
      
    
    </summary>
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;PolarNet&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-PolarNet/"/>
    <id>https://leijiezhang001.github.io/paper-reading-PolarNet/</id>
    <published>2020-04-16T01:19:12.000Z</published>
    <updated>2020-04-17T02:09:42.539Z</updated>
    
    <content type="html"><![CDATA[<p>　　Point-wise 特征提取在 <a href="/PointCloud-Feature-Extraction/" title="PointCloud-Feature-Extraction">PointCloud-Feature-Extraction</a> 中已经有较为详细的描述，虽然 Point-wise 提取的特征更加精细，但是一般都有 KNN 构建及索引操作，计算量较大，而且实践中发现学习收敛较慢。Voxel-based 虽然理论上损失了一定的信息，但是能直接应用 2D 卷积网络，网络学习效率很高。本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种在极坐标下栅格化后进行点云 Semantic Segmentation 的方法，相比传统的笛卡尔坐标系下栅格化有一定的优势。</p><h2 id="voxelization">1. Voxelization</h2><p><img src="/paper-reading-PolarNet/pts.png" width="98%" height="98%" title="图 1. Cartesian VS. Polar"> 　　如图 1. 所示，传统的笛卡尔坐标系下栅格化的栅格是矩形，而极坐标系下栅格是饼状的。激光雷达是在极坐标方式下获取点云的，所以由图可知，<strong>极坐标栅格化下，每个栅格拥有的点数更加均匀</strong>，有利于网络学习并减少计算量。此外，本文统计后显示，相比笛卡尔坐标栅格，极坐标的栅格内点属于同一目标的概率更大。</p><h2 id="polarnet-framework">2. PolarNet Framework</h2><p><img src="/paper-reading-PolarNet/framework.png" width="98%" height="98%" title="图 2. PolarNet"> 　　如图 2. 所示，点云经过 Polar 栅格化后，对每个栅格首先进行 PointNet 特征提取，然后对所有栅格作 ring-convolution 操作。<br>　　ring-convolution 是指卷积在环形方向进行，没有边缘截断效应。实现上，将栅格从某处展开，然后边缘处用另一边对应的栅格进行 padding，即可用普通的卷积进行运算。<br>　　网络是作 Voxel-wise 的分割，然后直接将预测的类别应用到栅格内的点云中。统计上，同一栅格内的点云属于不同类别的概率很低，所以本文并没进一步作 Point-wise 的分割。</p><h2 id="rethinking">3. Rethinking</h2><p>　　PolarNet 作 Semantic Segmentation 比其它方法提升很多。但是实际应用时，PolarNet 不能指定各个方向的范围，所以计算效率较低。比如，自动驾驶中，我们可以设定前 100m，后 60m，左右各 30m 的检测范围，笛卡尔坐标系下很容易进行栅格化，而极坐标下则没法搞。所以为了解决点云的分布不均匀问题，另一种思路是在笛卡尔坐标系下，近处打高分辨率的栅格，远处打低分辨率的栅格。具体实现，可以先用低分辨率过一遍网络，然后再对感兴趣的特定区域作高分辨率检测。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Zhang, Yang, et al. &quot;PolarNet: An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation.&quot; arXiv preprint arXiv:2003.14032 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Point-wise 特征提取在 &lt;a href=&quot;/PointCloud-Feature-Extraction/&quot; title=&quot;PointCloud-Feature-Extraction&quot;&gt;PointCloud-Feature-Extraction&lt;/a&gt; 中已经有
      
    
    </summary>
    
      <category term="Semantic Segmentation" scheme="https://leijiezhang001.github.io/categories/Semantic-Segmentation/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/tags/Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>PointFlowNet</title>
    <link href="https://leijiezhang001.github.io/PointFlowNet/"/>
    <id>https://leijiezhang001.github.io/PointFlowNet/</id>
    <published>2020-04-13T01:54:59.000Z</published>
    <updated>2020-04-15T03:07:01.997Z</updated>
    
    <content type="html"><![CDATA[<p>　　点云的 Scene Flow 与 Semantic 一样是一个较低层的信息，通过 Point-Wise Semantic 信息可以作物体级别的检测，这种方式有很高的召回率，且超参数较少。同样，通过 Point-Wise Scene Flow 作目标级别的运动估计(当然也可作物体点级别聚类检测的线索)，也会非常鲁棒。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 将点级别/Voxel 级别的 Scene Flow 与 3D 目标检测融合在一起，作物体级别的运动估计，工作系统性较强。</p><h2 id="问题描述">1. 问题描述</h2><p>　　设 \(t\) 时刻点云 \(\mathbf{P} _ t\in\mathbb{R}^{M\times 3}\)，那么需要求解的未知量有：</p><ul><li>每个点的 Scene Flow: \(\mathbf{v} _ i\in\mathbb{R} ^3\);</li><li>每个点集的 Rigid Motion: \(\mathbf{R} _ i\in\mathbb{R}^{3\times 3}\)，\(\mathbf{t} _ i\in\mathbb{R}^{3}\);</li><li>每个物体的 3D 属性：Location，Orientation，Size，Rigid Motion;</li></ul><h2 id="算法框架">2. 算法框架</h2><p><img src="/PointFlowNet/framework.png" width="90%" height="90%" title="图 1. PoingFlowNet Framework"> 　　如图 1. 所示，PointFlowNet 由四部分组成，分别为：Feature Encoder，Scene Flow/Ego-motion Estimation and 3D Object Detection，Rigid Motion Estimation，Object Motion Decoder。Feature Encoder 将前后帧点云栅格化后作特征提取，然后 Context Encoder 作进一步的特征融合去提取；输出的特征第一个分支作 Voxel 级别的 Scene Flow 预测，进一步作每个点的 Rigid Motion 预测(<strong>每个点属于对应物体的 Motion 在该 Voxel 坐标系下的表示</strong>)；第二个分支作 Ego-Motion 的预测；第三个分支作 3D 目标检测，进一步作目标的 Motion Decoder。</p><h3 id="feature-encoder">2.1. Feature Encoder</h3><p>　　不同的点云特征提取方式都可采用，本文采用传统的 Bird-View Voxel 表示方式，然后作 2D/3D 卷积。同时还需要将前后帧的点云作特征融合，这里也完全可以采用 <a href="/paperreading-FlowNet3D/" title="FlowNet3D">FlowNet3D</a> 的特征提取形式。</p><h3 id="scene-flowrigid-motion-decoder">2.2. Scene Flow/Rigid Motion Decoder</h3><p>　　Scene Flow 是作 Bird-View 下 Voxel 级别的场景流预测，然后再预测 Rigid Motion。 <img src="/PointFlowNet/rigid-motion.png" width="80%" height="80%" title="图 2. Rigid MOtion Estimation"> 　　如图 2. 所示，世界坐标系 \(\mathbf{W}\) 下点 \(\mathbf{p}\) 的 scene flow 表示为 \(\mathbf{v}\)，刚体物体的局部坐标系从 \(\mathbf{A}\) 经过 \((\mathbf{R _ A, t _ A})\) 运动到 \(\mathbf{B}\) ，那么其 scene flow 可表示为： <span class="math display">\[\mathbf{v=[R _ A(p-o _ A)+t _ A]-(p-o _ A)} \tag{1}\]</span> 本文论证了两个定理：</p><ol type="1"><li>scene flow 只能通过刚体局部坐标系的运动导出，不能直接通过世界坐标系下的刚体运动导出(除非运动无旋转量)。所以如图 1. 所示，通过 scene flow 预测出的 voxel motion 是局部坐标系下的，还需通过坐标变换到世界坐标系下。<strong>这里每个 Voxel 预测量的局部坐标系采用 Voxel 中心点</strong>。作目标运动估计时，&quot;世界坐标系&quot;其实可以定义为物体坐标系(Voxel 为局部坐标系)，最后再通过 Ego-motion 变换到世界坐标系。</li><li>不管是局部坐标系 \(\mathbf{A}\) 还是 \(\mathbf{B}\)，都能导出 scene flow。</li></ol><p>　　如图 2. 所示，实验也验证了 scene flow 不能直接学习到世界坐标系下的 translation 运动。</p><h3 id="ego-motion-regressor">2.3. Ego-motion Regressor</h3><p>　　根据前后帧的点云回归本车的运动(ego-motion)，ego-motion 建立局部坐标系与世界坐标系的联系。如果有更精准的外部模块估计的 ego-motion，则可以直接替换采用。</p><h3 id="d-object-detection-and-object-motion-decoder">2.4. 3D Object Detection and Object Motion Decoder</h3><p>　　Bird-view 下 Voxel 后的 3D 检测方法很多，可以是 Anchor-based，Anchor-free，Semantic Segmentation 等方法，其中如果采用 Semantic Segmentation + cluster 方法，那么 scene flow 的结果也可作为 cluster 的线索。<br>　　有了 3D 目标以及目标内 Voxel 的 Rigid Motion 后，取平均或中值即可得到目标的 Motion。<br>　　<strong>Voxel Rigid Motion 可以有两种回归方法：</strong></p><ol type="1"><li>translation 真值为实际该 Voxel 的位移，rotation 为对应刚体的旋转量；</li><li>translation 与 rotation 均为对应刚体的位移与旋转量；</li></ol><p>我理解的本文是采用方法 1. 这种形式，这种形式的好处是回归的就是真实 Voxel 的位移，与输入的特征是 Voxel 级别对应的，但是简单的对目标内的 Voxel 取平均或中值只是目标位移的近似，实际目标的真实位移应该为旋转中心 Voxel 的位移。而方法 2. 是物体级别的回归量，均值即可反应物体的运动，只要构建物体级别的 Loss，用 Voxel 去学习物体级别的运动应该问题不大，所以可能方法 2. 更合理。</p><h2 id="loss-functions">3. Loss Functions</h2><p>　　采用 Voxel 级别的 Loss，总的 Loss 为： <span class="math display">\[\mathcal{L}=\alpha\mathcal{L} _ {flow}+\beta\mathcal{L} _ {rigmo} + \gamma\mathcal{L} _ {ego}+\mathcal{L} _ {det}\tag{2}\]</span> 这四部分具体的形式为：</p><ol type="1"><li>Scene Flow Loss<br>对于有效的 Voxel，作预测值与真值的 \(\mathcal{l} _ 1\) 误差： <span class="math display">\[\mathcal{L} _ {flow}=\frac{1}{K}\sum _ j\Vert \mathbf{v} _ j-\mathbf{v} _ j ^ * \Vert \tag{3}\]</span></li><li>Rigid Motion Loss<br>对于有效的 Voxel，作预测值与真值(真值有两种形式，详见 2.4 讨论)的 \(\mathcal{l} _ 1\) 误差： <span class="math display">\[\mathcal{L} _ {rigmo} = \frac{1}{K}\sum _ j\Vert\mathbf{t} _ j-\mathbf{t} _ j^ * \Vert+\lambda\Vert\theta _ j-\theta _ j^ * \Vert\tag{4}\]</span></li><li>Ego-motion Loss<br>同样的对预测值与真值作 \(\mathcal{l} _ 1\) Loss: <span class="math display">\[\mathcal{L} _ {ego}=\Vert\mathbf{t} _ {BG}-\mathbf{t} _ {BG}^ * \Vert+\lambda\Vert\theta _ {BG}-\theta _ {BG}^ * \Vert \tag{5}\]</span></li><li>Detection Loss<br>不作赘述。</li></ol><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Behl, Aseem, et al. &quot;Pointflownet: Learning representations for rigid motion estimation from point clouds.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　点云的 Scene Flow 与 Semantic 一样是一个较低层的信息，通过 Point-Wise Semantic 信息可以作物体级别的检测，这种方式有很高的召回率，且超参数较少。同样，通过 Point-Wise Scene Flow 作目标级别的运动估计(当然也
      
    
    </summary>
    
      <category term="Scene Flow" scheme="https://leijiezhang001.github.io/categories/Scene-Flow/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="Scene Flow" scheme="https://leijiezhang001.github.io/tags/Scene-Flow/"/>
    
  </entry>
  
  <entry>
    <title>The Normal Distributions Transform for Laser Scan Matching</title>
    <link href="https://leijiezhang001.github.io/paper-reading-The-Normal-Distributions-Transform/"/>
    <id>https://leijiezhang001.github.io/paper-reading-The-Normal-Distributions-Transform/</id>
    <published>2020-04-10T01:39:54.000Z</published>
    <updated>2020-04-11T11:29:19.873Z</updated>
    
    <content type="html"><![CDATA[<p>　　机器人系统中，定位是非常重要的模块。基于 SLAM/VO/VIO 技术的算法能实时作机器人的自定位，但是这种开环下的里程计方案很容易累积绝对误差，使得定位漂移。而离线建立的地图因为有闭环检测，精度很高，所以基于地图的定位方法有很高的绝对定位精度。<br>　　<a href="/LOAM/" title="LOAM">LOAM</a> 是一种基于点云的实时建图与定位方法，其中当前帧点云与前序建立的地图点云配准的方法，采用了提取线、面特征并建立点-线，点-面特征匹配误差函数，从而最小二乘非线性优化求解位姿。这种方案如果特征点噪声较大无匹配对，那么就会有较大的误差。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 将地图点云栅格化，每个栅格又统计点云的高斯分布，匹配的时候计算该帧点云在每个栅格的概率，从而迭代至最优匹配位姿。<br>　　<strong>有闭环检测</strong>的 SLAM 建立的地图即可作为离线定位地图，定位的过程就是当前时刻点云与地图配准的过程，当然后续可以融合其它传感器(GPS，IMU)输出最终的绝对位姿。<strong>点云与地图配准的过程与建图时点云与局部地图或上一时刻点云配准的过程非常相似</strong>。本文介绍一种区别于 <a href="/LOAM/" title="LOAM">LOAM</a> 特征匹配的基于概率统计优化的 NDT 配准方法。</p><h2 id="点云配准算法过程">1. 点云配准算法过程</h2><p>　　考虑二维情况，本文点云配准算法过程为：</p><ol type="1"><li>建立 \(t-1\) 帧点云的 NDT；</li><li>初始化待优化的相对位姿参数 \(T\);</li><li>用 \(T\) 将 \(t\) 帧点云变换到 \(t-1\) 坐标系；</li><li>找到变换每个变换点对应的 \(t-1\) 帧栅格的高斯分布；</li><li>该变换 \(T\) 的度量分数为变换点在高斯分布下的概率和；</li><li>用 Newton 法迭代优化 \(T\);</li><li>重复 3. 直到收敛；</li></ol><p>　　这里主要涉及 NDT，目标函数构建(即 \(T\) 的度量分数)，Newton 法优化三个内容。</p><h3 id="ndt">1.1. NDT</h3><p>　　NDT 是点云栅格化后一系列高斯分布的表示，其过程为：</p><ol type="1"><li>将点云进行栅格化；</li><li>统计每个栅格的点 \(\mathbf{x} _ {i=1..n}\)；</li><li>计算每个栅格高斯分布的 Mean: \(\mathbf{q} = \frac{1}{n}\sum _ i\mathbf{x} _ i\);</li><li>计算 Covariance Matrix: \(\Sigma = \frac{1}{n}\sum _ i(\mathbf{x} _ i -\mathbf{q})(\mathbf{x} _ i-\mathbf{q})^t\)；</li></ol><p>　　由此，<strong>NDT 描述了栅格内每个位置出现点的概率</strong>，即 \(\mathbf{x}\) 有点的概率为： <span class="math display">\[ p(\mathbf{x}) \sim \mathrm{exp}\left(-\frac{(\mathbf{x-q})^t\sum ^ {-1}(\mathbf{x-q})}{2}\right) \tag{1}\]</span> 需要注意的是 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 描述的是每个栅格有点的概率，NDT 描述的是每个栅格点云的概率分布。为了更准确的建模，采用重叠栅格化的设计以消除离散化的影响，以及限定 Covariance 矩阵的最小奇异值。</p><h3 id="目标函数构建">1.2. 目标函数构建</h3><p>　　考虑二维情况，需要优化的位姿参数为 \(\mathbf{p}=(t _ x, t _ y, \varphi)^t\)，第2个点云(待配准点云)中的点为 \(\mathbf{x} _ i\)，其变换到第1个点云坐标系后的表示为 \(\mathbf{x}' _ i\)，对应的第1个点云栅格的 NDT 表示为 \(\mathbf{\Sigma} _ i, \mathbf{q} _ i\)。由此可计算该变换位姿下，其度量分数为： <span class="math display">\[\mathrm{score}(\mathbf{p})=\sum _ i\mathrm{exp}\left(-\frac{(\mathbf{x}&#39; _ i-\mathbf{q} _ i)^t\sum _ i ^ {-1}(\mathbf{x}&#39; _ i-\mathbf{q} _ i)}{2}\right) \tag{2}\]</span> 最大化度量函数即可求解最优的位姿，优化过程一般都是最小化目标函数，所以设定目标函数为 \(-\mathrm{score}\)。</p><h3 id="newton-法优化迭代">1.3. Newton 法优化迭代</h3><p>　　设 \(\mathbf{q}=\mathbf{x}' _ i-\mathbf{q} _ i\)，那么目标函数为： <span class="math display">\[ s = -\mathrm{exp}\frac{-\mathbf{q^t\sum ^ {-1}q}}{2} \tag{3}\]</span> 每次迭代过程为： <span class="math display">\[\mathbf{p\gets p+\Delta p} \tag{4}\]</span> 而 \(\mathbf{\Delta p}\) 来自： <span class="math display">\[\mathbf{H\Delta p} = \mathbf{-g} \tag{5}\]</span> 其中 \(\mathbf{g}\) 是目标函数对优化参数的导数，\(\mathbf{H}\) 为目标函数的 Hessian 矩阵： <span class="math display">\[\left\{\begin{array}{l}g _ i=\frac{\partial s}{\partial p _ i}\\H _ {ij} = \frac{\partial s}{\partial p _ i\partial p _ j}\end{array}\tag{6}\right.\]</span></p><h2 id="建图与定位">2. 建图与定位</h2><p>　　本文的建图是通过<strong>关键帧集合与关键帧之间的位姿变化实现的</strong>，定位的时候去找重合度最高的关键帧作点云配准。此外，当找不到重合度较高的关键帧时，可以实时更新当前帧作为关键帧添加到地图中，还可以对地图作进一步的全局，半全局优化。</p><h2 id="一些思考">3. 一些思考</h2><p>　　本文建图是关键帧的形式，更鲁棒的做法是将点云配准到一起，在世界坐标系下获得场景的稠密点云，然后再 NDT 化，这样能更准确的建模点云分布。<br>　　<a href="/LOAM/" title="LOAM">LOAM</a> 维护的是栅格化的地图，每个栅格限制特征点的数量，所以本质上存储的是原始点云图(被选出是特征点的点云)。为了更好的描述栅格内的特征分布，可以对其作类似 NDT 近似，同时加入能描述该分布的特征，比如对于面特征，加入法向量。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Biber, Peter &amp; Straßer, Wolfgang. (2003). The Normal Distributions Transform: A New Approach to Laser Scan Matching. IEEE International Conference on Intelligent Robots and Systems. 3. 2743 - 2748 vol.3. 10.1109/IROS.2003.1249285.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　机器人系统中，定位是非常重要的模块。基于 SLAM/VO/VIO 技术的算法能实时作机器人的自定位，但是这种开环下的里程计方案很容易累积绝对误差，使得定位漂移。而离线建立的地图因为有闭环检测，精度很高，所以基于地图的定位方法有很高的绝对定位精度。&lt;br&gt;
　　&lt;a hr
      
    
    </summary>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/categories/SLAM/"/>
    
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/tags/SLAM/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="Localization" scheme="https://leijiezhang001.github.io/tags/Localization/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Probabilistic 3D Multi-Object Tracking for Autonomous Driving&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-Probabilistic-3D-Multi-Object-Tracking-for-Autonomous-Driving/"/>
    <id>https://leijiezhang001.github.io/paper-reading-Probabilistic-3D-Multi-Object-Tracking-for-Autonomous-Driving/</id>
    <published>2020-04-07T09:28:57.000Z</published>
    <updated>2020-05-06T06:32:45.376Z</updated>
    
    <content type="html"><![CDATA[<p>　　<a href="/卡尔曼滤波器在三维目标状态估计中的应用/" title="卡尔曼滤波器在三维目标状态估计中的应用">卡尔曼滤波器在三维目标状态估计中的应用</a>中已经较详细得阐述了 3D MOT 状态估计过程，文章末提到观测过程的协方差矩阵初始化问题可以用观测的不确定性解决，<a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 就是通过贝叶斯深度神经网络来建模该不确定性。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 提供了另一种简化的观测不确定性计算方法，同时估计运动模型与观测模型的不确定性，即过程噪声与测量噪声。</p><h2 id="kalman-filter">1. Kalman Filter</h2><p><img src="/paper-reading-Probabilistic-3D-Multi-Object-Tracking-for-Autonomous-Driving/framework.png" width="80%" height="80%" title="图 1. MOT Framework"> 　　如图 1. 所示，本文采用的卡尔曼滤波框架与传统的一样，分为预测与更新。预测阶段，根据上一时刻结果通过 Motion Model(Process Model) 预测当前时刻的状态(先验)；数据关联阶段，将预测的状态与观测的状态作目标数据关联，出 ID；更新阶段，融合预测与观测的状态，得到状态的后验估计。</p><h3 id="predict-step">1.1. Predict Step</h3><p>　　本文采用 CTRV(Constant Turn Rate and Velocity) 运动模型。不同与<a href="/卡尔曼滤波器在三维目标状态估计中的应用/" title="卡尔曼滤波器在三维目标状态估计中的应用">卡尔曼滤波器在三维目标状态估计中的应用</a>中描述的 CTRV，本文作了<strong>线性简化</strong>，其运动方程为： <span class="math display">\[\begin{align}&amp;\begin{bmatrix}\hat{x}\\\hat{y}\\\hat{z}\\\hat{a}\\\hat{l}\\\hat{w}\\\hat{h}\\\hat{d} _ x\\\hat{d} _ y\\\hat{d} _ z\\\hat{d} _ a\\\end{bmatrix} _ {t+1}=\begin{bmatrix}1 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0\\0 &amp;1 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0\\0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0\\0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1\\0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0\\0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0\\0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0 &amp;0\\0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0\\0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0 &amp;0\\0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1 &amp;0\\0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;1\\\end{bmatrix}\begin{bmatrix}x\\y\\z\\a\\l\\w\\h\\d _ x\\d _ y\\d _ z\\d _ a\\\end{bmatrix} _ {t}  +\begin{bmatrix}q _ x\\q _ y\\q _ z\\q _ a\\0\\0\\0\\q _ {d _ x}\\q _ {d _ y}\\q _ {d _ z}\\q _ {d _ a}\\\end{bmatrix} _ {t}\\\Longleftrightarrow &amp; \\&amp;\hat{\mu} _ {t+1} = \mathbf{A}\mu _ t \\\end{align}\tag{1}\]</span> 其中未知的线加速度与角加速度 \((q _ x, q _ y, q _ z, q _ a)\)，\((q _ {d _ x},q _ {d _ y},q _ {d _ z},q _ {d _ a})\) 符合\((0,\mathbf{Q})\)高斯分布。<br>　　根据 Motion Model，卡尔曼的预测过程计算状态量的先验： <span class="math display">\[\begin{align}\hat{\mu} _ {t+1} &amp;= \mathbf{A}\mu _ t \\\hat{\Sigma} _ {t+1} &amp;= \mathbf{A}\Sigma _ t\mathbf{A}^T + \mathbf{Q}\\\end{align}\tag{2}\]</span> 　　观测模型为每一时刻检测的结果，包括位置，朝向，目标框尺寸，即观测矩阵 \(\mathbf{H} _ {7\times 11} = [\mathbf{I}, \mathbf{0}]\)。观测噪声也符合高斯分布，由此得到预测的观测量： <span class="math display">\[\begin{align}\hat{o} _ {t+1} &amp;= \mathbf{H}\hat{\mu} _ {t+1} \\\mathbf{S} _ {t+1} &amp;= \mathbf{H}\hat{\Sigma} _ {t+1}\mathbf{H}^T + \mathbf{R}\\\end{align}\tag{3}\]</span></p><h3 id="update-step">1.2. Update Step</h3><p>　　首先将预测的观测量与实际的观测量作数据关联。基本思想是将预测目标与观测目标作 Cost Matrix，然后用匈牙利/贪心算法求解最优匹配对。本文采用 Mahalanobis distance： <span class="math display">\[ m = \sqrt{(o _ {t+1}- \mathbf{H}\hat{\mu} _ {t+1})^T\mathbf{S} _ {t+1} ^{-1}(o _ {t+1}-\mathbf{H}\hat{\mu} _ {t+1})} \tag{4}\]</span> 需要注意的是，计算距离前先做角度矫正，如果两个目标框角度相差大于 90 度，那么作 180 度旋转。<br>　　得到预测与观测的匹配对后，计算后验概率更新该目标的状态： <span class="math display">\[\begin{align}\mathbf{K} _ {t+1} &amp;= \hat{\Sigma} _ {t+1}\mathbf{H} ^T\mathbf{S} _ {t+1}^{-1}\\\mu _ {t+1} &amp;= \hat{\mu} _ {t+1} + \mathbf{K} _ {t+1}(o _ {t+1}-\mathbf{H}\hat{\mu} _ {t+1})\\\Sigma _ {t+1} &amp;=(\mathbf{I}-\mathbf{K} _ {t+1}\mathbf{H})\hat{\Sigma} _ {t+1}\\\end{align}\tag{5}\]</span> 　　以上卡尔曼过程与<a href="/卡尔曼滤波器在三维目标状态估计中的应用/" title="卡尔曼滤波器在三维目标状态估计中的应用">卡尔曼滤波器在三维目标状态估计中的应用</a>，以及<a href="/卡尔曼滤波详解/" title="卡尔曼滤波详解">卡尔曼滤波详解</a>完全一致。</p><h2 id="covariance-matrices-estimation">2. Covariance Matrices Estimation</h2><p>　　如何确定卡尔曼滤波过程中的 \(\Sigma _ 0, \mathbf{Q, R}\)？传统方法是直接用一个确定的经验矩阵赋值；理想的是用<a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 建模处理，但是会相对较复杂；本文用更简单的基于统计方法来确定协方差矩阵。<br>　　<strong>观测量的方差(不确定性)与目标的属性有关</strong>，如距离，遮挡，类别等。本文没有区分这些属性，只统计了一种观测量的方差，<strong>更好的处理方式是按照不同属性，统计不同的方差</strong>。而 <a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 是 Instance 级别的方差预测。<strong>这种统计出来的方差虽然细粒度差一点，但是非常合理，因为只要模型训练好后，模型预测的分布是与训练集分布相似的(理想情况)</strong>，所以用训练集的方差来直接代替模型预测的方差也较为合理。<br>　　<span style="color:red"><strong>更准确的来说，不确定性与物体的属性以及标注误差有关，这里只统计了标注误差(标注误差在大多数情况下都是同分布的)，而实际上遮挡大的目标，是更难学习的(目标学习有难易之分，即预测分布与训练集分布会有偏差)，即预测结果会有额外量的不确定性，所以这种离线统计方法也有很大的局限性</strong>。</span><br>　　设训练集的真值标签：\(\left\{\left\{x _ t^m, y _ t^m, z _ t^m, a _ t^m\right\} _ {m=1}^M\right\} _ {t = 1}^T\)。</p><h2 id="motionprocess-noise-model">2.1. Motion/Process Noise Model</h2><p>　　假设各状态量的噪声独立同分布，那么对于位置与朝向噪声，有： <span class="math display">\[\begin{align}Q _ {xx} &amp;= \mathbf{Var}\left(\left(x _ {t+1}^m-x _ t^m\right)-\left(x _ t^m-x _ {t-1}^m\right)\right)\\Q _ {yy} &amp;= \mathbf{Var}\left(\left(y _ {t+1}^m-y _ t^m\right)-\left(y _ t^m-y _ {t-1}^m\right)\right)\\Q _ {zz} &amp;= \mathbf{Var}\left(\left(z _ {t+1}^m-z _ t^m\right)-\left(z _ t^m-z _ {t-1}^m\right)\right)\\Q _ {aa} &amp;= \mathbf{Var}\left(\left(a _ {t+1}^m-a _ t^m\right)-\left(a _ t^m-a _ {t-1}^m\right)\right)\\\end{align}\tag{6}\]</span> 　　对于线速度与角速度，因为： <span class="math display">\[\begin{align}q _ {x _ t} &amp;\approx x _ {x+1} - x _ t - d _ {x _ t}\\&amp; \approx (x _ {t+1}-x _ t) - (x _ t-x _ {t-1})\\q _ {d _ {x _ t}} &amp;\approx d _ {x _ {t+1}} - d _ {x _ t}\\&amp; \approx (x _ {t+1}-x _ t) - (x _ t-x _ {t-1})\\\end{align}\tag{7}\]</span> 所以： <span class="math display">\[ (Q _ {d _ xd _ x}, Q _ {d _ yd _ y}, Q _ {d _ zd _ z}, Q _ {d _ ad _ a}) = (Q _ {xx}, Q _ {yy}, Q _ {zz}, Q _ {aa})\tag{8}\]</span></p><h2 id="observation-noise-model">2.2. Observation Noise Model</h2><p>　　在训练集上，找到检测与真值的匹配对 \(\left\{\left\{(D _ t^k, G _ t^k)\right\} _ {k=1}^K\right\} _ {t=1}^T\)，从而计算观测噪声： <span class="math display">\[\begin{align}&amp;R _ {xx} = \mathbf{Var}\left(D _ {x _ t}^k-G _ {x _ t}^k\right)\\&amp;R _ {yy} = \mathbf{Var}\left(D _ {y _ t}^k-G _ {y _ t}^k\right)\\&amp;R _ {zz} = \mathbf{Var}\left(D _ {z _ t}^k-G _ {z _ t}^k\right)\\&amp;R _ {aa} = \mathbf{Var}\left(D _ {a _ t}^k-G _ {a _ t}^k\right)\\&amp;R _ {ll} = \mathbf{Var}\left(D _ {l _ t}^k-G _ {l _ t}^k\right)\\&amp;R _ {ww} = \mathbf{Var}\left(D _ {w _ t}^k-G _ {w _ t}^k\right)\\&amp;R _ {hh} = \mathbf{Var}\left(D _ {h _ t}^k-G _ {h _ t}^k\right)\\\end{align}\tag{8}\]</span> 初始的状态协方差 \(\Sigma _ 0 = \mathbf{R}\)。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Chiu, Hsu-kuang, et al. &quot;Probabilistic 3D Multi-Object Tracking for Autonomous Driving.&quot; arXiv preprint arXiv:2001.05673 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　&lt;a href=&quot;/卡尔曼滤波器在三维目标状态估计中的应用/&quot; title=&quot;卡尔曼滤波器在三维目标状态估计中的应用&quot;&gt;卡尔曼滤波器在三维目标状态估计中的应用&lt;/a&gt;中已经较详细得阐述了 3D MOT 状态估计过程，文章末提到观测过程的协方差矩阵初始化问题可以用观测的不
      
    
    </summary>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/categories/MOT/"/>
    
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/tags/MOT/"/>
    
      <category term="Uncertainty" scheme="https://leijiezhang001.github.io/tags/Uncertainty/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;LaserNet&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-LaserNet-An-Efficient-Probabilistic-3D-Object-Detector-for-Autonomous-Driving/"/>
    <id>https://leijiezhang001.github.io/paper-reading-LaserNet-An-Efficient-Probabilistic-3D-Object-Detector-for-Autonomous-Driving/</id>
    <published>2020-04-06T07:36:13.000Z</published>
    <updated>2020-04-07T09:22:24.202Z</updated>
    
    <content type="html"><![CDATA[<p>　　3D 目标检测中，目标定位的不确定性也很关键，<a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 中已经较为详细的描述了在 Bayesian Deep Networks 中如何建模异方差偶然不确定性(Aleatoric Uncertainty)。在贝叶斯深度神经网络框架下，网络不仅预测目标的位置(Mean)，还预测出该预测位置的方差(Variance)。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 延续了 <a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 中预测 Corner 点位置方差的思路，提出了一种预测目标位置方差的方法。</p><h2 id="算法框架">1. 算法框架</h2><p><img src="/paper-reading-LaserNet-An-Efficient-Probabilistic-3D-Object-Detector-for-Autonomous-Driving/framework.png" width="90%" height="90%" title="图 1. LaserNet Framework"> 　　如图 1. 所示，输入为激光点云的 Sensor Range View 表示方式，输出为点级别的目标框3D属性，框顶点位置方差，以及类别概率。最后在 Bird View 下作目标框的聚类与 NMS 等后处理。</p><h3 id="点云输入方式">1.1. 点云输入方式</h3><p>　　不同于目前主流的 Bird View 点云栅格化方式，本文将点云直接根据线束在 Sensor Range View 下进行表示，高为激光线数量，宽为 HFOV 除以角度分辨率。设计 5 个 channel：距离，高度，角度，反射值，以及是否有点的标志位。<br>　　本文认为这种点云表示方式的优点被忽视了，该视角下，点云的表达是紧促的，而且能高效得取得局部区域点，此外，能保留点云获取方式的信息。另一方面，该表达方式的缺点有，访问局部区域时，并不是空间一致的；以及需要处理物体的不同形状和遮挡问题。本文实验结果是，在 Kitti 上效果不如 Bird View 方法，但是在一个较大数据集上，能克服这些缺点。</p><h3 id="网络输出">1.2. 网络输出</h3><p>　　网络输出为点级别的预测，由三部分组成：</p><ol type="1"><li><strong>类别概率</strong><br>每个类别的概率；</li><li><strong>3D 框属性</strong><br>包括相对中心距离 \((d _ x, d _ y)\)；相对朝向 \((\omega _ x, \omega _ y)=(\mathrm{cos}\omega, \mathrm{sin}\omega)\)；以及尺寸 \((l,w)\)。最终目标框中心点位置及朝向表示为： <span class="math display">\[\left\{\begin{array}{l}\mathbf{b} _ c = [x,y]^T+\mathbf{R} _ \theta [d _ x,d _ y]^T \\\varphi = \theta + \mathrm{atan2}(\omega _ y,\omega _ x)\end{array}\tag{1}\right.\]</span> 其中 \(\theta\) 为该点的雷达扫描角度。由此可得到四个目标框角点坐标： <span class="math display">\[\left\{\begin{array}{l}\mathbf{b} _ 1 = \mathbf{b} _ c + \frac{1}{2}\mathbf{R} _ \varphi [l,w]^T\\\mathbf{b} _ 2 = \mathbf{b} _ c + \frac{1}{2}\mathbf{R} _ \varphi [l,-w]^T\\\mathbf{b} _ 3 = \mathbf{b} _ c + \frac{1}{2}\mathbf{R} _ \varphi [-l,-w]^T\\\mathbf{b} _ 4 = \mathbf{b} _ c + \frac{1}{2}\mathbf{R} _ \varphi [-l,w]^T\end{array}\tag{2}\right.\]</span></li><li><strong>顶点位置方差</strong><br>当观测不完全时(遮挡，远处)，目标框的概率分布是多模态的，所以如 <a href="/Heteroscedastic-Aleatoric-Uncertainty/" title="Heteroscedastic Aleatoric Uncertainty">Heteroscedastic Aleatoric Uncertainty</a> 中所述，输出为混合高斯模型。对于每个点的每个类别，输出 \(K\) 个目标框属性：\(\{d _ {x,k}, d _ {y,k}, \omega _ {x,k}, \omega _ {y,k}, l _ k, w _ k\} _ {k=1}^K\)；对应的方差 \(\{s _ k\} _ {k=1}^K\)；以及模型权重 \(\{\alpha _ k\} _ {k=1}^K\)。</li></ol><h3 id="bird-view-后处理">1.3. Bird View 后处理</h3><p>　　网络其实就做了一个点级别的分割，接下来需要作聚类以得到目标框。本文采用 Mean-Shift 方法作聚类。由于是点级别的概率分布，得到目标点集后，需要用 BCN(详见 <a href="/MOT-Fusion/" title="MOT-Fusion">MOT-Fusion</a>) 转换为目标级别的概率分布： <span class="math display">\[\left\{\begin{array}{l}\hat{\mathbf{b}} _ i = \frac{\sum _ {j\in S _ i} w _ j\mathbf{b} _ j}{\sum _ {j\in S _ i}w _ j}\\\hat{\sigma} _ i^2 = \left(\sum _ {j\in S _ i}\frac{1}{\sigma ^2 _ j}\right)^{-1}\end{array}\tag{3}\right.\]</span> 其中 \(w=\frac{1}{\sigma ^ 2}\)。</p><h2 id="loss-形式">2. Loss 形式</h2><p>　　分类采用 Focal Loss。对于每个点 3D 属性的回归，首先找到最靠近真值的预测模型： <span class="math display">\[k ^ * = \mathrm{arg}\min \limits _ k\Vert\hat{\mathbf{b}} _ k-\mathbf{b} ^{gt}\Vert\tag{4}\]</span> 对该预测模型作 Loss： <span class="math display">\[\mathcal{L} _ {box}=\sum _ n\frac{1}{\hat{\sigma} _ {k ^ * }} \left\vert\hat{\mathbf{b}} _ {n,k^ * }-\mathbf{b} _ n^{gt}\right\vert + \mathrm{log}\hat{\sigma} _ {k ^ * }\tag{5}\]</span> 实际回归的是 \(s:=\mathrm{log} \sigma\)。然后对混合模型的权重 \(\{\alpha _ k\} _ {k=1}^K\) 作 cross entry loss \(\mathcal{L} _ {mix}\)。最终的回归 Loss 为： <span class="math display">\[\mathcal{L} _ {reg} = \frac{1}{N}\sum _ i \frac{\mathcal{L} _ {box, i} + \lambda \mathcal{L} _ {mix,i}}{n _ i} \tag{6}\]</span></p><h2 id="adaptive-nms">3. Adaptive NMS</h2><p>　　类别概率不能反应目标框的质量，所以本文采用预测的目标框方差作为 NMS 的参考量。将目标框方差转换为目标框的质量分数：\(\alpha _ k/2\hat{\sigma} _ k\)。<br><img src="/paper-reading-LaserNet-An-Efficient-Probabilistic-3D-Object-Detector-for-Autonomous-Driving/nms.png" width="50%" height="50%" title="图 2. Adaptive NMS"> 　　此外不同目标在 Bird-View 下 IoU 最大值有一定的限制，如图 2. 所示，最坏的情况，Bird-View 下两个框的 IoU 最大限制为设计为： <span class="math display">\[t=\left\{\begin{array}{l}\frac{\sigma _ 1+\sigma _ 2}{2w-\sigma _ 1 - \sigma _ 2} &amp; \sigma _ 1+\sigma _ 2 &lt; w\\1 &amp; otherwise\end{array}\tag{7}\right.\]</span> 当两个目标框的 IoU 大于阈值时，可能的情况是：1. 目标框错误，则删除低分数的目标框；2. 方差估计错误，那么增大方差使最大阈值满足 IoU 条件。</p><h2 id="预测分布的分析">4. 预测分布的分析</h2><p><img src="/paper-reading-LaserNet-An-Efficient-Probabilistic-3D-Object-Detector-for-Autonomous-Driving/calibration.png" width="80%" height="80%" title="图 3. Uncertainty(Variance) Calibration"> 　　评价 Variance(Uncertainty) 预测的好坏，可以画 Calibration 图。如图 3. 所示，横坐标为预测的 Mean 与真值形成的高斯概率分布下的 CDF，而纵坐标为预测的 Variance 统计出的高斯分布下的 CDF。理想情况下，两者是 \(y=x\) 的关系，如图所示，在 ATG4D 大数据集上，预测的 Variance 效果更好。</p><h2 id="一些思考">5. 一些思考</h2><p>　　不管是 2D 检测还是 3D 检测，这种先(语义)分割后聚类出目标的思想，有很强的优势：召回率高，超参数少，自带分割信息等。本文又应用 Aleatoric Uncertainty 来建模检测的不确定性--位置方差(不确定性干嘛用，怎么用，不多说了)，有很好的借鉴意义。</p><h2 id="reference">6. Reference</h2><p><a id="1" href="#1ref">[1]</a> Meyer, Gregory P., et al. &quot;Lasernet: An efficient probabilistic 3d object detector for autonomous driving.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　3D 目标检测中，目标定位的不确定性也很关键，&lt;a href=&quot;/Heteroscedastic-Aleatoric-Uncertainty/&quot; title=&quot;Heteroscedastic Aleatoric Uncertainty&quot;&gt;Heteroscedastic 
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Uncertainty" scheme="https://leijiezhang001.github.io/tags/Uncertainty/"/>
    
  </entry>
  
</feed>
