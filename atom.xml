<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LeijieZhang</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://leijiezhang001.github.io/"/>
  <updated>2020-09-16T01:34:12.000Z</updated>
  <id>https://leijiezhang001.github.io/</id>
  
  <author>
    <name>Leijie</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>[paper_reading]-&quot;PnPNet&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-PnPNet/"/>
    <id>https://leijiezhang001.github.io/paper-reading-PnPNet/</id>
    <published>2020-09-11T01:35:32.000Z</published>
    <updated>2020-09-16T01:34:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　自动驾驶的障碍物状态估计功能模块中，包含 perception/Detection，tracking，prediction 三个环节。传统的做法这三个环节是分步进行的，Detection 出目标框检测结果；Tracking 则作前后帧目标的数据关联然后用卡尔曼平滑并估计目标状态；Prediction 预测目标未来的运动轨迹。 <img src="/paper-reading-PnPNet/diff-pipe.png" width="60%" height="60%" title="图 1. Perception and Prediction"> 　　如图 1. 所示，(a) 代表传统的做法，每个步骤都是独立优化并出结果，这种方式将功能模块解耦，容易找到具体问题的位置，但是会降低算法找到最优解的概率；(b) 则将 Detection 与 Prediction 用同一个网络预测，然后用 Tracking 来平滑估计整个运动轨迹(代表方法是 <a href="/paperreading-Fast-and-Furious/" title="Fast and Furious">Fast and Furious</a>)，这种方法下 Tracking 中丰富的时序及空域特征信息没有作用于 Detection 和 Prediction；本文提出的 PnP<a href="#1" id="1ref"><sup>[1]</sup></a>方法则将三个环节作深度的特征再利用，即整个功能模块是 End-to-End 可训练的，更容易得到目标状态及预测的全局最优解，更容易处理遮挡等问题。</p><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-PnPNet/framework.png" width="90%" height="90%" title="图 2. Framework"> 　　如图 2. 所示，PnP 网络包含 Detection、Tracking，Motion Forecasting 三个模块。网络输入为点云及 HD Map。检测模块包含一个任意的 3D 目标检测网络，以及一个存储历史 BEV 特征图的 Memory；Tracking 跟踪模块包含一个存储目标历史轨迹的 Memory，首先作 Track-Detection 的数据关联，然后优化目标历史轨迹并更新存储；Motion Forecasting 模块则根据历史轨迹作目标的运动预测。</p><h2 id="object-detection">2. Object Detection</h2><p>　　网络的输入为序列点云(本文采用 0.5s)及 HD Map，分别将点云在俯视图下体素化后在特征通道维度进行串联得到 \(\mathbf{x} ^ t\)，然后输入 Backbone 网络得到俯视图下特征图： <span class="math display">\[\mathcal{F} ^ t _ {bev}(\mathbf{x} ^ t) = \mathrm{CNN} _ {bev}(\mathbf{x} ^ t) \tag{1}\]</span> 最后加入 3D 目标检测头，得到 3D 目标框属性 \((u _ i ^ t, v _ i ^ t,w _ i,l _ i,\theta _ i ^ t)\) 的预测： <span class="math display">\[\mathcal{D} ^ t=\mathrm{CNN} _ {det}(\mathcal{F} ^ t _ {bev})\tag{2}\]</span></p><h2 id="discrete-continuous-tracking">3. Discrete-Continuous Tracking</h2><p>　　<strong>Tracking 模块包括离散的数据关联问题，以及连续的目标运动轨迹(状态)估计问题。</strong>目标运动轨迹的优化估计对之后的目标运动预测非常重要。</p><h3 id="trajectory-level-object-representation">3.1. Trajectory Level Object Representation</h3><p><img src="/paper-reading-PnPNet/trajectory.png" width="90%" height="90%" title="图 3. Trajectory Level Object Representation"> 　　Tracking 需要优化历史轨迹，Prediction 需要预测未来轨迹，所以轨迹级别的目标特征提取及表达非常重要。本文采用 LSTM 网络来表征。如图 3. 所示，对于轨迹 \(\mathcal{P} _ i ^ t=\mathcal{D} _ i ^ {t _ 0...t}\)，首先提取每个时刻目标的感知特征： <span class="math display">\[f _ i^{bev,t} = \mathrm{BilinearInterp}(\mathcal{F} _ {bev} ^ t,(u _ i ^ t, v _ i ^ t)) \tag{3}\]</span> 然后提取目标运动特征： <span class="math display">\[f _ i ^ {velocity,t}=(\dot{x} _ i ^ t,\dot{x} _ {ego} ^ t, \dot{\theta} _ {ego} ^ t)\tag{4}\]</span> 其中 \(\dot{x} _ i,\dot{x} _ {ego}\) 分别是第 \(i\) 个目标及本车的二维速度，通过位置差计算得到，对于新目标，将其设定为 0。由此得到第 \(i\) 个目标的特征： <span class="math display">\[f(\mathcal{D} _ i ^ t)=\mathrm{MLP} _ {merge}\left(f _ i^{bev,t},f _ i ^ {velocity,t}\right)\tag{5}\]</span> 最后通过 LSTM 网络来提取轨迹级别目标特征： <span class="math display">\[h(\mathcal{P} _ i ^ t)=\mathrm{LSTM}(f(\mathcal{D} _ i ^ {t _ 0...t}))\tag{6}\]</span></p><h3 id="data-association">3.2. Data Association</h3><p>　　当前时刻检测的目标数量为 \(N _ t\)，上一时刻目标轨迹数量为 \(M _ {t-1}\)，将二者关联匹配就是数据关联问题。这在有新目标出现以及目标出现遮挡的时候变得较为困难。类似传统方法，这里设计检测与跟踪轨迹的相似性矩阵 \(C\in\mathbb{R} ^ {N _ t\times (M _ {t-1}+N _ t)}\)(跟踪轨迹加入\(N _ t\)个目标是为了处理新出现目标的情况)： <span class="math display">\[C _ {i,j}=\left\{\begin{array}{l}\mathrm{MLP} _ {pair}\left(f(\mathcal{D} _ i ^ t),h(\mathcal{P} _ j ^ {t-1})\right) &amp;\;\; \mathrm{if}\; 1\leq j\leq M _ {t-1},\\\mathrm{MLP} _ {unary}\left(f(\mathcal{D} _ i ^ t)\right) &amp;\;\; \mathrm{if}\; j=  M _ {t-1} + i,\\-\mathrm{inf} &amp;\;\; \mathrm{otherwise}\end{array}\tag{7}\right.\]</span> 其中 \(\mathrm{MLP} _ {pair}\) 计算检测与跟踪轨迹的相似性分数，\(\mathrm{MLP} _ {unary}\) 计算目标是新出现的概率。有了该相似性矩阵，即可通过匈牙利算法求解最佳匹配对。<br>　　对于被遮挡的物体，跟踪轨迹在当前帧容易出现没有检测的情况，本文引入单目标跟踪的思想作跟踪搜索。设未匹配的跟踪轨迹为 \(\mathcal{P} _ j ^ {t-1}\)，那么根据上一帧该轨迹目标的位置 \(u _ j ^ {t-1}, v _ j ^ {t-1}\)，进行运动补偿后为 \(\tilde{u} _ j ^ {t}, \tilde{v} _ j ^ {t}\)，在其邻域 \(\Omega _ j\) 内寻找最优的检测(跟踪)结果 \(\tilde{\mathcal{D}} _ k ^ t\)： <span class="math display">\[k = \mathop{\arg\max}\limits _ {i\in\Omega _ j} \mathrm{MLP} _ {pair}\left(f(\tilde{\mathcal{D}} _ i ^ t),h(\mathcal{P} _ j ^ {t-1})\right)\tag{8}\]</span> 其中 \(\Omega _ j\) 设计为目标的最大假设速度，如 \(110 km/h\)。<br>　　最终可得到 \(N _ t+ K _ t\) 个目标轨迹，其中 \(K _ t\) 为未匹配的目标轨迹而通过单目标跟踪方法召回的轨迹数量。</p><h3 id="trajectory-estimation">3.3. Trajectory Estimation</h3><p>　　当前帧的观测加入到目标轨迹后，可进一步对目标轨迹作优化以减少 FP 以及提高轨迹定位精度。网络预测轨迹的置信度以及最近 \(T _ 0\) 时间内目标位置的残差： <span class="math display">\[\mathrm{score} _ i,\Delta u _ i ^ {t-T _ 0+1:t},\Delta v _ i ^ {t- T _ 0+1:t}=\mathrm{MLP} _ {refine}(h(\mathcal{P} _ i^t))\tag{9}\]</span> 其中 \(T _ 0\) 小于轨迹的总时间。最后用 NMS 去掉重叠的目标轨迹以消除 FP 与重叠项。</p><h2 id="motion-forecasting">4. Motion Forecasting</h2><p>　　根据优化后的目标轨迹，通过网络预测目标的未来轨迹： <span class="math display">\[\Delta u _ i^{t:t+\Delta T}, \Delta v _ i ^ {t:t+\Delta T}=\mathrm{MLP} _ {predict}(h(\mathcal{P} _ i^t))\tag{10}\]</span></p><h2 id="end-to-end-learning">5. End-to-End Learning</h2><p>　　整个网络多任务联合训练的 Loss 为： <span class="math display">\[\begin{align}\mathcal{L} &amp;= \mathcal{L} _ {detect} + \mathcal{L} _ {track} + \mathcal{L} _ {predict}\\&amp;= \mathcal{L} _ {detect} + \mathcal{L} _ {score} ^ {affinity} + \mathcal{L} _ {score} ^ {sot} + \mathcal{L} _ {socre} ^ {refine} + \mathcal{L} _ {reg} ^ {refine} + \mathcal{L} _ {predict}\end{align}\tag{11}\]</span> 其中 \(\mathcal{L} _ {score}\) 为 \(max-margin \;loss\): <span class="math display">\[\mathcal{L} _ {score} = \frac{1}{N _ {i,j}}\sum _ {i\in pos,j\in neg} \mathrm{max}(0,m-(a _ i-a _ j))\tag{12}\]</span> 对于 \(\mathcal{L} _ {score} ^ {affinity}\) 和 \(\mathcal{L} _ {score} ^ {sot}\)，计算正样本与所有负样本的 Loss；对于 \(\mathcal{L} _ {score} ^ {refine}\)，与真值框 IoU 较高的，则 score 较高，这样作 NMS 时可以该 score 为准则。</p><h2 id="reference">6. Reference</h2><p><a id="1" href="#1ref">[1]</a> Liang, Ming, et al. &quot;PnPNet: End-to-End Perception and Prediction with Tracking in the Loop.&quot; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　自动驾驶的障碍物状态估计功能模块中，包含 perception/Detection，tracking，prediction 三个环节。传统的做法这三个环节是分步进行的，Detection 出目标框检测结果；Tracking 则作前后帧目标的数据关联然后用卡尔曼平滑并估计
      
    
    </summary>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/categories/MOT/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/tags/MOT/"/>
    
      <category term="Prediction" scheme="https://leijiezhang001.github.io/tags/Prediction/"/>
    
  </entry>
  
  <entry>
    <title>自动驾驶系统之 Sensing</title>
    <link href="https://leijiezhang001.github.io/Self-Driving-Sensing/"/>
    <id>https://leijiezhang001.github.io/Self-Driving-Sensing/</id>
    <published>2020-09-10T03:09:33.000Z</published>
    <updated>2020-09-10T03:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="本文介绍了自动驾驶中基于多传感器融合的 Sensing 模块，请输入密码查看：" />    <label for="pass">本文介绍了自动驾驶中基于多传感器融合的 Sensing 模块，请输入密码查看：</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">U2FsdGVkX1+lyG3+lBoDr34E3KaoX0p4DJVvBLxQ/i4=</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      从零到一构建自动驾驶系统之 Sensing。
    
    </summary>
    
      <category term="Autonomous Driving System" scheme="https://leijiezhang001.github.io/categories/Autonomous-Driving-System/"/>
    
      <category term="Sensing" scheme="https://leijiezhang001.github.io/categories/Autonomous-Driving-System/Sensing/"/>
    
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="Tracking" scheme="https://leijiezhang001.github.io/tags/Tracking/"/>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/tags/MOT/"/>
    
      <category term="Sensing" scheme="https://leijiezhang001.github.io/tags/Sensing/"/>
    
      <category term="Detection" scheme="https://leijiezhang001.github.io/tags/Detection/"/>
    
  </entry>
  
  <entry>
    <title>自动驾驶系统之 Localization</title>
    <link href="https://leijiezhang001.github.io/Self-Driving-Localization/"/>
    <id>https://leijiezhang001.github.io/Self-Driving-Localization/</id>
    <published>2020-09-10T03:08:01.000Z</published>
    <updated>2020-09-10T03:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="本文介绍了自动驾驶中基于多传感器融合的 Localization 模块，请输入密码查看：" />    <label for="pass">本文介绍了自动驾驶中基于多传感器融合的 Localization 模块，请输入密码查看：</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">U2FsdGVkX19F5L3CXaEq80FegG7fGZVbXFX+FNboXkQ=</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      从零到一构建自动驾驶系统之 Localization。
    
    </summary>
    
      <category term="Autonomous Driving System" scheme="https://leijiezhang001.github.io/categories/Autonomous-Driving-System/"/>
    
      <category term="Localization" scheme="https://leijiezhang001.github.io/categories/Autonomous-Driving-System/Localization/"/>
    
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/tags/SLAM/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>自动驾驶系统之 Mapping</title>
    <link href="https://leijiezhang001.github.io/Self-Driving-Mapping/"/>
    <id>https://leijiezhang001.github.io/Self-Driving-Mapping/</id>
    <published>2020-09-10T02:49:01.000Z</published>
    <updated>2020-09-10T03:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="本文介绍了自动驾驶中用于定位、感知、规划的 Mapping 模块，请输入密码查看：" />    <label for="pass">本文介绍了自动驾驶中用于定位、感知、规划的 Mapping 模块，请输入密码查看：</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">U2FsdGVkX1/EH27hUH+F5Rq/I4jDDcT3XZs2VZMxGaE=</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      从零到一构建自动驾驶系统之 Mapping。
    
    </summary>
    
      <category term="Autonomous Driving System" scheme="https://leijiezhang001.github.io/categories/Autonomous-Driving-System/"/>
    
      <category term="Mapping" scheme="https://leijiezhang001.github.io/categories/Autonomous-Driving-System/Mapping/"/>
    
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/tags/SLAM/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;AFDet&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-AFDet/"/>
    <id>https://leijiezhang001.github.io/paper-reading-AFDet/</id>
    <published>2020-08-28T01:45:09.000Z</published>
    <updated>2020-09-02T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　点云目标检测方法已趋于完善，为了能在嵌入式系统上高效运行点云目标检测算法，地平线提出了 AFDet <a href="#1" id="1ref"><sup>[1]</sup></a>，该文章发表在 CVPR2020 Workshop 上，算是很工程化的一个工作了，对工程产品落地有很好的参考价值。AFDet 应用了很多 Anchor-Free 2D 目标检测思想，可参考 <a href="/Anchor-Free-Detection/" title="Anchor-Free Detection">Anchor-Free Detection</a>。</p><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-AFDet/framework.png" width="90%" height="90%" title="图 1. Framework"> 　　AFDet 是一种 Anchor-Free，NMS-Free 的检测方法，所以后处理非常简单，高效。如图 1. 所示，AFDet 采用了传统的 Birdview 下的 Point Cloud Encoder，Backbone &amp; Necks，Anchor-Free Detector 三种网络结构。Point Cloud Encoder 可采用 <a href="/paperreading-PointPillars/" title="PointPillars">PointPillars</a> 结构，Backbone &amp; Necks 这里也不作展开。这里最重要的设计是 Anchor-Free 的检测头，由 Keypoint Heatmap，Local Offset Head，z-axis Location Head，3D Object Size Head，Orientation Head 等五个分支构成。</p><h3 id="keypoint-heatmap-local-offset-head">1.1. Keypoint Heatmap &amp; Local Offset Head</h3><p>　　BEV 下目标定位由 Heatmap \(M\in\mathbb{R} ^ {W\times H\times C}\) 和 Offset Regression Map \(O\in\mathbb{R} ^ {W\times H\times 2}\) 组成，其中 \(C\) 为 Keypoint 类型。Offset Head 是为了消除 Voxel 后的量化误差以预测更准确的目标位置。<br>　　对于第 \(k\) 个类别为 \(c _ k\) 的目标，其 3D 属性为：\((x ^ {(k)},y ^ {(k)},z ^ {(k)},w ^ {(k)},l ^ {(k)},h ^ {(k)},\theta ^ {(k)})\)。设 Pillar 边长为 \(b\)，那么在 BEV 栅格图上，目标中心点作为关键点的坐标为 \(\bar{p}=\left(\left\lfloor\frac{x ^ {(k)}-back}{b}\right\rfloor,\left\lfloor\frac{y ^ {(k)}-left}{b}\right\rfloor\right)\in\mathbb{R} ^ 2\)，其中 \([(back,front),(left,right)]\) 为 \(x-y\) 平面检测范围。由此，目标在 BEV 下的 2D 属性框表示为 \(\left(\left\lfloor\frac{x ^ {(k)}-back}{b}\right\rfloor,\left\lfloor\frac{y ^ {(k)}-left}{b}\right\rfloor,\left\lfloor\frac{w ^ {(k)}}{b}\right\rfloor,\left\lfloor\frac{l ^ {(k)}}{b}\right\rfloor,\theta ^ {(k)}\right)\)。<br>　　对于 BEV Heatmap 分支的真值，需要根据目标框真值来生成。对于 Heatmap 中的像素点 \((x,y)\)，设计其值为： <span class="math display">\[M _ {x,y,z} =\left\{\begin{array}{l}1, &amp;\mathrm{if}\;d=0\\0.8, &amp;\mathrm{if}\; d=1\\\frac{1}{d}, &amp;\mathrm{else}\end{array}\tag{1}\right.\]</span> 其中 \(d\) 表示目标框中心点与对应像素点的距离，Heatmap 中预测量 \(\hat{M} _ {x,y,c}=1\) 表示其为目标框中心点，\(\hat{M} _ {x,y,c}=0\) 则表示是背景。Heatmap 中 \(\bar{p}\) 位置定义为正样本点，其余 Pillars 为负样本点，使用 Focal Loss： <span class="math display">\[\mathcal{L} _ {heat} = -\frac{1}{N}\sum _ {x,y,c}\left\{\begin{array}{l}\left(1-\hat{M} _ {x,y,c}\right) ^ {\alpha}\;\mathrm{log}\left(\hat{M} _ {x,y,c}\right), \;\mathrm{if}\; M _ {x,y,c} = 1 \\\left(1-\hat{M} _ {x,y,c}\right) ^ {\beta}\; \left(\hat{M} _ {x,y,c}\right) ^ {\alpha}\mathrm{log}\left(1-\hat{M} _ {x,y,c}\right), \;\mathrm{else} \\\end{array}\tag{2}\right.\]</span> 　　另一方面，Offset Regression 分支可以解决量化误差，以及当 Heatmap 中心点分类错误的时候，补救预测准确的中心点位置。选择中心点周围半径 \(r\) 区域作 Offset 预测： <span class="math display">\[\mathcal{L} _ {off} = \frac{1}{N}\sum _ p\sum ^ r _ {\sigma =-r}\sum ^ r _ {\epsilon = -r}\left\vert\hat{O} _ {\bar{p}}-b(p-\bar{p}+(\sigma,\epsilon))\right\vert\tag{3}\]</span> 只对 \(2r+1\) 的矩形区域作 Offset 预测。</p><h3 id="z-axis-location-head">1.2. z-axis Location Head</h3><p>　　高度预测值 \(\hat{Z}\in\mathbb{R} ^ {W\times H\times 1}\)，其 Loss 为： <span class="math display">\[\mathcal{L _ z} = \frac{1}{N}\sum _ {k=1} ^ N\left\vert\hat{Z} _ {p ^ {(k)}}-z ^ {(k)}\right\vert\tag{4}\]</span></p><h3 id="d-object-size-head">1.3. 3D Object Size Head</h3><p>　　尺寸预测值 \(\hat{S}\in\mathbb{R} ^ {W\times H\times 3}\)，其 Loss 为： <span class="math display">\[\mathcal{L} _ {size} = \frac{1}{N}\sum _ {k=1} ^ N\left\vert\hat{S} _ {p ^ {(k)}}-s ^ {(k)}\right\vert\tag{5}\]</span> 其中 \(s ^ {(k)} = (w ^ {(k)},l ^ {(k)}, h ^ {(k)})\)。</p><h3 id="orientation-head">1.4. Orientation Head</h3><p>　　与传统的一样，将角度预测分解为 bin 分类＋ offset 回归两个任务。具体的，分成两个 bin：\(\Psi _ 1 =[-\frac{7\pi}{6}, \frac{\pi}{6}]\)；\(\Psi _ 2 =[-\frac{\pi}{6}, \frac{7\pi}{6}]\)。对于每个 bin，softmax 分类 \(\hat{\mu} _ i ^ {(k)}\in\mathbb{R} ^ 2\)，与 bin 中心夹角 \(\gamma _ i\) 的 sin/cos 值 \(\hat{v} _ i ^ {(k)}\)。Loss 为： <span class="math display">\[\mathcal{L} _ {ori} = \frac{1}{N}\sum _ {k=1}^N\sum _ {i=1}^2\left(\mathrm{softmax}\left(\hat{\mu} _ i ^ {(k)},\eta _ i ^ {(k)}\right)+\eta _ i ^ {(k)}\left\vert\hat{v} _ i ^ {(k)}-v _ i ^ {(k)}\right\vert\right)\tag{6}\]</span> 其中当 \(\theta ^ {(k)}\in\Psi _ i\) 时，\(\eta _ i ^ {(k)} = \mathbb{1}\)，\(v _ i^ {(k)}=\left(\mathrm{sin}(\theta ^ {(k)}-\gamma _ i), \mathrm{cos}(\theta ^ {(k)}-\gamma _ i)\right)\)。由此，预测的角度可通过如下方式解码： <span class="math display">\[\hat{\theta} ^ {(k)}=\mathrm{arctan2}\left(\hat{v} _ {j,1} ^ {(k)},\hat{v} _ {j,2} ^ {(k)}\right)+\gamma _ j\tag{7}\]</span> 　　<strong>因为是 Anchor Free 的方式，所以如果按照传统的方式， bin 数量较大，那么最后输出的 map 所占的内存也会相当大，所以这里只采用了两个 bin</strong>。这么做有很大好处，比如量化时数值的稳定性，所以在工程应用中非常值得借鉴思考。</p><h2 id="experiments">2. Experiments</h2><p><img src="/paper-reading-AFDet/res1.png" width="70%" height="70%" title="图 2. res1"> <img src="/paper-reading-AFDet/res2.png" width="70%" height="70%" title="图 3. res2"> 　　如图 2. 所示，AFDet 在同等计算量下，基本能达到 <a href="/paperreading-PointPillars/" title="PointPillars">PointPillars</a> 水平。图 3. 则对比了几种 Anchor-Based 方法，效果也较好。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Ge, Runzhou, et al. &quot;Afdet: Anchor free one stage 3d object detection.&quot; arXiv preprint arXiv:2006.12671 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　点云目标检测方法已趋于完善，为了能在嵌入式系统上高效运行点云目标检测算法，地平线提出了 AFDet &lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt;，该文章发表在 CVPR2020 Workshop 上，算是很工程化的一个工作了，对工程
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;EPNet&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-EPNet/"/>
    <id>https://leijiezhang001.github.io/paper-reading-EPNet/</id>
    <published>2020-08-24T01:38:40.000Z</published>
    <updated>2020-08-28T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　在大多数场景下，融合激光雷达与图像数据能有效提升各种深度学习任务性能。本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种图像数据与激光雷达的前融合框架，并且考虑到分类分数与定位置信度的不一致性，提出了一种约束两者一致性的 Loss。</p><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-EPNet/framework.png" width="90%" height="90%" title="图 1. Framework"> 　　如图 1. 所示，EPNet 主要由 Image Stream，Geometric Stream，LI-Fusion Module，及 Detect Head 组成。<br>　　Image Stream 中，提取不同尺度的图像特征 \(F _ i(i = 1,2,3,4)\)，然后经过 2D Transposed Convolution 将不同尺度的特征变换到图像分辨率，得到特征 \(F _ U\)。<br>　　Geometric Stream 用 PointNet++(<a href="/PointNet-系列论文详读/" title="PointNet-系列论文详读">PointNet-系列论文详读</a>) 作特征提取，对应的 SA，FP 特征为 \(S _ i,P _ i(i =1,2,3,4)\)。\(S _ i,F _ i\) 通过 LI-Fusion 模块进行深度融合，此外 \(F _ U\) 与 \( P _ 4\) 也通过 LI-Fusion 进行融合。</p><h2 id="li-fusion-module">2. LI-Fusion Module</h2><p><img src="/paper-reading-EPNet/LI.png" width="70%" height="70%" title="图 2. LI-Fusion"> 　　LiDAR-guided Image Fusion Module 是图像点云两个数据流融合的核心模块。如图 2. 所示，LI-Fusion 由 Point-wise Correspondence Generation 和 LiDAR-guided fusion 两部分组成。Point-wise Correspondence Generation 又由 Grid Generator 和 Image Sampler 实现，对于点云中的点 \(p(x,y,z)\)，可得到不同尺度图像上的像素点 \(p'(x',y')\)： <span class="math display">\[p&#39;=M\times p\tag{1}\]</span> 其中 \(M\in\mathbb{R}^ {3\times 4}\)。\(p'\) 可能不是正好位于图像坐标像素点上，所以用双线性插值的方法取邻近像素点的特征值： <span class="math display">\[V^{(p)}=\mathcal{K}\left(F^{\mathcal{N}(p&#39;)}\right)\tag{2}\]</span> 其中 \(V^{(p)}\) 表示点 \(p\) 对应的图像点特征，\(\mathcal{K}\) 表示双线性插值，\(\left(F^{\mathcal{N}(p')}\right)\) 表示图像 \(p'\) 邻近点的特征。<br>　　LiDAR-guided Fusion 考虑到不能直接将点的图像特征与点特征进行串联融合，因为图像特征容易受光照，遮挡等因素影响，所以通过点云对图像点特征进行重要性权重融合。如图 2. 所示，重要性权重设计为： <span class="math display">\[\mathbf{w}=\sigma\left(\mathcal{W}\;\mathrm{tanh}(\mathcal{U} F _ P+\mathcal{V}F _ I)\right)\tag{3}\]</span> 其中 \(\mathcal{W,U,V}\) 为 MLP 网络，\(\sigma\) 为 sigmoid 归一化函数。<br>　　最终的融合特征为： <span class="math display">\[F _ {LI}=\mathrm{Concate}(F _ P,\mathbf{w}F _ I)\tag{4}\]</span></p><h2 id="consistency-enforcing-loss">2. Consistency Enforcing Loss</h2><p>　　NMS 操作时，一般用分类的分数，但是分类分数与定位置信度是不一致的。本文提出 Consistency Enforcing Loss，将定位与分类的分数监督成一致： <span class="math display">\[L _ {ce}=-log\left(c\times\frac{Area(D\cap G)}{Area(D\cup G)}\right)\tag{5}\]</span> 其中 \(D,G\) 分别为预测框与真值框，\(c\) 为分类分数，该 Loss 鼓励定位准的框分类分数越高。<br>　　这与 IoU Loss 作用相似！</p><h2 id="ablation-study">3. Ablation Study</h2><p><img src="/paper-reading-EPNet/ablation.png" width="90%" height="90%" title="图 3. Ablation Study"> 　　如图 3. 所示，LI-Fusion 和 CE Loss 对检测性能提升还是比较明显的。此外，本文还对比了三种 Fusion 方式，另外两种为：SC(simple concatenation)，将原始图像像素值串联到对应的点云原始数据中，没有 Image Stream；SS(single scale)，只用最后一层的图像点云特征作融合。<br>　　实验表明，SC 性能反而下降，SS 有所提升，但是 Multi-scale 的性能最好。结论就是，在一个尺度下，相对靠后的前融合可能比相对靠前的前融合效果更好(类比 <a href="/PointPainting/" title="PointPainting">PointPainting</a>，其虽然是前融合，但是直接提取的是图像的语义分割结果，所以相对靠后，效果也好)，当然多尺度的效果会是最好的。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Huang, Tengteng, et al. &quot;EPNet: Enhancing Point Features with Image Semantics for 3D Object Detection.&quot; arXiv preprint arXiv:2007.08856 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　在大多数场景下，融合激光雷达与图像数据能有效提升各种深度学习任务性能。本文&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt;提出了一种图像数据与激光雷达的前融合框架，并且考虑到分类分数与定位置信度的不一致性，提出了一种约束两者一致性的 L
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;PV-RCNN&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-PV-RCNN/"/>
    <id>https://leijiezhang001.github.io/paper-reading-PV-RCNN/</id>
    <published>2020-08-19T01:24:10.000Z</published>
    <updated>2020-08-20T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　PV-RCNN<a href="#1" id="1ref"><sup>[1]</sup></a> 目前在 Waymo 数据集上排名第二，性能还是比较强悍的，顺便也看了下港中文多媒体实验室开源的 OpenPCDet<a href="#2" id="2ref"><sup>[2]</sup></a> 代码，收获还是蛮多，与图像点云通用的 mmdetection3d<a href="#3" id="3ref"><sup>[3]</sup></a> 各有优劣吧。虽然 PV-RCNN 对于实际应用还是略显复杂，以及超参数较麻烦，但是其相关思想还是非常值得借鉴。本文主要关注其 Point + Vexol 特征提取并融合的方式。</p><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-PV-RCNN/framework.png" width="90%" height="90%" title="图 1. Framework"> 　　如图 1. 所示，PV-RCNN 首先将原始点云体素化，然后用 3D Sparse Convolution(<a href="/Rethinking-of-Sparse-3D-Convolution/" title="Rethinking of Sparse 3D Convolution">Rethinking of Sparse 3D Convolution</a>) 作 Voxel-level 的特征提取，并预测俯视图下的目标 ROI Proposal；另一方面，在原始点云中用 FPS 采样出特定数量的 key-point，然后通过 Voxel Set Abstraction 模块，将提取到的多尺度的 Voxel-level 特征融合到 key-point 特征；最后用 ROI-Grid 模块将 key-point 特征融合到 ROI Grid-Point 中，作进一步的 3D 目标框属性精细化预测。<br>　　由此不仅利用了 Voxel-level 3D Sparse Convolution 的高效性，还利用了 Point-based 模型对局部信息提取更加精细有效的特性。总体上，PV-RCNN 框架中特征提取操作由两大块组成：1. Voxel-to-Keypoint Scene Encoding；2. Point-to-Grid RoI Feature Abstraction。</p><h2 id="voxel-to-keypoint-scene-encoding">2. Voxel-to-Keypoint Scene Encoding</h2><p>　　该模块的作用是将提取的特征用特定数量的 Keypoint 来表示。所以有 Keypoints Sampling，Voxel Set Abstraction Module，Extended VSA Module，Predicted Keypoint Weighting 等组成。</p><h3 id="keypoints-sampling">2.1. Keypoints Sampling</h3><p>　　因为期望采样的 Keypoints 能完全覆盖整个场景，所以采用 Furthest-Point-Sampling(FPS) 来采样 \(n\) 个点 \(\mathcal{K}=\{p _ 1,...,p _ n\}\)。对于 KITTI 数据集，取 \(n=2048\)，对于 Waymo 数据集，取 \(n=4096\)。</p><h3 id="voxel-set-abstraction-module">2.2. Voxel Set Abstraction Module</h3><p>　　VSA 模块将经过 3D Sparse Convolution 得到的多尺度的 Voxel 特征编码为 Keypoints 表达形式，与 <a href="/PointNet-系列论文详读/" title="PointNet-系列论文详读">PointNet-系列论文详读</a> 类似，只不过这里点周围不是点，而是 Voxel。<br>　　具体的，设尺度 \(k\) 的 3D voxel 特征集合为 \(\mathcal{F}^ {(l _ k)}=\{f _ 1 ^ {(l _ k)},...,f _ {N _ k}^{(l _ k)}\}\)，对应的 Voxel 3D 坐标为 \(\mathcal{V}^ {(l _ k)}=\{v _ 1 ^ {(l _ k)},...,v _ {N _ k}^{(l _ k)}\}\)，其中 \(N _ k\) 为非零 Voxel 数量。对于 keypoint \(p _ i\)，在半径 \(r _ k\) 内找到所有 voxel 的特征向量： <span class="math display">\[S _ i ^ {(l _ k)} = \left\{\left[f _ j^{(l _ k)};v _ j^{(l _ k)}-p _ i\right] ^ T \left\vert\begin{array}{l}\Vert v _ j^{(l _ k)}-p _ i\Vert ^ 2 &lt; r _ k,\\\forall v _ j^{(l _ k)} \in \mathcal{V} ^ {(l _ k)},\\\forall f _ j ^ {(l _ k)} \in \mathcal{F} ^ {(l _ k)}\end{array}\right.\right\}\tag{1}\]</span> 其中 \(v _ j^{(l _ k)}-p _ i\) 为对应的 Voxel 与该点 \(p _ i\) 的相对位置，与特征向量串联得到该 Voxel 在该点的投影特征。由此用 PointNet 方式可得到该 Keypoint 融合领域内 Voxel 特征集 \(S _ i ^ {(l _ k)}\) 后的特征： <span class="math display">\[f _ i ^ {(pv _ k)}=\mathrm{max}\left\{G\left(\mathcal{M}(S _ i^{(l _ k)})\right)\right\}\tag{2}\]</span> 其中 \(\mathcal{M}\) 为随机采样 Voxel 的操作，目的是为了减少计算量；\(G\) 为 MLP 网络。<br>　　本文采用了 4 个尺度的 Voxel 特征，每个尺度的领域半径 \(r _ k\) 根据感受野而变化。最终得到的多尺度的语义的 Keypoint 特征为： <span class="math display">\[f _ i^{(pv)} = \left[f _ i^{(pv _ 1)},f _ i^{(pv _ 2)},f _ i^{(pv _ 3)},f _ i^{(pv _ 4)}\right],\;\mathrm{for}\; i = 1,...,n\tag{3}\]</span></p><h3 id="extended-vsa-module">2.3. Extended VSA Module</h3><p>　　此外，Keypoint 还利用了原始点云的特征以及经过 3D Sparse Convolution 和 ToBEV 后的 2D BEV 特征。Keypoint 通过双线性插值的方式从 BEV 特征层中计算对应空间位置的特征。综上，Keypoint 特征为： <span class="math display">\[f _ i^{(p)} = \left[f _ i^{(pv)},f _ i^{(raw)},f _ i^{(bev)}\right],\;\mathrm{for}\; i = 1,...,n\tag{4}\]</span></p><h3 id="predicted-keypoint-weighting">2.4. Predicted Keypoint Weighting</h3><p><img src="/paper-reading-PV-RCNN/weight.png" width="50%" height="50%" title="图 2. PKW"> 　　通过 FPS 采样得到的 Keypoint 也包含了大量的背景点，所以需要弱化背景点的特征，强化前景点特征，以便之后前景目标框属性的精细化估计。如图 2. 所示，训练阶段，对 Keypoint 进行点级别前景背景分类，标签可由目标框内点自动生成；测试阶段，直接预测点的类别，然后作点特征的权重化整合： <span class="math display">\[\tilde{f _ i} ^ {(p)} = \mathcal{A}(f _ i^{(p)})\cdot f _ i^{(p)}\tag{5}\]</span> 其中 \(\mathcal{A}(\cdot)\) 为 MLP 网络。</p><h2 id="point-to-grid-roi-feature-abstraction">3. Point-to-Grid RoI Feature Abstraction</h2><p><img src="/paper-reading-PV-RCNN/roi.png" width="50%" height="50%" title="图 3. RoI-grid Pooling"> 　　得到了 Keypoint 特征 \(\mathcal{\tilde{F}}=\{\tilde{f} _ 1^{(p)},...,\tilde{f} _ n^{(p)}\}\) 以及俯视图下的 3D proposal ROI 后，可进一步提取 ROI 特征，以作 3D 目标框属性的精细化估计。如图 3. 所示，类似 Voxel-to-Point 的 Voxel Set Abstraction 模块，本文提出了 Point-to-Grid 的 Set Abstraction，称之为 ROI-Grid Pooling。具体的，对每个 ROI Proposal，采样 \(6\times 6\times 6\) 个栅格点：\(\mathcal{G}=\{g _ 1,...,g _ {216}\}\)。Set Abstraction 操作将 Keypoint 的特征映射到栅格点处。类似 VSA，首先在 \(\tilde{r}\) 半径内查找栅格点的周围 Keypoint： <span class="math display">\[\tilde{\psi} = \left\{\left[\tilde{f} _ j^{(p)};p _ j-g _ i\right] ^ T \left\vert\begin{array}{l}\Vert p _ j-g _ i\Vert ^ 2 &lt; \tilde{r},\\\forall p _ j\in \mathcal{K},\\\forall \tilde{f} _ j ^ {(p)} \in \mathcal{\tilde{F}}\end{array}\right.\right\}\tag{6}\]</span> 类似的，再通过 PointNet 得到栅格点 \(g _ i\) 的特征： <span class="math display">\[\tilde{f} _ i ^ {(g)}=\mathrm{max}\left\{G\left(\mathcal{M}(\tilde{\psi})\right)\right\}\tag{7}\]</span> 　　由此得到所有 ROI 固定长度的特征向量，进而可在 ROI Proposal 的基础上，作最后的尺寸，角度，位置等属性的精细化估计，这里不做展开。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Shi, Shaoshuai, et al. &quot;Pv-rcnn: Point-voxel feature set abstraction for 3d object detection.&quot; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.<br><a id="2" href="#2ref">[2]</a> https://github.com/open-mmlab/OpenPCDet<br><a id="3" href="#3ref">[3]</a> https://github.com/open-mmlab/mmdetection3d</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　PV-RCNN&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt; 目前在 Waymo 数据集上排名第二，性能还是比较强悍的，顺便也看了下港中文多媒体实验室开源的 OpenPCDet&lt;a href=&quot;#2&quot; id=&quot;2ref&quot;&gt;&lt;sup&gt;[
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;MotionNet&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-MotionNet/"/>
    <id>https://leijiezhang001.github.io/paper-reading-MotionNet/</id>
    <published>2020-08-17T01:23:18.000Z</published>
    <updated>2020-08-18T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　基于点云的 3D 感知一般通过 3D 目标框实现，但是如果直接网络出 3D 目标框属性，那么召回率很难达到非常高的水平(所以之前提到检测的任务可以分解为分割+后处理聚类来作，这样召回率会比较高)。本文提出的 MotionNet<a href="#1" id="1ref"><sup>[1]</sup></a> 用于检测俯视图下每个栅格的类别及轨迹，可作为检测的辅助，能召回更小的目标，以及没有标注的动态障碍物目标。这种栅格级别或点级别的目标探测能力相比直接出目标框的优势有：</p><ol type="1"><li>目标框形式一般依赖于目标框区域的特征，不同目标类别之间的特征很难泛化，所以无法检测未见过的类别；</li><li>目标框形式一般会作 NMS 等处理去掉不确定性较大的框，而栅格级别的会保留；</li></ol><p><img src="/paper-reading-MotionNet/compare.png" width="40%" height="40%" title="图 1. Detection VS. Motion Prediction"> 　　如图 1. 所示，对于轮椅这种非正常类别(或预定义类别)的目标，检测任务可能会失效，此时 MotionNet 则会根据时序信息输出该区域的目标速度(未来运动轨迹)，这可作为 Motion Planning 阶段的另一重要线索。</p><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-MotionNet/framework.png" width="80%" height="80%" title="图 2. Framework"> 　　如图 2. 所示，MotionNet 输入为连续帧点云在当前坐标系下的俯视图表示，然后经过 Spatio-temporal Pyramid 网络作特征提取，最后三个分支网络输出三个信息：</p><ul><li>Cell Classification，每个 cell 的类别；</li><li>Motion Prediction，每个 cell 的未来轨迹；</li><li>State Estimation，每个 cell 是否静止的判断；</li></ul><h2 id="spatio-temporal-pyramid-network">2. Spatio-temporal Pyramid Network</h2><p><img src="/paper-reading-MotionNet/stpn.png" width="50%" height="50%" title="图 3. STPN"> 　　 将时序点云组织成多层的俯视图表达后，面临两个问题：1. 如何整合时序信息；2. 如何提取多尺度的空间及时序特征。本文设计了 STPN 网络，如图 3. 所示，STC 模块由 2D Convolution 以及 \(k\times 1\times 1\) 的 3D Convolution (本质上退化成 1D Convolution)组成；此外设计了金字塔式的特征提取结构，最终输出 \(1\times C\times H\times W\) 大小的特征图。相比于直接 3D 卷积形式，这种方式 2D + 1D 卷积方式极大提高了网络计算效率(很多地方用到这种方式，如 <a href="/paperreading-Fast-and-Furious/" title="FaF">FaF</a>)。</p><h2 id="output-heads">3. Output Heads</h2><p>　　三个分支输出的细节为：</p><ol type="1"><li>Cell Classification，输出尺寸为 \(H\times W\times C\)，其中 \(C\) 为类别；</li><li>Motion Prediction，输出尺寸为 \(N\times H\times W\times 2\)，表示时间 \(\tau\in (t,t+N)\) 内 cell 的位置 \(\{X ^ {(\tau)}\} _ {\tau =t} ^ {t + N}\)，其中 \(X ^ {(\tau)}\in\mathbb{R} ^ {H\times W\times 2}\) 只估计平面 2D 的位置。</li><li>State Estimation，输出尺寸为 \(H\times W\)，表示 cell 静止的概率。</li></ol><p>　　直接对静止 Cell 的 Motion Prediction 回归，会引入运动的微小跳变。这里采用两种策略来抑制这种跳变：1. 根据类别分支，如果是背景的类别，则将其 Motion 置 0；2. 根据静止判断分支，如果是静止的，则将其 Motion 也置为 0。这样就能较好的解决静态 cell 出速度轨迹的情况。</p><h2 id="loss-function">4. Loss Function</h2><p>　　Classification 和 State Estimation 用 Cross-Entropy Loss，Motion Prediction 用 Smooth L1 Loss。此外为了保证空域与时域的一致性，引入另外三种 Loss：</p><ul><li><strong>Spatial Consistency Loss</strong><br>属于同一物体的 Cell 的 Motion 应该是一致的(这里其实不太准确，考虑到转向情况，目标区域内 Cell 轨迹其实是不一样的，所以一般有两种思路，一种认为都一样，即每个 Cell 都建模成目标的运动；另一种则基于刚体假设，作类似 Flow 的建模，稍复杂些)。由此设计空间一致性损失函数： <span class="math display">\[L _ s = \sum _ {k}\sum _ {(i,j),(i&#39;,j&#39;)\in o _ k}\left\Vert X _ {i,j}^{(\tau)}-X _ {i&#39;,j&#39;} ^ {(\tau)}\right\Vert \tag{1}\]</span> 其中 \(||\cdot||\) 为 Smooth L1 Loss，\(o _ k\) 为第 \(k\) 个目标，\(X _ {i,j} ^ {(\tau)}\in\mathbb{R} ^ 2\) 为时间 \(\tau\) 时 Cell \((i,j)\) 的 motion。为了减少计算量，这里只是采样一些相邻的 \(X _ {i,j} ^ {(\tau)},X _ {i',j'} ^ {(\tau)}\) 匹配对。</li><li><strong>Foreground Temporal Consistency Loss</strong><br>类似的，属于同一物体的 Motion 在时域上也应该是一致的，所以设计损失函数： <span class="math display">\[L _ {ft} = \sum _ k\left\Vert X _ {o _ k} ^ {(\tau)} - X _ {o _ k} ^ {(\tau+\Delta t)}\right\Vert\tag{2}\]</span> 其中 \(X _ {o _ k} ^ {(\tau)}\in\mathbb{R} ^ 2\) 为第 \(k\) 个目标的 Motion，计算方式为 \(X _ {o _ k} ^ {(\tau)}=\sum _ {(i,j)\in o _ k}X _ {i,j} ^ {(\tau)}/M\)，其中 \(M\) 为目标中 Cell 的个数。</li><li><strong>Background Temporal Consistency Loss</strong><br>对静止的背景区域，其时域上 Motion 应该也是一致的(均为 0)。设计损失函数： <span class="math display">\[L _ {bt} = \sum _ {(i,j)\;\in\; X ^ {(\tau)}\;\cap\; T\left(\tilde{X} ^ {(\tau-\Delta t)}\right)}\left\Vert X _ {i,j} ^ {(\tau)}-T _ {i,j}\left(\tilde{X} ^ {(\tau-\Delta t)}\right)\right\Vert\tag{3}\]</span> 其中 \(T\in SE(3)\) 是 \(\tau-\Delta t\) 到 \(\tau\) 的位姿变换。将 \(\tilde{X} ^ {\tau-\Delta t}\) 变换到当前时刻后，与 \(X ^ {(\tau)}\) 会有一定的重合，将重合部分的背景区域的 Motion 约束为一致。有点 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 的味道。</li></ul><p>综上，所有的 Loss 为： <span class="math display">\[ L = L _ {cls} + L _ {motion} + L _ {state} + \alpha L _ s+ \beta L _ {ft} + \gamma L _ {bt} \tag{4}\]</span></p><h2 id="reference">5. Reference</h2><p><a id="1" href="#1ref">[1]</a> Wu, Pengxiang, Siheng Chen, and Dimitris N. Metaxas. &quot;MotionNet: Joint Perception and Motion Prediction for Autonomous Driving Based on Bird's Eye View Maps.&quot; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　基于点云的 3D 感知一般通过 3D 目标框实现，但是如果直接网络出 3D 目标框属性，那么召回率很难达到非常高的水平(所以之前提到检测的任务可以分解为分割+后处理聚类来作，这样召回率会比较高)。本文提出的 MotionNet&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;
      
    
    </summary>
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/"/>
    
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Deep Learning&quot;" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Cylinder3D&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-Cylinder3D/"/>
    <id>https://leijiezhang001.github.io/paper-reading-Cylinder3D/</id>
    <published>2020-08-12T03:43:33.000Z</published>
    <updated>2020-08-17T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　Voxel-based 点云分割/检测等任务中，点云的投影表示方法有三种：</p><ul><li>Spherical</li><li>Bird-eye View</li><li>Cylinder</li></ul><p>其中 Spherical 球坐标投影代表为 <a href="/paper-reading-RandLA-Net/" title="RandLA-Net">RandLA-Net</a>；Bird-eye View 则是目前主流的方法。有关 Bird-eye View 点云处理的优劣已经说了很多了，这里不再赘述。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 介绍一种 Cylinder 柱状投影的点云处理方式，类似 <a href="/paper-reading-Pillar-based-Object-Detection/" title="Pillar-based Object Detection">Pillar-based Object Detection</a>，也可以认为是 <a href="/paper-reading-PolarNet/" title="PolarNet">PolarNet</a> 的 3D 版本。 <img src="/paper-reading-Cylinder3D/vs.png" width="70%" height="70%" title="图 1. Comparison"> 　　<a href="/paper-reading-Pillar-based-Object-Detection/" title="Pillar-based Object Detection">Pillar-based Object Detection</a> 中详细说明了 Cylinder 投影比 Spherical 投影的优势，这里不做赘述，如图 1. 所示，相比 Spherical 投影，Cylinder 投影效果提升很明显。</p><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-Cylinder3D/framework.png" width="80%" height="80%" title="图 2. Framework"> 　　如图 2. Cylinder3D 由 3D 柱坐标投影和 3D U-Net 特征提取两部分组成。框架比较简单，网络结构主要由 DownSample，UpSample，Asymmetry Residual Block，Dimension-Decomposition based Context Modeling 四种组件构成。</p><h2 id="cylinder-partition">2. Cylinder Partition</h2><p><img src="/paper-reading-Cylinder3D/coord.png" width="80%" height="80%" title="图 3. Cylinder Partition"> 　　如图 3. 所示，将笛卡尔坐标系下的点云 \((x,y,z)\) 转换到柱坐标系下 \((\rho,\theta,z)\)。对于每个扇形 Voxel，作 PointNet 特征提取，最终得到 3D Cylinder 点云特征表示 \(\mathbb{R}\in C\times H\times W\times L\)。</p><h2 id="network">3. Network</h2><p><img src="/paper-reading-Cylinder3D/block.png" width="70%" height="70%" title="图 4. A & DDCM"></p><h3 id="asymmetry-residual-block">3.1. Asymmetry Residual Block</h3><p>　　如图 4. 所示，Asymmetry Residual Block 将 \(3\times 3\times 3\) 卷积拆分成 \(1\times 3\times 3\) 和 \(3\times 1\times 3\) 两种，这样作有两个好处：</p><ul><li>由于待检测的目标都接近于长方体，这种卷积形式更有利于提取长方体样式的特征；</li><li>减少 33% 的计算量，类似 Depth-wise Convolution；</li></ul><p>该模块作为 3D 卷积的基本模块，嵌入在下采样前，以及上采样后。</p><h3 id="dimension-decomposition-based-context-modeling">3.2. Dimension-Decomposition based Context Modeling</h3><p>　　由于 3D 空间的特征表达是 high-rank 的，所以利用矩阵分解的思想，将其用 height，width，depth 三维的 low-rank 向量来权重化表达，由此设计如图 4. 中的 DDCM 模块。该模块将三个方向的特征计算各自的权重，然后与原始特征作权重化整合。输出的特征用于最终的预测，预测输出是 Voxel-based，维度为 \(Class\times H\times W\times L\)。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Zhou, Hui, et al. &quot;Cylinder3D: An Effective 3D Framework for Driving-scene LiDAR Semantic Segmentation.&quot; arXiv preprint arXiv:2008.01550 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Voxel-based 点云分割/检测等任务中，点云的投影表示方法有三种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spherical&lt;/li&gt;
&lt;li&gt;Bird-eye View&lt;/li&gt;
&lt;li&gt;Cylinder&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中 Spherical 球坐标投影代
      
    
    </summary>
    
      <category term="Semantic Segmentation" scheme="https://leijiezhang001.github.io/categories/Semantic-Segmentation/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/tags/Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;RadarNet&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-RadarNet/"/>
    <id>https://leijiezhang001.github.io/paper-reading-RadarNet/</id>
    <published>2020-08-07T01:25:41.000Z</published>
    <updated>2020-08-12T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　Radar 相比 LiDAR/Camera，对天气影响鲁棒性较强，而且能直接测量目标速度，所以是多传感器融合感知里比较重要的一个输入。其在 ADAS 领域应用已经较为广泛。但是 Radar 的测量噪声较大，这给有效的多传感器融合带来了难度。本文提出了 RadarNet<a href="#1" id="1ref"><sup>[1]</sup></a>，利用 Radar 的几何数据以及动态数据，同时做特征级别的前融合以及注意力机制下的后融合，达到了较好的效果。</p><h2 id="comparison-between-lidar-and-radar">1. Comparison between LiDAR and Radar</h2><p><img src="/paper-reading-RadarNet/compare.png" width="90%" height="90%" title="图 1. Comparison"> 　　LiDAR 可分为三种类型：Spinning，Solid State，Flash。目前主要采用的是 Spinning 旋转式的激光雷达。激光雷达的缺点有：</p><ul><li>对雨雾雪，车尾气等比较敏感；</li><li>对玻璃等物体没有反射;</li><li>点云密度随着距离增加而下降，远距离探测能力较弱。</li></ul><p>毫米波雷达则能克服以上缺点，并且能直接测量速度；但是缺点也比较明显：</p><ul><li>分辨率低，对小目标探测能力较弱；</li><li>误检较多；</li><li>测量的速度只是径向速度。</li></ul><p>毫米波雷达可输出三种形式的数据：1. 原始点运数据；2. 经过 DBSCAN 等聚类算法获得的聚类点集数据；3. 对点集作跟踪的数据。三种数据越来越高层，但是噪音越来越大。本文考虑输出点集的数据，设探测到的点集目标为 \(Q = (q, v _ {||},m,t)\)，其中 \(q = (x,y)\) 是俯视图下的位置，\(v _ {||}\) 是径向速度，\(m\) 代表目标是否运动，\(t\) 则为时间戳。我们需要进一步估计出目标的 2D 速度，由此在毫米波雷达的辅助下，能获得更长的检测距离，以及更准确的速度。 <img src="/paper-reading-RadarNet/fusion.png" width="90%" height="90%" title="图 2. Fusion"> 　　激光雷达数据与毫米波雷达数据融合示意图如图 2. 所示。</p><h2 id="radarnet">2. RadarNet</h2><p><img src="/paper-reading-RadarNet/framework.png" width="90%" height="90%" title="图 3. Framework"> 　　如图 3. 所示，RadarNet 主要由 Voxel-Based Early Fusion，Detection Network 以及 Attention-Based Late Fusion 来做融合检测。前融合将各传感器数据通过俯视图形式进行表示并融合；后融合则通过基于注意力的数据关联及整合机制来对目标速度进行精细估计。检测网络是在传统的分类＋回归基础上，多了速度预测的分支。具体的，所有回归的预测量为 \((x-p _ x,y - p _ y,w,l,\mathrm{cos}(\theta),\mathrm{sin}(\theta),m,v _ x, v _ y)\)，其中 \(p _ x,p _ y\) 为体素/栅格的中心点，\(m\) 为目标是运动的概率，如果 \(m &lt; 0.5\)，则将速度置为 0。</p><h3 id="early-fusion">2.1. Early Fusion</h3><p>　　对于激光雷达数据，类似 <a href="/paperreading-Fast-and-Furious/" title="FAF">FAF</a>，将时序多帧(0.5s)的点云数据在本车坐标系下打成俯视图体素表示，然后在通道维度进行串联。如果体素内没有点，那么该体素值为 0；如果体素内有点 \(\{(x _ i,y _ i,z _ i),i=1,...,N\}\)，那么体素值为 \(\sum _ i\left( 1- \frac{|x _ i-a|}{dx / 2}\right)\left( 1- \frac{|y _ i-b|}{dy / 2}\right)\left( 1- \frac{|z _ i-c|}{dz / 2}\right)\)，其中 \((a,b,c)\) 为体素中心坐标，\(dx,dy,dz\) 为体素尺寸。<br>　　对于毫米波雷达数据，将其转到激光雷达坐标系后，也进行 BEV 时序串联，并将每一帧中不同线束的数据在 BEV 下体素化，然后串联。具体的，如果体素(本文丢掉了高度信息，所以退化为栅格)中没有毫米波探测到的目标，那么置为 0，如果探测到动态目标，则置为 1，如果探测到静态目标，则置为 -1。<br>　　分别得到激光雷达与毫米波雷达的 BEV 表示后，将二者在通道维度串联起来，就完成了前融合。</p><h3 id="late-fusion">2.2. Late Fusion</h3><p>　　<strong>前融合关注毫米波雷达探测到的目标的位置和密度，后融合则使用毫米波雷达探测到的目标径向速度信息</strong>。检测网络输出的目标状态为 \(D = (c,x,y,w,l,\theta,\mathbf{v})\)，毫米波雷达输出的目标状态为 \(Q=(q,v _ {||},m,t)\)。两者的目标级别的后融合通过 Association 和 Aggregation 两步骤组成。 <img src="/paper-reading-RadarNet/late-fusion.png" width="90%" height="90%" title="图 4. Late-Fusion"> 　　本文将 Association 和 Aggregation 用 End-to-End 的网络来处理。如图 4. 所示，首先进行检测目标与毫米波目标的成对特征提取，然后经过 MLP/Softmax 作匹配分数估计，最后根据分数作速度的加权优化。</p><h4 id="pairwise-detection-radar-association">2.2.1. Pairwise Detection-Radar Association</h4><p>　　定义 Pairwise Feature 为： <span class="math display">\[\begin{align}f(D,Q) &amp;= \left(f ^ {det}(D),f ^ {det-radar}(D,Q)\right) \tag{1}\\f ^ {det}(D) &amp;= \left(w,l,||\mathbf{v}||,\frac{v _ x}{||\mathbf{v}||},\frac{v _ y}{||\mathbf{v}||},\mathrm{cos}(\gamma)\right) \tag{2}\\f ^ {det-radar}(D,Q) &amp;= \left(dx,dy,dt,v ^ {bp}\right) \tag{3}\\v ^ {bp} &amp;= \mathrm{min}\left(50,\frac{v _ {||}}{\mathrm{cos}(\phi)}\right) \tag{4}\end{align}\]</span> 其中 \((\cdot,\cdot)\) 是 Concatenation 操作；\(\gamma\) 是网络检测 \(D\) 的运动方向与径向方向的夹角；\(\phi\) 是 \(D\) 运动方向与雷达探测目标 \(Q\) 的径向方向的夹角；\(v ^ {bp}\) 是径向速度反投影到运动方向(朝向)的速度值；\((dx,dy,dt)\) 为俯视图下 \(D,Q\) 的相对位置和时间。由此，通过 MLP 学习匹配分数： <span class="math display">\[s _ {i,j}=\mathrm{MLP} _ {match}\left(f(D _ i,Q _ j)\right)\tag{5}\]</span></p><h4 id="velocity-aggregation">2.2.2. Velocity Aggregation</h4><p>　　根据 \(D,Q\) 间的匹配分数，优化<strong>目标的绝对速度值</strong>。为了解决没有匹配的情况，分数 Concate 1，然后计算归一化的匹配分数： <span class="math display">\[s _ i ^ {norm}=\mathrm{softmax}((1,s _ {i,:})) \tag{6}\]</span> 然后优化每个检测目标 \(i\) 的绝对速度值： <span class="math display">\[v _ i&#39; = s _ i ^ {norm}\cdot\begin{bmatrix}||\mathbf{v} _ i|| \\v _ {i,:} ^ {bp}\end{bmatrix}\tag{7}\]</span> 最终可得到目标的 2D 速度： <span class="math display">\[\mathbf{v}&#39; _ i = v&#39; _ i\cdot\left(\frac{v _ x}{||\mathbf{v}||},\frac{v _ y}{||\mathbf{v}||}\right) \tag{8}\]</span> 　　<strong>由此可知，该后融合优化的只是目标的绝对速度(毫米波雷达也没办法探测目标的朝向或运动方向)，目标的朝向准确度还是由检测网络决定。</strong></p><h2 id="experiments">3. Experiments</h2><p>　　根据式(2,3)提取的特征，作者设计了基于 Heuristic 的关联方法，毫米波雷达探测的目标与网络检测的目标关联的条件为： <span class="math display">\[\left\{\begin{array}{rl}\sqrt{(dx) ^ 2+(dy) ^ 2} &amp;&lt; 3m \\\gamma &amp;&lt; 40°\\||\mathbf{v}|| &amp;&gt; 1m/s\\v ^ {bp} &amp;&lt; 30m/s \\\end{array}\tag{9}\right.\]</span> 一旦关联上后，去毫米波雷达速度的中位数作目标速度的进一步优化。这种传统的 Heuristic 与本文的 Attention 方法对比如下，优势明显。 <img src="/paper-reading-RadarNet/exp.png" width="80%" height="80%" title="图 5. Heuristic VS. Attention"></p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Yang, Bin, et al. &quot;RadarNet: Exploiting Radar for Robust Perception of Dynamic Objects.&quot; arXiv preprint arXiv:2007.14366 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Radar 相比 LiDAR/Camera，对天气影响鲁棒性较强，而且能直接测量目标速度，所以是多传感器融合感知里比较重要的一个输入。其在 ADAS 领域应用已经较为广泛。但是 Radar 的测量噪声较大，这给有效的多传感器融合带来了难度。本文提出了 RadarNet&lt;
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Pillar-based Object Detection&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-Pillar-based-Object-Detection/"/>
    <id>https://leijiezhang001.github.io/paper-reading-Pillar-based-Object-Detection/</id>
    <published>2020-08-04T03:42:08.000Z</published>
    <updated>2020-08-06T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　<a href="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/" title="MVF">MVF</a> 在俯视图点云特征的基础上，融合了点云的前视图特征，由此解决点云在远处比较稀疏，以及行人等狭长型目标特征信息较少的问题。本文<a href="#1" id="1ref"><sup>[1]</sup></a>基于 <a href="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/" title="MVF">MVF</a> 作了三部分的改进：</p><ol type="1"><li>检测头改为 Anchor-Free 的形式，本文称之为 Pillar-based，其实就是图像中对应的像素点；</li><li>前视图用 Cylindrical View 代替 Spherical View，解决目标高度失真的问题；</li><li>两个视图的栅格特征反投影回点特征作融合时，采用双线性插值的形式，避免量化误差的影响。</li></ol><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-Pillar-based-Object-Detection/framework.png" width="90%" height="90%" title="图 1. Framework"> 　　如图 1. 所示，点云分别投影到 BEV(Brids-Eye)，CYV(Cylindrical) 视角，然后作类似图像卷积的 2D 卷积操作以提取特征，并将特征反投影回点作融合(与 <a href="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/" title="MVF">MVF</a> 一致)，接着将点云特征再次投影到 BEV 下，最后作 Anchor-Free 的分类与回归任务。<br>　　具体的，设 \(N\) 个点的点云 \(P=\{p _ i\} _ {i=0} ^ {N-1}\subseteq\mathbb{R} ^ 3\)，对应的特征向量为 \(F = \{f _ i\} _ {i=0} ^ {N-1}\subseteq\mathbb{R} ^ K\)。令 \(F _ V(p _ i)\) 返回点 \(p _ i\) 对应的栅格柱子 \(v _ j\) 的索引 \(j\)；\(F _ P(v _ j)\) 则返回栅格柱子 \(v _ j\) 对应的点集。对每个柱子进行特征整合，一般采用类似 PointNet(PN) 的方法： <span class="math display">\[f _ j ^{pillar} = \mathrm{PN} (\{f _ i|\forall p _ i\in F _ P(v _ j)\}) \tag{1}\]</span> pillar 级别的特征经过 CNN \(\phi\) 后得到进一步的 pillar 级别特征：\(\varphi=\phi(f ^ {pillar})\)。然后分别对 BEV，CYV 作 pillar-to-point 的特征投影变换： <span class="math display">\[f _ i^{point}=f _ j^{pillar}\;\mathrm{and}\;\varphi _ i^{point} = \varphi _ j^{pillar},\;\mathrm{where}\; j = F _ V(p _ i) \tag{2}\]</span> 最后的检测头是应用已经较为广泛的 Anchor-Free 形式。</p><h2 id="cylindrical-view">2. Cylindrical View</h2><p><img src="/paper-reading-Pillar-based-Object-Detection/proj.png" width="60%" height="60%" title="图 2. Projection"> 　　<a href="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/" title="MVF">MVF</a> 采用 Spherical 投影方式，对于点 \(p _ i=(x _ i, y _ i, z _ i)\)，其球坐标 \(\varphi _ i,\theta _ i,d _ i\) 为： <span class="math display">\[\left\{\begin{array}{l}\varphi _ i &amp;= \mathrm{arctan}\frac{y _ i}{x _ i}\\\theta _ i &amp;= \mathrm{arccos}\frac{z _ i}{d _ i}\\d _ i &amp;= \sqrt{x _ i ^ 2+y _ i ^ 2+z _ i^2}\end{array}\tag{3}\right.\]</span> 如图 2. 所示，球坐标系下目标高度的形变比较严重，本文采用柱坐标系，其柱坐标 \(\rho _ i,\varphi _ i,z _ i\) 表示为： <span class="math display">\[\left\{\begin{array}{l}\rho _ i &amp;=\sqrt{x _ i ^ 2+y _ i^2}\\\varphi _ i &amp;= \mathrm{arctan}\frac{y _ i}{x _ i}\\z _ i &amp;= z _ i\end{array}\tag{4}\right.\]</span> 　　在此视角下作 pillar-level 的特征提取，与俯视图视角一样，只不过作卷积的时候，是环状卷积。具体实现方式是，将柱坐标系下的 pillar 展开，然后边缘补对应展开处另一边的 pillar 值，最后作传统的 2D 卷积即可。</p><h2 id="pillar-based-prediction">3. Pillar-based Prediction</h2><p>　　这里所谓的 Pillar-based 预测，本质上就是图像中常说的 Anchor-Free 的 Pixel-Level 的检测方法。最后特征图上的每个点预测类别概率，以及 3D 框属性 \(\Delta _ x,\Delta _ y,\Delta _ z,\Delta _ l,\Delta _ w,\Delta _ h,\theta ^ p\)。这里不作展开。</p><h2 id="bilinear-interpolation">4. Bilinear Interpolation</h2><p><img src="/paper-reading-Pillar-based-Object-Detection/bilinear.png" width="60%" height="60%" title="图 3. Bilinear"> 　　将 Pillar-Level 提取的特征反投影到 Point-Level 的特征时，需要进行插值处理。如图 3. 所示，传统的方式是最近邻插值，这种方式会引入量化误差，使得点投影反投影后的空间坐标不一致，产生的影响是同一 Pillar 内的点特征都是一样了。本文采用双线性插值的方法，使得 Point-Pillar-Point 的空间坐标一致，这样保证了 Pillar 内点特征的原始精度。该思想还是非常有借鉴意义的，实验效果提升也比较明显。</p><h2 id="reference">5. Reference</h2><p><a id="1" href="#1ref">[1]</a> Wang, Yue, et al. &quot;Pillar-based Object Detection for Autonomous Driving.&quot; arXiv preprint arXiv:2007.10323 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　&lt;a href=&quot;/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/&quot; title=&quot;MVF&quot;&gt;MVF&lt;/a&gt; 在俯视图点云特征的基础上，融合
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>VLOAM(Visual-lidar Odometry and Mapping)</title>
    <link href="https://leijiezhang001.github.io/VLOAM/"/>
    <id>https://leijiezhang001.github.io/VLOAM/</id>
    <published>2020-07-29T08:36:38.000Z</published>
    <updated>2020-08-04T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　<a href="/LOAM/" title="LOAM">LOAM</a> 中 Lidar Odometry 模块将当前累积的 Sweep 点云通过 Sweep-to-Sweep 注册到上一时刻的 Sweep 点云，从而生成高频低精度的位姿；Lidar Mapping 则将完整的当前 Sweep 点云通过 Sweep-to-Model 注册到全局地图中，从而生成低频高精度的位姿。这其中高频低精度的位姿可通过其它方式获得，如 IMU 等其它高频传感器。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 采用高频的 Visual Odometry 来生成高频低精度的位姿，低频高精度的位姿则还是通过 Lidar Odometry(Mapping) 获得，但做了细微的改变。 <img src="/VLOAM/demo.png" width="55%" height="55%" title="图 1. Visual & Lidar Odometry"> 　　如图 1. 所示，VSLAM 结合了高频低精度的 Visual Odometry，以及低频高精度的 Lidar Odometry，最终得到高频高精度的位姿，以及准确的全局点云地图。</p><h2 id="framework">1. Framework</h2><p>　　本文坐标系以相机坐标系 \(\{S\}\) 为主(x left，y upward，z forward)，所有点云都会通过外参转换到该坐标系下；设世界坐标系 \(\{W\}\) 为起始点。那么位姿求解问题的数学描述为：给定各个坐标系 \(\{S\}\) 下的图像和点云，求解所有 \(\{S\}\) 在 \(\{W\}\) 下的表示，以及 \(\{W\}\) 下地图的构建。 <img src="/VLOAM/framework.png" width="95%" height="95%" title="图 2. VSLAM Framework"> 　　如图 2. 所示，Visual Odometry 作前后帧的特征跟踪(或匹配)，结合点云深度信息，作 Frame-to-Frame 的运动位姿估计；Lidar Odometry 则先通过 Sweep-to-Sweep 作运动粗估计，然后用 Sweep-to-Map 作精估计(其中 Sweep 的定义可见 <a href="/LOAM/" title="LOAM">LOAM</a>)。由此输出低频的全局地图，以及高频的位姿估计。</p><h2 id="visual-odometry">2. Visual Odometry</h2><p>　　首先用 Visual Odometry 得到的高频位姿估计将点云注册为一个局部的深度图。由此维护三种类型的特征点：1. 从深度图获得深度的特征点；2. 从前后帧三角化获得深度的特征点；3. 没有深度的特征点。这里的特征点提取可采用任意的特征点提取方法，如果采用前后帧特征匹配的策略，则还得作相应的特征描述子提取，如果采用特征跟踪策略，则不需要。<br>　　设图像帧序号 \(k\in Z ^ +\)，特征点序号 \(i\in\mathcal{I}\)，那么在相机坐标系 \(\{S ^ k\}\) 下，特征点坐标表示为 \(\sideset{^S}{}X ^ k _ i = [\sideset{^S}{}x ^ k _ i,\sideset{^S}{}y ^ k _ i,\sideset{^S}{}z ^ k _ i] ^ T\)，其归一化表示为 \({\sideset{^S}{}{\overline{X}}} ^ k _ i = [\sideset{^S}{}{\overline{x}} ^ k _ i,\sideset{^S}{}{\overline{y}} ^ k _ i,\sideset{^S}{}{\overline{z}} ^ k _ i] ^ T\)。前后匹配的特征点与运动位姿的关系为： <span class="math display">\[{\sideset{^S}{}X} ^ k _ i=R\;{\sideset{^S}{}X} ^ {k-1} _ i+T \tag{1}\]</span> 其中 \({\sideset{^S}{}X} ^ k _ i\) 为当前帧的特征点坐标，由于还未估计出当前帧的位姿，所以该特征点是没有深度信息的。根据特征点 \({\sideset{^S}{}X} ^ {k-1} _ i\) 是否有深度信息，可归纳出方程：</p><ol type="1"><li>\({\sideset{^S}{}X} ^ {k-1} _ i\) 有深度信息 <span class="math display">\[\begin{align}&amp;{\sideset{^S}{}{\overline{d}}} ^ k _ i{\sideset{^S}{}{\overline{X}}} ^ k _ i=R\;{\sideset{^S}{}X} ^ {k-1} _ i+T\\\Longrightarrow &amp; \left\{\begin{array}{l}\left({\sideset{^S}{}{\overline{z}}} ^ k _ i R _ 1-{\sideset{^S}{}{\overline{x}}} ^ k _ i R _ 3\right){\sideset{^S}{}{X}} ^ k _ i + {\sideset{^S}{}{\overline{z}}} ^ k _ i T _ 1-{\sideset{^S}{}{\overline{x}}} ^ k _ i T _ 3 = 0\\\left({\sideset{^S}{}{\overline{z}}} ^ k _ i R _ 2-{\sideset{^S}{}{\overline{y}}} ^ k _ i R _ 3\right){\sideset{^S}{}{X}} ^ k _ i + {\sideset{^S}{}{\overline{z}}} ^ k _ i T _ 2-{\sideset{^S}{}{\overline{y}}} ^ k _ i T _ 3 = 0\\\end{array}\tag{2}\right.\end{align}\]</span> 其中 \(\sideset{^S}{}{d} ^ k _ i = \left\Vert \sideset{^S}{}{\overline{X}} ^ k _ i\right\Vert \)，\(R _ l, T _ l\) 为第 \(l\in\{1,2,3\}\) 行的 \(R,T\)。</li><li>\({\sideset{^S}{}X} ^ {k-1} _ i\) 无深度信息 <span class="math display">\[\begin{align}&amp;{\sideset{^S}{}{\overline{d}}} ^ k _ i{\sideset{^S}{}{\overline{X}}} ^ k _ i=R\;{\sideset{^S}{}{\overline{d}}} ^ {k-1} _ i\;{\sideset{^S}{}X} ^ {k-1} _ i+T\\\Longrightarrow &amp;\begin{bmatrix}-{\sideset{^S}{}{\overline{y}}} ^ k _ i T _ 3  +{\sideset{^S}{}{\overline{z}}} ^ k _ i T _ 2 &amp;{\sideset{^S}{}{\overline{x}}} ^ k _ i T _ 3-{\sideset{^S}{}{\overline{z}}} ^ k _ i T _ 1 &amp;-{\sideset{^S}{}{\overline{x}}} ^ k _ i T _ 2+{\sideset{^S}{}{\overline{y}}} ^ k _ i T _ 1\end{bmatrix}R\;{\sideset{^S}{}{\overline{X}}} ^ {k-1} _ i = 0\tag{3}\end{align}\]</span> 推导过程比较繁杂，但是也比较简单，依次消去 \(\sideset{^S}{}{d} ^ k _ i,\sideset{^S}{}{d} ^ {k-1} _ i\) 即可。</li></ol><p>将所有特征点所构成的 residual 累积，然后可用 LM 法求解该非线性问题中 6-DOF 的位姿。考虑到有较大 residual 的特征点大概率是离群点，所以对特征点的 residual 作权重处理，residual 越大，权重越小。<br><img src="/VLOAM/feats.png" width="55%" height="55%" title="图 3. Edge & Planar Feature"> 　　为了获取特征点的深度，维护一个从点云中采样的在上一帧图像坐标系下的深度图，深度图维护较新的点云深度信息，并且保持一定的点密度。深度图中的点用极坐标形式的 2D KD-tree 存储，具体的特征点深度值计算通过周围深度点构成的平面插值得到。在无法从深度图中获得特征点的深度信息时，如果特征点被跟踪了较长的距离，那么采用三角测量法获得该特征点深度。三种点的可视化如图 3. 所示。</p><h2 id="lidar-odometry">3. Lidar Odometry</h2><p>　　高频的 frame-to-frame Visual Odometry 得到的位姿估计是粗糙且有漂移的，接下来用 Lidar Odometry 作进一步的精估计。激光雷达里程计又基于 coarse-to-fine 的思想，分为 sweep-to-sweep 以及 sweep-to-map 两个步骤。这两个步骤的具体计算过程很相似，只不过前者是前后帧点云的匹配以消除运动引入的点云畸变，后者则是当前帧去畸变的点云与世界坐标系下的地图点云匹配，能消除累积误差。总体上这部分与 <a href="/LOAM/" title="LOAM">LOAM</a> 处理方式一致。</p><h3 id="sweep-to-sweep">3.1. Sweep-to-Sweep</h3><p><img src="/VLOAM/vo_drift.png" width="55%" height="55%" title="图 4. Drift"> 　　与 <a href="/LOAM/" title="LOAM">LOAM</a> 一样，对第 \(m\in Z ^ +\) 个 Sweep 点云 \(\mathcal{P} ^ m\)，提取线特征 \(\mathcal{E} ^ m\) 与面特征 \(\mathcal{H} ^ m\)。如图 4. 所示，将 Visual Odometry 产生的漂移建模为线性运动模型。假设第 \(m\) 个 Sweep 扫描期间其漂移的位姿为 \(T'\in\mathbb{R} ^ {6\times 1}\)，那么，对于点 \(i\in\mathcal{E} ^ m\cup\mathcal{H} ^ m\)，其接收时间 \(t _ i\) 对应的位姿漂移为： <span class="math display">\[T _ i&#39; = T&#39;(t _ i-t ^ m)/(t ^ {m+1}-t ^ m) \tag{4}\]</span> 　　为了求解 \(T'\)，分别找到当前帧特征点 \(\mathcal{E} ^ m,\mathcal{H} ^ m\) 与上一帧特征点的匹配，然后计算距离误差的 residual，累积后即可用 LM 法来求解该非线性最小二乘问题。对于 \(\mathcal{E} ^ m\)，在 \(\mathcal{P} ^ {m-1}\) 中找到最近的两个线特征点，从而计算 point-to-edge 距离；对于 \(\mathcal{H} ^ m\)，在 \(\mathcal{P} ^ {m-1}\) 中找到最近的三个面特征点，从而计算 point-to-plane 距离。找特征点的过程通过 3D KD-tree 实现(工程上为了加速，可以采用其它方法)。由此得到一系列方程： <span class="math display">\[f({\sideset{^S}{}X} ^ m _ i, T _ i&#39;)=d _ i \tag{5}\]</span> 其中 \({\sideset{^S}{}X} ^ m _ i\) 是点 \(i\in\mathcal{E} ^ m\cup\mathcal{H} ^ m\) 在 \(\{S ^ m\}\) 下的坐标。计算 \(T'\) 后，即可得到去畸变的当前帧点云 \(\mathcal{P} ^ m\)。</p><h3 id="sweep-to-map">3.2. Sweep-to-Map</h3><p>　　去畸变的点云 \(\mathcal{P} ^ m\) 可以进一步注册到点云地图 \(\mathcal{Q} ^ {m-1}\) 中。考虑到点云地图较为稠密，匹配过程为计算局部点集的分布特征值与特征向量。特征值一大两小，即为线特征；特征值两大一小则为面特征。因为没有 Sweep-to-Sweep 中的运动模型，所以可直接用 ICP 方法来优化求解位姿。最终得到低频高精度的位姿结果。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Zhang, Ji, and Sanjiv Singh. &quot;Visual-lidar odometry and mapping: Low-drift, robust, and fast.&quot; 2015 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2015.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　&lt;a href=&quot;/LOAM/&quot; title=&quot;LOAM&quot;&gt;LOAM&lt;/a&gt; 中 Lidar Odometry 模块将当前累积的 Sweep 点云通过 Sweep-to-Sweep 注册到上一时刻的 Sweep 点云，从而生成高频低精度的位姿；Lidar Mapping
      
    
    </summary>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/categories/SLAM/"/>
    
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/tags/SLAM/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>SuMa(Surfel-based Mapping)</title>
    <link href="https://leijiezhang001.github.io/SuMa/"/>
    <id>https://leijiezhang001.github.io/SuMa/</id>
    <published>2020-07-20T01:31:34.000Z</published>
    <updated>2020-07-28T03:11:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　目前业界比较流行的基于激光雷达的 SLAM 是 <a href="/LOAM/" title="LOAM">LOAM</a>，其中 Mapping 又是非常重要的一环，LOAM 提取 Edge 点与 Surf 点然后建立以 Voxel 约束点个数的点云地图，该地图用于 Lidar Odometry 时的匹配定位。实际应用于工业界时，Mapping 的数据结构设计及存取管理对整体系统的效率至关重要，具体可优化的细节以后再写文阐述。<br>　　本系列文章<a href="#1" id="1ref"><sup>[1]</sup></a><a href="#2" id="2ref"><sup>[2]</sup></a> 提出了一种基于 Surfel 和语义信息的建图及定位方法。整体框架与 LOAM 类似，只是这里只用了面区域的特征点，其它模块，如优化方式，也有很大的差异。</p><h2 id="suma">1. SuMa</h2><p>　　设 \(A\) 坐标系下的点 \(p _ A\)，\(B\) 坐标系下的点 \(p _ B\)，其变换矩阵 \(T _ {BA}\in\mathbb{R}^{4\times 4}\)，使得 \(p _ B = T _ {BA} p _ A\)。变换矩阵 \(T _ {BA}\) 又由 \(R _ {BA}\in\mathbf{SO}(3)\) 和 \(t _ {BA}\in\mathbb{R}^3\) 构成。设每帧点云的雷达坐标系为 \(C _ k,k\in\{0,...,t\}\)，那么 Lidar Odometry 要求解的问题就是当前雷达坐标系在世界坐标系下的表示： <span class="math display">\[T _ {WC _ t} = T _ {WC _ 0}T _ {C _ 0C _ 1}\cdots T _ {C _ {t-1}C _ t} \tag{1}\]</span> 其中 \(T _ {WC _ 0}\) 为已标定的变换矩阵。 <img src="/SuMa/suma.png" width="65%" height="65%" title="图 1. SuMa Framework"> 　　如图 1. 所示，SuMa 根据点云 \(\mathcal{P} = \{p\in\mathbb{R}^3\}\) 估计 \(T _ {WC _ t}\) 的步骤为：</p><ol type="1"><li>当前帧地图计算。将当前帧的三维点云投影到二维，得到顶点图 \(\mathcal{V} _ D\)，以及计算对应的法向量图 \(\mathcal{N} _ D\)；</li><li>当前地图计算。对上一帧优化出的 Surfel Map \(\mathcal{M} _ {active}\) 作顶点图和法向量图的渲染 \(\mathcal{V} _ M,\mathcal{N} _ M\)；</li><li>位姿计算。根据 \(\mathcal{V} _ D, \mathcal{N} _ D\) 以及 \(\mathcal{V} _ M,\mathcal{N} _ M\) 作 frame-to-model 的 ICP 匹配，得到相对位姿 \(T _ {C _ {t-1}C _ t}\)，最后用式(1)计算当前帧在世界坐标系下的位姿态 \(T _ {WC _ t}\)；</li><li>地图更新。根据 \(T _ {WC _ t}\)，更新 Surfel Map \(\mathcal{M} _ {active}\)：初始化首次观测的区域，优化更新再次观测的区域；</li><li>闭环检测。在未激活的 Surfel Map \(\mathcal{M} _ {inactive}\) 中搜索当前帧地图的匹配；</li><li>闭环检测验证。在接下来 \(\Delta _ {verification}\) 时间内，验证闭环检测的有效性，如果有效，那么加入之后的位姿图优化；</li><li>位姿图优化。另一个线程作位姿图优化，输入信息是前后帧的相对位姿里程计以及闭环检测的相对位姿结果，类似 <a href="/AVP-SLAM/" title="AVP-SLAM">AVP-SLAM</a> 中的位姿图优化。优化后的位姿用来更新 Surfel Map。</li></ol><h3 id="preprocessing">1.1. Preprocessing</h3><p>　　与 RangeNet++<a href="#3" id="3ref"><sup>[3]</sup></a> 中对点云的表示一样，顶点图 \(\mathcal{V} _ D\) 的计算方法为： <span class="math display">\[\left(\begin{matrix}u\\v\\\end{matrix}\right)=\left(\begin{matrix}\frac{1}{2}[1-\mathrm{arctan}(y,x)\cdot \pi ^ {-1}]\cdot w\\[1-(\mathrm{arcsin}(z\cdot r ^ {-1})+f _ {up})f ^ {-1}]\cdot h\end{matrix}\right)\tag{2}\]</span> 其中 \(r = \Vert p\Vert _ 2\) 为点的距离，\(f = f _ {up} + f _ {down}\) 是雷达的上下视野角，\(w,h\) 为顶点图的宽和高。然后基于 \(\mathcal{V} _ D\) 计算每个顶点的法向量，得到法向量图 \(\mathcal{N} _ D\): <span class="math display">\[\mathcal{N} _ D((u,v)) = \left(\mathcal{V} _ D((u+1,v))-\mathcal{V} _ D((u,v))\right)\times \left(\mathcal{V} _ D((u,v+1))-\mathcal{V} _ D((u,v))\right) \tag{3}\]</span> 其中只计算坐标点 \((u,v)\) 有顶点的法向量。因为 \(u\) 方向物理世界是环状的，所以对边界作环向处理。这种法向量计算的 \(\mathcal{N} _ D\) 由较大噪声，但是实验发现对 Frame-to-Model 的 ICP 匹配不会产生精度影响。 <img src="/SuMa/preprocess_suma.png" width="55%" height="55%" title="图 2. SuMa Preprocessing"> 　　顶点图 \(\mathcal{V} _ D\) 与法向量图 \(\mathcal{V} _ N\) 的可视化结果如图 2. 所示。</p><h3 id="map-representation">1.2. Map Representation</h3><p>　　不同于 <a href="/LOAM/" title="LOAM">LOAM</a> 中采用了 Edge 和 Surf 两种特征来表示地图，本文只用 Surfel 来表示地图 \(\mathcal{M}\)。<a href="/LOAM/" title="LOAM">LOAM</a> 中计算了每个点的曲率，然后将其归为 Edge 或是 Surf，实际工程应用中，为了存储的高效性，首先将点云地图体素化，然后将体素内的特征点用 Mean，Normal，协方差矩阵的 EigenVector 等信息来存储，Normal 可用来表征 Surf 特征点，EigenVector 则可用来表征 Edge 特征点，这块具体的细节以后开文再详细阐述。<br>　　本文的 Surfel Map 自然就提取了点云的 Surf 特征，每个Surfel 可以用位置 \(v _ s\in\mathbb{R} ^ 3\)，法向量 \(n _ s\in\mathbb{R} ^ 3\)，半径 \(r _ s\in\mathbb{R}\) 来表示。此外每个 Surf 包含两个时间戳：首次建立的时间 \(t _ c\)，以及最新更新的时间 \(t _ u\)。然后采用贝叶斯滤波方法(详见 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a>)，定义及计算 Surfel 特征的稳定概率： <span class="math display">\[\begin{align}l _ s ^ {(t)} &amp;= l _ s ^ {t-1} + \mathrm{log}(p\cdot (1-p) ^ {-1}) - \mathrm{log}(p _ {prior}\cdot (1-p _ {prior}) ^ {-1})\\&amp;= l _ s ^ {t-1} + \mathrm{odds}(p) - \mathrm{odds}(p _ {prior})\\&amp;= l _ s ^ {t-1} + \mathrm{odds}\left(p _ {stable}\cdot \mathrm{exp}\left(-\frac{\alpha ^ 2}{\sigma _ {\alpha} ^ 2}\right)\mathrm{exp}\left(-\frac{d ^ 2}{\sigma _ d ^ 2}\right)\right) - \mathrm{odds}(p _ {prior})\end{align} \tag{4}\]</span> 其中 \(p _ {stable}, p _ {prior}\) 分别为测量为 surfel 是 stable 的概率，以及先验概率。\(\sigma ^ 2\) 为测量噪声方差。\(\alpha\) 为测量的 Surfel 法向量与对应的地图中 Surfel 法向量的夹角，\(d\) 则为测量的 Surfel 与对应的地图中 Surfel 的距离。<br>　　每个 Surfel 的位置及法向量都是以建立时的位置作为参考系，即 \(C _ {t _ c}\)。这样经过全局位姿优化后，就不需要重新建图，只需要通过 \(T _ {WC _ {t _ c}}\) 将 Surfel 地图更新到世界坐标系即可。<br>　　\(\mathcal{M} _ {active}\) 与 \(\mathcal{M} _ {inactive}\) 的区分也比较简单：\(\mathcal{M} _ {active}\) 定义为最近更新的 Surfels，即 \(t _ u\geq t - \Delta _ {active}\)；\(\mathcal{M} _ {inactive}\) 则定义为不是最近建立的 Surfels，即 \(t _ c&lt; t - \Delta _ {active}\)。Odometry 只在 \(\mathcal{M} _ {active}\) 中作匹配计算，Loop Closure 则只在 \(\mathcal{M} _ {inactive}\) 中搜索。</p><h3 id="odometry-estimation">1.3. Odometry Estimation</h3><p>　　里程计是将当前帧点云与地图点云匹配的过程。将上一时刻的地图 \(\mathcal{M} _ {active}\) 渲染成上一时刻局部坐标系下的顶点图 \(\mathcal{V} _ M\) 与法向量图 \(\mathcal{N} _ M\) 形式。然后采用 point-to-plane 的 ICP 匹配方法，其最小化误差为： <span class="math display">\[ E(\mathcal{V} _ D,\mathcal{V} _ M, \mathcal{N} _ M) = \sum _ {u\in\mathcal{V} _ D}n _ u ^ T\cdot\left(T _ {C _ {t-1}\;C _ t}^{(k)}\;u-v _ u\right) ^ 2 \tag{5}\]</span> 其中 \(u\in\mathcal{V} _ D\)，\(v _ u\in\mathcal{V} _ M,n _ u\in\mathcal{N} _ M\) 是地图上对应关联上的点，关联过程为： <span class="math display">\[\begin{align}v _ u &amp;= \mathcal{V} _ M\left(\Pi\left(T _ {C _ {t-1}\;C _ t}^{(k)}\;u\right)\right)\\n _ u &amp;= \mathcal{N} _ M\left(\Pi\left(T _ {C _ {t-1}\;C _ t}^{(k)}\;u\right)\right)\end{align} \tag{6}\]</span> 其中 \(T _ {C _ {t-1}\;C _ t} ^ {(t)}\) 为 frame-to-model ICP 得到的里程计估计的相对位姿。\(\Pi(u)\) 是特征点的关联方式，<a href="/LOAM/" title="LOAM">LOAM</a> 中根据前后线束的关系来寻找关联方式，本方案则采用直接坐标映射的方式。<strong>因为点云均投影到了前视图，所以可根据坐标直接搜索关联，这也是本方案最重要的优势之一</strong>。具体的，如图对应的地图顶点图中没有顶点，或者地图法向量点没有定义，那么忽略该待关联的特征点；对于关联的特征点对距离大于 \(\sigma _ {ICP}\) 或是法向量夹角大于 \(\theta _ {ICP}\) 的情况，则认为是离群点，不计入误差项。ICP 初始化为上一帧的相对位姿结果。<br>　　该问题是典型的非线性最小二乘问题，可在李空间下对位姿进行线性化并用 Gaussian-Newton 求解，这里不做展开。</p><h3 id="map-update">1.4. Map Update</h3><p>　　得到里程计估计的相对位姿后，要将当前帧的特征点更新到地图中，即要确定哪些 Surfel 要更新，哪些要重新构建新的 Surfel。对于 \(v _ s\in\mathcal{V} _ D\)，首先计算其面元的半径： <span class="math display">\[ r _ s = \frac{\sqrt{2}\Vert v _ s\Vert _ 2\cdot p}{\mathrm{clam}(-v _ s ^ T n _ s\cdot\Vert v _ s\Vert _ 2 ^ {-1}, 0.5, 1.0)} \tag{7}\]</span> 其中 \(p=\mathrm{max}(w\cdot f _ {horiz} ^ {-1}, h\cdot f _ {vert} ^ {-1})\)。<strong>根据式(2)，每个 \(v _ s\) 均能找到地图中对应的 Surfel \(s '\)。</strong>然后通过 \(\vert n _ {s'} ^ T(v _ s-v _ {s'})\vert &lt; \sigma _ M \;\mathrm{and}\; \Vert n _ s\times n _ {s'}\Vert &lt; \mathrm{sin}(\theta _ M)\) 判定当前帧的 Surfel 与地图中的 \(s'\) 是否一致：</p><ul><li>如果一致。那么更新地图中的 Surfel，如果估计的半径更准，那么也更新： <span class="math display">\[\begin{align}v _ {s&#39;} ^ {(t)} &amp;= (1-\gamma)\cdot v _ s + \gamma\cdot v _ {s&#39;} ^ {(t-1)}\\n _ {s&#39;} ^ {(t)} &amp;= (1-\gamma)\cdot n _ s + \gamma\cdot n _ {s&#39;} ^ {(t-1)}\\r _ {s&#39;} ^ {(t)} &amp;= r _ s, \; \mathrm{if} \; r _ s &lt; r _ {s&#39;}\end{align} \tag{8}\]</span></li><li>如果不一致。那么将地图中匹配上的 Surfel 作 Stable 概率衰减，然后创建新的 Surfel。如果地图中没有匹配的 Surfel，那么也创建新的 Surfel。</li></ul><p>最后将 Stable 概率较小的 Surfel 以及时间较早的 Surfel 删除，以此删除动态障碍物特征点以及较老的无关的特征点。</p><h3 id="loop-closures">1.5. Loop Closures</h3><p>　　检测到闭环后就可以作 Pose Graph 优化。闭环检测由检测与验证两部分组成，检测的过程为在未激活的地图 \(\mathcal{M} _ {inactive}\) 中找到一个最相近的位姿： <span class="math display">\[ j ^ * = \mathop{\arg\min}\limits _ {j\in 0,...,t-\Delta _ {active}} \Vert t _ {WC _ t}-t _ {WC _ j}\Vert \tag{9}\]</span> 然后类似 Odometry 的过程，将当前帧的点云特征注册到 \(T _ {WC _ j ^ * }\) 的地图特征中。为了用 ICP 求解两者的相对位姿 \(T _ {C _ {j ^ * }C _ t}\)，初始化 \(T ^ {(0) } _ {C _ {j ^ * }C _ t}\) 为： <span class="math display">\[\begin{align}R _ {C _ {j^ * }C _ t} &amp;= R ^ {-1} _ {WC _ {j ^ * }}R _ {WC _ t}\\t _ {C _ {j^ * }C _ t} &amp;= R ^ {-1} _ {WC _ {j ^ * }}(t _ {WC _ t}-t _ {WC _ {j ^ * }})\\\end{align} \tag{10}\]</span> 本文将 \(T ^ {(0) } _ {C _ {j ^ * }C _ t}\) 中的位移用 \(\lambda t _ {C _ {j ^ * }C _ t}\) 代替，其中 \(\lambda = \{0.0,0.5,1.0\}\)。由此可得到三种初始化后 ICP 迭代的结果，选择最合理的结果即可。<br>　　验证阶段，在 \(t + 1,...,t+ \Delta _ {verification}\) 时间段内，在 \(\mathcal{M} _ {active}\) 与 \(\mathcal{M} _ {inactive}\) 地图中分别作 Odometry 累加，查看两者的一致性，如果一致则认为该闭环检测是有效的。</p><h2 id="suma-1">2. SuMa++</h2><p><img src="/SuMa/suma++.png" width="95%" height="95%" title="图 3. SuMa++ Framework"> 　　SuMa++ 相比 SuMa，只增加了语义信息，算法框架没有改变。如图 3. 所示，SuMa++ 也有当前帧地图计算，当前地图计算，位姿计算，地图更新，闭环检测，闭环检测验证，位姿图优化等七个部分组成，其中，在地图计算中加入了有 RangeNet++ 产生的语义信息，在 \(\mathcal{V} _ D,\mathcal{N} _ D\) 的基础上，增加 \(\mathcal{S} _ D\) 特征；在地图更新中，根据语义信息加入了动态障碍物过滤的策略；在位姿计算中，用语义信息来权重化特征的 ICP 匹配迭代。</p><h3 id="semantic-map">2.1. Semantic Map</h3><p>　　RangeNet++ 也是基于式(2)投影试图下的分割模型，由此可得到 Surfel 特征图 \(\mathcal{V} _ D\) 中每个像素点的语义类别以及对应的类别概率。由于语义分割预测的噪声，本文用 Flood-fill 算法对网络输出的语义分割图 \(\mathcal{S} _ {raw}\) 作优化，得到顶点图对应的语义信息 \(\mathcal{S} _ D\)。 <img src="/SuMa/preprocess.png" width="65%" height="65%" title="图 4. SuMa++ Preprocessing"> 　　考虑到语义分割在物体中心区域确定性较高，而在边缘处不确定性较高，所以 Flood-fill 算法采用两个步骤：</p><ol type="1"><li>用腐蚀算法将与周围语义类别不一致的像素点移除，得到腐蚀后的语义图 \(\mathcal{S} _ {raw} ^ {eroded}\)；</li><li>结合有深度信息的顶点图 \(\mathcal{V} _ D\)，对腐蚀的边缘像素点填充为周围相近距离的顶点对应的语义类别，得到 \(\mathcal{S} _ D\)；</li></ol><p>如图 4. 所示，该方法能修正边缘类别错误的情况。由此，\(\mathcal{V} _ D, \mathcal{N} _ D,\mathcal{S} _ D\)组成每一帧的特征点信息。</p><h3 id="filtering-dynamics">2.2. Filtering Dynamics</h3><p><img src="/SuMa/res.png" width="65%" height="65%" title="图 5. Filterring Dynamics"> 　　有了语义类别信息后，在更新地图时，可计算当前帧每个 Surfel 与地图中对应 Surfel 的类别一致性，由此作为地图贝叶斯更新的惩罚项，如果类别不一致，地图中的 Surfel 稳定性概率会降低，直到去除。如图 5. 所示，这种方法能去除大部分动态障碍物区域所构成的 Surfel。地图具体的贝叶斯更新为： <span class="math display">\[\begin{align}l _ s ^ {(t)} = l _ s ^ {t-1} + \mathrm{odds}\left(p _ {stable}\cdot \mathrm{exp}\left(-\frac{\alpha ^ 2}{\sigma _ {\alpha} ^ 2}\right)\mathrm{exp}\left(-\frac{d ^ 2}{\sigma _ d ^ 2}\right)\right) - \mathrm{odds}(p _ {prior}) - \mathrm{odds}(p _ {penalty})\end{align} \tag{11}\]</span></p><h3 id="semantic-icp">2.3. Semantic ICP</h3><p>　　在式(5)的 ICP 误差项基础上，可加入语义约束，对误差项作权重化： <span class="math display">\[ E(\mathcal{V} _ D,\mathcal{V} _ M, \mathcal{N} _ M) = \sum _ {u\in\mathcal{V} _ D}w _ u n _ u ^ T\cdot\left(T _ {C _ {t-1}\;C _ t}^{(k)}\;u-v _ u\right) ^ 2 \tag{12}\]</span> 其中权重项结合了语义约束与几何约束，以此来减少离群特征点对优化的影响： <span class="math display">\[w _ u ^{(k)} = \rho _ {Huper}\left(r _ u ^ {(k)}C _ {semantic}(\mathcal{S} _ D(u),\mathcal{S} _ M(u))\right)\mathbb{1}\left\{l _ s ^ {(k)}\geq l _ {stable}\right\} \tag{13}\]</span> 其中 \(\rho _ {Huber}(r)\) 是 Huber 核函数： <span class="math display">\[\rho _ {Huber}(r)=\left\{\begin{array}{l}1 &amp;,\mathrm{if}\;\vert r\vert &lt; \sigma\\\sigma\vert r\vert ^ {-1} &amp;,\mathrm{otherwise}\end{array}\tag{14}\right.\]</span> 语义约束项为： <span class="math display">\[C _ {semantic}\left((y _ u,P _ u),(y _ {v _ u}, P _ {v _ u})\right)=\left\{\begin{array}{l}P(y _ u|u) &amp;,\mathrm{if}\;y _ u=y _ {v _ u}\\1-P(y _ u|u) &amp;,\mathrm{otherwise}\end{array}\tag{15}\right.\]</span></p><p><img src="/SuMa/icp.png" width="65%" height="65%" title="图 6. Weights of ICP"> 　　如图 6. 所示，在语义信息的约束下，如果当前帧某个 Surfel 的类别与地图中对应的 Surfel 类别不一致，那么就会减少该 Surfel 匹配对的误差项。</p><h2 id="thinkings">3. Thinkings</h2><p>　　利用检测或分割得到的语义信息去过滤当前帧以及地图中的动态障碍物，在 SLAM/Odometry 中已经非常常见，其实可以大概率相信语义信息，然后直接将对应的点云干掉。而本文以融合迭代的思路，想通过将信将疑的方式来完成有效的 ICP 匹配（既要滤掉大多数动态障碍物的影响，也期望一堆车停在场景中时然后保留足够匹配的特征点）。但是一般工程上，直接干掉也够用，毕竟场景够大，不太可能出现特征点不够匹配的情景。<strong>而本方法的高效性在于，寻找当前帧与地图中的 Surfel 匹配时，直接采用图像索引然后顶点图距离及法向量图角度判断有效性的形式，没有 KD-Tree，极大提高效率</strong>，类似 ICPCUDA<a href="#4" id="4ref"><sup>[4]</sup></a>。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Behley, Jens, and Cyrill Stachniss. &quot;Efficient Surfel-Based SLAM using 3D Laser Range Data in Urban Environments.&quot; Robotics: Science and Systems. 2018.<br><a id="2" href="#2ref">[2]</a> Chen, Xieyuanli, et al. &quot;Suma++: Efficient lidar-based semantic slam.&quot; 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2019.<br><a id="3" href="#3ref">[3]</a> Milioto, Andres, et al. &quot;RangeNet++: Fast and accurate LiDAR semantic segmentation.&quot; 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2019.<br><a id="4" href="#4ref">[4]</a> https://github.com/mp3guy/ICPCUDA</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　目前业界比较流行的基于激光雷达的 SLAM 是 &lt;a href=&quot;/LOAM/&quot; title=&quot;LOAM&quot;&gt;LOAM&lt;/a&gt;，其中 Mapping 又是非常重要的一环，LOAM 提取 Edge 点与 Surf 点然后建立以 Voxel 约束点个数的点云地图，该地图用于 
      
    
    </summary>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/categories/SLAM/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/tags/SLAM/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>AVP-SLAM</title>
    <link href="https://leijiezhang001.github.io/AVP-SLAM/"/>
    <id>https://leijiezhang001.github.io/AVP-SLAM/</id>
    <published>2020-07-15T01:17:56.000Z</published>
    <updated>2020-07-17T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　Visual-SLAM 一般采用特征点或像素直接法来建图定位，这种方式对光照较为敏感。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 提出了一种基于语义特征的 Visual Semantic SLAM，应用于光照条件较为复杂的室内停车场环境，相比于采用特征点的 ORB-SLAM，性能较为鲁棒。</p><h2 id="framework">1. Framework</h2><p><img src="/AVP-SLAM/framework.png" width="95%" height="95%" title="图 1. AVP-SLAM Framework"> 　　如图 1. 所示，AVP-SLAM 由 Mapping，Localization 两部分组成。Mapping 阶段，将车周围的四张图通过 IPM 拼接并变换到俯视图，然后作 Guide Signs，Parking Lines，Speed Bumps 等语义信息的提取，接着通过 Odometry 将每帧的特征累积成局部地图，最后通过回环检测，全局优化出全局地图。Localization 阶段，提取出每帧的语义信息后，用 Odometry 初始化位姿，然后用 ICP 匹配求解当前帧在全局地图中的位姿，得到基于地图的位姿观测量，最后用 EKF 融合该观测量与 Odometry 信息，得到本车的最终位姿。<br>　　有了本车在全局地图下的位姿后，然后通过语义信息识别停车位，即可达到本车自动泊车的目的。</p><h2 id="mapping">2. Mapping</h2><h3 id="ipm">2.1. IPM</h3><p>　　传感器为车身四周四个鱼眼相机，相机内外参已知。IPM(Inverse Perspective Mapping) 是将图像中的像素点投影到车身物理坐标系下的俯视图中，具体的： <span class="math display">\[\frac{1}{\lambda}\;\begin{bmatrix}x ^ v \\y ^ v \\1\end{bmatrix} =[\mathbf{R} _ c \;\mathbf{t} _ c] ^ {-1} _ {col:1,2,4} \;\pi _ c ^ {-1}\begin{bmatrix}u \\v \\1\end{bmatrix}\tag{1}\]</span> 其中 \(\pi _ c(\cdot)\) 为鱼眼相机的内参矩阵，\([\mathbf{R} _ c\;\mathbf{t} _ c]\) 为每个相机到车身坐标系的外参矩阵，\([x ^ v\; y ^ v]\) 为车身坐标系下语义特征的位置。关于 IPM 更多细节可参考 <a href="/lane-det-from-BEV/" title="Apply IPM in Lane Detection from BEV">Apply IPM in Lane Detection from BEV</a>。<br>　　进一步将 IPM 图拼接成一张全景图： <span class="math display">\[\begin{bmatrix}u _ {ipm}\\v _ {ipm}\\1\end{bmatrix}=\mathbf{K} _ {ipm}\begin{bmatrix}x ^ v \\y ^ v \\1\end{bmatrix}\tag{2}\]</span> 其中 \(\mathbf{K} _ {ipm}\) 是全景图的内参。</p><h3 id="feature-detection">2.2. Feature Detection</h3><p><img src="/AVP-SLAM/segment.png" width="65%" height="65%" title="图 2. Segmentation in IPM Image"> 　　将每张 IPM 图拼接成一张大全景图，然后用基于深度学习的语义分割方法，对全景图作像素级别作 lane，parking line，guide sign，speed bump，free space，obstacle，wall 等类别的语义分割。如图 4. 所示，parking line，guide sign，speed bump 是稳定的特征，用于定位；parking line 用于车位的识别；free space 与 obstacle 用于路径规划。</p><h3 id="local-mapping">2.3. Local Mapping</h3><p>　　全景图语义分割得到的用于定位的特征(parking line，guide sign，speed bump)需要反投影回车身物理坐标系： <span class="math display">\[\begin{bmatrix}x ^ v \\y ^ v \\1\end{bmatrix}=\mathbf{K} _ {ipm} ^ {-1}\begin{bmatrix}u _ {ipm}\\v _ {ipm}\\1\end{bmatrix}\tag{3}\]</span> 然后基于 Odometry 的相对位姿，将当前的语义特征点转换到世界坐标系下： <span class="math display">\[\begin{bmatrix}x ^ w \\y ^ w \\z ^ w\end{bmatrix}=\mathbf{R _ o}\begin{bmatrix}x ^ v \\y ^ v \\0\end{bmatrix} + \mathbf{t _ o}\tag{4}\]</span> 由此得到局部地图，本文保持车身周边 30m 内的局部地图。</p><h3 id="loop-detection">2.4. Loop Detection</h3><p><img src="/AVP-SLAM/loop_det.png" width="65%" height="65%" title="图 3. Loop Detection"> 　　因为 Odometry 有累计误差，所以这里对局部地图作一个闭环检测。如图 3. 所示，通过 ICP 对两个局部地图作匹配，一旦匹配成功，就说明检测到了闭环，ICP 匹配的相对位姿用于之后的全局位子图优化，以消除里程计累计误差。</p><h3 id="global-optimization">2.5. Global Optimization</h3><p>　　检测到闭环后，需进行全局位姿图优化。位姿图中，节点(node)为每个局部地图的位姿：\((\mathbf{r, t})\)；边(edge)有两种：odometry 相对位姿以及闭环检测中 ICP 匹配位姿。由此位姿图优化的损失函数为： <span class="math display">\[\chi ^ * = \mathop{\arg\min}\limits _ \chi \sum _ t\Vert f(\mathbf{r} _ {t+1},\mathbf{t} _ {t+1}, \mathbf{r} _ t, \mathbf{t} _ t) - \mathbf{z} ^ o _ {t,t+1}\Vert ^ 2 + \sum _ {i,j\in\mathcal{L}}\Vert f(\mathbf{r} _ i,\mathbf{t} _ i,\mathbf{r} _ j, \mathbf{t} _ j)-\mathbf{z} ^ l _ {i,j}\Vert ^ 2 \tag{5}\]</span> 其中 \(\chi = [\mathbf{r} _ 0,\mathbf{t} _ 0,...,\mathbf{r} _ t,\mathbf{t} _ t] ^ T\) 是所有局部地图的位姿，也是待优化的参数。\(\mathbf{z} ^ 0 _ {t,t+1}\) 为 Odometry 得到的位姿。\(\mathbf{z} ^ l _ {i,j}\) 为闭环检测 ICP 得到的位姿。\(f(\cdot)\) 为计算两个局部地图相对位姿的方程。该优化问题可通过 Gauss-Newton 法求解。<br>　　用优化后的位姿将局部地图叠加起来，就获得了整个场景的全局地图。</p><h2 id="localization">3. Localization</h2><p><img src="/AVP-SLAM/loc.png" width="65%" height="65%" title="图 4. Localization"> 　　有了全局地图后，基于全局地图的定位观测量可通过当前帧与全局地图的匹配得到。如图 4. 所示，绿色为当前帧检测到的语义特征，与全局地图匹配后即可得到当前的绝对位置。匹配通过 ICP 算法实现： <span class="math display">\[ \mathbf{r ^ * ,t ^ * } =  \mathop{\arg\min}\limits _ {\mathbf{r,t}}\sum _ {k\in\mathcal{S}}\Vert\mathbf{R(r)}\begin{bmatrix}x ^ v  _ k\\y ^ v  _ k\\0\end{bmatrix} + \mathbf{t} - \begin{bmatrix}x ^ w _ k \\y ^ w _ k\\z ^ w _ k\end{bmatrix}\Vert ^ 2 \tag{6}\]</span> 其中 \(\mathcal{S}\) 为当前帧语义特征点集，\([x _ k ^ w\; y _ k ^ w\; z _ k ^ w]\) 分别为对应的全局地图中最近的特征点集。<br>　　ICP 的初始化非常重要，本文提出了两种初始化方法：1. 直接在地图上标记车库入口作为全局坐标点；2. 室外 GPS 信号初始化，然后用 Odometry 累积到车库。</p><h2 id="extended-kalman-filter">4. Extended Kalman Filter</h2><p>　　Visual Localization 在语义特征较少的情况下，比如车辆停满了，定位会不稳定，所以这里采用 EKF 对 Visual Localization 与 Odometry 中的轮速计和 IMU 作扩展卡尔曼融合，这里不做展开。</p><h2 id="thinkings">5. Thinkings</h2><p>　　Semantic SLAM 相比基于几何特征点的 SLAM 更加鲁棒。但是在车库场景下，一旦车子停满后，停车线等语义信息会急剧减少，所以实际商业应用中，AVP-SLAM 能满足室内自动泊车的需求吗？<br>　　对此我持怀疑态度。我认为，对于车库自动泊车的商业落地，可能最有效且低成本的方法还是得基于室内 UWB 定位技术。至少 UWB 可作为辅助。当然要将 UWB 应用于车载装置，目前好像还没有，不过随着车载软硬件系统的完善，手机上能做的事，车载平台问题也不大。</p><h2 id="reference">6. Reference</h2><p><a id="1" href="#1ref">[1]</a> Qin, Tong, et al. &quot;AVP-SLAM: Semantic Visual Mapping and Localization for Autonomous Vehicles in the Parking Lot.&quot; arXiv preprint arXiv:2007.01813 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Visual-SLAM 一般采用特征点或像素直接法来建图定位，这种方式对光照较为敏感。本文&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt; 提出了一种基于语义特征的 Visual Semantic SLAM，应用于光照条件较为复杂的室内
      
    
    </summary>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/categories/SLAM/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/tags/SLAM/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-Point-GNN/"/>
    <id>https://leijiezhang001.github.io/paper-reading-Point-GNN/</id>
    <published>2020-07-10T01:22:07.000Z</published>
    <updated>2020-07-14T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种基于图网络来提取点云特征的方法，理论上可在不损失原始信息的情况下，高效的学习点云特征，其在点云 3D 检测任务中效果提升明显。</p><h2 id="different-point-cloud-representations">1. Different Point Cloud Representations</h2><p><img src="/paper-reading-Point-GNN/repr.png" width="65%" height="65%" title="图 1. Point Cloud Representations"> 　　如图 1. 所示，目前点云表示方式以及对应的特征学习方式有三种：Grids，栅格化后类似图像 2D/3D 卷积的形式；Sets，以 PointNet 为代表的最近邻查找周围点并学习的形式；Graph，将无序点集转换为图模型，特征信息通过点云顶点传递学习的形式。Grids 及 Sets 形式我们已经比较熟悉了，Graph 则查询效率比 Sets 高，特征提取能力又比 Grids 高。Graph 的建图时间复杂度为 \(\mathcal{O}(cN)\)，领域查询复杂度则为 \(\mathcal{O}(1)\)，Sets 中的 KNN 建树及查询复杂度可见 <a href="/PointCloud-Feature-Extraction/" title="PointCloud Feature Extraction">PointCloud Feature Extraction</a>。当然 KNN 式的领域查询方式可以用近似 \(\mathcal{O}(1)\) 方法实现，但是会影响特征学习的准确度。</p><h2 id="framework">2. Framework</h2><p><img src="/paper-reading-Point-GNN/framework.png" width="95%" height="95%" title="图 2. Framework of Point-GNN"> 　　如图 2. 所示，基于 Graph 的 3D 点云检测，首先对点云作 Graph Construction，然后用 GNN 来学习每个顶点的特征，接着对每个顶点预测目标框，最后作目标框的整合和 NMS。</p><h3 id="graph-construction">2.1. Graph Construction</h3><p>　　设点云集：\(P=\{p _ 1,...,p _ N\}\)，其中 \(p _ i=(x _ i, s _ i)\) 分别表示坐标 \(x _ i\in\mathbb{R} ^ 3\)，以及该点反射率，领域点相对位置等信息 \(s _ i\in\mathbb{R} ^ k\)。对该点集建图 \(G=(P,E)\)，将距离小于一定阈值的两个点进行连接，即： <span class="math display">\[E = \{(p _ i, p _ j)|\Vert x _ i-x _ j\Vert _ 2 &lt; r\} \tag{1}\]</span> 这种建图方式是 Fixed Radius Near-Neighbors 问题，可在 \(\mathcal{O}(cN)\) 时间复杂度下解决，其中 \(c\) 为最大连接数。<br>　　建图完成后，要对每个点信息状态 \(s _ i\) 作初始化。这里采用类似 Sets 的特征提取方式，即将该点的反射率，以及与领域内点的相对位置，串联成特征向量，然后用 MLP 作空间变换，最后在点维度上作 Max Pooling，即可得到初始化的该点特征状态量 \(s _ i\)。</p><h3 id="graph-neural-network-with-auto-registration">2.2. Graph Neural Network with Auto-Registration</h3><p>　　传统的图神经网络，通过边迭代每个顶点的特征。在 \((t+1) ^ {th}\) 迭代时： <span class="math display">\[\begin{align} v _ i ^ {t+1} &amp;= g ^ t\left(\rho\left(\{e _ {ij} ^ t|(i,j)\in E\}\right), v _ i ^ t\right) \\e _ {ij} ^ t &amp;= f ^ t(v _ i ^ t, v _ j ^ t) \tag{2}\end{align}\]</span> 其中 \(e ^ t,v ^ t\) 分别是边和顶点特征，\(f ^ t(\cdot)\) 计算两个顶点之间边的特征，\(\rho(\cdot)\) 将与该点连接的边特征整合，得到该点特征增量，\(g ^ t(\cdot)\) 将该点特征增量与原特征进行整合得到本次迭代后该点的最终特征。<br>　　对于边特征，一种设计方式为，描述领域特征对该点位置的作用力，重写式 (2)： <span class="math display">\[s _ i ^ {t+1} = g ^ t\left(\rho\left(\{f ^ t(x _ j-x _ i,s _ j^t)|(i,j)\in E\}\right), s _ i ^ t\right) \tag{3}\]</span> 这样就得到了图神经网络的迭代模型。此外，本文还指出，由于边特征对领域点的距离较为敏感，所以作者提出对相对位置作自动补偿，实验表明其实意义不大： <span class="math display">\[\begin{align}\Delta x _ i ^ t &amp;= h ^ t(s _ i^t) \\s _ i ^ {t+1} &amp;= g ^ t\left(\rho\left(\{f ^ t(x _ j-x _ i+\Delta x _ i ^ t,s _ j^t)|(i,j)\in E\}\right), s _ i ^ t\right) \tag{4}\end{align}\]</span> 　　具体的，\(f ^ t(\cdot),g ^ t(\cdot), h ^ t(\cdot)\) 可用 MLP 来建模，\(\rho(\cdot)\) 则采用 Max 操作： <span class="math display">\[\begin{align}\Delta x _ i ^ t &amp;= MLP _ h ^ t(s _ i^t) \\e _ {ij} ^ t &amp;= MLP _ f ^ t([x _ j - x _ i + \Delta x _ i ^ t, s _ j ^ t]) \\s _ i ^ {t+1} &amp;= MLP _ g ^ t\left(MAX(\{e _ {ij}|(i,j)\in E\})\right)+ s _ i ^ t \tag{5}\end{align}\]</span></p><h3 id="loss">2.3. Loss</h3><p>　　为了作 3D 检测的任务，网络头输出为每个顶点的类别，目标框中心的 offset，以及目标框的尺寸，朝向。这与传统的基于 Ancho-Free 的 3D 目标检测基本一致，这里不做展开。</p><h3 id="box-merging-and-scoring">2.4. Box Merging and Scoring</h3><p>　　本方法的 3D 检测需要作 NMS 后处理，由于分类的 Score 不能反应目标框的 Uncertainty，所以基于 Score 的 NMS 是不合理的。这个问题在 2D 检测中也有比较多的研究，比如采用预测 IoU 值的方式来作权重。本文则认为遮挡信息能作为 NMS 操作的指导，由此定义了遮挡值的计算方式。但是实验显示，其实提升并不明显，所以这里不做具体展开。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Shi, Weijing, and Raj Rajkumar. &quot;Point-gnn: Graph neural network for 3d object detection in a point cloud.&quot; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　本文&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt;提出了一种基于图网络来提取点云特征的方法，理论上可在不损失原始信息的情况下，高效的学习点云特征，其在点云 3D 检测任务中效果提升明显。&lt;/p&gt;
&lt;h2 id=&quot;different-p
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>CenterTrack</title>
    <link href="https://leijiezhang001.github.io/CenterTrack/"/>
    <id>https://leijiezhang001.github.io/CenterTrack/</id>
    <published>2020-07-02T01:16:36.000Z</published>
    <updated>2020-07-08T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　障碍物感知由目标检测，目标跟踪(MOT)，目标状态估计等三个模块构成。目标状态估计一般是指将位置，速度等观测量作卡尔曼滤波平滑；广义的目标跟踪也包含了状态估计过程，这里采用狭义的目标跟踪定义方式，主要指出目标 ID 的过程。传统的做法，目标检测与目标跟踪是分开进行的，检测模块分别对前后帧作目标检测，目标跟踪模块则接收前后帧检测结果，然后用 Motion Model 将上一帧的检测结果预测到这一帧，最后与这一帧的检测结果作数据关联(Data Association)出目标 ID。这里的 Motion Model 可以是 3D 下目标的物理运动模型，也可以是图像下的单目标跟踪结果，如 KCF 算法。详细介绍可参考 <a href="/MOT-综述-Multiple-Object-Tracking-A-Literature-Review/" title="Multiple Object Tracking: A Literature Review">Multiple Object Tracking: A Literature Review</a>。<br>　　随着检测技术的发展，检测与跟踪的整合成为了趋势。<a href="#1" id="1ref">[1]</a> 是较早将跟踪的 “Motion Model” 用 Anchor-based Two-stage 网络来预测的方法，其网络输入为前后帧图像，其中一个分支输出当前帧的检测框，另一个分支用上一帧的检测结果作为 proposal，输出这一帧的跟踪框，最后用传统的数据关联方法得到目标的 ID。随着检测技术往 Anchor-Free One-stage 方向发展，在此基础上整合目标检测与跟踪也就顺理成章。<br>　　<a href="/Anchor-Free-Detection/" title="Anchor-Free Detection">Anchor-Free Detection</a> 中详细描述了 Anchor-Free 的目标检测方法，相比于 Anchor-Based 的目标检测，其有很多优势，这里不做赘述。本文基于 CenterNet<a href="#2" id="2ref"><sup>[2]</sup></a>，总结了 CenterTrack<a href="#3" id="3ref"><sup>[3]</sup></a>，以及 CenterPoint(3D CenterTrack)<a href="#4" id="4ref"><sup>[4]</sup></a>方法。</p><h2 id="centernet">1. CenterNet</h2><p>　　CenterNet 在 <a href="/Anchor-Free-Detection/" title="Anchor-Free Detection">Anchor-Free Detection</a> 中已经较为详细得阐述了。需要补充的是，中心点的正负样本设计为：正样本只有中心点像素，负样本则为其它区域，并加入以中心点为中心的高斯权重，越靠近中心点，负样本权重越小。其 Loss 基于 Focal Loss，数学描述为： <span class="math display">\[L _ k = \frac{1}{N}\sum _ {xyc}\left\{\begin{array}{l}(1-\hat{Y} _ {xyc})^{\alpha}\mathrm{log}(\hat{Y} _ {xyc}) &amp; \mathrm{if}\; Y _ {xyc} = 1\\(1- Y _ {xyc})^{\beta}(\hat{Y} _ {xyc})^{\alpha}\mathrm{log}(1-\hat{Y} _ {xyc}) &amp; \mathrm{otherwise}\end{array}\tag{1}\right.\]</span> 其中 \(Y _ {xyc}\) 为高斯权重后的正负样本分布值。<br>　　具体的，设图像 \(I\in \mathbb{R}^{W\times H\times 3}\)，CenterNet 输出的每个类别 \(c\in\{0,...,C-1\}\) 的目标为 \(\{(\mathbf{p} _ i, \mathbf{s} _ i)\} _ {i=0} ^ {N-1}\)。其中 \(\mathbf{p}\in \mathbb{R} ^ 2\)，\(\mathbf{s}\in\mathbb{R} ^ 2\) 为目标框的尺寸。对应的，最终输出的 heatmap 位置和尺寸图为：\(\hat{Y}\in [0,1]^{\frac{W}{R}\times\frac{H}{R}\times C}\)，\(\hat{S}\in\mathbb{R}^{\frac{W}{R}\times\frac{H}{R}\times 2}\)。对 \(\hat{Y}\) 作 \(3\times 3\) 的 max pooling，即可获得目标中心点，\(\hat{S}\) 上对应的的点即为该目标的尺寸。此外还用额外的 heatmap 作位置 offset 的回归，因为 \(\hat{Y}\) 存在量化误差。最终由中心点位置 loss，位置 offset loss，尺寸 loss 三部分组成。</p><h2 id="centertrack">2. CenterTrack</h2><h3 id="framework">2.1. Framework</h3><p><img src="/CenterTrack/centertrack.png" width="85%" height="85%" title="图 1. CenterTrack"> 　　如图 1. 所示，CenterTrack 基于 CenterNet，框架也较为简单：输入前后帧图像，以及上一帧跟踪到的目标中心点所渲染的 heatmap，经过网络后输出为当前帧的检测 heatmap，size map，以及这一帧相对上一帧跟踪的 offset map。最后通过最近距离匹配即可作数据关联获得目标的 ID。算法得到的目标属性有 \(b = (\mathbf{p,s},w,id)\)，分别为目标的 location，size，confidence，identity。<br>　　相比于 CenterNet，CenterTrack 还预测了这一帧相对上一帧，目标的 2D displacement：\(\hat{D}\in\mathbb{R}^{\frac{W}{R}\times\frac{H}{R}\times 2}\)。这相当于 Tracking 中 Motion Model 的结果，分别计算上一帧目标经过该 displacement 变换到这一帧后的目标位置与当前帧检测的目标位置的距离误差，用最小距离的贪心法即可将目标作数据关联，得到目标的 ID。</p><h3 id="experiments">2.2. Experiments</h3><p>　　网络结构相比于 CenterNet 只是增加了输入的四个通道特征，输出的两个通道特征。网络可在视频流图像或者单帧图像上训练，对于单帧图像，可对图像中的目标作伸缩平移变换来模拟目标运动，实验表明，也非常有效。 <img src="/CenterTrack/motion_models2d.png" width="85%" height="85%" title="图 2. Motion Models"> 　　如图 2. 所示，本文比较了 displacement 与 kalman filter，optical flow 等 Motion Model，显示本文效果是最好的，我猜测是因为 displacement 回归的直接是物体级别的像素运动，抗噪性更强。</p><h2 id="center-based-3d-object-detection-and-tracking">3. Center-based 3D Object Detection and Tracking</h2><h3 id="framework-1">3.1. Framework</h3><p><img src="/CenterTrack/centertrack3d.png" width="85%" height="85%" title="图 3. 3D CenterTrack"> 　　如图 3. 所示，CenterPoint 将点云在俯视图下栅格化，然后采用 CenterTrack 一样的网络结构，只是输出为目标的 3D location，size，orientation，velocity。<br>　　点云俯视图下的栅格化，如果对栅格不做点云的精细化估计，那么会影响到目标位置及速度估计的精度，所以理论上 PointPillars 这种栅格点云特征学习方式能更有效的提取点云的信息，保留特征的连续化信息(但是论文的实验表明 VoxelNet 比 PointPillars 效果更好)。否则，虽然目标位置等信息的监督项是连续量，但是栅格化的特征是离散量，这会降低预测精度。<br>　　具体的，网络输出为：\(K\) 个类别的 \(K\)-channel heatmap 表示目标中心点，目标的尺寸 \(\mathbf{s}=(w,l,h)\) heatmap，目标的中心点 offset \(\mathbf{o}=(o _ x,o _ y,o _ z)\) heatmap，朝向角 \(\mathbf{e} = (\mathrm{sin}(\alpha),\mathrm{cos}(\alpha))\) heatmap，目标速度 \(\mathbf{v}=(v _ x,v _ y)\) heatmap。与 CenterTrack 非常相似，只不过这里的速度就是真实的物理速度。</p><h3 id="experiments-1">3.2. Experiments</h3><p><img src="/CenterTrack/detmap.png" width="85%" height="85%" title="图 4. 3D Detection Benchmark"> 　　如图 4. 所示，引入 Velocity 预测，能有效提升检测的性能，这应该是网络输入前一帧信息的结果，对半遮挡情况能有较好效果。 <img src="/CenterTrack/experiment3d.png" width="85%" height="85%" title="图 5. 3D MOT Benchmark"> 　　如图 5. 所示，跟踪性能也是有很大提升，而且数据关联等后处理相对比较简单。</p><h2 id="reference">5. Reference</h2><p><a id="1" href="#1ref">[1]</a> Feichtenhofer, Christoph, Axel Pinz, and Andrew Zisserman. &quot;Detect to track and track to detect.&quot; Proceedings of the IEEE International Conference on Computer Vision. 2017.<br><a id="2" href="#2ref">[2]</a> Zhou, Xingyi, Dequan Wang, and Philipp Krähenbühl. &quot;Objects as points.&quot; arXiv preprint arXiv:1904.07850 (2019).<br><a id="3" href="#3ref">[3]</a> Zhou, Xingyi, Vladlen Koltun, and Philipp Krähenbühl. &quot;Tracking Objects as Points.&quot; arXiv preprint arXiv:2004.01177 (2020).<br><a id="4" href="#4ref">[4]</a> Yin, Tianwei, Xingyi Zhou, and Philipp Krähenbühl. &quot;Center-based 3D Object Detection and Tracking.&quot; arXiv preprint arXiv:2006.11275 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　障碍物感知由目标检测，目标跟踪(MOT)，目标状态估计等三个模块构成。目标状态估计一般是指将位置，速度等观测量作卡尔曼滤波平滑；广义的目标跟踪也包含了状态估计过程，这里采用狭义的目标跟踪定义方式，主要指出目标 ID 的过程。传统的做法，目标检测与目标跟踪是分开进行的，检
      
    
    </summary>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/categories/MOT/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="Tracking" scheme="https://leijiezhang001.github.io/tags/Tracking/"/>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/tags/MOT/"/>
    
  </entry>
  
  <entry>
    <title>Rethinking of Sparse 3D Convolution</title>
    <link href="https://leijiezhang001.github.io/Rethinking-of-Sparse-3D-Convolution/"/>
    <id>https://leijiezhang001.github.io/Rethinking-of-Sparse-3D-Convolution/</id>
    <published>2020-06-23T09:37:12.000Z</published>
    <updated>2020-06-25T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　Sparse 3D Convolution 最早在<a href="#1" id="1ref">[1]</a>中提出，然后该作者又提出了 Submanifold Sparse Convolution<a href="#2" id="2ref"><sup>[2]</sup></a>，并将其应用于 3D 语义分割中<a href="#3" id="3ref"><sup>[3]</sup></a>。<a href="#4" id="4ref">[4]</a>则改进了 Sparse 3D Convolution 的实现方式，并应用于 3D 目标检测中。之前一直没仔细看 Sparse 3D Convolution 原理，以为只是基于稀疏矩阵的矩阵相乘加速，最近的一些实验发现 Sparse 3D Convolution 在点云相关的任务中不仅仅是加速，还能提升网络特征提取的性能，所以回过头来重新思考 Sparse 3D Convolution 原理及作用。</p><h2 id="sparse-convolution">1. Sparse Convolution</h2><p><img src="/Rethinking-of-Sparse-3D-Convolution/spconv.png" width="85%" height="85%" title="图 1. sparse VS. submanifold sparse"> 　　如图 1. 左图所示，对于稀疏的特征输入，传统的 Sparse Convolution 与 Convolution 一致，只是对于卷积核覆盖的输入特征为零的区域不做计算，直接置为零。这种方式下，随着卷积层的增加，特征层会变得不那么稀疏，这样不仅使得计算量上升，而且会使得提取的信息变得不那么准确。</p><h2 id="submanifold-sparse-convolution">2. Submanifold Sparse Convolution</h2><p>　　如图 1. 右图所示，Submanifold Sparse Convolution 解决了 Sparse Convolution 存在的问题。原理也很直观：只计算输出特征层映射到输入特征层不为零的位置区域。这种方式下，随着卷积层的增加，不仅能保持稀疏性，而且能保证原始信息的准确性。 <img src="/Rethinking-of-Sparse-3D-Convolution/flops.png" width="85%" height="85%" title="图 2. Flops"> 　　如图 2. 所示，Sparse Convolution 相比传统的 Convolution 已经能减少较多的计算量，而 Submanifold Sparse Convolution 则能减少更多的计算量。特征输入越稀疏，减少的计算量就越多，这对点云的三维特征提取，或者是俯视图下的二维特征提取有很大的帮助。</p><h2 id="implementation">3. Implementation</h2><p><img src="/Rethinking-of-Sparse-3D-Convolution/speed.png" width="85%" height="85%" title="图 3. Speed"> 　　<a href="#2" id="2ref">[2]</a> 中实现了 Submanifold Sparse Convolution，其中的卷积运算是手写的矩阵相乘，所以速度较慢；<a href="#4" id="4ref">[4]</a> 则基于 GEMM 实现了更高效的 Submanifold Sparse Convolution。如图 3. 所示，其有将近一倍的速度提升。 <img src="/Rethinking-of-Sparse-3D-Convolution/imple.png" width="90%" height="90%" title="图 4. Implementation"> 　　图 4. 描述了<a href="#4" id="4ref">[4]</a>实现的 Submanifold Sparse Convolution 原理。其首先通过 gather 操作将非零的元素进行矩阵相乘，然后通过 scatter 操作将结果映射回原位置。为了加速，前后元素的映射矩阵计算比较关键，这里实现了一种 GPU 计算方法，这里不做展开。</p><h2 id="application">4. Application</h2><p><img src="/Rethinking-of-Sparse-3D-Convolution/second.png" width="90%" height="90%" title="图 5. SECOND Framework"> 　　Submanifold Sparse Convolution 可应用于点云的分类，分割，检测等任务的特征提取中，SECOND<a href="#4" id="4ref"><sup>[4]</sup></a>是一种点云检测方法，如图 5. 所示，其检测框架与传统的一致，只是将体素化后的点云特征信息，进一步用 Sparse Convolution 来作特征提取。该方法不仅速度较快，而且性能也有不少提升。所以 Submanifold Sparse Convolution 是非常高效的，可作为点云特征提取的基本操作。但是传统的 Convolution，在 GPU 平台下，已经有较多的硬件级优化(cudnn)，在 CPU 平台下也有很多的指令集优化，所以最终在特定硬件下作 Inference 时，到底 Submanifold Sparse Convolution 速度能提升多少，还得看 Submanifold Sparse Convolution 实现的好不好。不过可以猜测，在目前的实现下，Submanifold Sparse Convolution 在 GPU 平台下应该能有不少的速度提升。<br>　　此外，传统的卷积量化操作也比较成熟，cudnn 已经有基本的操作引擎，而 Submanifold Sparse Convolution 的 INT8 引擎则目前还没有。所以 float32/float16 的 Submanifold Sparse Convolution 与 INT8 的 Convolution，孰快孰慢？这两条路大概就是部署的思路了，当然 INT8 的 Submanifold Sparse Convolution 会更好，但是开发成本会比较高。</p><h2 id="reference">5. Reference</h2><p><a id="1" href="#1ref">[1]</a> Graham, Ben. &quot;Sparse 3D convolutional neural networks.&quot; arXiv preprint arXiv:1505.02890 (2015).<br><a id="2" href="#2ref">[2]</a> Graham, Benjamin, and Laurens van der Maaten. &quot;Submanifold sparse convolutional networks.&quot; arXiv preprint arXiv:1706.01307 (2017).<br><a id="3" href="#3ref">[3]</a> Graham, Benjamin, Martin Engelcke, and Laurens Van Der Maaten. &quot;3d semantic segmentation with submanifold sparse convolutional networks.&quot; Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.<br><a id="4" href="#4ref">[4]</a> Yan, Yan, Yuxing Mao, and Bo Li. &quot;Second: Sparsely embedded convolutional detection.&quot; Sensors 18.10 (2018): 3337.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Sparse 3D Convolution 最早在&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;[1]&lt;/a&gt;中提出，然后该作者又提出了 Submanifold Sparse Convolution&lt;a href=&quot;#2&quot; id=&quot;2ref&quot;&gt;&lt;sup&gt;[2]&lt;/sup
      
    
    </summary>
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection&quot;</title>
    <link href="https://leijiezhang001.github.io/End-to-End-Pseudo-LiDAR-for-3D-Det/"/>
    <id>https://leijiezhang001.github.io/End-to-End-Pseudo-LiDAR-for-3D-Det/</id>
    <published>2020-06-22T01:19:12.000Z</published>
    <updated>2020-06-25T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　基于视觉的 3D 目标检测方法因为成本较低，所以在 ADAS 领域应用非常广泛。其基本思路有以下几种：</p><ul><li>单目\(\rightarrow\)3D 框，代表文章有<a href="#1" id="1ref">[1]</a>。</li><li>单目\(\rightarrow\)深度图\(\rightarrow\)3D 框</li><li>双目\(\rightarrow\)3D 框，代表文章有<a href="#2" id="2ref">[2]</a>。</li><li>双目\(\rightarrow\)深度图\(\rightarrow\)3D 框</li></ul><p>由单目或双目直接回归 3D 目标框的属性，这种方法优势是 latency 小，缺点则是，没有显式的预测深度图，导致目标 3D 位置回归较为困难。而在深度图基础上回归 3D 目标位置则相对容易些，这种方法由两个模块构成：深度图预测，3D 目标预测。得到深度图后，可以在前视图下将深度图直接 concate 到 rgb 图上来做，另一种方法是将深度图转换为 pseudo-LiDAR 点云，然后用基于点云的 3D 目标检测方法来做，目前学术界基本有结论：pseudo-LiDAR 效果更好。<br>　　本文<a href="#3" id="3ref"><sup>[3]</sup></a>即采用双目出深度图，然后基于 pseudo-LiDAR 来作 3D 目标检测的方案，并且解决了两个模块需要两个网络来优化的大 lantency 问题，实现了 End-to-End 联合优化的方式。</p><h2 id="framework">1. Framework</h2><p><img src="/End-to-End-Pseudo-LiDAR-for-3D-Det/framework.png" width="60%" height="60%" title="图 1. Framework"> 　　基于点云作 3D 目标检测大致可分为 point-based 与 voxel-based 两大类，详见 <a href="/Point-based-3D-Det/" title="Point-based 3D Detection">Point-based 3D Detection</a>，传统的基于双目的 pseudo-LiDAR 方案无法 End-to-End 作俯视图下 voxel-based 3D 检测，因为点云信息需要作俯视图离散化，离散的过程是无法作反向传播训练的，本文提出了 Change of Representation(CoR) 模块有效解决了这个问题。如图 1. 所示，本方案中 Depth Estimation 可由任何深度估计网络实现，然后经过 CoR 模块，将深度图变换成点云形式用于 point-based 3D detection，或者是 Voxel 形式用于 voxel-based 3D detection。这里的关键是可求导的 CoR 模块设计。</p><h2 id="cor">2. CoR</h2><h3 id="quantization">2.1. Quantization</h3><p>　　点云检测模块如果采用 voxel-based 方案，那么点云到俯视图栅格的离散化(quantization)是必不可少的。假设点云 \(P = \{p _ 1,...,p _ N\}\)，待生成的 3D 占据栅格(最简单的特征形式) \(T\) 包含 \(M\) 个 bins，即 \(m\in\{1,...,M\}\)，每个 bin 的中心点设为 \(\hat{p} _ m\)。那么生成的 \(T\) 可表示为： <span class="math display">\[ T(m) = \left\{\begin{array}{l}1, &amp; \mathrm{if}\;\exists p\in P \; \mathrm{s.t.}\; m = \mathop{\arg\min}\limits _ {m &#39;}\Vert p - \hat{p} _ {m&#39;}\Vert _ 2 \\0, &amp; \mathrm{otherwise}.\end{array}\tag{1}\right.\]</span> 即如果有点落在该 bin 里，那么该 bin 对应的值置为 1。这种离散方式是无法求导的。<br>　　本文提出了一种可导的软量化模块(soft quantization module)，即用 RBF 作权重计数，另一种角度来看，<strong>这其实类似于点的空间概率密度表示</strong>。设 \(P _ m\) 为落在 bin \(m\) 的点集： <span class="math display">\[ P _ m=\left\{p\in |, \mathrm{s.t.}\; m=\mathop{\arg\min}\limits _ {m &#39;}\Vert p - \hat{p} _ {m&#39;}\Vert _ 2\right\} \tag{2}\]</span> 那么，\(m'\) 作用于 \(m\) 的值为： <span class="math display">\[ T(m, m&#39;) = \left\{\begin{array}{l}0 &amp; \mathrm{if}\; \vert P _ {m&#39;}\vert = 0;\\\frac{1}{\vert P _ {m&#39;}\vert} \sum _ {p\in P _ {m&#39;}} e^{-\frac{\Vert p-\hat{p} _ m\Vert ^2}{\sigma ^ 2}} &amp; \mathrm{if}\; \vert P _ {m&#39;}\vert &gt; 0.\end{array}\tag{3}\right.\]</span> 最终的 bin 值为： <span class="math display">\[ T(m) = T(m,m)+\frac{1}{\vert \mathcal{N} _ m\vert}\sum _ {m&#39;\in\mathcal{N} _ m}T(m,m&#39;) \tag{4}\]</span> 当 \(\sigma ^2\gg 0\) 以及 \(\mathcal{N} _ m=\varnothing\) 时，回退到式 (1) 的离散化方式。本文实验中采用 \(\sigma ^2 = 0.01\)，\(\mathcal{N} _ m=3\times 3\times 3 -1 = 26\)。传统的点云栅格概率密度计算方式为：将点云中的每个点高斯化，然后统计每个栅格中心坐标上覆盖到的值。与上述方法的高斯原点不一样，但是计算结果是一致的。 <img src="/End-to-End-Pseudo-LiDAR-for-3D-Det/quantization.png" width="90%" height="90%" title="图 2. Quantization"> 　　这种方法可将导数反向传播到 \(m'\) 中的每个点：\(\frac{\partial\mathcal{L} _ {det}}{\partial T(m)}\times\frac{\partial T(m)}{\partial T(m,m')}\times\bigtriangledown _ pT(m,m')\)。如图 2. 所示，蓝色 voxel 表示梯度为正，即 \(\frac{\partial\mathcal{L} _ {det}}{\partial T(m)} &gt; 0\)，红色 voxel 表示梯度为负。那么蓝色 voxel 期望没有点，所以将点往外推，红色 voxel 则将点往里拉，最终使点云与 LiDAR 点云，即 GT 点云一致。</p><h3 id="subsampling">2.1. Subsampling</h3><p>　　点云检测模块如果采用 point-based 方案，那么就比较容易直接与深度图网络进行 End-to-End 整合。point-based 3D Detection 一般通过 sampling 来扩大感受野，提取局部信息，因为这种方法的计算量对点数比较敏感，所以 sampling 也是降低计算量的有效手段。一个 \(640\times 480\) 的深度图所包含的点云超过 30 万，远远超过一个 64 线的激光雷达，所以对其进行采样就非常关键。<br>　　本文对深度图点云进行模拟雷达式的采样，即定义球坐标系下栅格化参数：\((r,\theta,\phi)\)。其中 \(\theta\) 为水平分辨率，\(\phi\) 为垂直分辨率。对每个栅格内采样一个点，即可得到一个较为稀疏，且接近激光雷达扫描属性的点云。</p><h2 id="loss">3. Loss</h2><p>　　Loss 由 depth 估计与 3D Detection 两项构成： <span class="math display">\[\mathcal{L} = \lambda _ {det}\mathcal{L} _ {det} + \lambda _ {depth}\mathcal{L} _ {depth} \tag{5}\]</span></p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Mousavian, Arsalan, et al. &quot;3d bounding box estimation using deep learning and geometry.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.<br><a id="2" href="#2ref">[2]</a> Li, Peiliang, Xiaozhi Chen, and Shaojie Shen. &quot;Stereo r-cnn based 3d object detection for autonomous driving.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br><a id="3" href="#3ref">[3]</a> Qian, Rui, et al. &quot;End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection.&quot; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　基于视觉的 3D 目标检测方法因为成本较低，所以在 ADAS 领域应用非常广泛。其基本思路有以下几种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;单目\(\rightarrow\)3D 框，代表文章有&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;[1]&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;单目
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;PointPainting: Sequential Fusion for 3D Object Detection&quot;</title>
    <link href="https://leijiezhang001.github.io/PointPainting/"/>
    <id>https://leijiezhang001.github.io/PointPainting/</id>
    <published>2020-06-17T03:27:38.000Z</published>
    <updated>2020-06-22T09:00:45.632Z</updated>
    
    <content type="html"><![CDATA[<p>　　相机能很好的捕捉场景的语义信息，激光雷达则能很好的捕捉场景的三维信息，所以图像与点云的融合，对检测，分割等任务有非常大的帮助。融合可分为，<strong>数据级或特征级的前融合</strong>，以及<strong>任务级的后融合</strong>。本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种将图像分割结果的语义信息映射到点云，进而作 3D 检测的方法。这种串行方式的融合，既有点前融合的意思，也有点后融合的意思，暂且可归为前融合吧。本方法可认为是个框架，该框架下，基于图像的语义分割，以及基于点云的 3D 检测，均为独立模块。实验表明，融合了图像的语义信息后，点云针对行人等小目标的检测有较大的性能提升。</p><h2 id="framework">1. Framework</h2><p><img src="/PointPainting/framework.png" width="100%" height="100%" title="图 1. Framework"> 　　如图 1. 所示，算法框架非常简单，一句话能说明白：1). 首先经过图像语义分割获得语义图；2). 然后将点云投影到图像上，查询点云的语义信息，并连接到坐标信息中；3). 最后用点云 3D 检测的方法作 3D 检测。</p><h2 id="experiments">2. Experiments</h2><p><img src="/PointPainting/sota.png" width="90%" height="90%" title="图 2. PointPainting Applied to SOTA"> 　　采用 DeepLabv3+ 作为语义分割模块，应用到不同的点云 3D 检测后，结果如图 2. 所示，均有不同程度的提升，尤其是行人这种小目标。 <img src="/PointPainting/pointrcnn.png" width="90%" height="90%" title="图 3. Painted PointRCNN"> 　　图 3. 显示了 Painted PointRCNN 与各个方法的对比结果，mAP 是最高的。 <img src="/PointPainting/per-class.png" width="90%" height="90%" title="图 4. 不同类别的提升程度"> 　　由图 4. 可知，对行人，自行车，雪糕筒等小目标(俯视图下来说)，本方法提升非常显著。这也比较好理解，因为前视图下，这些目标所占的像素会比较多，所以更容易在前视相机图像下提取有效信息，辅助俯视图下作更准确的检测。</p><h2 id="rethinking-of-early-fusion">3. Rethinking of Early Fusion</h2><p>　　这里将本方法归为前融合，但是并不是真正意义上的前融合。如果是前融合，那么一般是 concate 语义分割网络的中低层特征到点云信息中，然而本文是直接取语义分割网络的最高层特征(即分类结果)。<strong>所以问题来了，所谓的前融合，一定比后融合更好吗？</strong>我想，这篇文章可能给了一些答案(不知道作者有没有做过取其它特征的实验，姑且认为做过，然后选择了本方法的策略)，虽然理论上前融合信息最完整，但是，如果这种完整的信息无法有效学出来或者对标定外参比较敏感，那么这种前融合也提升不了后续任务的性能，更有甚者，由于信息空间的变大或紊乱，导致后续任务性能下降。相反，对于不是那么“前”的后融合，我们能极大得保证各个任务学习结果的有效性，基于此，融合后学习的有效性也会比较确定。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Vora, Sourabh, et al. &quot;Pointpainting: Sequential fusion for 3d object detection.&quot; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　相机能很好的捕捉场景的语义信息，激光雷达则能很好的捕捉场景的三维信息，所以图像与点云的融合，对检测，分割等任务有非常大的帮助。融合可分为，&lt;strong&gt;数据级或特征级的前融合&lt;/strong&gt;，以及&lt;strong&gt;任务级的后融合&lt;/strong&gt;。本文&lt;a href=
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Joint 3D Instance Segmentation and Objection Detection for Autonomous Driving&quot;</title>
    <link href="https://leijiezhang001.github.io/Instance-Seg-and-Obj-Det/"/>
    <id>https://leijiezhang001.github.io/Instance-Seg-and-Obj-Det/</id>
    <published>2020-05-22T03:27:38.000Z</published>
    <updated>2020-06-17T02:16:44.008Z</updated>
    
    <content type="html"><![CDATA[<p>　　检测的发展基本上是从 Anchor-based 这种稀疏的方式到 Anchor-free 这种密集检测方案演进的。相比于 Anchor-free 这种特征层像素级别的回归与分类来检测，更密集的方式，是直接作 Instance Segmentation，然后经过聚类等后处理来得到目标框属性。越密集的检测方案，因为样本较多(一定程度增大了样本空间)，所以学习越困难，但是理论上有极高的召回率。随着一系列技术的发展，如 Focal-loss 等，密集检测性能得以超过二阶段的 Anchor-based 方案，具体描述可参考 <a href="/Anchor-Free-Detection/" title="Anchor-Free Detection">Anchor-Free Detection</a>。<br>　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>借鉴 2D Instance Segmentation 思路，提出了一种同时作 3D Instance Segmentation 与 Detection 的方法。百度 Apollo 中的点云分割方法就是俯视图下 Instance Segmentation 然后后处理得到目标 Polygon 与 BBox 的思路，这种方法虽然后处理较为复杂，但是有超参数较少且召回率高的特点。本文算是该方法的 3D 版本(想法很自然，被人捷足先登。。)。</p><h2 id="framework">1. Framework</h2><p><img src="/Instance-Seg-and-Obj-Det/framework.png" width="100%" height="100%" title="图 1. Framework"> 　　如图 1. 所示，本方法由三部分构成：点级别的分类及回归，候选目标聚类，目标框优化。</p><ul><li><strong>点级别的分类与回归</strong><br>原始点云经过 Backbone 网络提取局部及全局特征，这里的 Backbone 网络可以是任意能提取点级别特征的网络。基于 Backbone 网络提取的特征，可进行点级别的 Semantic Segmentation 以及 Instance-aware SE(Spatial Embedding)。SE 回归的是每个点距离目标中心点的 offset，该目标的 size，以及该目标的朝向。</li><li><strong>候选目标聚类</strong><br>基于预测的 SE，将每个点的位置加上距离目标中心点的 offset，然后可通过简单的聚类算法(如 K-means)即可得到各个目标的点云集合，取 top k 个该点云集合回归的目标框属性，作下一步的目标框进一步优化。</li><li><strong>目标框优化</strong><br>基于候选目标聚类得到的目标框，提取目标点集，将其转换到该目标 Local 坐标系下，作进一步的目标框优化。</li></ul><h2 id="instance-aware-se">2. Instance-aware SE</h2><p>　　该框架的关键是 Instance-aware SE 的回归，回归量有：距离目标中心点的 offset，目标 size，目标 orientation。传统的 Instance Segmentation 做法是 Feature Embedding，将相同 Instance 的特征拉近，不同的 Instance 的特征推远，这种方法很难构造有效的 Loss 函数，而且同为车的不同 Instance，其特征已经非常接近。而本文 Spatial Embedding 中 offset 的回归量，经过聚类后处理，可以很容易的得到 Instance Segmentation 结果。<br>　　Apollo 点云分割的方案中，是在俯视图的 2D 栅格下做的，主要回归量也是这三种，不同的是，2D 栅格是离散的，所以根据 offset 找某一点的中心点时，可以迭代的进行，然后投票出中心点位置，后处理可以做的更细致。这里不做展开，有机会以后写一篇详解。</p><h2 id="loss">3. Loss</h2><p>　　Loss 项由 Semantic Segmentation，SE，3D BBox regression 组成： <span class="math display">\[ L = L _ {seg-cls}+L _ {SE}+L _ {reg} \tag{1}\]</span> Semantic Segmentation Loss 为： <span class="math display">\[ L _ {seg-cls}=-\sum _ {i=1}^C (y _ i\mathrm{log}(p _ i)(1-p _ i)^{\gamma}\alpha _ i+(1- y _ i)\mathrm{log}(1-p _ i)(p _ i)^{\gamma}(1-\alpha _ i)) \tag{2}\]</span> 其中 \(C\) 表示类别数；如果某点属于某类，那么 \(y _ i=1\)；\(p _ i\) 表示预测为第 \(i\) 类的概率；\(\gamma,\alpha\) 为超参数。<br>SE Loss 为： <span class="math display">\[ L _ {SE} = \frac{1}{N}\sum _ {i=1}^N\frac{1}{N _ c}\sum _ {i\in ins _ c}^{N _ c}(\mathcal{l} _ {offset}^i+\mathcal{l} _ {size}^i+\mathcal{l} _ {\theta}^i) \tag{3}\]</span> 其中 \(N\) 为 Instance 个数，\(N _ c\) 为内部点数，\(\mathcal{l}\) 为 L1 Smooth Loss。<br>BBox regression Loss 为 rotated 3D IOU Loss： <span class="math display">\[ L _ {reg} = 1-\mathbf{IoU}(B _ g,B _ d)\tag{4}\]</span></p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Zhou, Dingfu, et al. &quot;Joint 3D Instance Segmentation and Object Detection for Autonomous Driving.&quot; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　检测的发展基本上是从 Anchor-based 这种稀疏的方式到 Anchor-free 这种密集检测方案演进的。相比于 Anchor-free 这种特征层像素级别的回归与分类来检测，更密集的方式，是直接作 Instance Segmentation，然后经过聚类等后处
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="Semantic Segmentation" scheme="https://leijiezhang001.github.io/tags/Semantic-Segmentation/"/>
    
  </entry>
  
</feed>
