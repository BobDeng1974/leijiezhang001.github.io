<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LeijieZhang</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://leijiezhang001.github.io/"/>
  <updated>2019-06-11T01:43:51.655Z</updated>
  <id>https://leijiezhang001.github.io/</id>
  
  <author>
    <name>Leijie</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>[paper_reading]-&quot;Stereo R-CNN based 3D Object Detection for Autonomous Driving&quot;</title>
    <link href="https://leijiezhang001.github.io/%5Bpaper_reading%5D-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/"/>
    <id>https://leijiezhang001.github.io/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/</id>
    <published>2019-06-08T06:21:14.000Z</published>
    <updated>2019-06-11T01:43:51.655Z</updated>
    
    <content type="html"><![CDATA[<p>　　Learning 方法有什么致命缺点吗？我认为目前 Learning 方法还存在的较为棘手的问题是，有时候结果会出现非常低级的错误，或是说不可思议不合常理的 cornercases。所以我认为一个工程系统或是一个鲁棒的算法系统，在 Learning 之后做一个基于常理（如 geometry 约束或专家系统）的验证，能有效抑制这个问题。本文就是一个比较好的 learning+geometry 想结合的方法。<br>　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>基于图像语义及几何信息，通过 3D 目标的稀疏与密集约束，提出了一种准确的 3D 目标检测方法。根据输入数据的类型，作者将 3D 检测分为三大类：</p><ul><li>LiDAR-based，近期被研究的较多，基本是自动驾驶所必须的；</li><li>Monocular-based，低成本方案；</li><li>Stereo-based，相比 Monocular-based，有优势，但是研究较少；</li></ul><p>本文就是 Stereo-based 3D 检测方案。不同于一般的 rgb+depth 作为输入的方案，本文直接将左右目 rgb 作为输入，没有显示地 depth 生成过程。工程上来说，这也极大地缩短了 3D Detection 的时延(latency)。<br>　　本文方法如图 1 所示，主要有三部分组成：</p><ol><li>&ensp;Network，又有三部分构成：<ul><li>Stereo RPN Module，输出左右图的 RoI；</li><li>Classification and Regression branches，输出目标类别，朝向，尺寸；</li><li>Keypoint branch，输出左目目标的关键点；</li></ul></li><li>&ensp;Sparse constraints，3D 框-2D 框的稀疏约束；</li><li>&ensp;Dense constraints，准确定位的关键模块；</li></ol><p><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/net_arch.png" width="100%" height="100%" title="图 1. 网络结构"></p><h2 id="1-ensp-Stereo-R-CNN-Network"><a href="#1-ensp-Stereo-R-CNN-Network" class="headerlink" title="1.&ensp;Stereo R-CNN Network"></a>1.&ensp;Stereo R-CNN Network</h2><p>　　Stereo R-CNN 是在 Faster R-CNN 基础上，同时检测与关联左右目图像 2D 框的微小差异。</p><h3 id="1-1-ensp-Stereo-RPN"><a href="#1-1-ensp-Stereo-RPN" class="headerlink" title="1.1.&ensp;Stereo RPN"></a>1.1.&ensp;Stereo RPN</h3><p>　　在传统 RPN 网络的基础上，本文先对左右图做 paramid features 提取，然后将不同尺度的特征 concatenate 一起，进入 RPN 网络。<br><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/target.png" width="60%" height="60%" title="图 2. 真值框定义方式"><br>　　关键的一点是 objectness classification与 stereo box regression 的真值框定义不一样。如图 2 所示，</p><ul><li>对于 objectness classification，真值框定义为左右目真值框的外接合并（union GT box），一个 anchor 在与真值框的交并比（Intersection-over-Union）大于 0.7 时标记为正样本，小于 0.3 时标记为负样本。分类任务的候选框包含了左右目真值框区域的信息。</li><li>对于 stereo box regression，真值框定义为左右目分别的真值框。待回归的参数定义为 \([\Delta u, \Delta w, \Delta u’, \Delta w’, \Delta v, \Delta h]\)，分别为左目的水平位置及宽，右目的水平位置及宽，垂直位置及高。因为输入为矫正过的左右目图像，所以可认为左右目的垂直方向上已经对齐。</li></ul><p>每个左右目的 proposal 都是通过同一个 anchor 产生的，自然左右目的 proposal 是关联的。通过 NMS 后，保留左右目都还存在的 proposal 关联对，取前 2000 个用于训练，测试时取前 300 个。</p><h3 id="1-2-ensp-Stereo-R-CNN"><a href="#1-2-ensp-Stereo-R-CNN" class="headerlink" title="1.2.&ensp;Stereo R-CNN"></a>1.2.&ensp;Stereo R-CNN</h3><p><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/viewpoint.png" width="50%" height="50%" title="图 3. 各角度关系"><br>　　网络头包含两大部分：</p><ol><li>&ensp;<strong>Stereo Regression</strong><br>左右目的 proposal 关联对，分别在左右目的 feature 上进行 RoI Align 的操作，然后 concatenate 输入到全链接层。左右目的 RoI 对与真值框的 IoU 均大于 0.5 时定位正样本，左右目的 RoI 对与真值框的 IoU 有一个小于 0.5 且大于 0.1，则定位负样本。用四个分支分别预测：<ul><li>object class；</li><li>stereo bounding boxes，与 stereo rpn 中一致，左右目的高度已对齐；</li><li>dimension，先统计平均的尺寸，然后预测相对量；</li><li>viewpoint angle，如图 3 所示，\(\theta\) 为相机坐标系下的朝向角，\(\beta\) 为相机中心点下的方位角(azimuth)，这三个目标在相机视野下是一样的，所以我们回归的量是视野角(viewpoint angle) \(\alpha=\theta+\beta\)，其中 \(\beta=arctan\left(-\frac{x}{z} \right) \)。并且为了连续性，回归量为 \([sin\,\alpha,cos\,\alpha]\)。<br><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/keypoints.png" width="70%" height="70%" title="图 4. 语义关键点"></li></ul></li><li>&ensp;<strong>Keypoint Prediction</strong><br>如图 4 所示，考虑 3D 框底部矩形的四个关键点，投影到图像平面后，最多只有一个关键点会在图像 2D 矩形框内。对左目图像进行关键点预测，类似 Mask R-CNN，在 6×28×28 的基础上，因为关键点只有图像坐标 u 方向才提供了额外的信息，所以对每列进行累加，最终输出 6×28 的向量。前 4 个通道代表每个关键点作为 perspective keypoint 投影到该 u 坐标下的概率；后 2 个通道代表该 u 坐标是左右边缘关键点(boundary keypoints)的概率。为了找出 perspective keypoint，softmax 应用于 4×28 的输出上；为了找出左右边缘关键点，softmax 分别应用于后两个 1×28 的输出上。训练的时候，4×28 中只有一个被赋予 perspective keypoint，忽略没有 perspective keypoint 的情况（遮挡等），然后最小化 cross-entropy loss；对于边缘关键点，则分别最小化 1×28 维度上的 cross-entropy loss，前景中也会被赋予边缘关键点。</li></ol><h2 id="2-ensp-3D-Box-Estimation"><a href="#2-ensp-3D-Box-Estimation" class="headerlink" title="2.&ensp;3D Box Estimation"></a>2.&ensp;3D Box Estimation</h2><p><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/projection.png" width="70%" height="70%" title="图 5. 关键点投影关系"><br>　　已知关键点，2D 框，尺寸，朝向角，我们可以求解出 3D 框 \(\{x,y,z,\theta\}\)。求解目标是最小化 3D 框投影到 2D 框以及关键点的误差。如图 5 所示，已知 7 个观测量 \(z = \{u_l,v_t,u_r,v_b,u’_l,u’_r,u_p\}\)，分别代表左目 2D 框的左上坐标，右下坐标，右目 2D 框的左右 u 方向坐标，以及 perspective keypoint 的 u 方向坐标。在图 5 的情况下（其它视角下，注意符号变化），左上点投影关系如下：<br>$$\require{cancel}<br>\begin{bmatrix}<br>u_l\\<br>v_t\\<br>1\\<br>\end{bmatrix}=K\cdot<br>\begin{bmatrix}<br>x_{cam}^{tl}\\<br>y_{cam}^{tl}\\<br>z_{cam}^{tl}\\<br>\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot<br>\begin{bmatrix}<br>x_{obj}^{tl}\\<br>y_{obj}^{tl}\\<br>z_{obj}^{tl}\\<br>\end{bmatrix}=\begin{bmatrix}<br>x\\<br>y\\<br>z\\<br>\end{bmatrix}+<br>\begin{bmatrix}<br>cos\theta &amp; 0 &amp;sin\theta\\<br>0 &amp; 1 &amp; 0\\<br>-sin\theta &amp; 0 &amp; cos\theta\\<br>\end{bmatrix} \cdot<br>\begin{bmatrix}<br>-\frac{w}{2}\\<br>-\frac{h}{2}\\<br>-\frac{l}{2}\\<br>\end{bmatrix}$$<br>其中 \(K\) 为相机内参，\(T_{cam}^{obj}\) 为目标中心坐标系在相机坐标系下的表示，\((\cdot)_{cam/obj}\) 分别为点在相机坐标系，目标中心坐标系下的表示。同样的，这个视野下，右下点为：<br>$$\require{cancel}<br>\begin{bmatrix}<br>u_l\\<br>v_t\\<br>1\\<br>\end{bmatrix}=K\cdot<br>\begin{bmatrix}<br>x_{cam}^{tl}\\<br>y_{cam}^{tl}\\<br>z_{cam}^{tl}\\<br>\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot<br>\begin{bmatrix}<br>x_{obj}^{tl}\\<br>y_{obj}^{tl}\\<br>z_{obj}^{tl}\\<br>\end{bmatrix}=\begin{bmatrix}<br>x\\<br>y\\<br>z\\<br>\end{bmatrix}+<br>\begin{bmatrix}<br>cos\theta &amp; 0 &amp;sin\theta\\<br>0 &amp; 1 &amp; 0\\<br>-sin\theta &amp; 0 &amp; cos\theta\\<br>\end{bmatrix} \cdot<br>\begin{bmatrix}<br>\frac{w}{2}\\<br>\frac{h}{2}\\<br>-\frac{l}{2}\\<br>\end{bmatrix}$$<br>右目两个边缘点以及 perspective keypoint 点也可同样得到，由此可整理出 7 个方程组（论文中第一个公式符号有错）：<br>$$\left\{\begin{array}{l}<br>u_l=(x- \frac{w}{2} cos\theta- \frac{l}{2} sin\theta) / (z+ \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\<br>v_t=(y- \frac{h}{2}) / (z+ \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\<br>u_r=(x+ \frac{w}{2} cos\theta+ \frac{l}{2} sin\theta) / (z- \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)\\<br>v_b=(y+ \frac{h}{2}) / (z- \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)\\<br>u’_l=(x-b- \frac{w}{2} cos\theta- \frac{l}{2} sin\theta) / (z+ \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\<br>u’_r=(x-b+ \frac{w}{2} cos\theta+ \frac{l}{2} sin\theta) / (z- \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)\\<br>u_p=(x+ \frac{w}{2} cos\theta- \frac{l}{2} sin\theta) / (z- \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\<br>\end{array}\right.$$<br>其中 \(b\) 为双目的基线长(baseline)。以上方程组可用 Gauss-Newton 法求解。</p><h2 id="3-ensp-Dense-3D-Box-Alignment"><a href="#3-ensp-Dense-3D-Box-Alignment" class="headerlink" title="3.&ensp;Dense 3D Box Alignment"></a>3.&ensp;Dense 3D Box Alignment</h2><p>　　以上得到的目标 3D 位置是 object-level 求解得到的，利用像素信息，还可以进行优化精确求解。首先在图像 2D 目标框内扣取一块 RoI，要使 RoI 能较为确定的在目标上，扣取方式定义为：</p><ul><li>目标一半以下区域；</li><li>perspective keypoint 与边缘关键点包围区域；</li></ul><p>关键点预测的时候只预测了 u 方向的坐标，边缘关键点无 v 方向的信息，看起来会使某些背景像素被划入为目标像素，更好的方法是加入 instance segmentation 信息。定义误差函数为：<br>$$E=\sum_{i=0}^N e_i=\sum_{i=0}^N \left\| I_l(u_i,v_i)-I_r(u_i-\frac{b}{z+\Delta z_i},v_i)\right\|$$<br>可由三角测量关系 \(z=\frac{bf}{d}\) 推出。上式中，\(\Delta z_i=z_i-z\) 表示某个像素点 \(i\) 所对应的 3D 点与目标中心点之间的距离。最小化总误差即可求得最优的中心点距离 \(z\)。优化过程可以用 coarse-to-fine 的策略，先以 0.5m 的精度找 50 步，再以 0.05m 的精度找 20 次。<br>　　这个 dense alignment 模块是独立的，可以应用到任意的左右目 3D 检测的后处理中。因为目标 RoI 是物理约束，所以这个方法避免了深度估计中不连续、病态的问题，且对光照是鲁棒的，因为每个像素都会对估计起作用。这里，本文只做了中心点的 align，尺寸，甚至朝向角是否能加入优化?</p><h2 id="4-ensp-Other-Details"><a href="#4-ensp-Other-Details" class="headerlink" title="4.&ensp;Other Details"></a>4.&ensp;Other Details</h2><p><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/r1.png" width="110%" height="110%"><br><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/r2.png" width="70%" height="70%"><br><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/r3.png" width="100%" height="100%"><br><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/r4.png" width="90%" height="90%"></p><p><a id="1" href="#1ref">[1]</a> Li, Peiliang, Xiaozhi Chen, and Shaojie Shen. “Stereo R-CNN based 3D Object Detection for Autonomous Driving.” arXiv preprint arXiv:1902.09738 (2019).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Learning 方法有什么致命缺点吗？我认为目前 Learning 方法还存在的较为棘手的问题是，有时候结果会出现非常低级的错误，或是说不可思议不合常理的 cornercases。所以我认为一个工程系统或是一个鲁棒的算法系统，在 Learning 之后做一个基于常理（
      
    
    </summary>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/categories/paper-reading/"/>
    
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving&quot;</title>
    <link href="https://leijiezhang001.github.io/%5Bpaper_reading%5D-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/"/>
    <id>https://leijiezhang001.github.io/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/</id>
    <published>2019-06-08T06:21:14.000Z</published>
    <updated>2019-06-12T10:16:22.478Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>结合 Semantic SLAM 与 Learning-based 3D Det 技术，提出了一种用于自动驾驶的动态目标定位与本车状态估计的方法。本文系统性较强，集成了较多成熟的模块，对工程应用也有较强的指导意义。<br><img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/arch.png" width="100%" height="100%" title="图 1. 语义跟踪系统框架"><br>　　如图 1. 所示，整个系统框架由三部分组成：</p><ul><li>2D object detection and viewpoint classification，目标位姿通过 2D-3D 约束求解出来；</li><li>feature extraction and matching，双目及前后帧的特征提取与匹配；</li><li>ego-motion and object tracking，将语义信息及特征量加入到优化中，并且加入车辆动力学约束以获得平滑的运动估计。</li></ul><h2 id="1-ensp-Viewpoint-Classification-and-3D-Box-Inference"><a href="#1-ensp-Viewpoint-Classification-and-3D-Box-Inference" class="headerlink" title="1.&ensp;Viewpoint Classification and 3D Box Inference"></a>1.&ensp;Viewpoint Classification and 3D Box Inference</h2><h3 id="1-1-ensp-Viewpoint-Classification"><a href="#1-1-ensp-Viewpoint-Classification" class="headerlink" title="1.1.&ensp;Viewpoint Classification"></a>1.1.&ensp;Viewpoint Classification</h3><p>　　选用 Faster R-CNN 作为 2D 检测框架，在此基础上，加入车辆视野（viewpoint）分类分支。由图 2. 所示，水平视野分为八类，垂直视野分为两类，总共 16 类。<br><img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/view.png" width="90%" height="90%" title="图 2. 车辆视野分类"></p><h3 id="1-2-ensp-3D-Box-Inference-Based-on-Viewpoint"><a href="#1-2-ensp-3D-Box-Inference-Based-on-Viewpoint" class="headerlink" title="1.2.&ensp;3D Box Inference Based on Viewpoint"></a>1.2.&ensp;3D Box Inference Based on Viewpoint</h3><p>　　网络输出图像 2D 框以及目标车辆的视野类别（viewpoint），此时我们假设：</p><ul><li>2D 框准确；</li><li>每种车辆的尺寸相同；</li><li>2D 框能紧密包围 3D 框；</li></ul><p>在以上假设条件下，我们可以求得 3D 框，该 3D 框作为后续优化的初始值。约束方程的表示在论文中比较晦涩，在这里我做细致的推倒。 3D 框可表示为 \(\{x,y,z,\theta,w,h,l\}\)，其中 \(\{w,h,l\}\) 分别对应 \(\{x,y,z\}\) 维度。如图 2.(b) 所示，这个视角下，四个 3D 框的顶点，可得四个约束方程。推倒过程为：<br>$$\require{cancel}<br>\begin{bmatrix}<br>u_{min}\\<br>v_1\\<br>1\\<br>\end{bmatrix}=K\cdot<br>\begin{bmatrix}<br>x_{1}^{cam}\\<br>y_{1}^{cam}\\<br>z_{1}^{cam}\\<br>\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot<br>\begin{bmatrix}<br>x_{1}^{obj}\\<br>y_{1}^{obj}\\<br>z_{1}^{obj}\\<br>\end{bmatrix}=\begin{bmatrix}<br>x\\<br>y\\<br>z\\<br>\end{bmatrix}+<br>\begin{bmatrix}<br>cos\theta &amp; 0 &amp;sin\theta\\<br>0 &amp; 1 &amp; 0\\<br>-sin\theta &amp; 0 &amp; cos\theta\\<br>\end{bmatrix} \cdot<br>\begin{bmatrix}<br>\frac{w}{2}\\<br>\frac{h}{2}\\<br>\frac{l}{2}\\<br>\end{bmatrix}$$<br>其中 \(K\) 为相机内参，做归一化处理消去；\(T_{cam}^{obj}\) 为目标中心坐标系在相机坐标系下的表示，\((\cdot)^{cam/obj}\) 分别为点在相机坐标系，目标中心坐标系下的表示。同样的，这个视野下，②，③，④ 点都可以由此获得：<br>$$\left\{\begin{array}{l}<br>\require{cancel}<br>\begin{bmatrix}<br>u_{min}\\<br>v_1\\<br>1\\<br>\end{bmatrix}=K\cdot<br>\begin{bmatrix}<br>x_{1}^{cam}\\<br>y_{1}^{cam}\\<br>z_{1}^{cam}\\<br>\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot<br>\begin{bmatrix}<br>x_{1}^{obj}\\<br>y_{1}^{obj}\\<br>z_{1}^{obj}\\<br>\end{bmatrix}=\begin{bmatrix}<br>x\\<br>y\\<br>z\\<br>\end{bmatrix}+<br>\begin{bmatrix}<br>cos\theta &amp; 0 &amp;sin\theta\\<br>0 &amp; 1 &amp; 0\\<br>-sin\theta &amp; 0 &amp; cos\theta\\<br>\end{bmatrix} \cdot<br>\begin{bmatrix}<br>\frac{w}{2}\\<br>\frac{h}{2}\\<br>\frac{l}{2}\\<br>\end{bmatrix}\\<br>\begin{bmatrix}<br>u_{max}\\<br>v_2\\<br>1\\<br>\end{bmatrix}=K\cdot<br>\begin{bmatrix}<br>x_{2}^{cam}\\<br>y_{2}^{cam}\\<br>z_{2}^{cam}\\<br>\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot<br>\begin{bmatrix}<br>x_{2}^{obj}\\<br>y_{2}^{obj}\\<br>z_{2}^{obj}\\<br>\end{bmatrix}=\begin{bmatrix}<br>x\\<br>y\\<br>z\\<br>\end{bmatrix}+<br>\begin{bmatrix}<br>cos\theta &amp; 0 &amp;sin\theta\\<br>0 &amp; 1 &amp; 0\\<br>-sin\theta &amp; 0 &amp; cos\theta\\<br>\end{bmatrix} \cdot<br>\begin{bmatrix}<br>-\frac{w}{2}\\<br>\frac{h}{2}\\<br>-\frac{l}{2}\\<br>\end{bmatrix}\\<br>\begin{bmatrix}<br>u_3\\<br>v_{min}\\<br>1\\<br>\end{bmatrix}=K\cdot<br>\begin{bmatrix}<br>x_{3}^{cam}\\<br>y_{3}^{cam}\\<br>z_{3}^{cam}\\<br>\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot<br>\begin{bmatrix}<br>x_{3}^{obj}\\<br>y_{3}^{obj}\\<br>z_{3}^{obj}\\<br>\end{bmatrix}=\begin{bmatrix}<br>x\\<br>y\\<br>z\\<br>\end{bmatrix}+<br>\begin{bmatrix}<br>cos\theta &amp; 0 &amp;sin\theta\\<br>0 &amp; 1 &amp; 0\\<br>-sin\theta &amp; 0 &amp; cos\theta\\<br>\end{bmatrix} \cdot<br>\begin{bmatrix}<br>\frac{w}{2}\\<br>-\frac{h}{2}\\<br>-\frac{l}{2}\\<br>\end{bmatrix}\\<br>\begin{bmatrix}<br>u_4\\<br>v_{max}\\<br>1\\<br>\end{bmatrix}=K\cdot<br>\begin{bmatrix}<br>x_{4}^{cam}\\<br>y_{4}^{cam}\\<br>z_{4}^{cam}\\<br>\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot<br>\begin{bmatrix}<br>x_{4}^{obj}\\<br>y_{4}^{obj}\\<br>z_{4}^{obj}\\<br>\end{bmatrix}=\begin{bmatrix}<br>x\\<br>y\\<br>z\\<br>\end{bmatrix}+<br>\begin{bmatrix}<br>cos\theta &amp; 0 &amp;sin\theta\\<br>0 &amp; 1 &amp; 0\\<br>-sin\theta &amp; 0 &amp; cos\theta\\<br>\end{bmatrix} \cdot<br>\begin{bmatrix}<br>-\frac{w}{2}\\<br>\frac{h}{2}\\<br>\frac{l}{2}\\<br>\end{bmatrix}<br>\end{array}\right.$$</p><p>将 \(z\) 方向归一化后，进一步得到最终的四个约束式子：<br>$$\left\{\begin{array}{l}<br>u_{min}=(x+ \frac{w}{2} cos\theta+ \frac{l}{2} sin\theta) / (z- \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)\\<br>u_{max}=(x- \frac{w}{2} cos\theta- \frac{l}{2} sin\theta) / (z+ \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\<br>v_{min}=(y- \frac{h}{2}) / (z- \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\<br>v_{max}=(y+ \frac{h}{2}) / (z+ \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)<br>\end{array}\right.$$<br>以上四个方程可以闭式求解 3D 框 \(\{x,y,z,\theta\}\)。该方法将 3D 框的回归求解分解成了 2D 框回归，视野角分类以及解方程组的过程，强依赖于前面的三点假设，实际情况 3D 框与 2D 框不会贴的很紧。这个 3D 框结果只用来作后续的特征提取区域及最大后验概率估计的初始化。</p><h2 id="2-ensp-Feature-Extraction-and-Matching"><a href="#2-ensp-Feature-Extraction-and-Matching" class="headerlink" title="2.&ensp;Feature Extraction and Matching"></a>2.&ensp;Feature Extraction and Matching</h2><p>　　这一部分做的是左右目及前后帧特征提取及匹配。选用 ORB 特征，目标区域由投影到图像的 3D 框确定。</p><ul><li><strong>目标区域内左右目的立体匹配</strong><br>由于已知目标的距离及尺寸，所以只需要在一定小范围内进行特征点的行搜索匹配。</li><li><strong>目标及背景区域下前后帧的时序匹配</strong><br>首先进行 2D 框的关联，2D 框经过相机旋转补偿后，最小化关联框的中心点距离及框形状相似度值。然后在关联上的目标框区域以及背景区域里，分别作 ORB 特征的匹配，异常值在 RANSAC 下通过基础矩阵测试去除。</li></ul><h2 id="3-ensp-Ego-motion-and-Object-Tracking"><a href="#3-ensp-Ego-motion-and-Object-Tracking" class="headerlink" title="3.&ensp;Ego-motion and Object Tracking"></a>3.&ensp;Ego-motion and Object Tracking</h2><p>　　首先进行本车运动状态估计，可在传统 SLAM 框架下做，不同的是将动态障碍物中的特征点去除。有了本车的位姿后，再估计动态障碍物的运动状态。</p><h3 id="3-1-ensp-Ego-motion-Tracking"><a href="#3-1-ensp-Ego-motion-Tracking" class="headerlink" title="3.1.&ensp;Ego-motion Tracking"></a>3.1.&ensp;Ego-motion Tracking</h3><p>　　给定左目前后帧背景区域特征点的观测，本车状态估计可以通过最大似然估计（Maximum Likelihood Estimation）得到。MLE 可以转化为非线性最小二乘问题，也就是 Bundle Adjustment 过程。</p><h3 id="3-2-ensp-Semantic-Object-Tracking"><a href="#3-2-ensp-Semantic-Object-Tracking" class="headerlink" title="3.2.&ensp;Semantic Object Tracking"></a>3.2.&ensp;Semantic Object Tracking</h3><p>　　得到本车相机的位姿后，运动目标的状态估计可以通过最大后验概率估计（Maximum-a-posterior, MAP）得到。类似的，可转为非线性优化问题进行求解。有四个 loss 项。</p><h3 id="3-2-1-ensp-Vehicle-Motion-Model"><a href="#3-2-1-ensp-Vehicle-Motion-Model" class="headerlink" title="3.2.1.&ensp;Vehicle Motion Model"></a>3.2.1.&ensp;Vehicle Motion Model</h3><p>　　<a href="#2" id="2ref">[2]</a> 中介绍了前转向车的两种模型：运动学模型(Kinematic Bicycle Model)，以及更复杂的动力学模型(Dynamic Bicycle Model)。运动学模型假设车辆不存在滑动，这在大多数情况下都是满足的，所以我们只介绍运动学模型。<br><img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/kinematic.png" width="30%" height="30%" title="图 3. 车辆运动学模型"><br>　　如图 3. 所示，前后轮无滑动的约束下，可得方程组：<br>$$\left\{\begin{array}{rl}<br>\dot{x}_fsin(\theta+\delta)-\dot{y}_fcos(\theta+\delta)=&amp;0\\<br>\dot{x}sin(\theta)-\dot{y}cos(\theta)=&amp;0\\<br>x+Lcos(\theta)=&amp;x_f  \quad\Rightarrow \quad \dot{x}-\dot{\theta}Lsin(\theta)=\dot{x}_f\\<br>y+Lsin(\theta)=&amp;y_f \quad\Rightarrow \quad \dot{y}+\dot{\theta}Lcos(\theta)=\dot{y}_f<br>\end{array}\right.$$<br>由此可得到:<br>$$\dot{x}sin(\theta+\delta)-\dot{y}cos(\theta+\delta)-\dot{\theta}Lcos(\delta)=0$$<br>用 \(\left(v \cdot cos(\theta),v\cdot sin(\theta)\right)\) 代替 \((\dot{x},\dot{y})\) 可得：<br>$$\dot{\theta}=\frac{tan(\delta)}{L}\cdot v$$<br>最终可整理成矩阵形式：<br>$$<br>\begin{bmatrix}<br>\dot{x}\\<br>\dot{y}\\<br>\dot{\theta}\\<br>\dot{\delta}\\<br>\dot{v}\\<br>\end{bmatrix}=<br>\begin{bmatrix}<br>0 &amp;0 &amp;0 &amp;0 &amp;cos(\theta)\\<br>0 &amp;0 &amp;0 &amp;0 &amp;sin(\theta)\\<br>0 &amp;0 &amp;0 &amp;0 &amp;\frac{tan(\delta)}{L}\\<br>0 &amp;0 &amp;0 &amp;0 &amp;0\\<br>0 &amp;0 &amp;0 &amp;0 &amp;0\\<br>\end{bmatrix}<br>\begin{bmatrix}<br>x\\<br>y\\<br>\theta\\<br>\delta\\<br>v\\<br>\end{bmatrix}+<br>\begin{bmatrix}<br>0 &amp;0\\<br>0 &amp;0\\<br>0 &amp;0\\<br>1 &amp;0\\<br>0 &amp;1\\<br>\end{bmatrix}<br>\begin{bmatrix}<br>\gamma\\<br>\alpha\\<br>\end{bmatrix}<br>$$<br>其中 \(L\) 为车辆参数。观测量有：</p><ul><li>\((x,y,\theta)\) 为车辆的位置及朝向角；</li><li>\(\delta\) 为方向盘/车轮转角；</li><li>\(v\) 为车辆速度；</li></ul><p>控制量有：</p><ul><li>\(\gamma\) 为方向盘角度比率；</li><li>\(\alpha\) 为加速度；</li></ul><p>本文的目的是要约束车辆时序上运动(速度及朝向)的平滑一致性，令控制量 \(\gamma,\alpha\) 为 0，然后可得状态量在相邻时刻的关系应满足：<br>$$\left\{\begin{array}{l}<br>\hat{x}^t=x^{t-1}+cos(\theta^{t-1})v^{t-1}\Delta t\\<br>\hat{y}^t=y^{t-1}+sin(\theta^{t-1})v^{t-1}\Delta t\\<br>\hat{\theta}^t=\theta^{t-1}+\frac{tan(\delta^{t-1})}{L}v^{t-1}\Delta t\\<br>\hat{\delta}^t=\delta^{t-1}\\<br>\hat{v}^t=v^{t-1}<br>\end{array}\right.$$<br>由此可整理成论文中矩阵的形式及误差项：<br><img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/15.png" width="80%" height="80%"></p><p><a id="1" href="#1ref">[1]</a> Li, Peiliang, and Tong Qin. “Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving.” Proceedings of the European Conference on Computer Vision (ECCV). 2018.<br><a id="2" href="#2ref">[2]</a> Gu, Tianyu. Improved trajectory planning for on-road self-driving vehicles via combined graph search, optimization &amp; topology analysis. Diss. Carnegie Mellon University, 2017.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　本文&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt;结合 Semantic SLAM 与 Learning-based 3D Det 技术，提出了一种用于自动驾驶的动态目标定位与本车状态估计的方法。本文系统性较强，集成了较多成熟的模块，
      
    
    </summary>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/categories/paper-reading/"/>
    
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
  </entry>
  
  <entry>
    <title>MOT Metrics in Academia and Industry</title>
    <link href="https://leijiezhang001.github.io/MOT-Metrics-in-Academia-and-Industry/"/>
    <id>https://leijiezhang001.github.io/MOT-Metrics-in-Academia-and-Industry/</id>
    <published>2019-06-03T05:47:00.000Z</published>
    <updated>2019-06-10T13:53:32.627Z</updated>
    
    <content type="html"><![CDATA[<p>　　MOT 是一个比较基本的技术模块，在视频监控中，常用于行人行为分析、姿态估计等任务的前序模块；在自动驾驶中，MOT 是动态目标状态估计的重要环节。在学术界，MOT 算法性能的评价准则已经较为完善，其指标主要关注，尽可能地覆盖所有性能维度，以及指标的简洁性（上一篇有较多介绍，<a href="https://leijiezhang001.github.io/MOT-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87-Evaluating-Multiple-Object-Tracking-Performance-the-CLEAR-MOT-Metrics/#more">the CLEAR MOT Metrics</a>）。而工业界则尚无统一的标准，实际的指标需求情况也比学术界复杂。<br>　　指标的计算过程可由三部分组成，真值过滤(Filter)，匹配构建(Establishing Correspondences)与指标计算(Calculating Metrics)。其中真值过滤，更多的是工程细节，学术界没有文章对这一部分进行讨论研究。本文首先介绍学术界各评价指标详情，然后讨论工业界需要的评价指标又是怎样的。</p><h2 id="1-ensp-Metrics-in-Academia"><a href="#1-ensp-Metrics-in-Academia" class="headerlink" title="1.&ensp;Metrics in Academia"></a>1.&ensp;Metrics in Academia</h2><p>　　在学术界，因为数据集质量较高，噪声相对较小，匹配构建中距离的度量偏向于严格且简单的方式。对于区域(框)跟踪器，采用重叠区域来度量；对于点跟踪器，采用中心点的欧式距离来度量。指标汇总如下：<br>A.&ensp;<strong>检测指标</strong><br>　\(\lozenge\)　准确性(Accuracy)</p><ul><li><strong>Recall</strong> = \(\frac{TP}{GT}\)；</li><li><strong>Precision</strong> = \(\frac{TP}{TP+FP}\)；</li><li><strong>FAF/FPPI</strong><a href="#1" id="1ref"><sup>[1]</sup></a><a href="#2" id="2ref"><sup>[2]</sup></a> ，Average False Alarms per Frame；False Positive Per Image;</li><li><strong>MODA</strong><a href="#3" id="3ref"><sup>[3]</sup></a>，Multipe Object Detection Precision，整合了 FN 与 FP，设 \(c_m, c_f\) 分别为 FN，FP 的权重：<br>$$MODA=1-\frac{\sum_{t=1}^{N_frames}(c_m(fn_t)+c_f(fp_t))}{\sum_{t=1}^{N_frames}gt_t}$$</li></ul><p>　\(\lozenge\)　精确性(Precision)</p><ul><li><strong>MODP</strong><a href="#3" id="3ref"><sup>[3]</sup></a>，Multiple Object Detection Accuracy，<br>$$MODP=\frac{\sum_{t=1}^{N_frames} \sum_{i=1}^{N_{mapped}^{(t)}} \;\; dist}{\sum_{t=1}^{N_frames} N_{mapped}^{(t)}}$$<br>其中 \(N_{mapped}^{(t)}\) 为第 \(t\) 帧匹配的目标数；\(dist\) 为距离度量方法，如框的交并比度量法：<br>$$Mapped Overlap Ratio = \frac{\lvert G_i^{(t)}\bigcap D_i^{(t)}\rvert}{|G_i^{(t)}\bigcup D_i^{(t)}|}$$</li></ul><p>B.&ensp;<strong>跟踪指标</strong><br>　\(\lozenge\)　准确性(Accuracy)</p><ul><li><strong>IDS</strong><a href="#4" id="4ref"><sup>[4]</sup></a>，ID switch，a tracked target changes its ID with another target(预测关联真值)；</li><li><strong>MOTA</strong><a href="#5" id="5ref"><sup>[5]</sup></a>，Multiple Object Tracking Accuracy，整合了 FN，FP，ID-Switch：<br>$$MOTA=1-\frac{\sum_{t=1}^{N_{frames}} \;\; (c_m(fn_t)+c_f(fp_t)+c_s(ID-SWITCHES_t))}{\sum_{t=1}^{N_{frames}} \;\; gt_t}$$<br>其中权重方程一般可设为：\(c_m=c_f=1, \quad c_s=log_{10}\)；</li></ul><p>　\(\lozenge\)　精确性(Precision)</p><ul><li><strong>MOTP</strong><a href="#5" id="5ref"><sup>[5]</sup></a>，Multiple Object Tracking Precision，<br>$$MODP=\frac{\sum_{t=1}^{N_frames} \sum_{i=1}^{N_{mapped}^{(t)}} \;\; \left(\frac{\lvert G_i^{(t)}\bigcap D_i^{(t)}\rvert}{|G_i^{(t)}\bigcup D_i^{(t)}|} \right)}{\sum_{t=1}^{N_frames} N_{mapped}^{(t)}}$$</li><li><strong>TDE</strong><a href="#6" id="6ref"><sup>[6]</sup></a>，Distance between the ground-truth annotation and the tracking result；像素级别的误差计算，适用于人群跟踪；</li><li><strong>OSPA</strong><a href="#7" id="7ref"><sup>[7]</sup></a><a href="#8" id="8ref"><sup>[8]</sup></a>，Optimal Subpattern assignment，由定位 (localization) 误差及基数 (cardinality) 误差构成，对于第 \(t\) 帧：<br>$$e^t=\left[\frac{1}{n^t}\left( \mathop{\min}_{\pi\in\Pi_n} \sum_{i=1}^{m^t} d^{(c)}(x_i^t,y_{\pi(i)}^t)^p + (n^t-m^t)\cdot c^p \right) \right]^{1/p}$$<br>其中，\(n^t\) 为目标真值与算法输出中数量较大者。\(\Pi_n\) 为从 \(n^t\) 中取出的 \(m\) 个目标。\(p\) 为距离指数范数。其中定位截断误差为：<br>$$d^{(c)}(x_i^t,y_{\pi(i)}^t) = \mathop{\min}\left(c,d(x_i^t,y_{\pi(i)}^t)\right)$$<br>\(c\) 为截断参数。定位误差又由距离误差和标签误差组成：<br>$$d(x_i^t,y_{\pi(i)}^t=\parallel x_i^t-y_{\pi(i)}^t\parallel + \alpha \; \bar{\delta}(l_x, l_y)$$<br>其中 \(\alpha\in[0,c]\)，为标签误差的权重系数。如果 \(l_x=l_y\)，\(\bar{\delta}(l_x, l_y)=0\)，否则 \(\bar{\delta}(l_x, l_y)=1\).</li></ul><p>　\(\lozenge\)　完整性(Completeness)</p><ul><li><strong>MT</strong><a href="#9" id="9ref"><sup>[9]</sup></a>，Mostly Tracked，真值轨迹长度被跟踪大于80%的比例；</li><li><strong>ML</strong><a href="#9" id="9ref"><sup>[9]</sup></a>，Mostly Lost，真值轨迹长度被跟踪小于20%的比例；</li><li><strong>PT</strong><a href="#9" id="9ref"><sup>[9]</sup></a>，Partially Tracked，\(1-MT-ML\);</li><li><strong>FM</strong><a href="#9" id="9ref"><sup>[9]</sup></a>，Fragments，ID of a target changed along a GT trajectory, or no ID(真值关联预测)；</li></ul><p>　\(\lozenge\)　鲁棒性(Robustne)</p><ul><li><strong>RS</strong><a href="#10" id="10ref"><sup>[10]</sup></a>，Recover from short term occlusion;</li><li><strong>RL</strong><a href="#10" id="10ref"><sup>[10]</sup></a>，Recover from long term occlusion;</li></ul><h2 id="2-ensp-Metrics-in-Industry"><a href="#2-ensp-Metrics-in-Industry" class="headerlink" title="2.&ensp;Metrics in Industry"></a>2.&ensp;Metrics in Industry</h2><p>　　工业界的数据噪声较大，传感器配置也比较多样，不同的产品（传感器+算法），对 MOT 性能维度要求也不一样。更重要的是，评价指标应该从功能层面进行定义，在模块层面 (MOT) 进行调整及细化。可以说，工业界是以学术界为基础来设计 MOT 指标的，不同的产品没有统一的标准，但有比较通用的设计准则。<br>　　这里以自动驾驶/辅助驾驶中动态目标状态估计模块为例，模块详细分析<a href>日后再写</a>。该模块的基本输入为：</p><ul><li><strong>传感器数据</strong>，可以是图像，激光等；</li><li><strong><em>自定位系统</em></strong>，可以是基于视觉的 VO，基于视觉-IMU 的 VINS等；<br>其中自定位系统能使目标状态估计在世界坐标系（惯性系）下优化，否则只能在本体（ego）非惯性系下优化，会减少一些约束量。该功能的基本输出为：</li><li><strong>位置</strong>，本体坐标系下目标的三维位置，\(x,y,z\)；</li><li><strong>尺寸</strong>，目标的物理尺寸大小，包括立方体的长宽高；或者图像坐标系下的像素大小；或者图像/点云下目标的 mask，即分割后的目标；</li><li><strong><em>朝向</em></strong>，一般只考虑目标的航向角；</li><li><strong>速度</strong>，本体坐标系或世界坐标系下的三维速度，一般只考虑航向平面的速度；  </li></ul><p>其中朝向是非必须项，有了朝向后，能更有效地进行状态优化。该模块的子模块有（注意，MOT 只包含前三者）：</p><ul><li><strong>检测(Detection)</strong>，进行多目标检测；</li><li><strong>跟踪(Tracking)</strong>，根据上一帧结果，进行多目标跟踪；</li><li><strong>数据关联(Association)</strong>，检测结果与跟踪结果的融合，出目标的 tracklets，生成 ID；</li><li><strong>状态估计(State Estimation)</strong>，不同的方法包括不同的部分；  </li></ul><p>　　工业界设计产品时，基本遵循自顶向下的策略：产品需求-功能需求-模块需求，层层推倒。所以我们设计评价准则时，一般会问几个问题：</p><ul><li>该模块服务的产品功能，其需求及对应的指标是什么？</li><li>要达到功能指标，本模块的输出需要哪些指标来评测？</li><li>各个子模块对模块的影响是怎样的，对应需要增加哪些指标？  </li></ul><p>这里提到了功能指标，模块指标，子模块指标三层概念。功能指标及部分模块指标是可以写入产品手册的，所以需要突出重点，易于理解；部分模块及子模块指标则主要是为了产品上工程优化迭代，这就要求这部分指标要相当细致，将模块的不足尽可能解耦，且完全暴露出来。以下通过两个例子来分析设计过程。</p><h3 id="2-1-ensp-ADAS-中的-FCW-功能"><a href="#2-1-ensp-ADAS-中的-FCW-功能" class="headerlink" title="2.1.&ensp;ADAS 中的 FCW 功能"></a>2.1.&ensp;ADAS 中的 FCW 功能</h3><p>　　FCW 基本功能要求为：</p><ul><li>不允许误报，尽可能不漏报；</li><li>在 V km/h 下，以一定的刹车加速度 a，能避免与静止的前车相碰撞；  </li></ul><p>　　由以上两个功能需求，可确定必须的功能指标：</p><ul><li>（百公里）误报率；</li><li>（百公里）漏报率；</li><li>观测距离，可由第二项功能要求推到出（人反应时间已知）；  </li></ul><p>　　相应的 MOT +状态估计模块输出的指标为<strong>各距离维度各类别维度</strong>下的：</p><ul><li>误检率；</li><li>漏检率；</li><li>ID Switch；</li><li>定位精度；</li><li>速度估计精度；  </li></ul><p>　　其中 MOT 主要涉及误检率，漏检率，ID Switch（直接影响状态估计模块）。这些指标的计算方式可以在学术界定义的基础上做进一步改进，比如漏检率，就需要体现出百公里漏报率的性能，所以可以考虑将连续 N 帧漏检的目标才归为漏检，分母可以定义为每多少帧。此外，要在各距离维度各类别维度下进行计算，这就涉及到过滤（filter）策略。对于 FCW 而言，首要关注的是本车前方近距离位置，距离维度上的功能重要程度要突显出来，类别维度也要区别对待，以便算法模块可以重点优化。</p><h3 id="2-2-ensp-自动驾驶中的动态障碍物检测功能"><a href="#2-2-ensp-自动驾驶中的动态障碍物检测功能" class="headerlink" title="2.2.&ensp;自动驾驶中的动态障碍物检测功能"></a>2.2.&ensp;自动驾驶中的动态障碍物检测功能</h3><p>　　自动驾驶中动态障碍物检测的要求就高了，子模块也较为复杂，指标除了评估功能模块的性能，还需要指导迭代各子模块算法，包括本子模块的迭代比较，以及上下游模块相关指标的对比。<br>　　功能需求，我们简单列举几项：</p><ul><li>不允许漏检，尽可能不误检；</li><li>前向，后向，侧向观测距离分别要达到 x, y, z；  </li></ul><p>　　相应的功能指标为：</p><ul><li>漏检率；</li><li>误检率；</li><li>观测距离；</li><li>观测精度；</li><li>观测时延(delay)；  </li></ul><p>　　MOT +状态估计模块输出的指标依然在<strong>各距离维度各类别维度</strong>下：</p><ul><li>误检率；</li><li>漏检率；</li><li>ID Switch；</li><li>定位精度；</li><li>尺寸，朝向，速度估计精度；</li><li>状态估计收敛时间；</li><li>一系列描述时序稳定性的指标；  </li></ul><p>　　与前述 FCW 功能类似，只是多了较多的指标。过滤操作也做的更加细致，我们还可以将目标做重要性等级划分，比如本车道前车多少米内，那指标基本都要达到 99%+；还可以将地面区域做重要性划分（比距离维度更加细致，可以认为是三维层面），周围几米内，那误检率肯定要非常低。除了过滤策略需要仔细设计外，匹配策略也需要进一步思考。如果传感器本身精度就有限，那么匹配策略就要相应放宽。还需注意的是引入过滤策略后，FP与FN计算的细微差别，比如有个过滤条件为去除目标像素面积小于一定阈值的目标集 A，观测值与真值匹配时，如果与 A 中的目标匹配上，那么不应该记为 FP，如果没匹配上 A 中的目标，那么 A 中地目标也不应该被记为 FN。这种类似的情况逻辑要思考清楚。</p><h2 id="3-ensp-Summary"><a href="#3-ensp-Summary" class="headerlink" title="3.&ensp;Summary"></a>3.&ensp;Summary</h2><p>　　以上设计的出发点是，我们要承认<strong>算法的不完美性</strong>以及<strong>传感器的局限性</strong>，在工程领域，一定要首先解决主要矛盾，再打磨细节。本文还对以下内容未作进一步分析（以后有机会再写文细究）：</p><ul><li>状态估计时序相关指标，描述估计的时序稳定性，也可以用于 MOT 的评估；</li><li>标注与过滤策略的关系，过滤策略往往依赖于标注策略；</li><li>各个指标的阈值确定，确定阈值也是产品中一件重要而又系统的事，有时候比指标设计更复杂；　　</li></ul><p><a id="1" href="#1ref">[1]</a> Yang B, Huang C, Nevatia R. <a href="https://scholar.google.com/scholar?lookup=0&amp;q=Learning+affinities+and+dependencies+for+multi-target+tracking+using+a+CRF+model&amp;hl=zh-CN&amp;as_sdt=0,5&amp;as_vis=1" target="_blank" rel="noopener">Learning affinities and dependencies for multi-target tracking using a CRF model</a>[C]//CVPR 2011. IEEE, 2011: 1233-1240.<br><a id="2" href="#2ref">[2]</a> Choi W, Savarese S. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=Multiple+target+tracking+in+world+coordinate+with+single%2C+minimally+calibrated+camera&amp;btnG=" target="_blank" rel="noopener">Multiple target tracking in world coordinate with single, minimally calibrated camera</a>[C]//European Conference on Computer Vision. Springer, Berlin, Heidelberg, 2010: 553-567.<br><a id="3" href="#3ref">[3]</a> Kasturi, Rangachar, et al. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=Framework+for+performance+evaluation+of+face%2C+text%2C+and+vehicle+detection+and+tracking+in+video%3A+Data%2C+metrics%2C+and+protocol&amp;btnG=" target="_blank" rel="noopener">Framework for performance evaluation of face, text, and vehicle detection and tracking in video: Data, metrics, and protocol</a> IEEE transactions on Pattern Analysis and Machine intelligence 31.2 (2008): 319-336.<br><a id="4" href="#4ref">[4]</a> Yamaguchi K, Berg A C, Ortiz L E, et al. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=who+are+you+with+and+where+are+you+going&amp;btnG=" target="_blank" rel="noopener">Who are you with and where are you going?</a>[C]//CVPR 2011. IEEE, 2011: 1345-1352.<br><a id="5" href="#5ref">[5]</a> Bernardin K, Stiefelhagen R. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=evaluating+multiple+object+tracking+performance+the+clear+mot+metrics&amp;btnG=" target="_blank" rel="noopener">Evaluating multiple object tracking performance: the CLEAR MOT metrics</a>[J]. Journal on Image and Video Processing, 2008, 2008: 1.<br><a id="6" href="#6ref">[6]</a> Kratz L, Nishino K. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=%E2%80%9CTracking+with+local+spatio-temporal+motion+patterns+in+extremely+crowded+scenes&amp;btnG=" target="_blank" rel="noopener">Tracking with local spatio-temporal motion patterns in extremely crowded scenes</a>[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE, 2010: 693-700.<br><a id="7" href="#7ref">[7]</a> Ristic B, Vo B N, Clark D, et al. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=a+metric+for+performance+evaluation+of+multi-target+tracking+algorithms&amp;btnG=" target="_blank" rel="noopener">A metric for performance evaluation of multi-target tracking algorithms</a>[J]. IEEE Transactions on Signal Processing, 2011, 59(7): 3452-3457.<br><a id="8" href="#8ref">[8]</a> Schuhmacher D, Vo B T, Vo B N. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=A+Consistent+Metric+for+Performance+Evaluation+of+Multi-Object+Filters&amp;btnG=" target="_blank" rel="noopener">A consistent metric for performance evaluation of multi-object filters</a>[J]. IEEE transactions on signal processing, 2008, 56(8): 3447-3457.<br><a id="9" href="#9ref">[9]</a> Li Y, Huang C, Nevatia R. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=Learning+to+associate%3A+Hybridboosted+multi-target+tracker+for+crowded+scene&amp;btnG=" target="_blank" rel="noopener">Learning to associate: Hybridboosted multi-target tracker for crowded scene</a>[C]//2009 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2009: 2953-2960.<br><a id="10" href="#10ref">[10]</a> Song B, Jeng T Y, Staudt E, et al. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=A+stochastic+graph+evolution+framework+for+robust+multi-target+tracking&amp;btnG=" target="_blank" rel="noopener">A stochastic graph evolution framework for robust multi-target tracking</a>[C]//European Conference on Computer Vision. Springer, Berlin, Heidelberg, 2010: 605-619.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　MOT 是一个比较基本的技术模块，在视频监控中，常用于行人行为分析、姿态估计等任务的前序模块；在自动驾驶中，MOT 是动态目标状态估计的重要环节。在学术界，MOT 算法性能的评价准则已经较为完善，其指标主要关注，尽可能地覆盖所有性能维度，以及指标的简洁性（上一篇有较多介
      
    
    </summary>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/categories/MOT/"/>
    
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/tags/MOT/"/>
    
      <category term="tracking" scheme="https://leijiezhang001.github.io/tags/tracking/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
  </entry>
  
  <entry>
    <title>MOT 评价指标-&quot;Evaluating Multiple Object Tracking Performance, the CLEAR MOT Metrics&quot;</title>
    <link href="https://leijiezhang001.github.io/MOT-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87-Evaluating-Multiple-Object-Tracking-Performance-the-CLEAR-MOT-Metrics/"/>
    <id>https://leijiezhang001.github.io/MOT-评价指标-Evaluating-Multiple-Object-Tracking-Performance-the-CLEAR-MOT-Metrics/</id>
    <published>2019-06-02T06:23:46.000Z</published>
    <updated>2019-06-05T14:32:36.448Z</updated>
    
    <content type="html"><![CDATA[<p>　　这篇文章介绍了两个综合性指标 MOTA 以及 MOTP 的计算过程，这两个指标有优劣势，但是作为综合性指标至今在学术界仍广泛应用。本文主要介绍其设计思想及计算过程。<br>　　一个理想的 MOT 算法，我们期望每一帧：</p><ul><li>准确检测目标的数量；</li><li>准确估计每个目标的状态，如位置，朝向，速度等；</li><li>准确估计每个目标的轨迹，即目标的 ID 不变性；  </li></ul><p>这就要求评价准则：</p><ul><li>能评估目标定位的精度；</li><li>能反映目标轨迹的追踪能力，即同一个目标产生唯一的 ID；  </li></ul><p>此外，为了提高评价准则的实用性：</p><ul><li>参数尽可能少，阈值可调；</li><li>易于理解，表现方式符合人们的直觉；</li><li>有较强的通用性，能评估各种跟踪算法；</li><li>指标个数少，但是能足够反映算法不同维度的性能；  </li></ul><p>假设第 \(t\) 帧，有目标集 \(\{o_1,…,o_n\}\)，跟踪算法的输出(hypotheses)：\(\{h_1,…h_m\}\)。根据上述设计准则，设计评价计算过程：</p><ol><li>&ensp;构建 \(h_j\) 与 \(o_i\) 的最优匹配；</li><li>&ensp;对于每个匹配对，计算位置估计误差；</li><li>&ensp;累加所有匹配对的误差，包括：<br>a. &ensp;计算漏检数(FN)；<br>b. &ensp;计算误检数(FP)；<br>c. &ensp;计算 ID swith 次数，包括两个邻近目标的 ID 交换，以及遮挡后，同一目标的 ID 跳变；  </li></ol><p>由此可得到两大指标：</p><ul><li>tracking precision，目标位置的估计精度；</li><li>tracking accuracy，包括 misses(FN), FP, mismatches(IDs), failures to recover；  </li></ul><p>下面分两块做细节分析，匹配构建 (Establishing Correspondences) 与评价指标 (Metrics)。</p><h2 id="1-ensp-匹配构建"><a href="#1-ensp-匹配构建" class="headerlink" title="1.&ensp;匹配构建"></a>1.&ensp;匹配构建</h2><p>　　算法估计与目标真值的匹配，大致还是基于匹配最近 object-hypothesis 的思想，没匹配上的估计就是 FP，没匹配上的真值就是 FN。但是这中间需要进一步考虑一些问题。</p><h3 id="1-1-ensp-有效匹配"><a href="#1-1-ensp-有效匹配" class="headerlink" title="1.1.&ensp;有效匹配"></a>1.1.&ensp;有效匹配</h3><p>　　如果算法估计 \(h_j\) 与目标 \(o_i\) 的最近距离 \(dist_{i,j}\) 超过了一定的阈值 \(T\)，那么这个匹配也是不合理的，因为这个距离误差加入到定位误差中是不合理的，所以只能说这个跟踪的结果不是这个目标。关于距离的度量：</p><ul><li>区域（框）跟踪器，距离可用两者的重叠区域来度量，\(T\) 可以设为 0；</li><li>点跟踪器，距离可用两者中心点的欧氏距离来度量，\(T\) 可以根据目标的尺寸来设定；</li></ul><h3 id="1-2-ensp-跟踪一致性"><a href="#1-2-ensp-跟踪一致性" class="headerlink" title="1.2.&ensp;跟踪一致性"></a>1.2.&ensp;跟踪一致性</h3><p>　　统计目标与算法输出的匹配跳变的次数，也就是目标 ID 的跳变数。文章还提到，当目标有两个有效地匹<br>配时，选择之前的匹配，即使那个匹配的距离大于另一个匹配，这点当存在两个很近的目标时，可能会有问题，需要全局来看。</p><h3 id="1-3-ensp-匹配过程"><a href="#1-3-ensp-匹配过程" class="headerlink" title="1.3.&ensp;匹配过程"></a>1.3.&ensp;匹配过程</h3><ol><li>&ensp;对 \(t\) 帧，考虑 \(M_{t-1}\) 中所有匹配是否还依然有效，包括目标真值及算法输出是否还存在，如果都存在，那么距离是否超出阈值 \(T\)；</li><li>&ensp;对于剩下的没找到匹配的真值目标，在唯一匹配以及阈值约束下，可采用匹配算法或者贪心算法来求解，使得距离误差的总和最小（文章的意思是排除了从上一帧继承的已有匹配，当目标密集时，这部分也应该加入进来优化）。统计当前帧目标真值匹配的跳变数 \(mme_t\)，作为 mismatch errors；</li><li>&ensp;经过之前两步后，找到了所有的匹配，统计匹配个数为 \(c_t\)，计算匹配上的目标真值与算法输出的定位误差 \(d_t^i\)；</li><li>&ensp;统计没有匹配上的算法输出 (hypotheses) 为 \(fp_t\)，没有匹配上的目标真值为 \(m_t\)，目标真值个数为 \(g_t\)；</li><li>&ensp;每一帧重复步骤１，第一帧没有 mismatch；</li></ol><h2 id="2-ensp-评价指标"><a href="#2-ensp-评价指标" class="headerlink" title="2.&ensp;评价指标"></a>2.&ensp;评价指标</h2><p>　　基于以上的匹配策略，得出两个合理的指标：</p><ul><li><strong>MOTP</strong>(multiple object tracking precision)，跟踪定位精度指标：$$MOTP=\frac{\sum_{i,t}d_t^i}{\sum_tc_t}$$</li><li><strong>MOTA</strong>(multiple object tracking accuracy)，综合了漏检率，误检率，以及 ID 跳变率：$$MOTA=1-\frac{\sum_t(m_t+fp_t+mme_t)}{\sum_tg_t}$$</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　这篇文章介绍了两个综合性指标 MOTA 以及 MOTP 的计算过程，这两个指标有优劣势，但是作为综合性指标至今在学术界仍广泛应用。本文主要介绍其设计思想及计算过程。&lt;br&gt;　　一个理想的 MOT 算法，我们期望每一帧：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;准确检测目标的数量；&lt;/
      
    
    </summary>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/categories/paper-reading/"/>
    
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/tags/MOT/"/>
    
      <category term="tracking" scheme="https://leijiezhang001.github.io/tags/tracking/"/>
    
  </entry>
  
  <entry>
    <title>MOT 综述-&#39;Multiple Object Tracking: A Literature Review&#39;</title>
    <link href="https://leijiezhang001.github.io/MOT-%E7%BB%BC%E8%BF%B0-Multiple-Object-Tracking-A-Literature-Review/"/>
    <id>https://leijiezhang001.github.io/MOT-综述-Multiple-Object-Tracking-A-Literature-Review/</id>
    <published>2019-05-28T08:22:58.000Z</published>
    <updated>2019-06-05T15:17:36.085Z</updated>
    
    <content type="html"><![CDATA[<p>　　之前做 MOT 还是沿着 SOT 的思路，这篇文章对 MOT 有一个很深入且很有框架性的综述，以下对这篇文章做一个提炼，并加入一些自己的想法。<br>　　MOT 作为一个中层任务，是一些高层任务的基础，比如行人的 pose estimation，action recognition，behavior analysis，车辆的 state estimation。单目标跟踪(SOT)主要关注 appearance model 以及 motion model 的设计，解决尺度、旋转、光照等影响因素。而 MOT 包含两个任务：目标数量以及目标ID，这就要求 MOT 还需要解决其它问题：</p><ul><li>frequent occlusions</li><li>initialization and termination of tracks</li><li>similar appearance</li><li>interactions among multiple objects</li></ul><h2 id="1-ensp-问题描述"><a href="#1-ensp-问题描述" class="headerlink" title="1.&ensp;问题描述"></a>1.&ensp;问题描述</h2><p>　　多目标跟踪实际上是多参数估计问题。给定图像序列\(\{I_1,I_2,…,I_t,…\}\)，第\(t\)帧中目标个数为\(M_t\)，第\(t\)帧中所有目标的状态表示为\(S_t=\{s_t^1,s_t^2,…,s_t^{M_t}\}\)，第\(i\)个目标的轨迹表示为\(s_{1:t}^i=\{s_1^i,s_2^i,…,s_t^i\}\)，所有图像中所有目标的状态序列为\(S_{1:t}=\{S_1,S_2,…,S_t\}\)。相应的，所有图像中所有目标观测到的状态序列为\(O_{1:t}=\{O_1,O_2,…,O_t\}\)。多目标跟踪的优化目标是求解最优的各目标状态，即求解一个后验概率问题，$$ \widehat{S} _ {1:t}=\mathop{\arg\max}_{S_{1:t}}P(S_{1:t}|O_{1:t})$$<br>这种形式有两种实现方法：</p><ul><li><strong>probabilistic inference</strong><br>适合用于 online tracking 任务，Dynamic Model 为 \(P(S_t|S_{t-1})\)，Observation Model 为 \(P(O_t|S_t)\)，两步求解过程：<br>　\(\circ\)　Predict: \(P(S_t|O_{1:t-1})=\int P(S_t|S_{t-1})dS_{t-1}\)<br>　\(\circ\)　Update: \(P(S_t|O_{1:t}) \propto P(O_t|S_t)P(S_t|O_{1:t-1})\)</li><li><strong>deterministic optimization</strong><br>适合用于 offline tracking 任务，直接利用多帧信息进行最优化求解。</li></ul><h2 id="2-ensp-分类方法"><a href="#2-ensp-分类方法" class="headerlink" title="2.&ensp;分类方法"></a>2.&ensp;分类方法</h2><ul><li><strong>initialization method</strong><br>初始化方式分为：<br>　\(\circ\)　Detection-Based Tracking，优势明显，除了只能处理特定的目标类型；<br>　\(\circ\)　Detection-Free Tracking，能处理任何目标类型；</li><li><strong>processing mode</strong><br>根据是否使用未来的观测，处理方式可分为：<br>　\(\circ\)　online tracking，适合在线任务，缺点是观测量会比较少；<br>　\(\circ\)　offline tracking，输出结果存在时延，理论上能获得全局最优解；</li><li><p><strong>type of output</strong><br>根据问题求解方式输出是否存在随机性：<br>　\(\circ\)　probabilistic inference，概率性推断；<br>　\(\circ\)　deterministic inference，求解最大后验概率；</p><p><strong>自动驾驶等在线任务主要关注 Detection-Based，online tracking。</strong></p></li></ul><h2 id="3-ensp-框架"><a href="#3-ensp-框架" class="headerlink" title="3.&ensp;框架"></a>3.&ensp;框架</h2><p>　　MOT 主要考虑两个问题：</p><ul><li>目标在不同帧之间的相似性度量，即对appearance, motion, interaction, exclusion, occlusion的建模；</li><li>恢复出目标的ID，即 inference 过程；  </li></ul><h3 id="3-1-ensp-Appearance-Model"><a href="#3-1-ensp-Appearance-Model" class="headerlink" title="3.1.&ensp;Appearance Model"></a>3.1.&ensp;Appearance Model</h3><h4 id="3-1-1-ensp-Visual-Representation"><a href="#3-1-1-ensp-Visual-Representation" class="headerlink" title="3.1.1.&ensp;Visual Representation"></a>3.1.1.&ensp;Visual Representation</h4><p>　　视觉表达即目标的特征表示方式：</p><ol><li><strong>local features</strong><br>本质上是点特征，点特征由 corner+descriptor(角点+描述子) 组成。KLT(good features to track)在 SOT 中应用广泛，用它可以生成短轨迹，估计相机运动位姿，运动聚类等；Optical Flow也是一种局部特征，在数据关联之前也可用于将检测目标连接到短轨迹中去。</li><li><strong>region features</strong><br>在一个块区域内提取特征，根据像素间作差的次数，可分为：<ul><li>zero-order, color histogram &amp; raw pixel template</li><li>first-order, HOG &amp; level-set formulation(?)</li><li>up-to-second-order, Region covariance matrix</li></ul></li><li><strong>others</strong><br>其它特征本质上也需要 local 或 region 的方式提取，只是原始信息并不是灰度或彩图。如 depth,probabilistic occupancy map, gait feature.  </li></ol><p>　　Local features，比如颜色特征，在计算上比较高效，但是对遮挡，旋转比较敏感；Region features 里，HOG 对光照有一定的鲁棒性，但是对遮挡及形变效果较差；Region covariance matrix 更加鲁棒，但是需要更高的计算量；深度特征也比较有效，但是需要额外的获取深度信息的代价。</p><h4 id="3-1-2-ensp-Statistical-Measuring"><a href="#3-1-2-ensp-Statistical-Measuring" class="headerlink" title="3.1.2.&ensp;Statistical Measuring"></a>3.1.2.&ensp;Statistical Measuring</h4><p>　　有了目标的特征表示方式之后，就可以评价两个观察的目标的相似性。特征表示的线索(cue)可分为：</p><ol><li><strong>single cue</strong><br>因为只有一个线索，相似性(similarity)可以直接通过两个向量的距离转换得到。可以将距离指数化，高斯化。也可以将不相似度转为可能性，用协方差矩阵表示。</li><li><strong>multiple cues</strong><br>多线索，即多种特征的融合，能极大提高鲁棒性，融合的策略有：<ul><li>Boosting, 选取一系列的特征，用 boost 算法选取表达能力最强的特征；</li><li>Concatenation, 各个特征直接在空间维度上串起来，形成一个 cue 的表达方式；</li><li>Summation, 加权融合各个特征，形成一个 cue 的表达方式；</li><li>Product, 各个特征相乘的方式，比如目标 \(s_0\) 的某个潜在匹配 \(s_1\) 的颜色，形状特征为 \(color\), \(shape\) 的概率为 \(p(color|s_0)\), \(p(shape|s_0)\), 假设特征独立，那么，<br>　　　　　　　$$p(s_1|s_0)=p(color, shape|s_0)=p(color|s_0)\cdot p(shape|s_0)$$</li><li>Cascading, coarse-to-fine 的方式，逐步精细化搜索；</li></ul></li></ol><h3 id="3-2-ensp-Motion-Model"><a href="#3-2-ensp-Motion-Model" class="headerlink" title="3.2.&ensp;Motion Model"></a>3.2.&ensp;Motion Model</h3><p>　　运动模型对关联两个 tracklets 比较管用，而 online tracking 任务，对输出的时延要求较高，所以其中一个 tracklet 可以任务就是当前帧与上一帧形成的轨迹，所以这里很难去计算两个 tracklets 的相似度。能看到的一个应用点就是，通过 motion model 模型，预测下一时刻目标的位置，作为一个线索项目。以下讨论的各模型主要是为了度量 tracklets 的相似性，从而做 tracklets 的匹配。  </p><h4 id="3-2-1-ensp-Linear"><a href="#3-2-1-ensp-Linear" class="headerlink" title="3.2.1.&ensp;Linear"></a>3.2.1.&ensp;Linear</h4><ul><li>Velocity Smoothness. N 帧 M 个目标轨迹: \(C_{dyn}=\sum_{t=1}^{N-2}\sum_{i=1}^{M}\parallel v_i^t-v_i^{t+1}\parallel^2\)</li><li>Position Smoothness. \(G(p^{tail}+v^{tail}\Delta t-p^{head}, \sum_p)\cdot G(p^{head}-v^{head}\Delta t-p^{tail}, \sum_p)\)</li><li>Acceleration Smoothness.  </li></ul><h4 id="3-2-2-ensp-Non-linear"><a href="#3-2-2-ensp-Non-linear" class="headerlink" title="3.2.2.&ensp;Non-linear"></a>3.2.2.&ensp;Non-linear</h4><p>　　运动模型假设是非线性的，相似度计算还是按照以上高斯形式。引为中提到，非线性运动模型并不作为目标的惩罚因子，因为目标并不需要满足该模型，但是只要有目标满足，就降低惩罚系数。</p><h3 id="3-3-ensp-Interaction-Model"><a href="#3-3-ensp-Interaction-Model" class="headerlink" title="3.3.&ensp;Interaction Model"></a>3.3.&ensp;Interaction Model</h3><h4 id="3-3-1-ensp-Social-Force-Models"><a href="#3-3-1-ensp-Social-Force-Models" class="headerlink" title="3.3.1.&ensp;Social Force Models"></a>3.3.1.&ensp;Social Force Models</h4><ol><li><strong>Individual Force</strong><ul><li>fidelity, 目标不会改变它的目的地方向；</li><li>constancy, 目标不会突然改变速度和方向；</li></ul></li><li><strong>Group Force</strong><ul><li>attraction, 目标间应该尽量靠近；</li><li>repulsion, 目标间也得保留适当的距离；</li><li>coherence, 同一个 group 里面的目标速度应该差不多；</li></ul></li></ol><h4 id="3-3-2-ensp-Crowd-Motion-Pattern-Models"><a href="#3-3-2-ensp-Crowd-Motion-Pattern-Models" class="headerlink" title="3.3.2.&ensp;Crowd Motion Pattern Models"></a>3.3.2.&ensp;Crowd Motion Pattern Models</h4><p>　　当一个 group 比较密集的时候，单个目标的运动模型不太显著了，这时候群体的运动模型更加有效，可以用一些方法来构建群体运动模型。</p><h3 id="3-4-ensp-Exclusion-Model"><a href="#3-4-ensp-Exclusion-Model" class="headerlink" title="3.4.&ensp;Exclusion Model"></a>3.4.&ensp;Exclusion Model</h3><h4 id="3-4-1-ensp-Detection-level"><a href="#3-4-1-ensp-Detection-level" class="headerlink" title="3.4.1.&ensp;Detection-level"></a>3.4.1.&ensp;Detection-level</h4><p>　　同一帧两个检测量不能指向同一个目标。匹配 tracklets 时，可以将这一项作为惩罚项。不过目前的检测技术都做了 NMS，基本可以消除这种情况。  </p><h4 id="3-4-2-ensp-Trajectory-level"><a href="#3-4-2-ensp-Trajectory-level" class="headerlink" title="3.4.2.&ensp;Trajectory-level"></a>3.4.2.&ensp;Trajectory-level</h4><p>　　两个轨迹不能非常靠近。对于 online tracking 来说，就是 tracking 结果的两个量不能挨在一起，如果挨在一起，就说明有问题，比如遮挡，或跟丢。</p><h3 id="3-5-ensp-Occlusion-Handling"><a href="#3-5-ensp-Occlusion-Handling" class="headerlink" title="3.5.&ensp;Occlusion Handling"></a>3.5.&ensp;Occlusion Handling</h3><ul><li>Part-to-whole, 将目标分成栅格来处理；</li><li>Hypothesize-and-test, </li><li>Buffer-and-recover, 在遮挡产生前，记录一定量的观测，遮挡后恢复；</li><li>Others</li></ul><h3 id="3-6-ensp-Inference"><a href="#3-6-ensp-Inference" class="headerlink" title="3.6.&ensp;Inference"></a>3.6.&ensp;Inference</h3><h4 id="3-6-1-ensp-Probabilistic-Inference"><a href="#3-6-1-ensp-Probabilistic-Inference" class="headerlink" title="3.6.1.&ensp;Probabilistic Inference"></a>3.6.1.&ensp;Probabilistic Inference</h4><p>　　概率法只需要用到当前时刻之前的信息，所以适合用于 online tracking 任务。首先，如果假设一阶马尔科夫，当前目标的状态之依赖于前一时刻目标的状态，即 <em>dynamic model</em>：<br>$$P(S_t|S_{1:t-1})=P(S_t|S_{t-1})$$<br>其次，观测是独立的，即当前目标的观测只由当前目标的状态决定，<em>observation model</em>：<br>$$P(O_{1:t}|S_{1:t})=\prod_{i=1}^t P(O_t|S_t)$$<br>dynamic model 对应的就是跟踪算法策略，observation model 是状态观测手段，包括检测方法。目标状态估计的迭代过程为：</p><ul><li><strong>predict step</strong><br>根据 dynamic model，由目标的上一状态预测当前状态的后验概率分布；</li><li><strong>update step</strong><br>根据 observation model，更新当前目标状态的后验概率分布；</li></ul><p>　　状态估计的过程伴随着噪音等因素的影响，常用的概率推断模型有：</p><ul><li>Kalman filter</li><li>Extended Kalman filter</li><li>Particle filter</li></ul><h4 id="3-6-2-ensp-Deterministic-Optimization"><a href="#3-6-2-ensp-Deterministic-Optimization" class="headerlink" title="3.6.2.&ensp;Deterministic Optimization"></a>3.6.2.&ensp;Deterministic Optimization</h4><p>  　确定性优化法需要至少一个时间窗口的观测量，所以适合 offline tracking 任务。优化方法有：</p><ul><li>Bipartite graph matching</li><li>Dynamic Programming</li><li>Min-cost max-flow network flow</li><li>Conditional random field</li><li>MWIS(Maximum-weight independent set)</li></ul><h2 id="4-ensp-评价方法"><a href="#4-ensp-评价方法" class="headerlink" title="4.&ensp;评价方法"></a>4.&ensp;评价方法</h2><p>　　评价方法是非常重要的，一方面对算法系统进行调参优化，另一方面比较各个不同算法的优劣。评价方法 (evaluation) 包括评价指标 (metrics) 以及数据集 (datasets)，多类别的数据集主要有：</p><ul><li><a href="https://motchallenge.net/results/MOT17/" target="_blank" rel="noopener">MOT Challenge</a></li><li><a href="http://www.cvlibs.net/datasets/kitti/eval_tracking.php" target="_blank" rel="noopener">KITTI</a>　　</li></ul><p>评价指标可分为：<br>A.&ensp;<strong>检测指标</strong><br>　\(\lozenge\)　准确性(Accuracy)</p><ul><li>Recall &amp; Precision</li><li>False Alarme per Frame(FAF) rate, from <a href="https://www.google.com/search?q=Learning+affinities+and+dependencies+for+multi-target+tracking+using+a+CRF+model&amp;oq=Learning+affinities+and+dependencies+for+multi-target+tracking+using+a+CRF+model&amp;aqs=chrome..69i57.1077j0j9&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">paper</a></li><li>False Positive Per Image(FPPI), from <a href="https://www.google.com/search?q=Multiple+target+tracking+in+world+coordinate+with+single%2C+minimally+calibrated+camera&amp;oq=Multiple+target+tracking+in+world+coordinate+with+single%2C+minimally+calibrated+camera&amp;aqs=chrome..69i57j0.1134j0j9&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">paper</a></li><li>MODA(Multiple Object Detection Accuracy), 包含了 false positive &amp; miss dets. from <a href="https://www.google.com/search?q=Framework+for+performance+evaluation+of+face%2C+text%2C+and+vehicle+detection+and+tracking+in+video%3A+Data%2C+metrics%2C+and+protocol&amp;oq=Framework+for+performance+evaluation+of+face%2C+text%2C+and+vehicle+detection+and+tracking+in+video%3A+Data%2C+metrics%2C+and+protocol&amp;aqs=chrome..69i57j69i61.973j0j9&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">paper</a></li></ul><p>　\(\lozenge\)　精确性(Precision)</p><ul><li>MODP(Multiple Object Detection Precision), 衡量检测框与真值框的位置对齐程度；from <a href="https://www.google.com/search?q=Framework+for+performance+evaluation+of+face%2C+text%2C+and+vehicle+detection+and+tracking+in+video%3A+Data%2C+metrics%2C+and+protocol&amp;oq=Framework+for+performance+evaluation+of+face%2C+text%2C+and+vehicle+detection+and+tracking+in+video%3A+Data%2C+metrics%2C+and+protocol&amp;aqs=chrome..69i57j69i61.973j0j9&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">paper</a></li></ul><p>B.&ensp;<strong>跟踪指标</strong><br>　\(\lozenge\)　准确性(Accuracy)</p><ul><li>ID switches(IDs), from <a href="https://www.google.com/search?safe=strict&amp;ei=agXyXMaQEKyl_Qa4zJrQCg&amp;q=who+are+you+with+and+where+are+you+going&amp;oq=Who+are+you+with+and+where+are+you+going&amp;gs_l=psy-ab.1.0.0i203.53050.53050..55771...0.0..0.559.559.5-1......0....2j1..gws-wiz.nigYYAJc4jQ" target="_blank" rel="noopener">paper</a></li><li>MOTA(Multiple Object Tracking Accuracy), 包含了FP，FN，mismatch；from <a href="https://www.google.com/search?safe=strict&amp;ei=0ATyXP6lPIO6ggfIk6GAAQ&amp;q=evaluating+multiple+object+tracking+performance+the+clear+mot+metrics&amp;oq=Evaluating+Multiple+Object+Tracking+Performance&amp;gs_l=psy-ab.1.1.35i39j0j0i30j0i67.46576.46576..50024...0.0..0.436.436.4-1......0....2j1..gws-wiz.KAREeooiDMo" target="_blank" rel="noopener">paper</a></li></ul><p>　\(\lozenge\)　精确性(Precision)</p><ul><li>MOTP(Multiple Object Tracking Precision), from <a href="https://www.google.com/search?safe=strict&amp;ei=0ATyXP6lPIO6ggfIk6GAAQ&amp;q=evaluating+multiple+object+tracking+performance+the+clear+mot+metrics&amp;oq=Evaluating+Multiple+Object+Tracking+Performance&amp;gs_l=psy-ab.1.1.35i39j0j0i30j0i67.46576.46576..50024...0.0..0.436.436.4-1......0....2j1..gws-wiz.KAREeooiDMo" target="_blank" rel="noopener">paper</a></li><li>TDE(Tracking Distance Error), from <a href="https://www.google.com/search?safe=strict&amp;ei=fATyXNnwEvCH_QaG17fwDA&amp;q=%E2%80%9CTracking+with+local+spatio-temporal+motion+patterns+in+extremely+crowded+scenes&amp;oq=%E2%80%9CTracking+with+local+spatio-temporal+motion+patterns+in+extremely+crowded+scenes&amp;gs_l=psy-ab.12..0i30.82181.82181..83291...0.0..0.292.292.2-1......0....2j1..gws-wiz.hs0Je90zzHU" target="_blank" rel="noopener">paper</a></li><li>OSPA(optimal subpattern assignment), from <a href="https://www.google.com/search?safe=strict&amp;ei=_gDyXPKINY21ggeKtb2oDg&amp;q=a+metric+for+performance+evaluation+of+multi-target+tracking+algorithms&amp;oq=A_Metric_for_Performance_Evaluation_of_Multi-Targe&amp;gs_l=psy-ab.1.0.0i30.106502.106502..109413...0.0..0.303.303.3-1......0....2j1..gws-wiz.vrzc0MG18OM" target="_blank" rel="noopener">paper</a></li></ul><p>　\(\lozenge\)　完整性(Completeness)</p><ul><li>MT, the numbers of Mostly Tracked, from <a href="https://www.google.com/search?q=Learning+to+associate%3A+Hybridboosted+multi-target+tracker+for+crowded+scene&amp;oq=Learning+to+associate%3A+Hybridboosted+multi-target+tracker+for+crowded+scene&amp;aqs=chrome..69i57.1261j0j9&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">paper</a></li><li>PT, the numbers of Partly Tracked</li><li>ML, the numbers of Mostly Lost</li><li>FM, the numbers of Fragmentation</li></ul><p>　\(\lozenge\)　鲁棒性(Robustness)</p><ul><li>RS(Recover from Short-term occlusion), from <a href="https://www.google.com/search?safe=strict&amp;ei=_gDyXPKINY21ggeKtb2oDg&amp;q=A+stochastic+graph+evolution+framework+for+robust+multi-target+tracking&amp;oq=A+stochastic+graph+evolution+framework+for+robust+multi-target+tracking&amp;gs_l=psy-ab.12..0i30.453442.453442..454691...0.0..0.315.315.3-1......0....2j1..gws-wiz.OPYJ8mRFgYg" target="_blank" rel="noopener">paper</a></li><li>RL(Recover from Long-term occlusion)  </li></ul><p>评价指标汇总：<br><img src="/MOT-综述-Multiple-Object-Tracking-A-Literature-Review/metrics.png" width="50%" height="50%"></p><h2 id="5-ensp-总结"><a href="#5-ensp-总结" class="headerlink" title="5.&ensp;总结"></a>5.&ensp;总结</h2><h3 id="5-1-ensp-还存在的问题"><a href="#5-1-ensp-还存在的问题" class="headerlink" title="5.1.&ensp;还存在的问题"></a>5.1.&ensp;还存在的问题</h3><p>　　MOT 算法模块较多，参数也较复杂，但是最依赖于检测模块的性能，所以算法间比较性能时，需要注意按模块进行变量控制。</p><h3 id="5-2-ensp-未来研究方向"><a href="#5-2-ensp-未来研究方向" class="headerlink" title="5.2.&ensp;未来研究方向"></a>5.2.&ensp;未来研究方向</h3><ul><li><strong>MOT with video adaptation</strong>，检测模块式预先训练的，需要在线更新学习；</li><li><strong>MOT under multiple camera</strong>:<br>\(\circ\)　multiple views，不同视野相同场景信息的记录，<br>\(\circ\)　non-overlapping multi-camera，不同视野不同场景的 reidentification；</li><li><strong>Multiple 3D object tracking</strong>，能更准确预测位置，大小，更有效处理遮挡；</li><li><strong>MOT with scene understanding</strong>，拥挤场景，用场景理解来有效跟踪；</li><li><strong>MOT with deep learning</strong></li><li><strong>MOT with other cv tasks</strong>，和其他任务融合，比如目标分割等；</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　之前做 MOT 还是沿着 SOT 的思路，这篇文章对 MOT 有一个很深入且很有框架性的综述，以下对这篇文章做一个提炼，并加入一些自己的想法。&lt;br&gt;　　MOT 作为一个中层任务，是一些高层任务的基础，比如行人的 pose estimation，action recog
      
    
    </summary>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/categories/paper-reading/"/>
    
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/tags/MOT/"/>
    
      <category term="tracking" scheme="https://leijiezhang001.github.io/tags/tracking/"/>
    
  </entry>
  
  <entry>
    <title>3D Detection Paper List</title>
    <link href="https://leijiezhang001.github.io/3D-Detection-paper-list/"/>
    <id>https://leijiezhang001.github.io/3D-Detection-paper-list/</id>
    <published>2019-05-22T03:43:33.000Z</published>
    <updated>2019-05-26T08:38:20.552Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章从输入数据类别上进行 3D Detection paper 的归类。</p><h2 id="RGB"><a href="#RGB" class="headerlink" title="RGB"></a>RGB</h2><h2 id="RGB-D-双目，单目-点云"><a href="#RGB-D-双目，单目-点云" class="headerlink" title="RGB-D(双目，单目+点云)"></a>RGB-D(双目，单目+点云)</h2><h2 id="Lidar"><a href="#Lidar" class="headerlink" title="Lidar"></a>Lidar</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这篇文章从输入数据类别上进行 3D Detection paper 的归类。&lt;/p&gt;
&lt;h2 id=&quot;RGB&quot;&gt;&lt;a href=&quot;#RGB&quot; class=&quot;headerlink&quot; title=&quot;RGB&quot;&gt;&lt;/a&gt;RGB&lt;/h2&gt;&lt;h2 id=&quot;RGB-D-双目，单目-点云&quot;
      
    
    </summary>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/categories/paper-reading/"/>
    
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
  </entry>
  
  <entry>
    <title>Study Topic List</title>
    <link href="https://leijiezhang001.github.io/study-topic-list/"/>
    <id>https://leijiezhang001.github.io/study-topic-list/</id>
    <published>2019-05-20T05:30:54.000Z</published>
    <updated>2019-06-05T15:08:37.617Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文罗列了相关领域知识的学习资料。</p><h2 id="1-ensp-Detection"><a href="#1-ensp-Detection" class="headerlink" title="1.&ensp;Detection"></a>1.&ensp;Detection</h2><h3 id="1-1-ensp-2D-Detection"><a href="#1-1-ensp-2D-Detection" class="headerlink" title="1.1.&ensp;2D Detection"></a>1.1.&ensp;2D Detection</h3><ul><li><a href="https://zhuanlan.zhihu.com/p/34142321" target="_blank" rel="noopener">入门</a></li><li><a href="https://github.com/amusi/awesome-object-detection" target="_blank" rel="noopener">amusi</a></li><li><a href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#yolov3" target="_blank" rel="noopener">Object Detection @handong</a></li><li><a href="http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/" target="_blank" rel="noopener">Object Detection and Classification using R-CNNs</a></li><li><a href="https://paperswithcode.com/task/object-detection" target="_blank" rel="noopener">Paper with Code</a></li></ul><h3 id="1-2-ensp-3D-Detection"><a href="#1-2-ensp-3D-Detection" class="headerlink" title="1.2.&ensp;3D Detection"></a>1.2.&ensp;3D Detection</h3><ul><li><a href="https://paperswithcode.com/task/3d-object-detection" target="_blank" rel="noopener">Paper with Code</a></li><li><a href="http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d" target="_blank" rel="noopener">KITTI Leaderboard</a></li></ul><hr><h2 id="2-ensp-Tracking"><a href="#2-ensp-Tracking" class="headerlink" title="2.&ensp;Tracking"></a>2.&ensp;Tracking</h2><h3 id="2-1-ensp-Single-Object-Tracking"><a href="#2-1-ensp-Single-Object-Tracking" class="headerlink" title="2.1.&ensp;Single Object Tracking"></a>2.1.&ensp;Single Object Tracking</h3><ul><li><a href="https://paperswithcode.com/task/visual-object-tracking" target="_blank" rel="noopener">Paper with Code</a></li></ul><h3 id="2-2-ensp-Multi-Object-Tracking"><a href="#2-2-ensp-Multi-Object-Tracking" class="headerlink" title="2.2.&ensp;Multi Object Tracking"></a>2.2.&ensp;Multi Object Tracking</h3><ul><li><a href="https://paperswithcode.com/task/multiple-object-tracking" target="_blank" rel="noopener">Paper with Code</a></li><li><a href="https://zhuanlan.zhihu.com/p/65177442" target="_blank" rel="noopener">Paper List</a></li><li><a href="https://motchallenge.net/results/MOT17/" target="_blank" rel="noopener">MOT Challenge</a></li><li><a href="https://arxiv.org/abs/1409.7618" target="_blank" rel="noopener">综述：Multiple Object Tracking: A Literature Review</a></li><li><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2013/html/Wu_Online_Object_Tracking_2013_CVPR_paper.html" target="_blank" rel="noopener">综述：Online object tracking: A benchmark</a></li><li><a href="https://arxiv.org/abs/1504.01942" target="_blank" rel="noopener">综述：MOTChallenge 2015: Towards a benchmark for multi-target tracking</a></li></ul><hr><h2 id="3-ensp-Computational-Photography"><a href="#3-ensp-Computational-Photography" class="headerlink" title="3.&ensp;Computational Photography"></a>3.&ensp;Computational Photography</h2><ul><li><a href="http://graphics.cs.cmu.edu/courses/15-463/2017_fall/" target="_blank" rel="noopener">2017年秋季的计算摄影学课程15-463</a></li></ul><hr><h2 id="4-ensp-CNN-ACC"><a href="#4-ensp-CNN-ACC" class="headerlink" title="4.&ensp;CNN ACC"></a>4.&ensp;CNN ACC</h2><hr><h2 id="5-ensp-SLAM"><a href="#5-ensp-SLAM" class="headerlink" title="5.&ensp;SLAM"></a>5.&ensp;SLAM</h2><h3 id="5-1-ensp-理论知识"><a href="#5-1-ensp-理论知识" class="headerlink" title="5.1.&ensp;理论知识"></a>5.1.&ensp;理论知识</h3><ul><li><a href="http://cvrs.whu.edu.cn/downloads/ebooks/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6%E6%96%B9%E6%B3%95.pdf" target="_blank" rel="noopener">计算机视觉中的数学方法</a></li><li><a href="http://cvrs.whu.edu.cn/downloads/ebooks/Multiple%20View%20Geometry%20in%20Computer%20Vision%20%28Second%20Edition%29.pdf" target="_blank" rel="noopener">Multiple View Geometry in Computer Vision</a></li><li><a href="https://docs.ufpr.br/~danielsantos/ProbabilisticRobotics.pdf" target="_blank" rel="noopener">Probabilistic Robotics</a>(有中文版)</li><li><a href="http://asrl.utias.utoronto.ca/~tdb/bib/barfoot_ser17.pdf" target="_blank" rel="noopener">State Estimation for Robotics</a>(有中文版)</li><li><a href="https://github.com/gaoxiang12/slambook" target="_blank" rel="noopener">视觉SLAM十四讲</a></li></ul><h3 id="5-2-ensp-综述"><a href="#5-2-ensp-综述" class="headerlink" title="5.2.&ensp;综述"></a>5.2.&ensp;综述</h3><ul><li>[Visual Odometry Part I: Fundamentals]</li><li>[Visual Odometry Part II: Matching, Robustness, Optimization, Applications]</li><li><a href="https://link.springer.com/content/pdf/10.1186%2Fs40064-016-3573-7.pdf" target="_blank" rel="noopener">Review of Visual Odometry: Types, Approaches, Challenges, and Applications</a></li><li><a href="https://ipsjcva.springeropen.com/track/pdf/10.1186/s41074-017-0027-2" target="_blank" rel="noopener">Visual SLAM algorithms: a Survey from 2010 to 2016</a></li><li><a href="http://www.cvc.uab.es/~asappa/publications/C__IEEE_IV_2012_W3.pdf" target="_blank" rel="noopener">Visual SLAM for Driverless Cars: a Brief Survey</a></li><li><a href="https://link.springer.com/article/10.1007/s10462-012-9365-8" target="_blank" rel="noopener">Visual Simultaneous Locations and Mapping: a Survey</a></li></ul><h3 id="5-3-ensp-工具"><a href="#5-3-ensp-工具" class="headerlink" title="5.3.&ensp;工具"></a>5.3.&ensp;工具</h3><ul><li><a href="http://www.guyuehome.com/column/ros-explore" target="_blank" rel="noopener">ROS</a></li><li><a href="https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html" target="_blank" rel="noopener">Opencv Camera Calibration</a></li><li><a href="http://www.vision.caltech.edu/bouguetj/calib_doc/" target="_blank" rel="noopener">Matlab Camera Calibration Toolbox</a></li><li><a href="http://wiki.ros.org/camera_calibration" target="_blank" rel="noopener">ROS Wiki Camera Calibration</a></li></ul><h3 id="5-4-ensp-算法"><a href="#5-4-ensp-算法" class="headerlink" title="5.4.&ensp;算法"></a>5.4.&ensp;算法</h3><ul><li><a href="https://openslam-org.github.io/" target="_blank" rel="noopener">OpenSLAM</a></li></ul><h3 id="5-5-ensp-其它资料"><a href="#5-5-ensp-其它资料" class="headerlink" title="5.5.&ensp;其它资料"></a>5.5.&ensp;其它资料</h3><ul><li><a href="https://www.zhihu.com/people/cheng-xu-yuan-10/posts" target="_blank" rel="noopener">计算机视觉life</a></li><li><a href="https://paperswithcode.com/task/visual-odometry" target="_blank" rel="noopener">Paper with Code</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　本文罗列了相关领域知识的学习资料。&lt;/p&gt;
&lt;h2 id=&quot;1-ensp-Detection&quot;&gt;&lt;a href=&quot;#1-ensp-Detection&quot; class=&quot;headerlink&quot; title=&quot;1.&amp;ensp;Detection&quot;&gt;&lt;/a&gt;1.&amp;ensp;De
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://leijiezhang001.github.io/hello-world/"/>
    <id>https://leijiezhang001.github.io/hello-world/</id>
    <published>2019-05-20T04:58:47.933Z</published>
    <updated>2019-05-20T04:58:47.933Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
