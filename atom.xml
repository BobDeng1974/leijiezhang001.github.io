<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LeijieZhang</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://leijiezhang001.github.io/"/>
  <updated>2020-08-17T01:19:12.000Z</updated>
  <id>https://leijiezhang001.github.io/</id>
  
  <author>
    <name>Leijie</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>[paper_reading]-&quot;Cylinder3D&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-Cylinder3D/"/>
    <id>https://leijiezhang001.github.io/paper-reading-Cylinder3D/</id>
    <published>2020-08-12T03:43:33.000Z</published>
    <updated>2020-08-17T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　Voxel-based 点云分割/检测等任务中，点云的投影表示方法有三种：</p><ul><li>Spherical</li><li>Bird-eye View</li><li>Cylinder</li></ul><p>其中 Spherical 球坐标投影代表为 <a href="/paper-reading-RandLA-Net/" title="RandLA-Net">RandLA-Net</a>；Bird-eye View 则是目前主流的方法。有关 Bird-eye View 点云处理的优劣已经说了很多了，这里不再赘述。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 介绍一种 Cylinder 柱状投影的点云处理方式，类似 <a href="/paper-reading-Pillar-based-Object-Detection/" title="Pillar-based Object Detection">Pillar-based Object Detection</a>，也可以认为是 <a href="/paper-reading-PolarNet/" title="PolarNet">PolarNet</a> 的 3D 版本。 <img src="/paper-reading-Cylinder3D/vs.png" width="70%" height="70%" title="图 1. Comparison"> 　　<a href="/paper-reading-Pillar-based-Object-Detection/" title="Pillar-based Object Detection">Pillar-based Object Detection</a> 中详细说明了 Cylinder 投影比 Spherical 投影的优势，这里不做赘述，如图 1. 所示，相比 Spherical 投影，Cylinder 投影效果提升很明显。</p><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-Cylinder3D/framework.png" width="80%" height="80%" title="图 2. Framework"> 　　如图 2. Cylinder3D 由 3D 柱坐标投影和 3D U-Net 特征提取两部分组成。框架比较简单，网络结构主要由 DownSample，UpSample，Asymmetry Residual Block，Dimension-Decomposition based Context Modeling 四种组件构成。</p><h2 id="cylinder-partition">2. Cylinder Partition</h2><p><img src="/paper-reading-Cylinder3D/coord.png" width="80%" height="80%" title="图 3. Cylinder Partition"> 　　如图 3. 所示，将笛卡尔坐标系下的点云 \((x,y,z)\) 转换到柱坐标系下 \((\rho,\theta,z)\)。对于每个扇形 Voxel，作 PointNet 特征提取，最终得到 3D Cylinder 点云特征表示 \(\mathbb{R}\in C\times H\times W\times L\)。</p><h2 id="network">3. Network</h2><p><img src="/paper-reading-Cylinder3D/block.png" width="70%" height="70%" title="图 4. A & DDCM"></p><h3 id="asymmetry-residual-block">3.1. Asymmetry Residual Block</h3><p>　　如图 4. 所示，Asymmetry Residual Block 将 \(3\times 3\times 3\) 卷积拆分成 \(1\times 3\times 3\) 和 \(3\times 1\times 3\) 两种，这样作有两个好处：</p><ul><li>由于待检测的目标都接近于长方体，这种卷积形式更有利于提取长方体样式的特征；</li><li>减少 33% 的计算量，类似 Depth-wise Convolution；</li></ul><p>该模块作为 3D 卷积的基本模块，嵌入在下采样前，以及上采样后。</p><h3 id="dimension-decomposition-based-context-modeling">3.2. Dimension-Decomposition based Context Modeling</h3><p>　　由于 3D 空间的特征表达是 high-rank 的，所以利用矩阵分解的思想，将其用 height，width，depth 三维的 low-rank 向量来权重化表达，由此设计如图 4. 中的 DDCM 模块。该模块将三个方向的特征计算各自的权重，然后与原始特征作权重化整合。输出的特征用于最终的预测，预测输出是 Voxel-based，维度为 \(Class\times H\times W\times L\)。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Zhou, Hui, et al. &quot;Cylinder3D: An Effective 3D Framework for Driving-scene LiDAR Semantic Segmentation.&quot; arXiv preprint arXiv:2008.01550 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Voxel-based 点云分割/检测等任务中，点云的投影表示方法有三种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spherical&lt;/li&gt;
&lt;li&gt;Bird-eye View&lt;/li&gt;
&lt;li&gt;Cylinder&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中 Spherical 球坐标投影代
      
    
    </summary>
    
      <category term="Semantic Segmentation" scheme="https://leijiezhang001.github.io/categories/Semantic-Segmentation/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/tags/Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;RadarNet&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-RadarNet/"/>
    <id>https://leijiezhang001.github.io/paper-reading-RadarNet/</id>
    <published>2020-08-07T01:25:41.000Z</published>
    <updated>2020-08-12T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　Radar 相比 LiDAR/Camera，对天气影响鲁棒性较强，而且能直接测量目标速度，所以是多传感器融合感知里比较重要的一个输入。其在 ADAS 领域应用已经较为广泛。但是 Radar 的测量噪声较大，这给有效的多传感器融合带来了难度。本文提出了 RadarNet<a href="#1" id="1ref"><sup>[1]</sup></a>，利用 Radar 的几何数据以及动态数据，同时做特征级别的前融合以及注意力机制下的后融合，达到了较好的效果。</p><h2 id="comparison-between-lidar-and-radar">1. Comparison between LiDAR and Radar</h2><p><img src="/paper-reading-RadarNet/compare.png" width="90%" height="90%" title="图 1. Comparison"> 　　LiDAR 可分为三种类型：Spinning，Solid State，Flash。目前主要采用的是 Spinning 旋转式的激光雷达。激光雷达的缺点有：</p><ul><li>对雨雾雪，车尾气等比较敏感；</li><li>对玻璃等物体没有反射;</li><li>点云密度随着距离增加而下降，远距离探测能力较弱。</li></ul><p>毫米波雷达则能克服以上缺点，并且能直接测量速度；但是缺点也比较明显：</p><ul><li>分辨率低，对小目标探测能力较弱；</li><li>误检较多；</li><li>测量的速度只是径向速度。</li></ul><p>毫米波雷达可输出三种形式的数据：1. 原始点运数据；2. 经过 DBSCAN 等聚类算法获得的聚类点集数据；3. 对点集作跟踪的数据。三种数据越来越高层，但是噪音越来越大。本文考虑输出点集的数据，设探测到的点集目标为 \(Q = (q, v _ {||},m,t)\)，其中 \(q = (x,y)\) 是俯视图下的位置，\(v _ {||}\) 是径向速度，\(m\) 代表目标是否运动，\(t\) 则为时间戳。我们需要进一步估计出目标的 2D 速度，由此在毫米波雷达的辅助下，能获得更长的检测距离，以及更准确的速度。 <img src="/paper-reading-RadarNet/fusion.png" width="90%" height="90%" title="图 2. Fusion"> 　　激光雷达数据与毫米波雷达数据融合示意图如图 2. 所示。</p><h2 id="radarnet">2. RadarNet</h2><p><img src="/paper-reading-RadarNet/framework.png" width="90%" height="90%" title="图 3. Framework"> 　　如图 3. 所示，RadarNet 主要由 Voxel-Based Early Fusion，Detection Network 以及 Attention-Based Late Fusion 来做融合检测。前融合将各传感器数据通过俯视图形式进行表示并融合；后融合则通过基于注意力的数据关联及整合机制来对目标速度进行精细估计。检测网络是在传统的分类＋回归基础上，多了速度预测的分支。具体的，所有回归的预测量为 \((x-p _ x,y - p _ y,w,l,\mathrm{cos}(\theta),\mathrm{sin}(\theta),m,v _ x, v _ y)\)，其中 \(p _ x,p _ y\) 为体素/栅格的中心点，\(m\) 为目标是运动的概率，如果 \(m &lt; 0.5\)，则将速度置为 0。</p><h3 id="early-fusion">2.1. Early Fusion</h3><p>　　对于激光雷达数据，类似 <a href="/paperreading-Fast-and-Furious/" title="FAF">FAF</a>，将时序多帧(0.5s)的点云数据在本车坐标系下打成俯视图体素表示，然后在通道维度进行串联。如果体素内没有点，那么该体素值为 0；如果体素内有点 \(\{(x _ i,y _ i,z _ i),i=1,...,N\}\)，那么体素值为 \(\sum _ i\left( 1- \frac{|x _ i-a|}{dx / 2}\right)\left( 1- \frac{|y _ i-b|}{dy / 2}\right)\left( 1- \frac{|z _ i-c|}{dz / 2}\right)\)，其中 \((a,b,c)\) 为体素中心坐标，\(dx,dy,dz\) 为体素尺寸。<br>　　对于毫米波雷达数据，将其转到激光雷达坐标系后，也进行 BEV 时序串联，并将每一帧中不同线束的数据在 BEV 下体素化，然后串联。具体的，如果体素(本文丢掉了高度信息，所以退化为栅格)中没有毫米波探测到的目标，那么置为 0，如果探测到动态目标，则置为 1，如果探测到静态目标，则置为 -1。<br>　　分别得到激光雷达与毫米波雷达的 BEV 表示后，将二者在通道维度串联起来，就完成了前融合。</p><h3 id="late-fusion">2.2. Late Fusion</h3><p>　　<strong>前融合关注毫米波雷达探测到的目标的位置和密度，后融合则使用毫米波雷达探测到的目标径向速度信息</strong>。检测网络输出的目标状态为 \(D = (c,x,y,w,l,\theta,\mathbf{v})\)，毫米波雷达输出的目标状态为 \(Q=(q,v _ {||},m,t)\)。两者的目标级别的后融合通过 Association 和 Aggregation 两步骤组成。 <img src="/paper-reading-RadarNet/late-fusion.png" width="90%" height="90%" title="图 4. Late-Fusion"> 　　本文将 Association 和 Aggregation 用 End-to-End 的网络来处理。如图 4. 所示，首先进行检测目标与毫米波目标的成对特征提取，然后经过 MLP/Softmax 作匹配分数估计，最后根据分数作速度的加权优化。</p><h4 id="pairwise-detection-radar-association">2.2.1. Pairwise Detection-Radar Association</h4><p>　　定义 Pairwise Feature 为： <span class="math display">\[\begin{align}f(D,Q) &amp;= \left(f ^ {det}(D),f ^ {det-radar}(D,Q)\right) \tag{1}\\f ^ {det}(D) &amp;= \left(w,l,||\mathbf{v}||,\frac{v _ x}{||\mathbf{v}||},\frac{v _ y}{||\mathbf{v}||},\mathrm{cos}(\gamma)\right) \tag{2}\\f ^ {det-radar}(D,Q) &amp;= \left(dx,dy,dt,v ^ {bp}\right) \tag{3}\\v ^ {bp} &amp;= \mathrm{min}\left(50,\frac{v _ {||}}{\mathrm{cos}(\phi)}\right) \tag{4}\end{align}\]</span> 其中 \((\cdot,\cdot)\) 是 Concatenation 操作；\(\gamma\) 是网络检测 \(D\) 的运动方向与径向方向的夹角；\(\phi\) 是 \(D\) 运动方向与雷达探测目标 \(Q\) 的径向方向的夹角；\(v ^ {bp}\) 是径向速度反投影到运动方向(朝向)的速度值；\((dx,dy,dt)\) 为俯视图下 \(D,Q\) 的相对位置和时间。由此，通过 MLP 学习匹配分数： <span class="math display">\[s _ {i,j}=\mathrm{MLP} _ {match}\left(f(D _ i,Q _ j)\right)\tag{5}\]</span></p><h4 id="velocity-aggregation">2.2.2. Velocity Aggregation</h4><p>　　根据 \(D,Q\) 间的匹配分数，优化<strong>目标的绝对速度值</strong>。为了解决没有匹配的情况，分数 Concate 1，然后计算归一化的匹配分数： <span class="math display">\[s _ i ^ {norm}=\mathrm{softmax}((1,s _ {i,:})) \tag{6}\]</span> 然后优化每个检测目标 \(i\) 的绝对速度值： <span class="math display">\[v _ i&#39; = s _ i ^ {norm}\cdot\begin{bmatrix}||\mathbf{v} _ i|| \\v _ {i,:} ^ {bp}\end{bmatrix}\tag{7}\]</span> 最终可得到目标的 2D 速度： <span class="math display">\[\mathbf{v}&#39; _ i = v&#39; _ i\cdot\left(\frac{v _ x}{||\mathbf{v}||},\frac{v _ y}{||\mathbf{v}||}\right) \tag{8}\]</span> 　　<strong>由此可知，该后融合优化的只是目标的绝对速度(毫米波雷达也没办法探测目标的朝向或运动方向)，目标的朝向准确度还是由检测网络决定。</strong></p><h2 id="experiments">3. Experiments</h2><p>　　根据式(2,3)提取的特征，作者设计了基于 Heuristic 的关联方法，毫米波雷达探测的目标与网络检测的目标关联的条件为： <span class="math display">\[\left\{\begin{array}{rl}\sqrt{(dx) ^ 2+(dy) ^ 2} &amp;&lt; 3m \\\gamma &amp;&lt; 40°\\||\mathbf{v}|| &amp;&gt; 1m/s\\v ^ {bp} &amp;&lt; 30m/s \\\end{array}\tag{9}\right.\]</span> 一旦关联上后，去毫米波雷达速度的中位数作目标速度的进一步优化。这种传统的 Heuristic 与本文的 Attention 方法对比如下，优势明显。 <img src="/paper-reading-RadarNet/exp.png" width="80%" height="80%" title="图 5. Heuristic VS. Attention"></p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Yang, Bin, et al. &quot;RadarNet: Exploiting Radar for Robust Perception of Dynamic Objects.&quot; arXiv preprint arXiv:2007.14366 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Radar 相比 LiDAR/Camera，对天气影响鲁棒性较强，而且能直接测量目标速度，所以是多传感器融合感知里比较重要的一个输入。其在 ADAS 领域应用已经较为广泛。但是 Radar 的测量噪声较大，这给有效的多传感器融合带来了难度。本文提出了 RadarNet&lt;
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Pillar-based Object Detection&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-Pillar-based-Object-Detection/"/>
    <id>https://leijiezhang001.github.io/paper-reading-Pillar-based-Object-Detection/</id>
    <published>2020-08-04T03:42:08.000Z</published>
    <updated>2020-08-06T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　<a href="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/" title="MVF">MVF</a> 在俯视图点云特征的基础上，融合了点云的前视图特征，由此解决点云在远处比较稀疏，以及行人等狭长型目标特征信息较少的问题。本文<a href="#1" id="1ref"><sup>[1]</sup></a>基于 <a href="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/" title="MVF">MVF</a> 作了三部分的改进：</p><ol type="1"><li>检测头改为 Anchor-Free 的形式，本文称之为 Pillar-based，其实就是图像中对应的像素点；</li><li>前视图用 Cylindrical View 代替 Spherical View，解决目标高度失真的问题；</li><li>两个视图的栅格特征反投影回点特征作融合时，采用双线性插值的形式，避免量化误差的影响。</li></ol><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-Pillar-based-Object-Detection/framework.png" width="90%" height="90%" title="图 1. Framework"> 　　如图 1. 所示，点云分别投影到 BEV(Brids-Eye)，CYV(Cylindrical) 视角，然后作类似图像卷积的 2D 卷积操作以提取特征，并将特征反投影回点作融合(与 <a href="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/" title="MVF">MVF</a> 一致)，接着将点云特征再次投影到 BEV 下，最后作 Anchor-Free 的分类与回归任务。<br>　　具体的，设 \(N\) 个点的点云 \(P=\{p _ i\} _ {i=0} ^ {N-1}\subseteq\mathbb{R} ^ 3\)，对应的特征向量为 \(F = \{f _ i\} _ {i=0} ^ {N-1}\subseteq\mathbb{R} ^ K\)。令 \(F _ V(p _ i)\) 返回点 \(p _ i\) 对应的栅格柱子 \(v _ j\) 的索引 \(j\)；\(F _ P(v _ j)\) 则返回栅格柱子 \(v _ j\) 对应的点集。对每个柱子进行特征整合，一般采用类似 PointNet(PN) 的方法： <span class="math display">\[f _ j ^{pillar} = \mathrm{PN} (\{f _ i|\forall p _ i\in F _ P(v _ j)\}) \tag{1}\]</span> pillar 级别的特征经过 CNN \(\phi\) 后得到进一步的 pillar 级别特征：\(\varphi=\phi(f ^ {pillar})\)。然后分别对 BEV，CYV 作 pillar-to-point 的特征投影变换： <span class="math display">\[f _ i^{point}=f _ j^{pillar}\;\mathrm{and}\;\varphi _ i^{point} = \varphi _ j^{pillar},\;\mathrm{where}\; j = F _ V(p _ i) \tag{2}\]</span> 最后的检测头是应用已经较为广泛的 Anchor-Free 形式。</p><h2 id="cylindrical-view">2. Cylindrical View</h2><p><img src="/paper-reading-Pillar-based-Object-Detection/proj.png" width="60%" height="60%" title="图 2. Projection"> 　　<a href="/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/" title="MVF">MVF</a> 采用 Spherical 投影方式，对于点 \(p _ i=(x _ i, y _ i, z _ i)\)，其球坐标 \(\varphi _ i,\theta _ i,d _ i\) 为： <span class="math display">\[\left\{\begin{array}{l}\varphi _ i &amp;= \mathrm{arctan}\frac{y _ i}{x _ i}\\\theta _ i &amp;= \mathrm{arccos}\frac{z _ i}{d _ i}\\d _ i &amp;= \sqrt{x _ i ^ 2+y _ i ^ 2+z _ i^2}\end{array}\tag{3}\right.\]</span> 如图 2. 所示，球坐标系下目标高度的形变比较严重，本文采用柱坐标系，其柱坐标 \(\rho _ i,\varphi _ i,z _ i\) 表示为： <span class="math display">\[\left\{\begin{array}{l}\rho _ i &amp;=\sqrt{x _ i ^ 2+y _ i^2}\\\varphi _ i &amp;= \mathrm{arctan}\frac{y _ i}{x _ i}\\z _ i &amp;= z _ i\end{array}\tag{4}\right.\]</span> 　　在此视角下作 pillar-level 的特征提取，与俯视图视角一样，只不过作卷积的时候，是环状卷积。具体实现方式是，将柱坐标系下的 pillar 展开，然后边缘补对应展开处另一边的 pillar 值，最后作传统的 2D 卷积即可。</p><h2 id="pillar-based-prediction">3. Pillar-based Prediction</h2><p>　　这里所谓的 Pillar-based 预测，本质上就是图像中常说的 Anchor-Free 的 Pixel-Level 的检测方法。最后特征图上的每个点预测类别概率，以及 3D 框属性 \(\Delta _ x,\Delta _ y,\Delta _ z,\Delta _ l,\Delta _ w,\Delta _ h,\theta ^ p\)。这里不作展开。</p><h2 id="bilinear-interpolation">4. Bilinear Interpolation</h2><p><img src="/paper-reading-Pillar-based-Object-Detection/bilinear.png" width="60%" height="60%" title="图 3. Bilinear"> 　　将 Pillar-Level 提取的特征反投影到 Point-Level 的特征时，需要进行插值处理。如图 3. 所示，传统的方式是最近邻插值，这种方式会引入量化误差，使得点投影反投影后的空间坐标不一致，产生的影响是同一 Pillar 内的点特征都是一样了。本文采用双线性插值的方法，使得 Point-Pillar-Point 的空间坐标一致，这样保证了 Pillar 内点特征的原始精度。该思想还是非常有借鉴意义的，实验效果提升也比较明显。</p><h2 id="reference">5. Reference</h2><p><a id="1" href="#1ref">[1]</a> Wang, Yue, et al. &quot;Pillar-based Object Detection for Autonomous Driving.&quot; arXiv preprint arXiv:2007.10323 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　&lt;a href=&quot;/paperreading-End-to-End-Multi-View-Fusion-for-3D-Object-Detection-in-LiDAR-Point-Clouds/&quot; title=&quot;MVF&quot;&gt;MVF&lt;/a&gt; 在俯视图点云特征的基础上，融合
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>VLOAM(Visual-lidar Odometry and Mapping)</title>
    <link href="https://leijiezhang001.github.io/VLOAM/"/>
    <id>https://leijiezhang001.github.io/VLOAM/</id>
    <published>2020-07-29T08:36:38.000Z</published>
    <updated>2020-08-04T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　<a href="/LOAM/" title="LOAM">LOAM</a> 中 Lidar Odometry 模块将当前累积的 Sweep 点云通过 Sweep-to-Sweep 注册到上一时刻的 Sweep 点云，从而生成高频低精度的位姿；Lidar Mapping 则将完整的当前 Sweep 点云通过 Sweep-to-Model 注册到全局地图中，从而生成低频高精度的位姿。这其中高频低精度的位姿可通过其它方式获得，如 IMU 等其它高频传感器。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 采用高频的 Visual Odometry 来生成高频低精度的位姿，低频高精度的位姿则还是通过 Lidar Odometry(Mapping) 获得，但做了细微的改变。 <img src="/VLOAM/demo.png" width="55%" height="55%" title="图 1. Visual & Lidar Odometry"> 　　如图 1. 所示，VSLAM 结合了高频低精度的 Visual Odometry，以及低频高精度的 Lidar Odometry，最终得到高频高精度的位姿，以及准确的全局点云地图。</p><h2 id="framework">1. Framework</h2><p>　　本文坐标系以相机坐标系 \(\{S\}\) 为主(x left，y upward，z forward)，所有点云都会通过外参转换到该坐标系下；设世界坐标系 \(\{W\}\) 为起始点。那么位姿求解问题的数学描述为：给定各个坐标系 \(\{S\}\) 下的图像和点云，求解所有 \(\{S\}\) 在 \(\{W\}\) 下的表示，以及 \(\{W\}\) 下地图的构建。 <img src="/VLOAM/framework.png" width="95%" height="95%" title="图 2. VSLAM Framework"> 　　如图 2. 所示，Visual Odometry 作前后帧的特征跟踪(或匹配)，结合点云深度信息，作 Frame-to-Frame 的运动位姿估计；Lidar Odometry 则先通过 Sweep-to-Sweep 作运动粗估计，然后用 Sweep-to-Map 作精估计(其中 Sweep 的定义可见 <a href="/LOAM/" title="LOAM">LOAM</a>)。由此输出低频的全局地图，以及高频的位姿估计。</p><h2 id="visual-odometry">2. Visual Odometry</h2><p>　　首先用 Visual Odometry 得到的高频位姿估计将点云注册为一个局部的深度图。由此维护三种类型的特征点：1. 从深度图获得深度的特征点；2. 从前后帧三角化获得深度的特征点；3. 没有深度的特征点。这里的特征点提取可采用任意的特征点提取方法，如果采用前后帧特征匹配的策略，则还得作相应的特征描述子提取，如果采用特征跟踪策略，则不需要。<br>　　设图像帧序号 \(k\in Z ^ +\)，特征点序号 \(i\in\mathcal{I}\)，那么在相机坐标系 \(\{S ^ k\}\) 下，特征点坐标表示为 \(\sideset{^S}{}X ^ k _ i = [\sideset{^S}{}x ^ k _ i,\sideset{^S}{}y ^ k _ i,\sideset{^S}{}z ^ k _ i] ^ T\)，其归一化表示为 \({\sideset{^S}{}{\overline{X}}} ^ k _ i = [\sideset{^S}{}{\overline{x}} ^ k _ i,\sideset{^S}{}{\overline{y}} ^ k _ i,\sideset{^S}{}{\overline{z}} ^ k _ i] ^ T\)。前后匹配的特征点与运动位姿的关系为： <span class="math display">\[{\sideset{^S}{}X} ^ k _ i=R\;{\sideset{^S}{}X} ^ {k-1} _ i+T \tag{1}\]</span> 其中 \({\sideset{^S}{}X} ^ k _ i\) 为当前帧的特征点坐标，由于还未估计出当前帧的位姿，所以该特征点是没有深度信息的。根据特征点 \({\sideset{^S}{}X} ^ {k-1} _ i\) 是否有深度信息，可归纳出方程：</p><ol type="1"><li>\({\sideset{^S}{}X} ^ {k-1} _ i\) 有深度信息 <span class="math display">\[\begin{align}&amp;{\sideset{^S}{}{\overline{d}}} ^ k _ i{\sideset{^S}{}{\overline{X}}} ^ k _ i=R\;{\sideset{^S}{}X} ^ {k-1} _ i+T\\\Longrightarrow &amp; \left\{\begin{array}{l}\left({\sideset{^S}{}{\overline{z}}} ^ k _ i R _ 1-{\sideset{^S}{}{\overline{x}}} ^ k _ i R _ 3\right){\sideset{^S}{}{X}} ^ k _ i + {\sideset{^S}{}{\overline{z}}} ^ k _ i T _ 1-{\sideset{^S}{}{\overline{x}}} ^ k _ i T _ 3 = 0\\\left({\sideset{^S}{}{\overline{z}}} ^ k _ i R _ 2-{\sideset{^S}{}{\overline{y}}} ^ k _ i R _ 3\right){\sideset{^S}{}{X}} ^ k _ i + {\sideset{^S}{}{\overline{z}}} ^ k _ i T _ 2-{\sideset{^S}{}{\overline{y}}} ^ k _ i T _ 3 = 0\\\end{array}\tag{2}\right.\end{align}\]</span> 其中 \(\sideset{^S}{}{d} ^ k _ i = \left\Vert \sideset{^S}{}{\overline{X}} ^ k _ i\right\Vert \)，\(R _ l, T _ l\) 为第 \(l\in\{1,2,3\}\) 行的 \(R,T\)。</li><li>\({\sideset{^S}{}X} ^ {k-1} _ i\) 无深度信息 <span class="math display">\[\begin{align}&amp;{\sideset{^S}{}{\overline{d}}} ^ k _ i{\sideset{^S}{}{\overline{X}}} ^ k _ i=R\;{\sideset{^S}{}{\overline{d}}} ^ {k-1} _ i\;{\sideset{^S}{}X} ^ {k-1} _ i+T\\\Longrightarrow &amp;\begin{bmatrix}-{\sideset{^S}{}{\overline{y}}} ^ k _ i T _ 3  +{\sideset{^S}{}{\overline{z}}} ^ k _ i T _ 2 &amp;{\sideset{^S}{}{\overline{x}}} ^ k _ i T _ 3-{\sideset{^S}{}{\overline{z}}} ^ k _ i T _ 1 &amp;-{\sideset{^S}{}{\overline{x}}} ^ k _ i T _ 2+{\sideset{^S}{}{\overline{y}}} ^ k _ i T _ 1\end{bmatrix}R\;{\sideset{^S}{}{\overline{X}}} ^ {k-1} _ i = 0\tag{3}\end{align}\]</span> 推导过程比较繁杂，但是也比较简单，依次消去 \(\sideset{^S}{}{d} ^ k _ i,\sideset{^S}{}{d} ^ {k-1} _ i\) 即可。</li></ol><p>将所有特征点所构成的 residual 累积，然后可用 LM 法求解该非线性问题中 6-DOF 的位姿。考虑到有较大 residual 的特征点大概率是离群点，所以对特征点的 residual 作权重处理，residual 越大，权重越小。<br><img src="/VLOAM/feats.png" width="55%" height="55%" title="图 3. Edge & Planar Feature"> 　　为了获取特征点的深度，维护一个从点云中采样的在上一帧图像坐标系下的深度图，深度图维护较新的点云深度信息，并且保持一定的点密度。深度图中的点用极坐标形式的 2D KD-tree 存储，具体的特征点深度值计算通过周围深度点构成的平面插值得到。在无法从深度图中获得特征点的深度信息时，如果特征点被跟踪了较长的距离，那么采用三角测量法获得该特征点深度。三种点的可视化如图 3. 所示。</p><h2 id="lidar-odometry">3. Lidar Odometry</h2><p>　　高频的 frame-to-frame Visual Odometry 得到的位姿估计是粗糙且有漂移的，接下来用 Lidar Odometry 作进一步的精估计。激光雷达里程计又基于 coarse-to-fine 的思想，分为 sweep-to-sweep 以及 sweep-to-map 两个步骤。这两个步骤的具体计算过程很相似，只不过前者是前后帧点云的匹配以消除运动引入的点云畸变，后者则是当前帧去畸变的点云与世界坐标系下的地图点云匹配，能消除累积误差。总体上这部分与 <a href="/LOAM/" title="LOAM">LOAM</a> 处理方式一致。</p><h3 id="sweep-to-sweep">3.1. Sweep-to-Sweep</h3><p><img src="/VLOAM/vo_drift.png" width="55%" height="55%" title="图 4. Drift"> 　　与 <a href="/LOAM/" title="LOAM">LOAM</a> 一样，对第 \(m\in Z ^ +\) 个 Sweep 点云 \(\mathcal{P} ^ m\)，提取线特征 \(\mathcal{E} ^ m\) 与面特征 \(\mathcal{H} ^ m\)。如图 4. 所示，将 Visual Odometry 产生的漂移建模为线性运动模型。假设第 \(m\) 个 Sweep 扫描期间其漂移的位姿为 \(T'\in\mathbb{R} ^ {6\times 1}\)，那么，对于点 \(i\in\mathcal{E} ^ m\cup\mathcal{H} ^ m\)，其接收时间 \(t _ i\) 对应的位姿漂移为： <span class="math display">\[T _ i&#39; = T&#39;(t _ i-t ^ m)/(t ^ {m+1}-t ^ m) \tag{4}\]</span> 　　为了求解 \(T'\)，分别找到当前帧特征点 \(\mathcal{E} ^ m,\mathcal{H} ^ m\) 与上一帧特征点的匹配，然后计算距离误差的 residual，累积后即可用 LM 法来求解该非线性最小二乘问题。对于 \(\mathcal{E} ^ m\)，在 \(\mathcal{P} ^ {m-1}\) 中找到最近的两个线特征点，从而计算 point-to-edge 距离；对于 \(\mathcal{H} ^ m\)，在 \(\mathcal{P} ^ {m-1}\) 中找到最近的三个面特征点，从而计算 point-to-plane 距离。找特征点的过程通过 3D KD-tree 实现(工程上为了加速，可以采用其它方法)。由此得到一系列方程： <span class="math display">\[f({\sideset{^S}{}X} ^ m _ i, T _ i&#39;)=d _ i \tag{5}\]</span> 其中 \({\sideset{^S}{}X} ^ m _ i\) 是点 \(i\in\mathcal{E} ^ m\cup\mathcal{H} ^ m\) 在 \(\{S ^ m\}\) 下的坐标。计算 \(T'\) 后，即可得到去畸变的当前帧点云 \(\mathcal{P} ^ m\)。</p><h3 id="sweep-to-map">3.2. Sweep-to-Map</h3><p>　　去畸变的点云 \(\mathcal{P} ^ m\) 可以进一步注册到点云地图 \(\mathcal{Q} ^ {m-1}\) 中。考虑到点云地图较为稠密，匹配过程为计算局部点集的分布特征值与特征向量。特征值一大两小，即为线特征；特征值两大一小则为面特征。因为没有 Sweep-to-Sweep 中的运动模型，所以可直接用 ICP 方法来优化求解位姿。最终得到低频高精度的位姿结果。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Zhang, Ji, and Sanjiv Singh. &quot;Visual-lidar odometry and mapping: Low-drift, robust, and fast.&quot; 2015 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2015.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　&lt;a href=&quot;/LOAM/&quot; title=&quot;LOAM&quot;&gt;LOAM&lt;/a&gt; 中 Lidar Odometry 模块将当前累积的 Sweep 点云通过 Sweep-to-Sweep 注册到上一时刻的 Sweep 点云，从而生成高频低精度的位姿；Lidar Mapping
      
    
    </summary>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/categories/SLAM/"/>
    
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/tags/SLAM/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>SuMa(Surfel-based Mapping)</title>
    <link href="https://leijiezhang001.github.io/SuMa/"/>
    <id>https://leijiezhang001.github.io/SuMa/</id>
    <published>2020-07-20T01:31:34.000Z</published>
    <updated>2020-07-28T03:11:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　目前业界比较流行的基于激光雷达的 SLAM 是 <a href="/LOAM/" title="LOAM">LOAM</a>，其中 Mapping 又是非常重要的一环，LOAM 提取 Edge 点与 Surf 点然后建立以 Voxel 约束点个数的点云地图，该地图用于 Lidar Odometry 时的匹配定位。实际应用于工业界时，Mapping 的数据结构设计及存取管理对整体系统的效率至关重要，具体可优化的细节以后再写文阐述。<br>　　本系列文章<a href="#1" id="1ref"><sup>[1]</sup></a><a href="#2" id="2ref"><sup>[2]</sup></a> 提出了一种基于 Surfel 和语义信息的建图及定位方法。整体框架与 LOAM 类似，只是这里只用了面区域的特征点，其它模块，如优化方式，也有很大的差异。</p><h2 id="suma">1. SuMa</h2><p>　　设 \(A\) 坐标系下的点 \(p _ A\)，\(B\) 坐标系下的点 \(p _ B\)，其变换矩阵 \(T _ {BA}\in\mathbb{R}^{4\times 4}\)，使得 \(p _ B = T _ {BA} p _ A\)。变换矩阵 \(T _ {BA}\) 又由 \(R _ {BA}\in\mathbf{SO}(3)\) 和 \(t _ {BA}\in\mathbb{R}^3\) 构成。设每帧点云的雷达坐标系为 \(C _ k,k\in\{0,...,t\}\)，那么 Lidar Odometry 要求解的问题就是当前雷达坐标系在世界坐标系下的表示： <span class="math display">\[T _ {WC _ t} = T _ {WC _ 0}T _ {C _ 0C _ 1}\cdots T _ {C _ {t-1}C _ t} \tag{1}\]</span> 其中 \(T _ {WC _ 0}\) 为已标定的变换矩阵。 <img src="/SuMa/suma.png" width="65%" height="65%" title="图 1. SuMa Framework"> 　　如图 1. 所示，SuMa 根据点云 \(\mathcal{P} = \{p\in\mathbb{R}^3\}\) 估计 \(T _ {WC _ t}\) 的步骤为：</p><ol type="1"><li>当前帧地图计算。将当前帧的三维点云投影到二维，得到顶点图 \(\mathcal{V} _ D\)，以及计算对应的法向量图 \(\mathcal{N} _ D\)；</li><li>当前地图计算。对上一帧优化出的 Surfel Map \(\mathcal{M} _ {active}\) 作顶点图和法向量图的渲染 \(\mathcal{V} _ M,\mathcal{N} _ M\)；</li><li>位姿计算。根据 \(\mathcal{V} _ D, \mathcal{N} _ D\) 以及 \(\mathcal{V} _ M,\mathcal{N} _ M\) 作 frame-to-model 的 ICP 匹配，得到相对位姿 \(T _ {C _ {t-1}C _ t}\)，最后用式(1)计算当前帧在世界坐标系下的位姿态 \(T _ {WC _ t}\)；</li><li>地图更新。根据 \(T _ {WC _ t}\)，更新 Surfel Map \(\mathcal{M} _ {active}\)：初始化首次观测的区域，优化更新再次观测的区域；</li><li>闭环检测。在未激活的 Surfel Map \(\mathcal{M} _ {inactive}\) 中搜索当前帧地图的匹配；</li><li>闭环检测验证。在接下来 \(\Delta _ {verification}\) 时间内，验证闭环检测的有效性，如果有效，那么加入之后的位姿图优化；</li><li>位姿图优化。另一个线程作位姿图优化，输入信息是前后帧的相对位姿里程计以及闭环检测的相对位姿结果，类似 <a href="/AVP-SLAM/" title="AVP-SLAM">AVP-SLAM</a> 中的位姿图优化。优化后的位姿用来更新 Surfel Map。</li></ol><h3 id="preprocessing">1.1. Preprocessing</h3><p>　　与 RangeNet++<a href="#3" id="3ref"><sup>[3]</sup></a> 中对点云的表示一样，顶点图 \(\mathcal{V} _ D\) 的计算方法为： <span class="math display">\[\left(\begin{matrix}u\\v\\\end{matrix}\right)=\left(\begin{matrix}\frac{1}{2}[1-\mathrm{arctan}(y,x)\cdot \pi ^ {-1}]\cdot w\\[1-(\mathrm{arcsin}(z\cdot r ^ {-1})+f _ {up})f ^ {-1}]\cdot h\end{matrix}\right)\tag{2}\]</span> 其中 \(r = \Vert p\Vert _ 2\) 为点的距离，\(f = f _ {up} + f _ {down}\) 是雷达的上下视野角，\(w,h\) 为顶点图的宽和高。然后基于 \(\mathcal{V} _ D\) 计算每个顶点的法向量，得到法向量图 \(\mathcal{N} _ D\): <span class="math display">\[\mathcal{N} _ D((u,v)) = \left(\mathcal{V} _ D((u+1,v))-\mathcal{V} _ D((u,v))\right)\times \left(\mathcal{V} _ D((u,v+1))-\mathcal{V} _ D((u,v))\right) \tag{3}\]</span> 其中只计算坐标点 \((u,v)\) 有顶点的法向量。因为 \(u\) 方向物理世界是环状的，所以对边界作环向处理。这种法向量计算的 \(\mathcal{N} _ D\) 由较大噪声，但是实验发现对 Frame-to-Model 的 ICP 匹配不会产生精度影响。 <img src="/SuMa/preprocess_suma.png" width="55%" height="55%" title="图 2. SuMa Preprocessing"> 　　顶点图 \(\mathcal{V} _ D\) 与法向量图 \(\mathcal{V} _ N\) 的可视化结果如图 2. 所示。</p><h3 id="map-representation">1.2. Map Representation</h3><p>　　不同于 <a href="/LOAM/" title="LOAM">LOAM</a> 中采用了 Edge 和 Surf 两种特征来表示地图，本文只用 Surfel 来表示地图 \(\mathcal{M}\)。<a href="/LOAM/" title="LOAM">LOAM</a> 中计算了每个点的曲率，然后将其归为 Edge 或是 Surf，实际工程应用中，为了存储的高效性，首先将点云地图体素化，然后将体素内的特征点用 Mean，Normal，协方差矩阵的 EigenVector 等信息来存储，Normal 可用来表征 Surf 特征点，EigenVector 则可用来表征 Edge 特征点，这块具体的细节以后开文再详细阐述。<br>　　本文的 Surfel Map 自然就提取了点云的 Surf 特征，每个Surfel 可以用位置 \(v _ s\in\mathbb{R} ^ 3\)，法向量 \(n _ s\in\mathbb{R} ^ 3\)，半径 \(r _ s\in\mathbb{R}\) 来表示。此外每个 Surf 包含两个时间戳：首次建立的时间 \(t _ c\)，以及最新更新的时间 \(t _ u\)。然后采用贝叶斯滤波方法(详见 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a>)，定义及计算 Surfel 特征的稳定概率： <span class="math display">\[\begin{align}l _ s ^ {(t)} &amp;= l _ s ^ {t-1} + \mathrm{log}(p\cdot (1-p) ^ {-1}) - \mathrm{log}(p _ {prior}\cdot (1-p _ {prior}) ^ {-1})\\&amp;= l _ s ^ {t-1} + \mathrm{odds}(p) - \mathrm{odds}(p _ {prior})\\&amp;= l _ s ^ {t-1} + \mathrm{odds}\left(p _ {stable}\cdot \mathrm{exp}\left(-\frac{\alpha ^ 2}{\sigma _ {\alpha} ^ 2}\right)\mathrm{exp}\left(-\frac{d ^ 2}{\sigma _ d ^ 2}\right)\right) - \mathrm{odds}(p _ {prior})\end{align} \tag{4}\]</span> 其中 \(p _ {stable}, p _ {prior}\) 分别为测量为 surfel 是 stable 的概率，以及先验概率。\(\sigma ^ 2\) 为测量噪声方差。\(\alpha\) 为测量的 Surfel 法向量与对应的地图中 Surfel 法向量的夹角，\(d\) 则为测量的 Surfel 与对应的地图中 Surfel 的距离。<br>　　每个 Surfel 的位置及法向量都是以建立时的位置作为参考系，即 \(C _ {t _ c}\)。这样经过全局位姿优化后，就不需要重新建图，只需要通过 \(T _ {WC _ {t _ c}}\) 将 Surfel 地图更新到世界坐标系即可。<br>　　\(\mathcal{M} _ {active}\) 与 \(\mathcal{M} _ {inactive}\) 的区分也比较简单：\(\mathcal{M} _ {active}\) 定义为最近更新的 Surfels，即 \(t _ u\geq t - \Delta _ {active}\)；\(\mathcal{M} _ {inactive}\) 则定义为不是最近建立的 Surfels，即 \(t _ c&lt; t - \Delta _ {active}\)。Odometry 只在 \(\mathcal{M} _ {active}\) 中作匹配计算，Loop Closure 则只在 \(\mathcal{M} _ {inactive}\) 中搜索。</p><h3 id="odometry-estimation">1.3. Odometry Estimation</h3><p>　　里程计是将当前帧点云与地图点云匹配的过程。将上一时刻的地图 \(\mathcal{M} _ {active}\) 渲染成上一时刻局部坐标系下的顶点图 \(\mathcal{V} _ M\) 与法向量图 \(\mathcal{N} _ M\) 形式。然后采用 point-to-plane 的 ICP 匹配方法，其最小化误差为： <span class="math display">\[ E(\mathcal{V} _ D,\mathcal{V} _ M, \mathcal{N} _ M) = \sum _ {u\in\mathcal{V} _ D}n _ u ^ T\cdot\left(T _ {C _ {t-1}\;C _ t}^{(k)}\;u-v _ u\right) ^ 2 \tag{5}\]</span> 其中 \(u\in\mathcal{V} _ D\)，\(v _ u\in\mathcal{V} _ M,n _ u\in\mathcal{N} _ M\) 是地图上对应关联上的点，关联过程为： <span class="math display">\[\begin{align}v _ u &amp;= \mathcal{V} _ M\left(\Pi\left(T _ {C _ {t-1}\;C _ t}^{(k)}\;u\right)\right)\\n _ u &amp;= \mathcal{N} _ M\left(\Pi\left(T _ {C _ {t-1}\;C _ t}^{(k)}\;u\right)\right)\end{align} \tag{6}\]</span> 其中 \(T _ {C _ {t-1}\;C _ t} ^ {(t)}\) 为 frame-to-model ICP 得到的里程计估计的相对位姿。\(\Pi(u)\) 是特征点的关联方式，<a href="/LOAM/" title="LOAM">LOAM</a> 中根据前后线束的关系来寻找关联方式，本方案则采用直接坐标映射的方式。<strong>因为点云均投影到了前视图，所以可根据坐标直接搜索关联，这也是本方案最重要的优势之一</strong>。具体的，如图对应的地图顶点图中没有顶点，或者地图法向量点没有定义，那么忽略该待关联的特征点；对于关联的特征点对距离大于 \(\sigma _ {ICP}\) 或是法向量夹角大于 \(\theta _ {ICP}\) 的情况，则认为是离群点，不计入误差项。ICP 初始化为上一帧的相对位姿结果。<br>　　该问题是典型的非线性最小二乘问题，可在李空间下对位姿进行线性化并用 Gaussian-Newton 求解，这里不做展开。</p><h3 id="map-update">1.4. Map Update</h3><p>　　得到里程计估计的相对位姿后，要将当前帧的特征点更新到地图中，即要确定哪些 Surfel 要更新，哪些要重新构建新的 Surfel。对于 \(v _ s\in\mathcal{V} _ D\)，首先计算其面元的半径： <span class="math display">\[ r _ s = \frac{\sqrt{2}\Vert v _ s\Vert _ 2\cdot p}{\mathrm{clam}(-v _ s ^ T n _ s\cdot\Vert v _ s\Vert _ 2 ^ {-1}, 0.5, 1.0)} \tag{7}\]</span> 其中 \(p=\mathrm{max}(w\cdot f _ {horiz} ^ {-1}, h\cdot f _ {vert} ^ {-1})\)。<strong>根据式(2)，每个 \(v _ s\) 均能找到地图中对应的 Surfel \(s '\)。</strong>然后通过 \(\vert n _ {s'} ^ T(v _ s-v _ {s'})\vert &lt; \sigma _ M \;\mathrm{and}\; \Vert n _ s\times n _ {s'}\Vert &lt; \mathrm{sin}(\theta _ M)\) 判定当前帧的 Surfel 与地图中的 \(s'\) 是否一致：</p><ul><li>如果一致。那么更新地图中的 Surfel，如果估计的半径更准，那么也更新： <span class="math display">\[\begin{align}v _ {s&#39;} ^ {(t)} &amp;= (1-\gamma)\cdot v _ s + \gamma\cdot v _ {s&#39;} ^ {(t-1)}\\n _ {s&#39;} ^ {(t)} &amp;= (1-\gamma)\cdot n _ s + \gamma\cdot n _ {s&#39;} ^ {(t-1)}\\r _ {s&#39;} ^ {(t)} &amp;= r _ s, \; \mathrm{if} \; r _ s &lt; r _ {s&#39;}\end{align} \tag{8}\]</span></li><li>如果不一致。那么将地图中匹配上的 Surfel 作 Stable 概率衰减，然后创建新的 Surfel。如果地图中没有匹配的 Surfel，那么也创建新的 Surfel。</li></ul><p>最后将 Stable 概率较小的 Surfel 以及时间较早的 Surfel 删除，以此删除动态障碍物特征点以及较老的无关的特征点。</p><h3 id="loop-closures">1.5. Loop Closures</h3><p>　　检测到闭环后就可以作 Pose Graph 优化。闭环检测由检测与验证两部分组成，检测的过程为在未激活的地图 \(\mathcal{M} _ {inactive}\) 中找到一个最相近的位姿： <span class="math display">\[ j ^ * = \mathop{\arg\min}\limits _ {j\in 0,...,t-\Delta _ {active}} \Vert t _ {WC _ t}-t _ {WC _ j}\Vert \tag{9}\]</span> 然后类似 Odometry 的过程，将当前帧的点云特征注册到 \(T _ {WC _ j ^ * }\) 的地图特征中。为了用 ICP 求解两者的相对位姿 \(T _ {C _ {j ^ * }C _ t}\)，初始化 \(T ^ {(0) } _ {C _ {j ^ * }C _ t}\) 为： <span class="math display">\[\begin{align}R _ {C _ {j^ * }C _ t} &amp;= R ^ {-1} _ {WC _ {j ^ * }}R _ {WC _ t}\\t _ {C _ {j^ * }C _ t} &amp;= R ^ {-1} _ {WC _ {j ^ * }}(t _ {WC _ t}-t _ {WC _ {j ^ * }})\\\end{align} \tag{10}\]</span> 本文将 \(T ^ {(0) } _ {C _ {j ^ * }C _ t}\) 中的位移用 \(\lambda t _ {C _ {j ^ * }C _ t}\) 代替，其中 \(\lambda = \{0.0,0.5,1.0\}\)。由此可得到三种初始化后 ICP 迭代的结果，选择最合理的结果即可。<br>　　验证阶段，在 \(t + 1,...,t+ \Delta _ {verification}\) 时间段内，在 \(\mathcal{M} _ {active}\) 与 \(\mathcal{M} _ {inactive}\) 地图中分别作 Odometry 累加，查看两者的一致性，如果一致则认为该闭环检测是有效的。</p><h2 id="suma-1">2. SuMa++</h2><p><img src="/SuMa/suma++.png" width="95%" height="95%" title="图 3. SuMa++ Framework"> 　　SuMa++ 相比 SuMa，只增加了语义信息，算法框架没有改变。如图 3. 所示，SuMa++ 也有当前帧地图计算，当前地图计算，位姿计算，地图更新，闭环检测，闭环检测验证，位姿图优化等七个部分组成，其中，在地图计算中加入了有 RangeNet++ 产生的语义信息，在 \(\mathcal{V} _ D,\mathcal{N} _ D\) 的基础上，增加 \(\mathcal{S} _ D\) 特征；在地图更新中，根据语义信息加入了动态障碍物过滤的策略；在位姿计算中，用语义信息来权重化特征的 ICP 匹配迭代。</p><h3 id="semantic-map">2.1. Semantic Map</h3><p>　　RangeNet++ 也是基于式(2)投影试图下的分割模型，由此可得到 Surfel 特征图 \(\mathcal{V} _ D\) 中每个像素点的语义类别以及对应的类别概率。由于语义分割预测的噪声，本文用 Flood-fill 算法对网络输出的语义分割图 \(\mathcal{S} _ {raw}\) 作优化，得到顶点图对应的语义信息 \(\mathcal{S} _ D\)。 <img src="/SuMa/preprocess.png" width="65%" height="65%" title="图 4. SuMa++ Preprocessing"> 　　考虑到语义分割在物体中心区域确定性较高，而在边缘处不确定性较高，所以 Flood-fill 算法采用两个步骤：</p><ol type="1"><li>用腐蚀算法将与周围语义类别不一致的像素点移除，得到腐蚀后的语义图 \(\mathcal{S} _ {raw} ^ {eroded}\)；</li><li>结合有深度信息的顶点图 \(\mathcal{V} _ D\)，对腐蚀的边缘像素点填充为周围相近距离的顶点对应的语义类别，得到 \(\mathcal{S} _ D\)；</li></ol><p>如图 4. 所示，该方法能修正边缘类别错误的情况。由此，\(\mathcal{V} _ D, \mathcal{N} _ D,\mathcal{S} _ D\)组成每一帧的特征点信息。</p><h3 id="filtering-dynamics">2.2. Filtering Dynamics</h3><p><img src="/SuMa/res.png" width="65%" height="65%" title="图 5. Filterring Dynamics"> 　　有了语义类别信息后，在更新地图时，可计算当前帧每个 Surfel 与地图中对应 Surfel 的类别一致性，由此作为地图贝叶斯更新的惩罚项，如果类别不一致，地图中的 Surfel 稳定性概率会降低，直到去除。如图 5. 所示，这种方法能去除大部分动态障碍物区域所构成的 Surfel。地图具体的贝叶斯更新为： <span class="math display">\[\begin{align}l _ s ^ {(t)} = l _ s ^ {t-1} + \mathrm{odds}\left(p _ {stable}\cdot \mathrm{exp}\left(-\frac{\alpha ^ 2}{\sigma _ {\alpha} ^ 2}\right)\mathrm{exp}\left(-\frac{d ^ 2}{\sigma _ d ^ 2}\right)\right) - \mathrm{odds}(p _ {prior}) - \mathrm{odds}(p _ {penalty})\end{align} \tag{11}\]</span></p><h3 id="semantic-icp">2.3. Semantic ICP</h3><p>　　在式(5)的 ICP 误差项基础上，可加入语义约束，对误差项作权重化： <span class="math display">\[ E(\mathcal{V} _ D,\mathcal{V} _ M, \mathcal{N} _ M) = \sum _ {u\in\mathcal{V} _ D}w _ u n _ u ^ T\cdot\left(T _ {C _ {t-1}\;C _ t}^{(k)}\;u-v _ u\right) ^ 2 \tag{12}\]</span> 其中权重项结合了语义约束与几何约束，以此来减少离群特征点对优化的影响： <span class="math display">\[w _ u ^{(k)} = \rho _ {Huper}\left(r _ u ^ {(k)}C _ {semantic}(\mathcal{S} _ D(u),\mathcal{S} _ M(u))\right)\mathbb{1}\left\{l _ s ^ {(k)}\geq l _ {stable}\right\} \tag{13}\]</span> 其中 \(\rho _ {Huber}(r)\) 是 Huber 核函数： <span class="math display">\[\rho _ {Huber}(r)=\left\{\begin{array}{l}1 &amp;,\mathrm{if}\;\vert r\vert &lt; \sigma\\\sigma\vert r\vert ^ {-1} &amp;,\mathrm{otherwise}\end{array}\tag{14}\right.\]</span> 语义约束项为： <span class="math display">\[C _ {semantic}\left((y _ u,P _ u),(y _ {v _ u}, P _ {v _ u})\right)=\left\{\begin{array}{l}P(y _ u|u) &amp;,\mathrm{if}\;y _ u=y _ {v _ u}\\1-P(y _ u|u) &amp;,\mathrm{otherwise}\end{array}\tag{15}\right.\]</span></p><p><img src="/SuMa/icp.png" width="65%" height="65%" title="图 6. Weights of ICP"> 　　如图 6. 所示，在语义信息的约束下，如果当前帧某个 Surfel 的类别与地图中对应的 Surfel 类别不一致，那么就会减少该 Surfel 匹配对的误差项。</p><h2 id="thinkings">3. Thinkings</h2><p>　　利用检测或分割得到的语义信息去过滤当前帧以及地图中的动态障碍物，在 SLAM/Odometry 中已经非常常见，其实可以大概率相信语义信息，然后直接将对应的点云干掉。而本文以融合迭代的思路，想通过将信将疑的方式来完成有效的 ICP 匹配（既要滤掉大多数动态障碍物的影响，也期望一堆车停在场景中时然后保留足够匹配的特征点）。但是一般工程上，直接干掉也够用，毕竟场景够大，不太可能出现特征点不够匹配的情景。<strong>而本方法的高效性在于，寻找当前帧与地图中的 Surfel 匹配时，直接采用图像索引然后顶点图距离及法向量图角度判断有效性的形式，没有 KD-Tree，极大提高效率</strong>，类似 ICPCUDA<a href="#4" id="4ref"><sup>[4]</sup></a>。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Behley, Jens, and Cyrill Stachniss. &quot;Efficient Surfel-Based SLAM using 3D Laser Range Data in Urban Environments.&quot; Robotics: Science and Systems. 2018.<br><a id="2" href="#2ref">[2]</a> Chen, Xieyuanli, et al. &quot;Suma++: Efficient lidar-based semantic slam.&quot; 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2019.<br><a id="3" href="#3ref">[3]</a> Milioto, Andres, et al. &quot;RangeNet++: Fast and accurate LiDAR semantic segmentation.&quot; 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2019.<br><a id="4" href="#4ref">[4]</a> https://github.com/mp3guy/ICPCUDA</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　目前业界比较流行的基于激光雷达的 SLAM 是 &lt;a href=&quot;/LOAM/&quot; title=&quot;LOAM&quot;&gt;LOAM&lt;/a&gt;，其中 Mapping 又是非常重要的一环，LOAM 提取 Edge 点与 Surf 点然后建立以 Voxel 约束点个数的点云地图，该地图用于 
      
    
    </summary>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/categories/SLAM/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/tags/SLAM/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>AVP-SLAM</title>
    <link href="https://leijiezhang001.github.io/AVP-SLAM/"/>
    <id>https://leijiezhang001.github.io/AVP-SLAM/</id>
    <published>2020-07-15T01:17:56.000Z</published>
    <updated>2020-07-17T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　Visual-SLAM 一般采用特征点或像素直接法来建图定位，这种方式对光照较为敏感。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 提出了一种基于语义特征的 Visual Semantic SLAM，应用于光照条件较为复杂的室内停车场环境，相比于采用特征点的 ORB-SLAM，性能较为鲁棒。</p><h2 id="framework">1. Framework</h2><p><img src="/AVP-SLAM/framework.png" width="95%" height="95%" title="图 1. AVP-SLAM Framework"> 　　如图 1. 所示，AVP-SLAM 由 Mapping，Localization 两部分组成。Mapping 阶段，将车周围的四张图通过 IPM 拼接并变换到俯视图，然后作 Guide Signs，Parking Lines，Speed Bumps 等语义信息的提取，接着通过 Odometry 将每帧的特征累积成局部地图，最后通过回环检测，全局优化出全局地图。Localization 阶段，提取出每帧的语义信息后，用 Odometry 初始化位姿，然后用 ICP 匹配求解当前帧在全局地图中的位姿，得到基于地图的位姿观测量，最后用 EKF 融合该观测量与 Odometry 信息，得到本车的最终位姿。<br>　　有了本车在全局地图下的位姿后，然后通过语义信息识别停车位，即可达到本车自动泊车的目的。</p><h2 id="mapping">2. Mapping</h2><h3 id="ipm">2.1. IPM</h3><p>　　传感器为车身四周四个鱼眼相机，相机内外参已知。IPM(Inverse Perspective Mapping) 是将图像中的像素点投影到车身物理坐标系下的俯视图中，具体的： <span class="math display">\[\frac{1}{\lambda}\;\begin{bmatrix}x ^ v \\y ^ v \\1\end{bmatrix} =[\mathbf{R} _ c \;\mathbf{t} _ c] ^ {-1} _ {col:1,2,4} \;\pi _ c ^ {-1}\begin{bmatrix}u \\v \\1\end{bmatrix}\tag{1}\]</span> 其中 \(\pi _ c(\cdot)\) 为鱼眼相机的内参矩阵，\([\mathbf{R} _ c\;\mathbf{t} _ c]\) 为每个相机到车身坐标系的外参矩阵，\([x ^ v\; y ^ v]\) 为车身坐标系下语义特征的位置。关于 IPM 更多细节可参考 <a href="/lane-det-from-BEV/" title="Apply IPM in Lane Detection from BEV">Apply IPM in Lane Detection from BEV</a>。<br>　　进一步将 IPM 图拼接成一张全景图： <span class="math display">\[\begin{bmatrix}u _ {ipm}\\v _ {ipm}\\1\end{bmatrix}=\mathbf{K} _ {ipm}\begin{bmatrix}x ^ v \\y ^ v \\1\end{bmatrix}\tag{2}\]</span> 其中 \(\mathbf{K} _ {ipm}\) 是全景图的内参。</p><h3 id="feature-detection">2.2. Feature Detection</h3><p><img src="/AVP-SLAM/segment.png" width="65%" height="65%" title="图 2. Segmentation in IPM Image"> 　　将每张 IPM 图拼接成一张大全景图，然后用基于深度学习的语义分割方法，对全景图作像素级别作 lane，parking line，guide sign，speed bump，free space，obstacle，wall 等类别的语义分割。如图 4. 所示，parking line，guide sign，speed bump 是稳定的特征，用于定位；parking line 用于车位的识别；free space 与 obstacle 用于路径规划。</p><h3 id="local-mapping">2.3. Local Mapping</h3><p>　　全景图语义分割得到的用于定位的特征(parking line，guide sign，speed bump)需要反投影回车身物理坐标系： <span class="math display">\[\begin{bmatrix}x ^ v \\y ^ v \\1\end{bmatrix}=\mathbf{K} _ {ipm} ^ {-1}\begin{bmatrix}u _ {ipm}\\v _ {ipm}\\1\end{bmatrix}\tag{3}\]</span> 然后基于 Odometry 的相对位姿，将当前的语义特征点转换到世界坐标系下： <span class="math display">\[\begin{bmatrix}x ^ w \\y ^ w \\z ^ w\end{bmatrix}=\mathbf{R _ o}\begin{bmatrix}x ^ v \\y ^ v \\0\end{bmatrix} + \mathbf{t _ o}\tag{4}\]</span> 由此得到局部地图，本文保持车身周边 30m 内的局部地图。</p><h3 id="loop-detection">2.4. Loop Detection</h3><p><img src="/AVP-SLAM/loop_det.png" width="65%" height="65%" title="图 3. Loop Detection"> 　　因为 Odometry 有累计误差，所以这里对局部地图作一个闭环检测。如图 3. 所示，通过 ICP 对两个局部地图作匹配，一旦匹配成功，就说明检测到了闭环，ICP 匹配的相对位姿用于之后的全局位子图优化，以消除里程计累计误差。</p><h3 id="global-optimization">2.5. Global Optimization</h3><p>　　检测到闭环后，需进行全局位姿图优化。位姿图中，节点(node)为每个局部地图的位姿：\((\mathbf{r, t})\)；边(edge)有两种：odometry 相对位姿以及闭环检测中 ICP 匹配位姿。由此位姿图优化的损失函数为： <span class="math display">\[\chi ^ * = \mathop{\arg\min}\limits _ \chi \sum _ t\Vert f(\mathbf{r} _ {t+1},\mathbf{t} _ {t+1}, \mathbf{r} _ t, \mathbf{t} _ t) - \mathbf{z} ^ o _ {t,t+1}\Vert ^ 2 + \sum _ {i,j\in\mathcal{L}}\Vert f(\mathbf{r} _ i,\mathbf{t} _ i,\mathbf{r} _ j, \mathbf{t} _ j)-\mathbf{z} ^ l _ {i,j}\Vert ^ 2 \tag{5}\]</span> 其中 \(\chi = [\mathbf{r} _ 0,\mathbf{t} _ 0,...,\mathbf{r} _ t,\mathbf{t} _ t] ^ T\) 是所有局部地图的位姿，也是待优化的参数。\(\mathbf{z} ^ 0 _ {t,t+1}\) 为 Odometry 得到的位姿。\(\mathbf{z} ^ l _ {i,j}\) 为闭环检测 ICP 得到的位姿。\(f(\cdot)\) 为计算两个局部地图相对位姿的方程。该优化问题可通过 Gauss-Newton 法求解。<br>　　用优化后的位姿将局部地图叠加起来，就获得了整个场景的全局地图。</p><h2 id="localization">3. Localization</h2><p><img src="/AVP-SLAM/loc.png" width="65%" height="65%" title="图 4. Localization"> 　　有了全局地图后，基于全局地图的定位观测量可通过当前帧与全局地图的匹配得到。如图 4. 所示，绿色为当前帧检测到的语义特征，与全局地图匹配后即可得到当前的绝对位置。匹配通过 ICP 算法实现： <span class="math display">\[ \mathbf{r ^ * ,t ^ * } =  \mathop{\arg\min}\limits _ {\mathbf{r,t}}\sum _ {k\in\mathcal{S}}\Vert\mathbf{R(r)}\begin{bmatrix}x ^ v  _ k\\y ^ v  _ k\\0\end{bmatrix} + \mathbf{t} - \begin{bmatrix}x ^ w _ k \\y ^ w _ k\\z ^ w _ k\end{bmatrix}\Vert ^ 2 \tag{6}\]</span> 其中 \(\mathcal{S}\) 为当前帧语义特征点集，\([x _ k ^ w\; y _ k ^ w\; z _ k ^ w]\) 分别为对应的全局地图中最近的特征点集。<br>　　ICP 的初始化非常重要，本文提出了两种初始化方法：1. 直接在地图上标记车库入口作为全局坐标点；2. 室外 GPS 信号初始化，然后用 Odometry 累积到车库。</p><h2 id="extended-kalman-filter">4. Extended Kalman Filter</h2><p>　　Visual Localization 在语义特征较少的情况下，比如车辆停满了，定位会不稳定，所以这里采用 EKF 对 Visual Localization 与 Odometry 中的轮速计和 IMU 作扩展卡尔曼融合，这里不做展开。</p><h2 id="thinkings">5. Thinkings</h2><p>　　Semantic SLAM 相比基于几何特征点的 SLAM 更加鲁棒。但是在车库场景下，一旦车子停满后，停车线等语义信息会急剧减少，所以实际商业应用中，AVP-SLAM 能满足室内自动泊车的需求吗？<br>　　对此我持怀疑态度。我认为，对于车库自动泊车的商业落地，可能最有效且低成本的方法还是得基于室内 UWB 定位技术。至少 UWB 可作为辅助。当然要将 UWB 应用于车载装置，目前好像还没有，不过随着车载软硬件系统的完善，手机上能做的事，车载平台问题也不大。</p><h2 id="reference">6. Reference</h2><p><a id="1" href="#1ref">[1]</a> Qin, Tong, et al. &quot;AVP-SLAM: Semantic Visual Mapping and Localization for Autonomous Vehicles in the Parking Lot.&quot; arXiv preprint arXiv:2007.01813 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Visual-SLAM 一般采用特征点或像素直接法来建图定位，这种方式对光照较为敏感。本文&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt; 提出了一种基于语义特征的 Visual Semantic SLAM，应用于光照条件较为复杂的室内
      
    
    </summary>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/categories/SLAM/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/tags/SLAM/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-Point-GNN/"/>
    <id>https://leijiezhang001.github.io/paper-reading-Point-GNN/</id>
    <published>2020-07-10T01:22:07.000Z</published>
    <updated>2020-07-14T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种基于图网络来提取点云特征的方法，理论上可在不损失原始信息的情况下，高效的学习点云特征，其在点云 3D 检测任务中效果提升明显。</p><h2 id="different-point-cloud-representations">1. Different Point Cloud Representations</h2><p><img src="/paper-reading-Point-GNN/repr.png" width="65%" height="65%" title="图 1. Point Cloud Representations"> 　　如图 1. 所示，目前点云表示方式以及对应的特征学习方式有三种：Grids，栅格化后类似图像 2D/3D 卷积的形式；Sets，以 PointNet 为代表的最近邻查找周围点并学习的形式；Graph，将无序点集转换为图模型，特征信息通过点云顶点传递学习的形式。Grids 及 Sets 形式我们已经比较熟悉了，Graph 则查询效率比 Sets 高，特征提取能力又比 Grids 高。Graph 的建图时间复杂度为 \(\mathcal{O}(cN)\)，领域查询复杂度则为 \(\mathcal{O}(1)\)，Sets 中的 KNN 建树及查询复杂度可见 <a href="/PointCloud-Feature-Extraction/" title="PointCloud Feature Extraction">PointCloud Feature Extraction</a>。当然 KNN 式的领域查询方式可以用近似 \(\mathcal{O}(1)\) 方法实现，但是会影响特征学习的准确度。</p><h2 id="framework">2. Framework</h2><p><img src="/paper-reading-Point-GNN/framework.png" width="95%" height="95%" title="图 2. Framework of Point-GNN"> 　　如图 2. 所示，基于 Graph 的 3D 点云检测，首先对点云作 Graph Construction，然后用 GNN 来学习每个顶点的特征，接着对每个顶点预测目标框，最后作目标框的整合和 NMS。</p><h3 id="graph-construction">2.1. Graph Construction</h3><p>　　设点云集：\(P=\{p _ 1,...,p _ N\}\)，其中 \(p _ i=(x _ i, s _ i)\) 分别表示坐标 \(x _ i\in\mathbb{R} ^ 3\)，以及该点反射率，领域点相对位置等信息 \(s _ i\in\mathbb{R} ^ k\)。对该点集建图 \(G=(P,E)\)，将距离小于一定阈值的两个点进行连接，即： <span class="math display">\[E = \{(p _ i, p _ j)|\Vert x _ i-x _ j\Vert _ 2 &lt; r\} \tag{1}\]</span> 这种建图方式是 Fixed Radius Near-Neighbors 问题，可在 \(\mathcal{O}(cN)\) 时间复杂度下解决，其中 \(c\) 为最大连接数。<br>　　建图完成后，要对每个点信息状态 \(s _ i\) 作初始化。这里采用类似 Sets 的特征提取方式，即将该点的反射率，以及与领域内点的相对位置，串联成特征向量，然后用 MLP 作空间变换，最后在点维度上作 Max Pooling，即可得到初始化的该点特征状态量 \(s _ i\)。</p><h3 id="graph-neural-network-with-auto-registration">2.2. Graph Neural Network with Auto-Registration</h3><p>　　传统的图神经网络，通过边迭代每个顶点的特征。在 \((t+1) ^ {th}\) 迭代时： <span class="math display">\[\begin{align} v _ i ^ {t+1} &amp;= g ^ t\left(\rho\left(\{e _ {ij} ^ t|(i,j)\in E\}\right), v _ i ^ t\right) \\e _ {ij} ^ t &amp;= f ^ t(v _ i ^ t, v _ j ^ t) \tag{2}\end{align}\]</span> 其中 \(e ^ t,v ^ t\) 分别是边和顶点特征，\(f ^ t(\cdot)\) 计算两个顶点之间边的特征，\(\rho(\cdot)\) 将与该点连接的边特征整合，得到该点特征增量，\(g ^ t(\cdot)\) 将该点特征增量与原特征进行整合得到本次迭代后该点的最终特征。<br>　　对于边特征，一种设计方式为，描述领域特征对该点位置的作用力，重写式 (2)： <span class="math display">\[s _ i ^ {t+1} = g ^ t\left(\rho\left(\{f ^ t(x _ j-x _ i,s _ j^t)|(i,j)\in E\}\right), s _ i ^ t\right) \tag{3}\]</span> 这样就得到了图神经网络的迭代模型。此外，本文还指出，由于边特征对领域点的距离较为敏感，所以作者提出对相对位置作自动补偿，实验表明其实意义不大： <span class="math display">\[\begin{align}\Delta x _ i ^ t &amp;= h ^ t(s _ i^t) \\s _ i ^ {t+1} &amp;= g ^ t\left(\rho\left(\{f ^ t(x _ j-x _ i+\Delta x _ i ^ t,s _ j^t)|(i,j)\in E\}\right), s _ i ^ t\right) \tag{4}\end{align}\]</span> 　　具体的，\(f ^ t(\cdot),g ^ t(\cdot), h ^ t(\cdot)\) 可用 MLP 来建模，\(\rho(\cdot)\) 则采用 Max 操作： <span class="math display">\[\begin{align}\Delta x _ i ^ t &amp;= MLP _ h ^ t(s _ i^t) \\e _ {ij} ^ t &amp;= MLP _ f ^ t([x _ j - x _ i + \Delta x _ i ^ t, s _ j ^ t]) \\s _ i ^ {t+1} &amp;= MLP _ g ^ t\left(MAX(\{e _ {ij}|(i,j)\in E\})\right)+ s _ i ^ t \tag{5}\end{align}\]</span></p><h3 id="loss">2.3. Loss</h3><p>　　为了作 3D 检测的任务，网络头输出为每个顶点的类别，目标框中心的 offset，以及目标框的尺寸，朝向。这与传统的基于 Ancho-Free 的 3D 目标检测基本一致，这里不做展开。</p><h3 id="box-merging-and-scoring">2.4. Box Merging and Scoring</h3><p>　　本方法的 3D 检测需要作 NMS 后处理，由于分类的 Score 不能反应目标框的 Uncertainty，所以基于 Score 的 NMS 是不合理的。这个问题在 2D 检测中也有比较多的研究，比如采用预测 IoU 值的方式来作权重。本文则认为遮挡信息能作为 NMS 操作的指导，由此定义了遮挡值的计算方式。但是实验显示，其实提升并不明显，所以这里不做具体展开。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Shi, Weijing, and Raj Rajkumar. &quot;Point-gnn: Graph neural network for 3d object detection in a point cloud.&quot; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　本文&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt;提出了一种基于图网络来提取点云特征的方法，理论上可在不损失原始信息的情况下，高效的学习点云特征，其在点云 3D 检测任务中效果提升明显。&lt;/p&gt;
&lt;h2 id=&quot;different-p
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>CenterTrack</title>
    <link href="https://leijiezhang001.github.io/CenterTrack/"/>
    <id>https://leijiezhang001.github.io/CenterTrack/</id>
    <published>2020-07-02T01:16:36.000Z</published>
    <updated>2020-07-08T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　障碍物感知由目标检测，目标跟踪(MOT)，目标状态估计等三个模块构成。目标状态估计一般是指将位置，速度等观测量作卡尔曼滤波平滑；广义的目标跟踪也包含了状态估计过程，这里采用狭义的目标跟踪定义方式，主要指出目标 ID 的过程。传统的做法，目标检测与目标跟踪是分开进行的，检测模块分别对前后帧作目标检测，目标跟踪模块则接收前后帧检测结果，然后用 Motion Model 将上一帧的检测结果预测到这一帧，最后与这一帧的检测结果作数据关联(Data Association)出目标 ID。这里的 Motion Model 可以是 3D 下目标的物理运动模型，也可以是图像下的单目标跟踪结果，如 KCF 算法。详细介绍可参考 <a href="/MOT-综述-Multiple-Object-Tracking-A-Literature-Review/" title="Multiple Object Tracking: A Literature Review">Multiple Object Tracking: A Literature Review</a>。<br>　　随着检测技术的发展，检测与跟踪的整合成为了趋势。<a href="#1" id="1ref">[1]</a> 是较早将跟踪的 “Motion Model” 用 Anchor-based Two-stage 网络来预测的方法，其网络输入为前后帧图像，其中一个分支输出当前帧的检测框，另一个分支用上一帧的检测结果作为 proposal，输出这一帧的跟踪框，最后用传统的数据关联方法得到目标的 ID。随着检测技术往 Anchor-Free One-stage 方向发展，在此基础上整合目标检测与跟踪也就顺理成章。<br>　　<a href="/Anchor-Free-Detection/" title="Anchor-Free Detection">Anchor-Free Detection</a> 中详细描述了 Anchor-Free 的目标检测方法，相比于 Anchor-Based 的目标检测，其有很多优势，这里不做赘述。本文基于 CenterNet<a href="#2" id="2ref"><sup>[2]</sup></a>，总结了 CenterTrack<a href="#3" id="3ref"><sup>[3]</sup></a>，以及 CenterPoint(3D CenterTrack)<a href="#4" id="4ref"><sup>[4]</sup></a>方法。</p><h2 id="centernet">1. CenterNet</h2><p>　　CenterNet 在 <a href="/Anchor-Free-Detection/" title="Anchor-Free Detection">Anchor-Free Detection</a> 中已经较为详细得阐述了。需要补充的是，中心点的正负样本设计为：正样本只有中心点像素，负样本则为其它区域，并加入以中心点为中心的高斯权重，越靠近中心点，负样本权重越小。其 Loss 基于 Focal Loss，数学描述为： <span class="math display">\[L _ k = \frac{1}{N}\sum _ {xyc}\left\{\begin{array}{l}(1-\hat{Y} _ {xyc})^{\alpha}\mathrm{log}(\hat{Y} _ {xyc}) &amp; \mathrm{if}\; Y _ {xyc} = 1\\(1- Y _ {xyc})^{\beta}(\hat{Y} _ {xyc})^{\alpha}\mathrm{log}(1-\hat{Y} _ {xyc}) &amp; \mathrm{otherwise}\end{array}\tag{1}\right.\]</span> 其中 \(Y _ {xyc}\) 为高斯权重后的正负样本分布值。<br>　　具体的，设图像 \(I\in \mathbb{R}^{W\times H\times 3}\)，CenterNet 输出的每个类别 \(c\in\{0,...,C-1\}\) 的目标为 \(\{(\mathbf{p} _ i, \mathbf{s} _ i)\} _ {i=0} ^ {N-1}\)。其中 \(\mathbf{p}\in \mathbb{R} ^ 2\)，\(\mathbf{s}\in\mathbb{R} ^ 2\) 为目标框的尺寸。对应的，最终输出的 heatmap 位置和尺寸图为：\(\hat{Y}\in [0,1]^{\frac{W}{R}\times\frac{H}{R}\times C}\)，\(\hat{S}\in\mathbb{R}^{\frac{W}{R}\times\frac{H}{R}\times 2}\)。对 \(\hat{Y}\) 作 \(3\times 3\) 的 max pooling，即可获得目标中心点，\(\hat{S}\) 上对应的的点即为该目标的尺寸。此外还用额外的 heatmap 作位置 offset 的回归，因为 \(\hat{Y}\) 存在量化误差。最终由中心点位置 loss，位置 offset loss，尺寸 loss 三部分组成。</p><h2 id="centertrack">2. CenterTrack</h2><h3 id="framework">2.1. Framework</h3><p><img src="/CenterTrack/centertrack.png" width="85%" height="85%" title="图 1. CenterTrack"> 　　如图 1. 所示，CenterTrack 基于 CenterNet，框架也较为简单：输入前后帧图像，以及上一帧跟踪到的目标中心点所渲染的 heatmap，经过网络后输出为当前帧的检测 heatmap，size map，以及这一帧相对上一帧跟踪的 offset map。最后通过最近距离匹配即可作数据关联获得目标的 ID。算法得到的目标属性有 \(b = (\mathbf{p,s},w,id)\)，分别为目标的 location，size，confidence，identity。<br>　　相比于 CenterNet，CenterTrack 还预测了这一帧相对上一帧，目标的 2D displacement：\(\hat{D}\in\mathbb{R}^{\frac{W}{R}\times\frac{H}{R}\times 2}\)。这相当于 Tracking 中 Motion Model 的结果，分别计算上一帧目标经过该 displacement 变换到这一帧后的目标位置与当前帧检测的目标位置的距离误差，用最小距离的贪心法即可将目标作数据关联，得到目标的 ID。</p><h3 id="experiments">2.2. Experiments</h3><p>　　网络结构相比于 CenterNet 只是增加了输入的四个通道特征，输出的两个通道特征。网络可在视频流图像或者单帧图像上训练，对于单帧图像，可对图像中的目标作伸缩平移变换来模拟目标运动，实验表明，也非常有效。 <img src="/CenterTrack/motion_models2d.png" width="85%" height="85%" title="图 2. Motion Models"> 　　如图 2. 所示，本文比较了 displacement 与 kalman filter，optical flow 等 Motion Model，显示本文效果是最好的，我猜测是因为 displacement 回归的直接是物体级别的像素运动，抗噪性更强。</p><h2 id="center-based-3d-object-detection-and-tracking">3. Center-based 3D Object Detection and Tracking</h2><h3 id="framework-1">3.1. Framework</h3><p><img src="/CenterTrack/centertrack3d.png" width="85%" height="85%" title="图 3. 3D CenterTrack"> 　　如图 3. 所示，CenterPoint 将点云在俯视图下栅格化，然后采用 CenterTrack 一样的网络结构，只是输出为目标的 3D location，size，orientation，velocity。<br>　　点云俯视图下的栅格化，如果对栅格不做点云的精细化估计，那么会影响到目标位置及速度估计的精度，所以理论上 PointPillars 这种栅格点云特征学习方式能更有效的提取点云的信息，保留特征的连续化信息(但是论文的实验表明 VoxelNet 比 PointPillars 效果更好)。否则，虽然目标位置等信息的监督项是连续量，但是栅格化的特征是离散量，这会降低预测精度。<br>　　具体的，网络输出为：\(K\) 个类别的 \(K\)-channel heatmap 表示目标中心点，目标的尺寸 \(\mathbf{s}=(w,l,h)\) heatmap，目标的中心点 offset \(\mathbf{o}=(o _ x,o _ y,o _ z)\) heatmap，朝向角 \(\mathbf{e} = (\mathrm{sin}(\alpha),\mathrm{cos}(\alpha))\) heatmap，目标速度 \(\mathbf{v}=(v _ x,v _ y)\) heatmap。与 CenterTrack 非常相似，只不过这里的速度就是真实的物理速度。</p><h3 id="experiments-1">3.2. Experiments</h3><p><img src="/CenterTrack/detmap.png" width="85%" height="85%" title="图 4. 3D Detection Benchmark"> 　　如图 4. 所示，引入 Velocity 预测，能有效提升检测的性能，这应该是网络输入前一帧信息的结果，对半遮挡情况能有较好效果。 <img src="/CenterTrack/experiment3d.png" width="85%" height="85%" title="图 5. 3D MOT Benchmark"> 　　如图 5. 所示，跟踪性能也是有很大提升，而且数据关联等后处理相对比较简单。</p><h2 id="reference">5. Reference</h2><p><a id="1" href="#1ref">[1]</a> Feichtenhofer, Christoph, Axel Pinz, and Andrew Zisserman. &quot;Detect to track and track to detect.&quot; Proceedings of the IEEE International Conference on Computer Vision. 2017.<br><a id="2" href="#2ref">[2]</a> Zhou, Xingyi, Dequan Wang, and Philipp Krähenbühl. &quot;Objects as points.&quot; arXiv preprint arXiv:1904.07850 (2019).<br><a id="3" href="#3ref">[3]</a> Zhou, Xingyi, Vladlen Koltun, and Philipp Krähenbühl. &quot;Tracking Objects as Points.&quot; arXiv preprint arXiv:2004.01177 (2020).<br><a id="4" href="#4ref">[4]</a> Yin, Tianwei, Xingyi Zhou, and Philipp Krähenbühl. &quot;Center-based 3D Object Detection and Tracking.&quot; arXiv preprint arXiv:2006.11275 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　障碍物感知由目标检测，目标跟踪(MOT)，目标状态估计等三个模块构成。目标状态估计一般是指将位置，速度等观测量作卡尔曼滤波平滑；广义的目标跟踪也包含了状态估计过程，这里采用狭义的目标跟踪定义方式，主要指出目标 ID 的过程。传统的做法，目标检测与目标跟踪是分开进行的，检
      
    
    </summary>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/categories/MOT/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="Tracking" scheme="https://leijiezhang001.github.io/tags/Tracking/"/>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/tags/MOT/"/>
    
  </entry>
  
  <entry>
    <title>Rethinking of Sparse 3D Convolution</title>
    <link href="https://leijiezhang001.github.io/Rethinking-of-Sparse-3D-Convolution/"/>
    <id>https://leijiezhang001.github.io/Rethinking-of-Sparse-3D-Convolution/</id>
    <published>2020-06-23T09:37:12.000Z</published>
    <updated>2020-06-25T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　Sparse 3D Convolution 最早在<a href="#1" id="1ref">[1]</a>中提出，然后该作者又提出了 Submanifold Sparse Convolution<a href="#2" id="2ref"><sup>[2]</sup></a>，并将其应用于 3D 语义分割中<a href="#3" id="3ref"><sup>[3]</sup></a>。<a href="#4" id="4ref">[4]</a>则改进了 Sparse 3D Convolution 的实现方式，并应用于 3D 目标检测中。之前一直没仔细看 Sparse 3D Convolution 原理，以为只是基于稀疏矩阵的矩阵相乘加速，最近的一些实验发现 Sparse 3D Convolution 在点云相关的任务中不仅仅是加速，还能提升网络特征提取的性能，所以回过头来重新思考 Sparse 3D Convolution 原理及作用。</p><h2 id="sparse-convolution">1. Sparse Convolution</h2><p><img src="/Rethinking-of-Sparse-3D-Convolution/spconv.png" width="85%" height="85%" title="图 1. sparse VS. submanifold sparse"> 　　如图 1. 左图所示，对于稀疏的特征输入，传统的 Sparse Convolution 与 Convolution 一致，只是对于卷积核覆盖的输入特征为零的区域不做计算，直接置为零。这种方式下，随着卷积层的增加，特征层会变得不那么稀疏，这样不仅使得计算量上升，而且会使得提取的信息变得不那么准确。</p><h2 id="submanifold-sparse-convolution">2. Submanifold Sparse Convolution</h2><p>　　如图 1. 右图所示，Submanifold Sparse Convolution 解决了 Sparse Convolution 存在的问题。原理也很直观：只计算输出特征层映射到输入特征层不为零的位置区域。这种方式下，随着卷积层的增加，不仅能保持稀疏性，而且能保证原始信息的准确性。 <img src="/Rethinking-of-Sparse-3D-Convolution/flops.png" width="85%" height="85%" title="图 2. Flops"> 　　如图 2. 所示，Sparse Convolution 相比传统的 Convolution 已经能减少较多的计算量，而 Submanifold Sparse Convolution 则能减少更多的计算量。特征输入越稀疏，减少的计算量就越多，这对点云的三维特征提取，或者是俯视图下的二维特征提取有很大的帮助。</p><h2 id="implementation">3. Implementation</h2><p><img src="/Rethinking-of-Sparse-3D-Convolution/speed.png" width="85%" height="85%" title="图 3. Speed"> 　　<a href="#2" id="2ref">[2]</a> 中实现了 Submanifold Sparse Convolution，其中的卷积运算是手写的矩阵相乘，所以速度较慢；<a href="#4" id="4ref">[4]</a> 则基于 GEMM 实现了更高效的 Submanifold Sparse Convolution。如图 3. 所示，其有将近一倍的速度提升。 <img src="/Rethinking-of-Sparse-3D-Convolution/imple.png" width="90%" height="90%" title="图 4. Implementation"> 　　图 4. 描述了<a href="#4" id="4ref">[4]</a>实现的 Submanifold Sparse Convolution 原理。其首先通过 gather 操作将非零的元素进行矩阵相乘，然后通过 scatter 操作将结果映射回原位置。为了加速，前后元素的映射矩阵计算比较关键，这里实现了一种 GPU 计算方法，这里不做展开。</p><h2 id="application">4. Application</h2><p><img src="/Rethinking-of-Sparse-3D-Convolution/second.png" width="90%" height="90%" title="图 5. SECOND Framework"> 　　Submanifold Sparse Convolution 可应用于点云的分类，分割，检测等任务的特征提取中，SECOND<a href="#4" id="4ref"><sup>[4]</sup></a>是一种点云检测方法，如图 5. 所示，其检测框架与传统的一致，只是将体素化后的点云特征信息，进一步用 Sparse Convolution 来作特征提取。该方法不仅速度较快，而且性能也有不少提升。所以 Submanifold Sparse Convolution 是非常高效的，可作为点云特征提取的基本操作。但是传统的 Convolution，在 GPU 平台下，已经有较多的硬件级优化(cudnn)，在 CPU 平台下也有很多的指令集优化，所以最终在特定硬件下作 Inference 时，到底 Submanifold Sparse Convolution 速度能提升多少，还得看 Submanifold Sparse Convolution 实现的好不好。不过可以猜测，在目前的实现下，Submanifold Sparse Convolution 在 GPU 平台下应该能有不少的速度提升。<br>　　此外，传统的卷积量化操作也比较成熟，cudnn 已经有基本的操作引擎，而 Submanifold Sparse Convolution 的 INT8 引擎则目前还没有。所以 float32/float16 的 Submanifold Sparse Convolution 与 INT8 的 Convolution，孰快孰慢？这两条路大概就是部署的思路了，当然 INT8 的 Submanifold Sparse Convolution 会更好，但是开发成本会比较高。</p><h2 id="reference">5. Reference</h2><p><a id="1" href="#1ref">[1]</a> Graham, Ben. &quot;Sparse 3D convolutional neural networks.&quot; arXiv preprint arXiv:1505.02890 (2015).<br><a id="2" href="#2ref">[2]</a> Graham, Benjamin, and Laurens van der Maaten. &quot;Submanifold sparse convolutional networks.&quot; arXiv preprint arXiv:1706.01307 (2017).<br><a id="3" href="#3ref">[3]</a> Graham, Benjamin, Martin Engelcke, and Laurens Van Der Maaten. &quot;3d semantic segmentation with submanifold sparse convolutional networks.&quot; Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.<br><a id="4" href="#4ref">[4]</a> Yan, Yan, Yuxing Mao, and Bo Li. &quot;Second: Sparsely embedded convolutional detection.&quot; Sensors 18.10 (2018): 3337.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Sparse 3D Convolution 最早在&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;[1]&lt;/a&gt;中提出，然后该作者又提出了 Submanifold Sparse Convolution&lt;a href=&quot;#2&quot; id=&quot;2ref&quot;&gt;&lt;sup&gt;[2]&lt;/sup
      
    
    </summary>
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection&quot;</title>
    <link href="https://leijiezhang001.github.io/End-to-End-Pseudo-LiDAR-for-3D-Det/"/>
    <id>https://leijiezhang001.github.io/End-to-End-Pseudo-LiDAR-for-3D-Det/</id>
    <published>2020-06-22T01:19:12.000Z</published>
    <updated>2020-06-25T01:19:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>　　基于视觉的 3D 目标检测方法因为成本较低，所以在 ADAS 领域应用非常广泛。其基本思路有以下几种：</p><ul><li>单目\(\rightarrow\)3D 框，代表文章有<a href="#1" id="1ref">[1]</a>。</li><li>单目\(\rightarrow\)深度图\(\rightarrow\)3D 框</li><li>双目\(\rightarrow\)3D 框，代表文章有<a href="#2" id="2ref">[2]</a>。</li><li>双目\(\rightarrow\)深度图\(\rightarrow\)3D 框</li></ul><p>由单目或双目直接回归 3D 目标框的属性，这种方法优势是 latency 小，缺点则是，没有显式的预测深度图，导致目标 3D 位置回归较为困难。而在深度图基础上回归 3D 目标位置则相对容易些，这种方法由两个模块构成：深度图预测，3D 目标预测。得到深度图后，可以在前视图下将深度图直接 concate 到 rgb 图上来做，另一种方法是将深度图转换为 pseudo-LiDAR 点云，然后用基于点云的 3D 目标检测方法来做，目前学术界基本有结论：pseudo-LiDAR 效果更好。<br>　　本文<a href="#3" id="3ref"><sup>[3]</sup></a>即采用双目出深度图，然后基于 pseudo-LiDAR 来作 3D 目标检测的方案，并且解决了两个模块需要两个网络来优化的大 lantency 问题，实现了 End-to-End 联合优化的方式。</p><h2 id="framework">1. Framework</h2><p><img src="/End-to-End-Pseudo-LiDAR-for-3D-Det/framework.png" width="60%" height="60%" title="图 1. Framework"> 　　基于点云作 3D 目标检测大致可分为 point-based 与 voxel-based 两大类，详见 <a href="/Point-based-3D-Det/" title="Point-based 3D Detection">Point-based 3D Detection</a>，传统的基于双目的 pseudo-LiDAR 方案无法 End-to-End 作俯视图下 voxel-based 3D 检测，因为点云信息需要作俯视图离散化，离散的过程是无法作反向传播训练的，本文提出了 Change of Representation(CoR) 模块有效解决了这个问题。如图 1. 所示，本方案中 Depth Estimation 可由任何深度估计网络实现，然后经过 CoR 模块，将深度图变换成点云形式用于 point-based 3D detection，或者是 Voxel 形式用于 voxel-based 3D detection。这里的关键是可求导的 CoR 模块设计。</p><h2 id="cor">2. CoR</h2><h3 id="quantization">2.1. Quantization</h3><p>　　点云检测模块如果采用 voxel-based 方案，那么点云到俯视图栅格的离散化(quantization)是必不可少的。假设点云 \(P = \{p _ 1,...,p _ N\}\)，待生成的 3D 占据栅格(最简单的特征形式) \(T\) 包含 \(M\) 个 bins，即 \(m\in\{1,...,M\}\)，每个 bin 的中心点设为 \(\hat{p} _ m\)。那么生成的 \(T\) 可表示为： <span class="math display">\[ T(m) = \left\{\begin{array}{l}1, &amp; \mathrm{if}\;\exists p\in P \; \mathrm{s.t.}\; m = \mathop{\arg\min}\limits _ {m &#39;}\Vert p - \hat{p} _ {m&#39;}\Vert _ 2 \\0, &amp; \mathrm{otherwise}.\end{array}\tag{1}\right.\]</span> 即如果有点落在该 bin 里，那么该 bin 对应的值置为 1。这种离散方式是无法求导的。<br>　　本文提出了一种可导的软量化模块(soft quantization module)，即用 RBF 作权重计数，另一种角度来看，<strong>这其实类似于点的空间概率密度表示</strong>。设 \(P _ m\) 为落在 bin \(m\) 的点集： <span class="math display">\[ P _ m=\left\{p\in |, \mathrm{s.t.}\; m=\mathop{\arg\min}\limits _ {m &#39;}\Vert p - \hat{p} _ {m&#39;}\Vert _ 2\right\} \tag{2}\]</span> 那么，\(m'\) 作用于 \(m\) 的值为： <span class="math display">\[ T(m, m&#39;) = \left\{\begin{array}{l}0 &amp; \mathrm{if}\; \vert P _ {m&#39;}\vert = 0;\\\frac{1}{\vert P _ {m&#39;}\vert} \sum _ {p\in P _ {m&#39;}} e^{-\frac{\Vert p-\hat{p} _ m\Vert ^2}{\sigma ^ 2}} &amp; \mathrm{if}\; \vert P _ {m&#39;}\vert &gt; 0.\end{array}\tag{3}\right.\]</span> 最终的 bin 值为： <span class="math display">\[ T(m) = T(m,m)+\frac{1}{\vert \mathcal{N} _ m\vert}\sum _ {m&#39;\in\mathcal{N} _ m}T(m,m&#39;) \tag{4}\]</span> 当 \(\sigma ^2\gg 0\) 以及 \(\mathcal{N} _ m=\varnothing\) 时，回退到式 (1) 的离散化方式。本文实验中采用 \(\sigma ^2 = 0.01\)，\(\mathcal{N} _ m=3\times 3\times 3 -1 = 26\)。传统的点云栅格概率密度计算方式为：将点云中的每个点高斯化，然后统计每个栅格中心坐标上覆盖到的值。与上述方法的高斯原点不一样，但是计算结果是一致的。 <img src="/End-to-End-Pseudo-LiDAR-for-3D-Det/quantization.png" width="90%" height="90%" title="图 2. Quantization"> 　　这种方法可将导数反向传播到 \(m'\) 中的每个点：\(\frac{\partial\mathcal{L} _ {det}}{\partial T(m)}\times\frac{\partial T(m)}{\partial T(m,m')}\times\bigtriangledown _ pT(m,m')\)。如图 2. 所示，蓝色 voxel 表示梯度为正，即 \(\frac{\partial\mathcal{L} _ {det}}{\partial T(m)} &gt; 0\)，红色 voxel 表示梯度为负。那么蓝色 voxel 期望没有点，所以将点往外推，红色 voxel 则将点往里拉，最终使点云与 LiDAR 点云，即 GT 点云一致。</p><h3 id="subsampling">2.1. Subsampling</h3><p>　　点云检测模块如果采用 point-based 方案，那么就比较容易直接与深度图网络进行 End-to-End 整合。point-based 3D Detection 一般通过 sampling 来扩大感受野，提取局部信息，因为这种方法的计算量对点数比较敏感，所以 sampling 也是降低计算量的有效手段。一个 \(640\times 480\) 的深度图所包含的点云超过 30 万，远远超过一个 64 线的激光雷达，所以对其进行采样就非常关键。<br>　　本文对深度图点云进行模拟雷达式的采样，即定义球坐标系下栅格化参数：\((r,\theta,\phi)\)。其中 \(\theta\) 为水平分辨率，\(\phi\) 为垂直分辨率。对每个栅格内采样一个点，即可得到一个较为稀疏，且接近激光雷达扫描属性的点云。</p><h2 id="loss">3. Loss</h2><p>　　Loss 由 depth 估计与 3D Detection 两项构成： <span class="math display">\[\mathcal{L} = \lambda _ {det}\mathcal{L} _ {det} + \lambda _ {depth}\mathcal{L} _ {depth} \tag{5}\]</span></p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Mousavian, Arsalan, et al. &quot;3d bounding box estimation using deep learning and geometry.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.<br><a id="2" href="#2ref">[2]</a> Li, Peiliang, Xiaozhi Chen, and Shaojie Shen. &quot;Stereo r-cnn based 3d object detection for autonomous driving.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br><a id="3" href="#3ref">[3]</a> Qian, Rui, et al. &quot;End-to-End Pseudo-LiDAR for Image-Based 3D Object Detection.&quot; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　基于视觉的 3D 目标检测方法因为成本较低，所以在 ADAS 领域应用非常广泛。其基本思路有以下几种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;单目\(\rightarrow\)3D 框，代表文章有&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;[1]&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;单目
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;PointPainting: Sequential Fusion for 3D Object Detection&quot;</title>
    <link href="https://leijiezhang001.github.io/PointPainting/"/>
    <id>https://leijiezhang001.github.io/PointPainting/</id>
    <published>2020-06-17T03:27:38.000Z</published>
    <updated>2020-06-22T09:00:45.632Z</updated>
    
    <content type="html"><![CDATA[<p>　　相机能很好的捕捉场景的语义信息，激光雷达则能很好的捕捉场景的三维信息，所以图像与点云的融合，对检测，分割等任务有非常大的帮助。融合可分为，<strong>数据级或特征级的前融合</strong>，以及<strong>任务级的后融合</strong>。本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种将图像分割结果的语义信息映射到点云，进而作 3D 检测的方法。这种串行方式的融合，既有点前融合的意思，也有点后融合的意思，暂且可归为前融合吧。本方法可认为是个框架，该框架下，基于图像的语义分割，以及基于点云的 3D 检测，均为独立模块。实验表明，融合了图像的语义信息后，点云针对行人等小目标的检测有较大的性能提升。</p><h2 id="framework">1. Framework</h2><p><img src="/PointPainting/framework.png" width="100%" height="100%" title="图 1. Framework"> 　　如图 1. 所示，算法框架非常简单，一句话能说明白：1). 首先经过图像语义分割获得语义图；2). 然后将点云投影到图像上，查询点云的语义信息，并连接到坐标信息中；3). 最后用点云 3D 检测的方法作 3D 检测。</p><h2 id="experiments">2. Experiments</h2><p><img src="/PointPainting/sota.png" width="90%" height="90%" title="图 2. PointPainting Applied to SOTA"> 　　采用 DeepLabv3+ 作为语义分割模块，应用到不同的点云 3D 检测后，结果如图 2. 所示，均有不同程度的提升，尤其是行人这种小目标。 <img src="/PointPainting/pointrcnn.png" width="90%" height="90%" title="图 3. Painted PointRCNN"> 　　图 3. 显示了 Painted PointRCNN 与各个方法的对比结果，mAP 是最高的。 <img src="/PointPainting/per-class.png" width="90%" height="90%" title="图 4. 不同类别的提升程度"> 　　由图 4. 可知，对行人，自行车，雪糕筒等小目标(俯视图下来说)，本方法提升非常显著。这也比较好理解，因为前视图下，这些目标所占的像素会比较多，所以更容易在前视相机图像下提取有效信息，辅助俯视图下作更准确的检测。</p><h2 id="rethinking-of-early-fusion">3. Rethinking of Early Fusion</h2><p>　　这里将本方法归为前融合，但是并不是真正意义上的前融合。如果是前融合，那么一般是 concate 语义分割网络的中低层特征到点云信息中，然而本文是直接取语义分割网络的最高层特征(即分类结果)。<strong>所以问题来了，所谓的前融合，一定比后融合更好吗？</strong>我想，这篇文章可能给了一些答案(不知道作者有没有做过取其它特征的实验，姑且认为做过，然后选择了本方法的策略)，虽然理论上前融合信息最完整，但是，如果这种完整的信息无法有效学出来或者对标定外参比较敏感，那么这种前融合也提升不了后续任务的性能，更有甚者，由于信息空间的变大或紊乱，导致后续任务性能下降。相反，对于不是那么“前”的后融合，我们能极大得保证各个任务学习结果的有效性，基于此，融合后学习的有效性也会比较确定。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Vora, Sourabh, et al. &quot;Pointpainting: Sequential fusion for 3d object detection.&quot; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　相机能很好的捕捉场景的语义信息，激光雷达则能很好的捕捉场景的三维信息，所以图像与点云的融合，对检测，分割等任务有非常大的帮助。融合可分为，&lt;strong&gt;数据级或特征级的前融合&lt;/strong&gt;，以及&lt;strong&gt;任务级的后融合&lt;/strong&gt;。本文&lt;a href=
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Boundary-Aware Dense Feature Indicator for Single-Stage 3D Object Detection from Point Clouds&quot;</title>
    <link href="https://leijiezhang001.github.io/DENFIDet/"/>
    <id>https://leijiezhang001.github.io/DENFIDet/</id>
    <published>2020-05-22T03:27:38.000Z</published>
    <updated>2020-06-12T02:22:23.113Z</updated>
    
    <content type="html"><![CDATA[<p>　　俯视图下 Voxel-based 点云 3D 目标检测一般会使用 2D 检测网络及相关策略。但是不同于图像的 2D 目标检测，俯视图下目标的点云信息基本在边缘处，所以如何准确得捕捉目标的边缘信息对特征提取的有效性非常关键。本文<a href="#1" id="1ref"><sup>[1]</sup></a> 提出了一种能捕捉目标边缘信息的网络结构，从而作更准确的目标检测。</p><h2 id="framework">1. Framework</h2><p><img src="/DENFIDet/framework.png" width="80%" height="80%" title="图 1. Framework of DENFIDet"> 　　如图 1. 所示，在传统的 One-Stage 2D/3D 检测框架下，嵌入了 DENFI 模块，该模块首先通过 DBPM 生成稠密的目标边缘 proposals，然后指导 DENFIConv 去提取更准确的目标特征，输出到检测头作 3D 目标框属性的分类与回归。</p><h2 id="denfidense-feature-indicator">2. DENFI(Dense Feature Indicator)</h2><h3 id="dbpmdense-boundary-proposal-module">2.1. DBPM(Dense Boundary Proposal Module)</h3><p><img src="/DENFIDet/DBPM.png" width="60%" height="60%" title="图 2. DBPM"> 　　DBPM 的输入为 Backbone 输出的 \(H\times W\times C\) 特征图，其由分类和回归两个分支构成：</p><ul><li>分类分支<br>分类分支经过 \(1\times 1\) 卷积输出 \(H\times W\times K\) 大小的 pixel 级别的 Score Map，其中 \(K\) 为类别数；</li><li>回归分支<br>回归分支也经过 \(1\times 1\) 卷积，输出 \(H\times W\times (4+n\times 2)\) 大小的回归量。回归量包括 \((l,t,r,b)\) 以及角度 \((\theta ^{bin}, \theta ^{res})\)(角度回归分解成了 n 个 bin 分类与 bin 内残差回归两个问题)。最终解码为描述目标边缘的信息：\((l,t,r,b,\theta)\)。</li></ul><p>　　Loss 的计算首先得区分正负样本。正负样本的划分思想与传统的差不多，主要思想是正负样本过渡区域引入 Ignore。如图 4. 所示，设 3D 真值框属性表示为 \((x,y,w,l,\theta)\)，正样本区域设计为 \((x,y,\sigma _ 1w,\sigma _ 1l,\theta)\)，定义另一缩小框 \((x,y,\sigma _ 2w,\sigma _ 2l,\theta)\)，其中 \(\sigma _ 1 &lt; \sigma _ 2\)。由此可得，灰色为正样本区域，黄色为 Ignore 区域，其它为负样本区域。<br>　　对于分类的 Loss，直接对正负样本进行 Focal Loss 计算。对于回归分支，则采用正样本的平均 Loss。回归 Loss 由目标框的 IoU Loss 以及角度 Loss 组成，角度 Loss 又由 bin 分类 Loss 加 bin 残差回归 Loss 组成。这里不做展开。<br>　　需要注意的是，分类分支只在训练的时候计算，Inference 时候只作回归分支的计算，从而得到每个像素感知到的目标边缘的信息。</p><h3 id="denficonv">2.2. DENFIConv</h3><p><img src="/DENFIDet/dconv.png" width="80%" height="80%" title="图 3. dconv"> 　　如图 3. 所示，Deformable Convolution 的思想是自动去寻找感兴趣的卷积区域。DBPM 获得每个像素点的目标边缘信息以后，自然的，接下来对像素点的卷积运算，运用可变形卷积可以捕捉更准确的目标区域信息。 <img src="/DENFIDet/DSDC.png" width="60%" height="60%" title="图 4. DSDC"> 　　如图 4. 所示，结合 Deformable Convolution 与 depth-wise separable Convolution，本文提出了 Depth-wise Separable Deformable Convolution。即将 \(3\times 3\) 的可变形卷积拆解成 \(3\times 3\) 的 Depth-wise 卷积以及 \(1\times 1\) 的可变形卷积，极大减少参数量。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Xu, Guodong, et al. &quot;Boundary-Aware Dense Feature Indicator for Single-Stage 3D Object Detection from Point Clouds.&quot; arXiv (2020): arXiv-2004.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　俯视图下 Voxel-based 点云 3D 目标检测一般会使用 2D 检测网络及相关策略。但是不同于图像的 2D 目标检测，俯视图下目标的点云信息基本在边缘处，所以如何准确得捕捉目标的边缘信息对特征提取的有效性非常关键。本文&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;SA-SSD: Structure Aware Single-stage 3D Object Detection from Point Cloud&quot;</title>
    <link href="https://leijiezhang001.github.io/SA-SSD/"/>
    <id>https://leijiezhang001.github.io/SA-SSD/</id>
    <published>2020-05-22T03:27:38.000Z</published>
    <updated>2020-05-22T10:04:17.044Z</updated>
    
    <content type="html"><![CDATA[<p>　　Voxel-based 3D Detection 相比 <a href="/Point-based-3D-Det/" title="Point-based 3D Detection">Point-based 3D Detection</a> 的缺点是特征提取不仅在 Voxel 阶段损失了一定的点云信息，而且 Voxel 化后丢失了点云之间的拓扑关系。<a href="/Point-based-3D-Det/" title="Point-based 3D Detection">Point-based 3D Detection</a> 中详细描述了几种 Point-based 方法，这种方法目前比较棘手的地方是，即使作 Inference 时，也需要作 kd-tree 搜索与采样等运算量较大的操作。那么如何榨干 Voxel-based 的性能对工业界落地就显得比较重要了，本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种单阶段的 Voxel-based 3D 检测方法，并借助了 Point 级别特征提取的相关策略，使得检测性能有较大提升。</p><h2 id="framework">1. Framework</h2><p><img src="/SA-SSD/framework.png" width="100%" height="100%" title="图 1. Framework of SA-SSD"> 　　如图 1. 所示，SA-SSD 由三部分组成：Backbone，Detection Head，Auxiliary Network。<br>　　Backbone 的输入是栅格化后的点云表示方式，文中栅格大小设定为 \(0.05m,0.05m,0.1m\)。Backbone 由一系列的 3D convolution 组成，因为需要保留空间三维位置信息，作 Voxel-to-Point 的映射。这里如果用 2D convolution 代替，那么 Auxiliary Network 估计也只能作 BridView 的分割了。<br>　　Detection Head 主体就是传统 Anchor-Free 结构，一个分支用于预测每个特征层像素点的 Confidence，另一个分支用于预测基于每个特征层像素点的 BBox 属性，如，以该点为 &quot;Anchor&quot; 的四个顶点坐标。此外，为了消除 One-Stage 方法中目标框与置信度不对齐的问题，本文引入 Part-sensitive Warping 来实现与 PSRoiAlign 类似的作用，实现两者的对齐。<br>　　Auxiliary Network 只在训练的阶段起作用，Inference 阶段不需要计算。该模块的作用是训练时通过 Voxel-to-Point 特征映射来反向传播监督 Backbone 中的 Voxel 特征学习 Point 级别的特征，包括点云的空间拓扑关系。<strong>当然 Inference 时也可以保留该分割模块，那么还可以增加点级别的特征反映射到 Voxel 的模块(Point-to-Voxel)，进一步作特征增强。</strong></p><h2 id="detachable-auxiliary-network">2. Detachable Auxiliary Network</h2><p><img src="/SA-SSD/sa.png" width="60%" height="60%" title="图 2. Structured Aware Feature Learning"> 　　如图 2. 所示，随着 Backbone 特征提取的感受野增大(特征分辨率下降)，背景点会接近目标的边缘，使得目标框大小不容易预测准确。本文提出的 Auxiliary Network，通过增加点级别分割及目标中心坐标预测任务，来监督 Backbone 特征层捕捉这种结构信息，从而达到更准确的目标检测的目的。<br>　　Auxiliary Network 的输入来自 Backbone 各个分辨率的特征层。将特征层上不为零的特征点，通过 Voxel-to-Point 反栅格化映射到三维空间，设该特征点表示为 \(\{(f _ j,p _ j):j=1,...,M\}\)，其中 \(f\) 为特征向量，\(p\) 为坐标向量。有了栅格对应的伪三维坐标点下的特征表示后，即可插值出实际点云中每个点的特征向量。设点云中点的插值特征为：\(\{(\hat{f} _ i,p _ i):i=1,...,N\}\)，采用 Inverse Distance Weighted 方法进行插值： <span class="math display">\[ \hat{f} _ i = \frac{\sum _ {j=1}^Mw _ j(p _ i)f _ j}{\sum _ {j=1}^Mw _ j(p _ i)} \tag{1}\]</span> 其中： <span class="math display">\[w _ j(p _ i)=\left\{\begin{array}{l}\frac{1}{\Vert p _ i-p _ j\Vert _ 2} &amp; \mathrm{if} p _ j\in\mathcal{N}(p _ i)\\0 &amp; \mathrm{otherwise}\end{array}\tag{2}\right.\]</span> \(\mathcal{N}(p _ i)\) 为球状区域，本文在四个分辨率下分别设定为：0.05m，0.1m，0.2m，0.4m。然后通过 cross-stage link 对各个分辨率下的点特征进行 concatenate 融合。最后通过感知机进行点云分割及目标中心点预测任务的构建。<br>　　对于点级别前景分割的任务，经过 sigmoid 函数后，应用二分类的 Focal Loss： <span class="math display">\[ \mathcal{L} _ {seg} = \frac{1}{N _ {pos}}\sum _ i^N -\alpha(1-\hat{s} _ i)^{\gamma}\mathrm{log}(\hat{s} _ i) \tag{3}\]</span> 该分割任务使得目标检测的框更加准确，如图 2.c 所示。但是还得优化其尺度与形状。<br>　　中心点的预测任务则能有效约束目标框的尺度与形状，具体的，预测的是每个属于目标的点云与中心点的相对位置(残差)。可用 Smooth-l1 来构建预测的中心点与实际中心点的 Loss。</p><h2 id="part-sensitive-warping">3. Part-sensitive Warping</h2><p><img src="/SA-SSD/psw.png" width="60%" height="60%" title="图 3. Part-sensitive Warping"> 　　One-Stage 方法都会有 Confidence 和 BBox 错位的现象，本文提出一种类似 PSROIAlign 但更有高效的 PSW 方法，具体步骤为：</p><ol type="1"><li>对于分类分支，修改为 \(K\) 个 Part-sensitive 的 cls maps，每个 map 包含目标的部分信息，比如当 \(K=4\) 时，可以理解为将目标切分为 \(2\times 2\) 部分；</li><li>对于回归分支，将每个目标框的 Feature map 划分为 \(K\) 个子区域，每个区域的中心点作为采样点；</li><li>如图 3. 所示，通过采样得到最终 cls map 的平均值。</li></ol><h2 id="experiment">4. Experiment</h2><p><img src="/SA-SSD/ablation.png" width="60%" height="60%" title="图 4. Ablation Study"> 　　如图 4. 所示，Auxiliary Network 能有效提升网络的定位精度，PSWarp 也能有效消除 Confidence 与 BBox 的错位影响。</p><h2 id="reference">5. Reference</h2><p><a id="1" href="#1ref">[1]</a> henhang, et al. &quot;Structure Aware Single-stage 3D Object Detection from Point Cloud.&quot;</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Voxel-based 3D Detection 相比 &lt;a href=&quot;/Point-based-3D-Det/&quot; title=&quot;Point-based 3D Detection&quot;&gt;Point-based 3D Detection&lt;/a&gt; 的缺点是特征提取不仅在 Vo
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Joint 3D Instance Segmentation and Objection Detection for Autonomous Driving&quot;</title>
    <link href="https://leijiezhang001.github.io/Instance-Seg-and-Obj-Det/"/>
    <id>https://leijiezhang001.github.io/Instance-Seg-and-Obj-Det/</id>
    <published>2020-05-22T03:27:38.000Z</published>
    <updated>2020-06-17T02:16:44.008Z</updated>
    
    <content type="html"><![CDATA[<p>　　检测的发展基本上是从 Anchor-based 这种稀疏的方式到 Anchor-free 这种密集检测方案演进的。相比于 Anchor-free 这种特征层像素级别的回归与分类来检测，更密集的方式，是直接作 Instance Segmentation，然后经过聚类等后处理来得到目标框属性。越密集的检测方案，因为样本较多(一定程度增大了样本空间)，所以学习越困难，但是理论上有极高的召回率。随着一系列技术的发展，如 Focal-loss 等，密集检测性能得以超过二阶段的 Anchor-based 方案，具体描述可参考 <a href="/Anchor-Free-Detection/" title="Anchor-Free Detection">Anchor-Free Detection</a>。<br>　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>借鉴 2D Instance Segmentation 思路，提出了一种同时作 3D Instance Segmentation 与 Detection 的方法。百度 Apollo 中的点云分割方法就是俯视图下 Instance Segmentation 然后后处理得到目标 Polygon 与 BBox 的思路，这种方法虽然后处理较为复杂，但是有超参数较少且召回率高的特点。本文算是该方法的 3D 版本(想法很自然，被人捷足先登。。)。</p><h2 id="framework">1. Framework</h2><p><img src="/Instance-Seg-and-Obj-Det/framework.png" width="100%" height="100%" title="图 1. Framework"> 　　如图 1. 所示，本方法由三部分构成：点级别的分类及回归，候选目标聚类，目标框优化。</p><ul><li><strong>点级别的分类与回归</strong><br>原始点云经过 Backbone 网络提取局部及全局特征，这里的 Backbone 网络可以是任意能提取点级别特征的网络。基于 Backbone 网络提取的特征，可进行点级别的 Semantic Segmentation 以及 Instance-aware SE(Spatial Embedding)。SE 回归的是每个点距离目标中心点的 offset，该目标的 size，以及该目标的朝向。</li><li><strong>候选目标聚类</strong><br>基于预测的 SE，将每个点的位置加上距离目标中心点的 offset，然后可通过简单的聚类算法(如 K-means)即可得到各个目标的点云集合，取 top k 个该点云集合回归的目标框属性，作下一步的目标框进一步优化。</li><li><strong>目标框优化</strong><br>基于候选目标聚类得到的目标框，提取目标点集，将其转换到该目标 Local 坐标系下，作进一步的目标框优化。</li></ul><h2 id="instance-aware-se">2. Instance-aware SE</h2><p>　　该框架的关键是 Instance-aware SE 的回归，回归量有：距离目标中心点的 offset，目标 size，目标 orientation。传统的 Instance Segmentation 做法是 Feature Embedding，将相同 Instance 的特征拉近，不同的 Instance 的特征推远，这种方法很难构造有效的 Loss 函数，而且同为车的不同 Instance，其特征已经非常接近。而本文 Spatial Embedding 中 offset 的回归量，经过聚类后处理，可以很容易的得到 Instance Segmentation 结果。<br>　　Apollo 点云分割的方案中，是在俯视图的 2D 栅格下做的，主要回归量也是这三种，不同的是，2D 栅格是离散的，所以根据 offset 找某一点的中心点时，可以迭代的进行，然后投票出中心点位置，后处理可以做的更细致。这里不做展开，有机会以后写一篇详解。</p><h2 id="loss">3. Loss</h2><p>　　Loss 项由 Semantic Segmentation，SE，3D BBox regression 组成： <span class="math display">\[ L = L _ {seg-cls}+L _ {SE}+L _ {reg} \tag{1}\]</span> Semantic Segmentation Loss 为： <span class="math display">\[ L _ {seg-cls}=-\sum _ {i=1}^C (y _ i\mathrm{log}(p _ i)(1-p _ i)^{\gamma}\alpha _ i+(1- y _ i)\mathrm{log}(1-p _ i)(p _ i)^{\gamma}(1-\alpha _ i)) \tag{2}\]</span> 其中 \(C\) 表示类别数；如果某点属于某类，那么 \(y _ i=1\)；\(p _ i\) 表示预测为第 \(i\) 类的概率；\(\gamma,\alpha\) 为超参数。<br>SE Loss 为： <span class="math display">\[ L _ {SE} = \frac{1}{N}\sum _ {i=1}^N\frac{1}{N _ c}\sum _ {i\in ins _ c}^{N _ c}(\mathcal{l} _ {offset}^i+\mathcal{l} _ {size}^i+\mathcal{l} _ {\theta}^i) \tag{3}\]</span> 其中 \(N\) 为 Instance 个数，\(N _ c\) 为内部点数，\(\mathcal{l}\) 为 L1 Smooth Loss。<br>BBox regression Loss 为 rotated 3D IOU Loss： <span class="math display">\[ L _ {reg} = 1-\mathbf{IoU}(B _ g,B _ d)\tag{4}\]</span></p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Zhou, Dingfu, et al. &quot;Joint 3D Instance Segmentation and Object Detection for Autonomous Driving.&quot; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　检测的发展基本上是从 Anchor-based 这种稀疏的方式到 Anchor-free 这种密集检测方案演进的。相比于 Anchor-free 这种特征层像素级别的回归与分类来检测，更密集的方式，是直接作 Instance Segmentation，然后经过聚类等后处
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Autonomous Driving" scheme="https://leijiezhang001.github.io/tags/Autonomous-Driving/"/>
    
      <category term="Semantic Segmentation" scheme="https://leijiezhang001.github.io/tags/Semantic-Segmentation/"/>
    
  </entry>
  
  <entry>
    <title>非线性最小二乘</title>
    <link href="https://leijiezhang001.github.io/Non-linear-Least-Squares/"/>
    <id>https://leijiezhang001.github.io/Non-linear-Least-Squares/</id>
    <published>2020-05-18T01:19:54.000Z</published>
    <updated>2020-05-24T04:47:24.293Z</updated>
    
    <content type="html"><![CDATA[<p>　　非线性最小二乘(Non-linear Least Squares)问题应用非常广泛，尤其是在 SLAM 领域。<a href="/LOAM/" title="LOAM">LOAM</a>，，<a href="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/" title="Stereo-RCNN">Stereo-RCNN</a>，<a href="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/" title="Stereo Vision-based Semantic and Ego-motion Tracking for Autonomous Driving">Stereo Vision-based Semantic and Ego-motion Tracking for Autonomous Driving</a> 等均需要求解非线性最小二乘问题。其中 <a href="/LOAM/" title="LOAM">LOAM</a> 作为非常流行的激光 SLAM 框架，其后端是一个典型的非线性最优化问题，本文会作为实践进行代码级讲解。</p><h2 id="问题描述">1. 问题描述</h2><p>　　在前端观测-后端优化框架下，设观测数据对集合为：\(\{y _ i,z _ i\} _ {i=1}^m\)，待求解的变量参数 \(x\in\mathbb{R}^n\) 定义了观测数据对的映射关系，即 \(z _ i=h(y _ i;x)\)，由此得到有 \(m\) 个参数方程 \(F(x)=[f _ 1(x),...,f _ m(x)]^T\)，其中 \(f _ i(x) = z _ i-h(y _ i;x)\)。我们要找到最优的参数 \(x\) 来描述观测数据对之间的关系，即求解的最优化问题为： <span class="math display">\[\begin{align}\mathop{\arg\min}\limits _ x \frac{1}{2}\Vert F(x)\Vert ^2 \iff \mathop{\arg\min}\limits _ x\frac{1}{2}\sum _ i \rho _ i\left(\Vert f _ i(x)\Vert ^ 2\right)\\L\leq x \leq U\end{align}\tag{1}\]</span> 其中 \(f _ i(\cdot)\) 为 Cost Function，\(\rho _ i(\cdot)\) 为 Loss Function，即核函数，用来减少离群点对非线性最小二乘优化的影响；\(L,U\) 分别为参数 \(x\) 的上下界。当核函数 \(\rho _ i(x)=x\) 时，就是常见的非线性最小二乘问题。<br>　　《视觉 SLAM 十四讲》<a href="#1" id="1ref"><sup>[1]</sup></a>在 SLAM 的状态估计问题中，从概率学角度导出了最大似然估计求解状态的方法，并进一步引出了最小二乘问题。回过头来看，本文很多内容在《视觉 SLAM 十四讲》中已经有非常清晰的描述，可作进一步参考。</p><h2 id="问题求解">2. 问题求解</h2><p>　　根据 \(F(x)\) 求得雅克比矩阵(Jacobian)：\(J(x) \in\mathbb{R}^{m\times n}\)，即 \(J _ {ij}(x)=\frac{\partial f _ i(x)}{\partial x _ j}\)。目标函数的梯度向量为 \(g(x) = \nabla\frac{1}{2}\Vert F(x)\Vert ^ 2=J(x)^TF(x)\)。在 \(x\) 处将目标函数线性化：\(F(x+\Delta x)\approx F(x)+J(x)\Delta x\)。由此非线性最小二乘问题可转换为线性最小二乘求解残差量 \(\Delta x\) 来近似求解： <span class="math display">\[\mathop{\arg\min}\limits _ {\Delta x}\frac{1}{2}\Vert J(x)\Delta x+F(x)\Vert ^ 2\tag{2}\]</span> 根据如何控制 \(\Delta x\) 的大小，非线性优化算法可分为两大类：</p><ul><li>Line Search<ul><li>Gradient Descent</li><li>Gaussian-Newton</li></ul></li><li>Trust Region<ul><li>Levenberg-Marquardt</li><li>Dogleg</li><li>Inner Iterations</li><li>Non-monotonic Steps</li></ul></li></ul><p>Line Search 首先确定迭代方向，然后最小化 \(\Vert f(x+\alpha \Delta x)\Vert ^2\) 确定迭代步长；Trust Region 则划分一个局部区域，在该区域内求解最优值，然后根据近似程度，扩大或缩减该局部区域范围。Trust Region 相比 Linear Search，数值迭代会更加稳定。这里介绍几种有代表性的方法：属于 Line Search 的梯度下降法，高斯牛顿法，以及属于 Trust Region 的 LM 法。</p><h3 id="梯度下降法">2.1. 梯度下降法</h3><p>　　将目标函数式(1)在 \(x\) 附近泰勒展开： <span class="math display">\[ \Vert F(x+\Delta x)\Vert ^2 \approx \Vert F(x)\Vert ^2 + J(x)\Delta x+\frac{1}{2}\Delta x^TH\Delta x \tag{3}\]</span> 其中 \(H\) 是二阶导数(Hessian 矩阵)。<br>　　如果保留一阶导数，那么增量的解就为： <span class="math display">\[\Delta x = -\lambda J^T(x) \tag{4}\]</span> 其中 \(\lambda\) 为步长，可预先由相关策略设定。<br>　　如果保留二阶导数，那么增量方程为： <span class="math display">\[\mathop{\arg\min}\limits _ {\Delta x} \Vert F(x)\Vert ^2+J(x)\Delta x+\frac{1}{2}\Delta x^TH\Delta x\tag{5}\]</span> 对 \(\Delta x\) 求导即可求解增量的解为： <span class="math display">\[\Delta x = -H^{-1}J^T \tag{6}\]</span> 　　一阶梯度法又称为最速下降法，二阶梯度法又称为牛顿法。一阶和二阶法都是将函数在当前值下泰勒展开，然后线性得求解增量值。最速下降法过于贪心，容易走出锯齿路线，反而增加迭代步骤。牛顿法需要计算 \(H\) 矩阵，计算量较大且困难。</p><h3 id="高斯牛顿法">2.2. 高斯牛顿法</h3><p>　　 将式(2)对 \(\Delta x\) 求导并令其为零，可得： <span class="math display">\[\begin{align}&amp;J(x)^TJ(x)\Delta x=-J(x)^TF(x)\\\iff &amp; H\Delta x=g\end{align}\tag{7}\]</span> 相比牛顿法，高斯牛顿法不用计算 \(H\) 矩阵，直接用 \(J^TJ\) 来近似，所以节省了计算量。但是高斯牛顿法要求 \(H\) 矩阵是可逆且正定的，而实际计算的 \(J^TJ\) 是半正定的，所以 \(J^TJ\) 会出现奇异或病态的情况，此时增量的稳定性就会变差，导致迭代发散。另一方面，增量较大时，目标近似函数式(2)就会产生较大的误差，也会导致迭代发散。这是高斯牛顿法的缺陷。高斯牛顿法的步骤为：</p><ol type="1"><li>根据式 (7) 求解迭代步长 \(\Delta x\)；</li><li>变量迭代：\(x ^ * \leftarrow x+\Delta x\)；</li><li>如果 \(\Vert F(x ^ * )-F(x)\Vert &lt; \epsilon\)，则收敛，退出迭代，否则重复步骤 1.；</li></ol><p>高斯牛顿法简单的将 \(\alpha\) 置为 1，而其它 Line Search 方法会最小化 \(\Vert f(x+\alpha \Delta x)\Vert ^2\) 来确定 \(\alpha\) 值。</p><h3 id="lm-法">2.3. LM 法</h3><p>　　Line Search 依赖线性化近似有较高的拟合度，但是有时候线性近似效果较差，导致迭代不稳定；Region Trust 就是解决了这种问题。高斯牛顿法中采用的近似二阶泰勒展开只在该点附近有较好的近似结果，对 \(\Delta x\) 添加一个信赖域区域，就变为 Trust Region 方法。其最优化问题转换为： <span class="math display">\[\begin{align}\mathop{\arg\min}\limits _ x \frac{1}{2}\Vert J(x)\Delta x+F(x)\Vert ^2 \\\Vert D(x)\Delta x\Vert ^2 \leq \mu\\L\leq x \leq U\\\end{align}\tag{8}\]</span> 用 Lagrange 乘子将其转换为无约束优化问题： <span class="math display">\[\mathop{\arg\min}\limits _ {\Delta x}\frac{1}{2}\Vert J(x)\Delta x+F(x)\Vert ^ 2+\frac{1}{\mu}\Vert D(x)\Delta x\Vert ^2 \tag{9}\]</span> 其中 Levenberg 提出的方法中 \(D=I\)，相当于把 \(\Delta x\) 约束在球中；Marquart 提出的方法中将 \(D\) 取为非负数对角阵，通常为 \(J(x)^TJ(x)\) 的对角元素平方根。<br>　　对于信赖域区域 \(\mu\) 的定义，一个比较好的方式是根据近似模型与实际函数之间的差异来确定这个范围：如果差异小，那么增大信赖域；反之减小信赖域。因此，考虑： <span class="math display">\[\rho = \frac{\Vert F(x+\Delta x)\Vert ^2-\Vert F(x)\Vert ^2}{\Vert J(x)\Delta x+F(x)\Vert ^2-\Vert F(x)\Vert ^2} \tag{10}\]</span> 　　 将式(9)对 \(\Delta x\) 求导并令其为零，可得： <span class="math display">\[\begin{align}&amp;\left(J(x)^TJ(x)+\frac{2}{\mu}D^T(x)D(x)\right)\Delta x=-J(x)^TF(x)\\\iff &amp; (H+\lambda D^TD)\Delta x=g\end{align}\tag{11}\]</span> 当 \(\lambda\) 较小时，接近于高斯牛顿法；当 \(\lambda\) 较大时，接近于最速下降法。LM 法的步骤为：</p><ol type="1"><li>根据式(11)求解迭代步长 \(\Delta x\);</li><li>根据式(10)求解 \(\rho\);</li><li>若 \(\rho &gt; \eta _ 1\)，则 \(\mu = 2\mu\);</li><li>若 \(\rho &lt; \eta _ 2\)，则 \(\mu = 0.5\mu\);</li><li>若 \(\rho &gt; \epsilon\)，则 \(x ^ * \leftarrow x+\Delta x\)；</li><li>如果满足收敛条件，则结束，否则继续步骤1.；</li></ol><h2 id="ceres-实践">3. Ceres 实践</h2><p>　　Ceres 是谷歌开发的一个用于非线性优化的库，使用 Ceres 库有以下几个步骤：</p><ul><li>构建 Cost Function，式(1)中的 \(\rho _ i\left(\Vert f _ i(x)\Vert ^ 2\right)\) 即为代码中需要增加的 ResidualBlock；</li><li>累加的 Cost Function 构成最终的 Loss Function 目标函数；</li><li>配置求解器参数并求解问题；</li></ul><h3 id="例子-曲线拟合">3.1. 例子-曲线拟合</h3><p>　　以下代码为拟合曲线参数的简单例子：</p><p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// copy from http://zhaoxuhui.top/blog/2018/04/04/ceres&amp;ls.html</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;opencv2/core/core.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;ceres/ceres.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> cv;</span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> ceres;</span><br><span class="line"></span><br><span class="line"><span class="comment">//vector,用于存放x、y的观测数据</span></span><br><span class="line"><span class="comment">//待估计函数为y=3.5x^3+1.6x^2+0.3x+7.8</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; xs;</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">double</span>&gt; ys;</span><br><span class="line"></span><br><span class="line"><span class="comment">//定义CostFunctor结构体用于描述代价函数</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">CostFunctor</span>&#123;</span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">double</span> x_guan,y_guan;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//构造函数，用已知的x、y数据对其赋值</span></span><br><span class="line">  CostFunctor(<span class="keyword">double</span> x,<span class="keyword">double</span> y)</span><br><span class="line">  &#123;</span><br><span class="line">    x_guan = x;</span><br><span class="line">    y_guan = y;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//重载括号运算符，两个参数分别是估计的参数和由该参数计算得到的残差</span></span><br><span class="line">  <span class="comment">//注意这里的const，一个都不能省略，否则就会报错</span></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">  <span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> T* <span class="keyword">const</span> params,T* residual)</span><span class="keyword">const</span></span></span><br><span class="line"><span class="function">  </span>&#123;</span><br><span class="line">    residual[<span class="number">0</span>]=y_guan-(params[<span class="number">0</span>]*x_guan*x_guan*x_guan+params[<span class="number">1</span>]*x_guan*x_guan+params[<span class="number">2</span>]*x_guan+params[<span class="number">3</span>]);</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;  </span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//生成实验数据</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">generateData</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  RNG rng;</span><br><span class="line">  <span class="keyword">double</span> w_sigma = <span class="number">1.0</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">100</span>;i++)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">double</span> x = i;</span><br><span class="line">    <span class="keyword">double</span> y = <span class="number">3.5</span>*x*x*x+<span class="number">1.6</span>*x*x+<span class="number">0.3</span>*x+<span class="number">7.8</span>;</span><br><span class="line">    xs.push_back(x);</span><br><span class="line">    ys.push_back(y+rng.gaussian(w_sigma));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;xs.size();i++)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"x:"</span>&lt;&lt;xs[i]&lt;&lt;<span class="string">" y:"</span>&lt;&lt;ys[i]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//简单描述我们优化的目的就是为了使我们估计参数算出的y'和实际观测的y的差值之和最小</span></span><br><span class="line"><span class="comment">//所以代价函数(CostFunction)就是y'-y，其对应每一组观测值与估计值的残差。</span></span><br><span class="line"><span class="comment">//由于我们优化的是残差之和，因此需要把代价函数全部加起来，使这个函数最小，而不是单独的使某一个残差最小</span></span><br><span class="line"><span class="comment">//默认情况下，我们认为各组的残差是等权的，也就是核函数系数为1。</span></span><br><span class="line"><span class="comment">//但有时可能会出现粗差等情况，有可能不等权，但这里不考虑。</span></span><br><span class="line"><span class="comment">//这个求和以后的函数便是我们优化的目标函数</span></span><br><span class="line"><span class="comment">//通过不断调整我们的参数值，使这个目标函数最终达到最小，即认为优化完成</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line">  generateData();</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//创建一个长度为4的double数组用于存放参数</span></span><br><span class="line">  <span class="keyword">double</span> params[<span class="number">4</span>]=&#123;<span class="number">1.0</span>&#125;;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//第一步，创建Problem对象，并对每一组观测数据添加ResidualBlock</span></span><br><span class="line">  <span class="comment">//由于每一组观测点都会得到一个残差，而我们的目的是最小化所有残差的和</span></span><br><span class="line">  <span class="comment">//所以采用for循环依次把每个残差都添加进来</span></span><br><span class="line">  Problem problem;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;xs.size();i++)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="comment">//利用我们之前写的结构体、仿函数，创建代价函数对象，注意初始化的方式</span></span><br><span class="line">    <span class="comment">//尖括号中的参数分别为误差类型，输出维度(因变量个数)，输入维度(待估计参数的个数)</span></span><br><span class="line">    CostFunction* cost_function = <span class="keyword">new</span> AutoDiffCostFunction&lt;CostFunctor,<span class="number">1</span>,<span class="number">4</span>&gt;(<span class="keyword">new</span> CostFunctor(xs[i],ys[i]));</span><br><span class="line">    <span class="comment">//三个参数分别为代价函数、核函数和待估参数</span></span><br><span class="line">    problem.AddResidualBlock(cost_function,<span class="literal">NULL</span>,params);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//第二步，配置Solver</span></span><br><span class="line">  Solver::Options options;</span><br><span class="line">  <span class="comment">//配置增量方程的解法</span></span><br><span class="line">  options.linear_solver_type=ceres::DENSE_QR;</span><br><span class="line">  <span class="comment">//是否输出到cout</span></span><br><span class="line">  options.minimizer_progress_to_stdout=<span class="literal">true</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//第三步，创建Summary对象用于输出迭代结果</span></span><br><span class="line">  Solver::Summary summary;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//第四步，执行求解</span></span><br><span class="line">  Solve(options,&amp;problem,&amp;summary);</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//第五步，输出求解结果</span></span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;summary.BriefReport()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;<span class="string">"p0:"</span>&lt;&lt;params[<span class="number">0</span>]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;<span class="string">"p1:"</span>&lt;&lt;params[<span class="number">1</span>]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;<span class="string">"p2:"</span>&lt;&lt;params[<span class="number">2</span>]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  <span class="built_in">cout</span>&lt;&lt;<span class="string">"p3:"</span>&lt;&lt;params[<span class="number">3</span>]&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="例子-loam">3.2. 例子-LOAM</h3><p>　　<a href="/LOAM/" title="LOAM">LOAM</a> 前端提取线和面特征，后端最小化线和面的匹配误差。其源码实现了整个最优化过程，ALOAM<a href="#2" id="2ref"><sup>[2]</sup></a> 将后端代码用 Ceres 实现，这里对其作理解与分析。</p><p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">LidarEdgeFactor</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">LidarEdgeFactor(Eigen::Vector3d curr_point_, Eigen::Vector3d last_point_a_,</span><br><span class="line">Eigen::Vector3d last_point_b_, <span class="keyword">double</span> s_)</span><br><span class="line">: curr_point(curr_point_), last_point_a(last_point_a_), last_point_b(last_point_b_), s(s_) &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> T *q, <span class="keyword">const</span> T *t, T *residual)</span> <span class="keyword">const</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; cp&#123;T(curr_point.x()), T(curr_point.y()), T(curr_point.z())&#125;;</span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; lpa&#123;T(last_point_a.x()), T(last_point_a.y()), T(last_point_a.z())&#125;;</span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; lpb&#123;T(last_point_b.x()), T(last_point_b.y()), T(last_point_b.z())&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Eigen::Quaternion&lt;T&gt; q_last_curr&#123;q[3], T(s) * q[0], T(s) * q[1], T(s) * q[2]&#125;;</span></span><br><span class="line">Eigen::Quaternion&lt;T&gt; q_last_curr&#123;q[<span class="number">3</span>], q[<span class="number">0</span>], q[<span class="number">1</span>], q[<span class="number">2</span>]&#125;;</span><br><span class="line">Eigen::Quaternion&lt;T&gt; q_identity&#123;T(<span class="number">1</span>), T(<span class="number">0</span>), T(<span class="number">0</span>), T(<span class="number">0</span>)&#125;;</span><br><span class="line">q_last_curr = q_identity.slerp(T(s), q_last_curr);</span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; t_last_curr&#123;T(s) * t[<span class="number">0</span>], T(s) * t[<span class="number">1</span>], T(s) * t[<span class="number">2</span>]&#125;;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; lp;</span><br><span class="line">lp = q_last_curr * cp + t_last_curr;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; nu = (lp - lpa).cross(lp - lpb);</span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; de = lpa - lpb;</span><br><span class="line"></span><br><span class="line">residual[<span class="number">0</span>] = nu.x() / de.norm();</span><br><span class="line">residual[<span class="number">1</span>] = nu.y() / de.norm();</span><br><span class="line">residual[<span class="number">2</span>] = nu.z() / de.norm();</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> ceres::<span class="function">CostFunction *<span class="title">Create</span><span class="params">(<span class="keyword">const</span> Eigen::Vector3d curr_point_, <span class="keyword">const</span> Eigen::Vector3d last_point_a_,</span></span></span><br><span class="line"><span class="function"><span class="params">   <span class="keyword">const</span> Eigen::Vector3d last_point_b_, <span class="keyword">const</span> <span class="keyword">double</span> s_)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> (<span class="keyword">new</span> ceres::AutoDiffCostFunction&lt;</span><br><span class="line">LidarEdgeFactor, <span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>&gt;(</span><br><span class="line"><span class="keyword">new</span> LidarEdgeFactor(curr_point_, last_point_a_, last_point_b_, s_)));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Eigen::Vector3d curr_point, last_point_a, last_point_b;</span><br><span class="line"><span class="keyword">double</span> s;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p><p>　　对于 Point2Line 误差，为了衡量该线特征上的点是否在地图对应的线特征上，在地图线特征上采样两个点，加上该点，组成两个向量，向量叉乘即可描述匹配误差。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">LidarPlaneFactor</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">LidarPlaneFactor(Eigen::Vector3d curr_point_, Eigen::Vector3d last_point_j_,</span><br><span class="line"> Eigen::Vector3d last_point_l_, Eigen::Vector3d last_point_m_, <span class="keyword">double</span> s_)</span><br><span class="line">: curr_point(curr_point_), last_point_j(last_point_j_), last_point_l(last_point_l_),</span><br><span class="line">  last_point_m(last_point_m_), s(s_)</span><br><span class="line">&#123;</span><br><span class="line">ljm_norm = (last_point_j - last_point_l).cross(last_point_j - last_point_m);</span><br><span class="line">ljm_norm.normalize();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> T *q, <span class="keyword">const</span> T *t, T *residual)</span> <span class="keyword">const</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; cp&#123;T(curr_point.x()), T(curr_point.y()), T(curr_point.z())&#125;;</span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; lpj&#123;T(last_point_j.x()), T(last_point_j.y()), T(last_point_j.z())&#125;;</span><br><span class="line"><span class="comment">//Eigen::Matrix&lt;T, 3, 1&gt; lpl&#123;T(last_point_l.x()), T(last_point_l.y()), T(last_point_l.z())&#125;;</span></span><br><span class="line"><span class="comment">//Eigen::Matrix&lt;T, 3, 1&gt; lpm&#123;T(last_point_m.x()), T(last_point_m.y()), T(last_point_m.z())&#125;;</span></span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; ljm&#123;T(ljm_norm.x()), T(ljm_norm.y()), T(ljm_norm.z())&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//Eigen::Quaternion&lt;T&gt; q_last_curr&#123;q[3], T(s) * q[0], T(s) * q[1], T(s) * q[2]&#125;;</span></span><br><span class="line">Eigen::Quaternion&lt;T&gt; q_last_curr&#123;q[<span class="number">3</span>], q[<span class="number">0</span>], q[<span class="number">1</span>], q[<span class="number">2</span>]&#125;;</span><br><span class="line">Eigen::Quaternion&lt;T&gt; q_identity&#123;T(<span class="number">1</span>), T(<span class="number">0</span>), T(<span class="number">0</span>), T(<span class="number">0</span>)&#125;;</span><br><span class="line">q_last_curr = q_identity.slerp(T(s), q_last_curr);</span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; t_last_curr&#123;T(s) * t[<span class="number">0</span>], T(s) * t[<span class="number">1</span>], T(s) * t[<span class="number">2</span>]&#125;;</span><br><span class="line"></span><br><span class="line">Eigen::Matrix&lt;T, <span class="number">3</span>, <span class="number">1</span>&gt; lp;</span><br><span class="line">lp = q_last_curr * cp + t_last_curr;</span><br><span class="line"></span><br><span class="line">residual[<span class="number">0</span>] = (lp - lpj).dot(ljm);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> ceres::<span class="function">CostFunction *<span class="title">Create</span><span class="params">(<span class="keyword">const</span> Eigen::Vector3d curr_point_, <span class="keyword">const</span> Eigen::Vector3d last_point_j_,</span></span></span><br><span class="line"><span class="function"><span class="params">   <span class="keyword">const</span> Eigen::Vector3d last_point_l_, <span class="keyword">const</span> Eigen::Vector3d last_point_m_,</span></span></span><br><span class="line"><span class="function"><span class="params">   <span class="keyword">const</span> <span class="keyword">double</span> s_)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> (<span class="keyword">new</span> ceres::AutoDiffCostFunction&lt;</span><br><span class="line">LidarPlaneFactor, <span class="number">1</span>, <span class="number">4</span>, <span class="number">3</span>&gt;(</span><br><span class="line"><span class="keyword">new</span> LidarPlaneFactor(curr_point_, last_point_j_, last_point_l_, last_point_m_, s_)));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Eigen::Vector3d curr_point, last_point_j, last_point_l, last_point_m;</span><br><span class="line">Eigen::Vector3d ljm_norm;</span><br><span class="line"><span class="keyword">double</span> s;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>　　对于 Point2Plane 误差，为了衡量该面特征上的点是否在地图对应的面特征上，在地图面特征上采样一个点，加上该点，组成向量，然后点乘面的法向量即可衡量匹配误差。</p><h3 id="例子-ba">3.3. 例子-BA</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// copy from https://www.jianshu.com/p/3df0c2e02b4c</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"ceres/ceres.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"ceres/rotation.h"</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Read a Bundle Adjustment in the Large dataset.</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BALProblem</span> &#123;</span></span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  ~BALProblem() &#123;</span><br><span class="line">    <span class="keyword">delete</span>[] point_index_;</span><br><span class="line">    <span class="keyword">delete</span>[] camera_index_;</span><br><span class="line">    <span class="keyword">delete</span>[] observations_;</span><br><span class="line">    <span class="keyword">delete</span>[] parameters_;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">int</span> <span class="title">num_observations</span><span class="params">()</span>       <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> num_observations_;               &#125;</span><br><span class="line">  <span class="function"><span class="keyword">const</span> <span class="keyword">double</span>* <span class="title">observations</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> observations_;                   &#125;</span><br><span class="line">  <span class="function"><span class="keyword">double</span>* <span class="title">mutable_cameras</span><span class="params">()</span>          </span>&#123; <span class="keyword">return</span> parameters_;                     &#125;</span><br><span class="line">  <span class="function"><span class="keyword">double</span>* <span class="title">mutable_points</span><span class="params">()</span>           </span>&#123; <span class="keyword">return</span> parameters_  + <span class="number">9</span> * num_cameras_; &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">double</span>* <span class="title">mutable_camera_for_observation</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> mutable_cameras() + camera_index_[i] * <span class="number">9</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">double</span>* <span class="title">mutable_point_for_observation</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> mutable_points() + point_index_[i] * <span class="number">3</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">bool</span> <span class="title">LoadFile</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span>* filename)</span> </span>&#123;</span><br><span class="line">    FILE* fptr = fopen(filename, <span class="string">"r"</span>);</span><br><span class="line">    <span class="keyword">if</span> (fptr == <span class="literal">NULL</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    FscanfOrDie(fptr, <span class="string">"%d"</span>, &amp;num_cameras_);</span><br><span class="line">    FscanfOrDie(fptr, <span class="string">"%d"</span>, &amp;num_points_);</span><br><span class="line">    FscanfOrDie(fptr, <span class="string">"%d"</span>, &amp;num_observations_);</span><br><span class="line"></span><br><span class="line">    point_index_ = <span class="keyword">new</span> <span class="keyword">int</span>[num_observations_];</span><br><span class="line">    camera_index_ = <span class="keyword">new</span> <span class="keyword">int</span>[num_observations_];</span><br><span class="line">    observations_ = <span class="keyword">new</span> <span class="keyword">double</span>[<span class="number">2</span> * num_observations_];</span><br><span class="line"></span><br><span class="line">    num_parameters_ = <span class="number">9</span> * num_cameras_ + <span class="number">3</span> * num_points_;</span><br><span class="line">    parameters_ = <span class="keyword">new</span> <span class="keyword">double</span>[num_parameters_];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_observations_; ++i) &#123;</span><br><span class="line">      FscanfOrDie(fptr, <span class="string">"%d"</span>, camera_index_ + i);</span><br><span class="line">      FscanfOrDie(fptr, <span class="string">"%d"</span>, point_index_ + i);</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; <span class="number">2</span>; ++j) &#123;</span><br><span class="line">        FscanfOrDie(fptr, <span class="string">"%lf"</span>, observations_ + <span class="number">2</span>*i + j);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num_parameters_; ++i) &#123;</span><br><span class="line">      FscanfOrDie(fptr, <span class="string">"%lf"</span>, parameters_ + i);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">FscanfOrDie</span><span class="params">(FILE *fptr, <span class="keyword">const</span> <span class="keyword">char</span> *format, T *value)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> num_scanned = <span class="built_in">fscanf</span>(fptr, format, value);</span><br><span class="line">    <span class="keyword">if</span> (num_scanned != <span class="number">1</span>) &#123;</span><br><span class="line">      LOG(FATAL) &lt;&lt; <span class="string">"Invalid UW data file."</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> num_cameras_;</span><br><span class="line">  <span class="keyword">int</span> num_points_;</span><br><span class="line">  <span class="keyword">int</span> num_observations_;</span><br><span class="line">  <span class="keyword">int</span> num_parameters_;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span>* point_index_;</span><br><span class="line">  <span class="keyword">int</span>* camera_index_;</span><br><span class="line">  <span class="keyword">double</span>* observations_;</span><br><span class="line">  <span class="keyword">double</span>* parameters_;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Templated pinhole camera model for used with Ceres.  The camera is</span></span><br><span class="line"><span class="comment">// parameterized using 9 parameters: 3 for rotation, 3 for translation, 1 for</span></span><br><span class="line"><span class="comment">// focal length and 2 for radial distortion. The principal point is not modeled</span></span><br><span class="line"><span class="comment">// (i.e. it is assumed be located at the image center).</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">SnavelyReprojectionError</span> &#123;</span></span><br><span class="line">  SnavelyReprojectionError(<span class="keyword">double</span> observed_x, <span class="keyword">double</span> observed_y)</span><br><span class="line">      : observed_x(observed_x), observed_y(observed_y) &#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line">  <span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">const</span> T* <span class="keyword">const</span> camera,</span></span></span><br><span class="line"><span class="function"><span class="params">                  <span class="keyword">const</span> T* <span class="keyword">const</span> point,</span></span></span><br><span class="line"><span class="function"><span class="params">                  T* residuals)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">    <span class="comment">// camera[0,1,2] are the angle-axis rotation.</span></span><br><span class="line">    T p[<span class="number">3</span>];</span><br><span class="line">    ceres::AngleAxisRotatePoint(camera, point, p);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// camera[3,4,5] are the translation.</span></span><br><span class="line">    p[<span class="number">0</span>] += camera[<span class="number">3</span>];</span><br><span class="line">    p[<span class="number">1</span>] += camera[<span class="number">4</span>];</span><br><span class="line">    p[<span class="number">2</span>] += camera[<span class="number">5</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Compute the center of distortion. The sign change comes from</span></span><br><span class="line">    <span class="comment">// the camera model that Noah Snavely's Bundler assumes, whereby</span></span><br><span class="line">    <span class="comment">// the camera coordinate system has a negative z axis.</span></span><br><span class="line">    T xp = - p[<span class="number">0</span>] / p[<span class="number">2</span>];</span><br><span class="line">    T yp = - p[<span class="number">1</span>] / p[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Apply second and fourth order radial distortion.</span></span><br><span class="line">    <span class="keyword">const</span> T&amp; l1 = camera[<span class="number">7</span>];</span><br><span class="line">    <span class="keyword">const</span> T&amp; l2 = camera[<span class="number">8</span>];</span><br><span class="line">    T r2 = xp*xp + yp*yp;</span><br><span class="line">    T distortion = <span class="number">1.0</span> + r2  * (l1 + l2  * r2);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Compute final projected point position.</span></span><br><span class="line">    <span class="keyword">const</span> T&amp; focal = camera[<span class="number">6</span>];</span><br><span class="line">    T predicted_x = focal * distortion * xp;</span><br><span class="line">    T predicted_y = focal * distortion * yp;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// The error is the difference between the predicted and observed position.</span></span><br><span class="line">    residuals[<span class="number">0</span>] = predicted_x - observed_x;</span><br><span class="line">    residuals[<span class="number">1</span>] = predicted_y - observed_y;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Factory to hide the construction of the CostFunction object from</span></span><br><span class="line">  <span class="comment">// the client code.</span></span><br><span class="line">  <span class="keyword">static</span> ceres::<span class="function">CostFunction* <span class="title">Create</span><span class="params">(<span class="keyword">const</span> <span class="keyword">double</span> observed_x,</span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">const</span> <span class="keyword">double</span> observed_y)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> (<span class="keyword">new</span> ceres::AutoDiffCostFunction&lt;SnavelyReprojectionError, <span class="number">2</span>, <span class="number">9</span>, <span class="number">3</span>&gt;(</span><br><span class="line">                <span class="keyword">new</span> SnavelyReprojectionError(observed_x, observed_y)));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">double</span> observed_x;</span><br><span class="line">  <span class="keyword">double</span> observed_y;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** argv)</span> </span>&#123;</span><br><span class="line">  google::InitGoogleLogging(argv[<span class="number">0</span>]);</span><br><span class="line">  <span class="keyword">if</span> (argc != <span class="number">2</span>) &#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cerr</span> &lt;&lt; <span class="string">"usage: simple_bundle_adjuster &lt;bal_problem&gt;\n"</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  BALProblem bal_problem;</span><br><span class="line">  <span class="keyword">if</span> (!bal_problem.LoadFile(argv[<span class="number">1</span>])) &#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cerr</span> &lt;&lt; <span class="string">"ERROR: unable to open file "</span> &lt;&lt; argv[<span class="number">1</span>] &lt;&lt; <span class="string">"\n"</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">double</span>* observations = bal_problem.observations();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Create residuals for each observation in the bundle adjustment problem. The</span></span><br><span class="line">  <span class="comment">// parameters for cameras and points are added automatically.</span></span><br><span class="line">  ceres::Problem problem;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; bal_problem.num_observations(); ++i) &#123;</span><br><span class="line">    <span class="comment">// Each Residual block takes a point and a camera as input and outputs a 2</span></span><br><span class="line">    <span class="comment">// dimensional residual. Internally, the cost function stores the observed</span></span><br><span class="line">    <span class="comment">// image location and compares the reprojection against the observation.</span></span><br><span class="line"></span><br><span class="line">    ceres::CostFunction* cost_function =</span><br><span class="line">        SnavelyReprojectionError::Create(observations[<span class="number">2</span> * i + <span class="number">0</span>],</span><br><span class="line">                                         observations[<span class="number">2</span> * i + <span class="number">1</span>]);</span><br><span class="line">    problem.AddResidualBlock(cost_function,</span><br><span class="line">                             <span class="literal">NULL</span> <span class="comment">/* squared loss */</span>,</span><br><span class="line">                             bal_problem.mutable_camera_for_observation(i),</span><br><span class="line">                             bal_problem.mutable_point_for_observation(i));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Make Ceres automatically detect the bundle structure. Note that the</span></span><br><span class="line">  <span class="comment">// standard solver, SPARSE_NORMAL_CHOLESKY, also works fine but it is slower</span></span><br><span class="line">  <span class="comment">// for standard bundle adjustment problems.</span></span><br><span class="line">  ceres::Solver::Options options;</span><br><span class="line">  options.linear_solver_type = ceres::DENSE_SCHUR;</span><br><span class="line">  options.minimizer_progress_to_stdout = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">  ceres::Solver::Summary summary;</span><br><span class="line">  ceres::Solve(options, &amp;problem, &amp;summary);</span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; summary.FullReport() &lt;&lt; <span class="string">"\n"</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>　　这里使用了 Bundle Adjustment in the Large<a href="#3" id="3ref"><sup>[3]</sup></a> 数据集，观测量为图像坐标系下路标(特征)的像素坐标系，待优化的参数为各路标的 3D 坐标以及相机内外参，这里相机内外参有 9 个，其中位置及姿态 6 个，畸变系数 2 个，焦距 1 个。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> 高翔. 视觉 SLAM 十四讲: 从理论到实践. 电子工业出版社, 2017.<br><a id="2" href="#2ref">[2]</a> https://github.com/HKUST-Aerial-Robotics/A-LOAM<br><a id="3" href="#3ref">[3]</a> http://grail.cs.washington.edu/projects/bal/</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　非线性最小二乘(Non-linear Least Squares)问题应用非常广泛，尤其是在 SLAM 领域。&lt;a href=&quot;/LOAM/&quot; title=&quot;LOAM&quot;&gt;LOAM&lt;/a&gt;，，&lt;a href=&quot;/[paper_reading]-Stereo-RCNN-ba
      
    
    </summary>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/categories/SLAM/"/>
    
    
      <category term="Optimization" scheme="https://leijiezhang001.github.io/tags/Optimization/"/>
    
  </entry>
  
  <entry>
    <title>如何搭建一个 ADAS 产品</title>
    <link href="https://leijiezhang001.github.io/How-to-Build-An-ADAS/"/>
    <id>https://leijiezhang001.github.io/How-to-Build-An-ADAS/</id>
    <published>2020-05-07T01:36:23.000Z</published>
    <updated>2020-06-23T07:20:24.170Z</updated>
    
    <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="本文介绍了基于视觉的 ADAS 产品构建方法，全文 6K 字 12 图，请输入密码查看：" />    <label for="pass">本文介绍了基于视觉的 ADAS 产品构建方法，全文 6K 字 12 图，请输入密码查看：</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      从零到一构建视觉 ADAS 产品。
    
    </summary>
    
      <category term="Autonomous Driving System" scheme="https://leijiezhang001.github.io/categories/Autonomous-Driving-System/"/>
    
      <category term="ADAS" scheme="https://leijiezhang001.github.io/categories/Autonomous-Driving-System/ADAS/"/>
    
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="ADAS" scheme="https://leijiezhang001.github.io/tags/ADAS/"/>
    
  </entry>
  
  <entry>
    <title>OctoMap</title>
    <link href="https://leijiezhang001.github.io/OctoMap/"/>
    <id>https://leijiezhang001.github.io/OctoMap/</id>
    <published>2020-04-23T02:23:05.000Z</published>
    <updated>2020-04-27T02:31:31.376Z</updated>
    
    <content type="html"><![CDATA[<p>　　地图是机器人领域非常重要的模块，也可以认为是自动驾驶保障安全的基础模块。根据存储建模类型，地图可分为拓扑地图，栅格地图，点云地图等。<a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 就是一种能在线检测静态障碍物的栅格地图。自动驾驶领域，地图的用处有：</p><ul><li><strong>高精度定位</strong>，一般是 3D 栅格地图，但是栅格中近似存储点云原始信息；</li><li><strong>路径规划</strong>，不同规划算法依赖不同地图，自动驾驶中比较靠谱又简单的规划算法一般依赖拓扑地图，俗称高精度语义地图，描述一些车道线等路面拓扑关系；而在室内或低速无道路信息场景，则会用如 \(A ^ * \) 算法在栅格地图上进行路径规划；</li><li><strong>辅助感知检测未定义类别的障碍物</strong>，有人称之为静态地图，一般是 2.5D 栅格地图，图层可以自定义一些语义信息；</li></ul><p>下游不同模块对不同存储方式的利用效率是不同的，所以需要针对不同下游任务设计不同地图建模方式。本文<a href="#1" id="1ref"><sup>[1]</sup></a>介绍了一种基于八叉树的栅格地图建模方法。<br>　　对于机器人而言，类似 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 能建模 FREE，OCCUPIED，UNMAPPED AREAS 的地图是信息量比较丰富的，但是 Grid-Mapping 是 2D 的。这里对 3D 地图有以下要求：</p><ul><li><strong>Probabilistic Representation</strong><br>测量都会有不确定性，这种不确定性需要用概率表征出来；另外多传感器融合也需要基于概率的表示；</li><li><strong>Modeling of Unmapped Areas</strong><br>对机器人导航而言，显式得表示哪些区域是观测未知的也非常重要；</li><li><strong>Efficiency</strong><br>地图构建与存储需要非常高效，一般而言，地图的内存消耗会是瓶颈；</li></ul><p><img src="/OctoMap/maps.png" width="90%" height="90%" title="图 1. Different Representations of Maps"> 　　如图 1. 所示，原始点云地图信息量丰富，但是不能结构化存储；Elevation Maps 与 Multi-level Surface Maps 虽然高效，但是不能表征未观测的区域信息。OctoMap 可以认为是 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 的 3D 版本，信息量丰富且高效。</p><h2 id="octomap-mapping-framework">1. OctoMap Mapping Framework</h2><h3 id="octrees">1.1. Octrees</h3><p><img src="/OctoMap/OctoMap.png" width="40%" height="40%" title="图 2. 八叉树地图存储"> 　　如图 2. 所示，八叉树是将空间递归得等分成八份(QuadTree 四叉树则等分为四份)，每个节点可以存储 Occupied，Free，Unknown 信息(Occupied 概率即可)。此外，如果子节点的状态都一样，那么可以进行剪枝，只保留大节点低分辨率的 Voxel，达到紧凑存储的目的。<br>　　时间复杂度上，对于有 \(n\) 个节点，深度为 \(d\) 的八叉树，那么单次查询的时间复杂度为 \(\mathcal{O}(d)=\mathcal{O}(\mathrm{log}\,n)\)；遍历节点的时间复杂度为 \(\mathcal{O}(n)\)。\(d = 16, r = 1cm\)，可以覆盖 \((655.36m)^3\)的区域。</p><h3 id="probabilistic-sensor-fusion">1.2. Probabilistic Sensor Fusion</h3><p>　　时序概率融合也是基于贝叶斯滤波，详见 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a>，只不过这里是 3D Mapping，作 Raycasting 的时候采用 <a href="/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/" title="What You See is What You Get">What You See is What You Get</a> 中提到的 Fast Voxel Traversal 算法。实际应用中，一般都会采用上下界限制概率值，这种限制也能提高八叉树的剪枝率。</p><h3 id="multi-resolution-queries">1.3. Multi-Resolution Queries</h3><p>　　由于八叉树的特性，OctoMap 支持低于最高分辨率的 Voxel 概率查询，即父节点是子节点的平均概率，或是子节点的最大概率: <span class="math display">\[\bar{l}(n)=\frac{1}{8}\sum _ {i=1}^8 L (n _ i)\\\hat{l}(n)=\max\limits _ iL(n _ i)\tag{1}\]</span> 其中 \(l\) 是测量模型下概率的 log-odds 值。</p><h2 id="implementation-details-statics">2. Implementation Details &amp; Statics</h2><h3 id="memory-efficient-node-map-file-generation">2.1. Memory-Efficient Node &amp; Map File Generation</h3><p><img src="/OctoMap/save.png" width="60%" height="60%" title="图 3. Node Memory Consumption and Serialization"> 　　如图 3. 左图所示，每个节点只分配一个 float 型的数据存储以及指向子节点地址数组的地址指针(而不是直接包含子节点地址的指针)，只有存在子节点时，才会分配子节点的地址数组空间。由此在 32-bit 系统中(4 字节对齐)，每个父节点需要 40B，子节点需要 8B；在 64-bit 系统中(8 字节对齐)，每个父节点需要 80B(\(4+9\times 8\))，子节点需要 16B(\(4+8)\)。<br>　　地图存储需要在信息量损失最小的情况下进行压缩。如图 3. 右图所示，存储序列化时，每个叶子节点总共需要 4B 概率值，不需要状态量；每个父节点总共需要 2B，表示 8 个子节点的 2bit 状态量(貌似与论文有出入，其不是最优的压缩)。在这种压缩方式下，大范围地图的存储大小一般也能接受。根据存储的地图重建地图时，只需要知道坐标原点即可。</p><h3 id="accessing-data-memory-consumption">2.2. Accessing Data &amp; Memory Consumption</h3><p><img src="/OctoMap/memusage1.png" width="60%" height="60%" title="图 4. Memory Usage VS. Scan Num."> 　　Freiburg 建图大小为 \((202\times 167\times 28) m^3\)，如图 4. 所示，随着点云扫描区域扩大，OctoMap 表示方式能有效降低建图大小。 <img src="/OctoMap/memusage2.png" width="60%" height="60%" title="图 5. Memory Usage VS. Resolution"> 　　图 5. 则说明建图大小与分辨率的关系。 <img src="/OctoMap/inserttime.png" width="60%" height="60%" title="图 6. Insert Date Time VS. Resolution"> <img src="/OctoMap/traversetime.png" width="60%" height="60%" title="图 7. Traverse Data Time VS. Depth"> 　　图 6. 显示了往图中插入一个节点所需时间，1000 个节点在毫秒级；图 7. 显示了遍历所有节点所需的时间，基本也在毫秒级。 <img src="/OctoMap/compress.png" width="60%" height="60%" title="图 8. Compression Ratio"> 　　通过限制概率上下界，可以剪枝压缩图，用 KL-diverge 来评估压缩前后图的分布相似性，图 8. 显示了压缩比与网络大小及相似性的关系。</p><h3 id="some-strategies">2.3. Some Strategies</h3><p><img src="/OctoMap/case.png" width="60%" height="60%" title="图 9. Corner Case Handle"> 　　如图 9. 所示，前后帧位姿的抖动，会导致 Occupied 持续观测的不稳定，所以需要一些领域约束策略来保证 Occupied 的稳定观测。这种类似的策略在 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 工程实现中也需要采用，因为实际的 Pose 肯定会有噪声，导致同一目标的栅格前后有一定概率不能完全命中。</p><h2 id="rethinking">3. ReThinking</h2><p>　　对于自动驾驶来说，高度方向的范围不需要很大，甚至四叉树足矣，如果采用八叉树，那么需要将高度方向的分辨率降低，从而更加紧凑的构建地图。<br>　　此外自动驾驶肯定是需要大范围建图的，如平方千公里级别。所以切片式的地图存储与查询就显得尤为重要，换句话说，需要动态得载入局部地图，这就有两种思路：</p><ul><li>动态载入完全局部地图<br>要求前后局部地图有一定的重叠，通过索引式的存储可以不存储重叠区域的地图信息；</li><li>动态载入部分局部地图<br>随着机器人本体的运动，实时动态载入前方更远处的地图，丢掉后方远处的历史地图。这对在线地图结构的灵活性要求比较高，如果基于八叉树，那么需要作片区域剪枝及插入的操作，效率不一定高；</li></ul><p>　　在自动驾驶领域，目前用于高精度定位的栅格地图与用于 PNC 规划控制的拓扑地图(高精地图)已经比较成熟；而用于环境感知的静态语义地图还没形成大范围的共识。不管从工程实现效果及效率上，还是语义信息描述定义上，还需作很多探索与实践。比如，可以定义最底层的语义信息：地面高度，此外也可以把车道线信息打到栅格图层中去(但是可能加大对 PNC 的搜索计算量)，等等。所以可能最优的存储查询方式并不是八叉树，<strong>可能还是栅格化后并对每个栅格哈希化，牺牲一定的内存空间，然后作 \(O(1)\) 的快速插入与查询</strong>。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Hornung, Armin, et al. &quot;OctoMap: An efficient probabilistic 3D mapping framework based on octrees.&quot; Autonomous robots 34.3 (2013): 189-206.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　地图是机器人领域非常重要的模块，也可以认为是自动驾驶保障安全的基础模块。根据存储建模类型，地图可分为拓扑地图，栅格地图，点云地图等。&lt;a href=&quot;/Grid-Mapping/&quot; title=&quot;Grid-Mapping&quot;&gt;Grid-Mapping&lt;/a&gt; 就是一种能在
      
    
    </summary>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/categories/SLAM/"/>
    
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/tags/SLAM/"/>
    
      <category term="Mapping" scheme="https://leijiezhang001.github.io/tags/Mapping/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;What You See is What You Get, Exploiting Visibility for 3D Object Detection&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/"/>
    <id>https://leijiezhang001.github.io/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/</id>
    <published>2020-04-22T01:19:59.000Z</published>
    <updated>2020-04-23T02:34:12.937Z</updated>
    
    <content type="html"><![CDATA[<p>　　Bird-View 3D Detection 都是将点云离散化到 Voxel，有点的 Voxel 提取区域特征，无点的 Voxel 则置为空。而 LiDAR 的测量特性其实还包含更多的信息，<a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 中较详细的阐述了 LiDAR 的测量模型，每个栅格可以标记为三个状态：UNKNOWN，FREE，OCCUPIED。传统的 Bird-View 3D Detection 没有显式得提取 UNKNOW 与 FREE 的信息(即没有提取 Visibility 信息)，而 UNKNOW 与 FREE 对数据增广及检测效果非常重要。 <img src="/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/visibility.png" width="90%" height="90%" title="图 1. Visibility or Freespace from LiDAR"> 　　如图 1. 所示，左图是传统的点云表示方式，无法区分红色区域是否有车，而右图则非常容易得区分哪个区域不可能有车，哪个区域可能有车。所以本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了显式提取点云 UNKNOWN 与 FREE 信息来辅助数据增广与提高目标检测精度的方法。</p><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/framework.png" width="90%" height="90%" title="图 2. Framework"> 　　如图 2. 所示，本文的 3D 检测框架与传统的差不多，是 Anchor-Based 方法，主要不同点是输入网络的特征，即点云栅格化后提取出的特征不一样以及融合时序信息。并且，训练过程中，对数据增广做了精心的设计。 <img src="/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/fusion.png" width="90%" height="90%" title="图 3. Frusion Strategy"> 　　如图 3. 所示，点云栅格化后提取的特征不一样是指增加了 Visibility 图层。有两种融合方式，前融合是与点云栅格化后提取的特征作 Concate，然后输入到主干网络；后融合则是二者分别通过主干网络，然后再作 Concate。实验表明前融合效果较好。</p><h3 id="object-augmentation">1.1. Object Augmentation</h3><p>　　传统的数据增广关注在全帧点云的平移，旋转，翻转变换。本文则采用目标级别的数据增广。首先生成目标的点云集合，可以用 CAD 模型，也可以直接扣实际的目标点云(扣出来的点云增广能力有限)；然后将目标点云集合随机得放到全帧点云中。在放置的过程中需要模拟 LiDAR 的测量模型，也就是 Visibility 计算过程，这在第 2. 节中详细描述。实验表面能提升 ~9 个点。</p><h3 id="temporal-aggregation">1.2. Temporal Aggregation</h3><p>　　时序点云信息的利用可以有以下几种方法：</p><ul><li>将每帧点云栅格化，然后直接在 Chanel 层作 Concate，之后作 3D 卷积，或者先在 Chanel 维度作 1D 卷积，然后作 2D 卷积；</li><li>将点云中的点增加相对时间戳属性，然后作整体的栅格化，之后直接作传统的 2D 卷积；</li></ul><p>本文采用第二种方法，实验表明能提升 ~8 个点。</p><h2 id="visibility-computing">2. Visibility Computing</h2><p>　　<a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 中已经应用了 Raycasting 来计算 Visibility/Free。对于点云中的每一个点，我们不仅能获得该点探测到障碍物的这个信息，还能知道，传感器与该点之间的连线上是 Free 的。这就要求能高效得计算该连线相交 Voxel 的集合。该计算模型也用来修正 Object Augmentation 时的点云。</p><h3 id="efficient-voxel-traversal">2.1. Efficient Voxel traversal</h3><p>　　对每个点，都需要遍历传感器原点到该点所经过的 Voxel，采用 Fast Voxel Traversal<a href="#2" id="2ref"><sup>[2]</sup></a>方法来进行高效的 Voxel 遍历。</p><h3 id="raycasting-with-augmented-objects">2.2. Raycasting with Augmented Objects</h3><p><img src="/paper-reading-What-You-See-is-What-You-Get-Exploiting-Visibility-for-3D-Object-Detection/augment.png" width="90%" height="90%" title="图 4. Rectify Object Augmentation"> 　　如图 4. 所示，本文设计了两种策略来修正物体增广：</p><ul><li>Culling，如果该物体是被遮挡的，那么直接去掉，这样会极大减少增广的物体；</li><li>Drilling，如果该物体是被遮挡的，那么将遮挡物去掉，即置为 Free；</li></ul><p>实验表明 Drilling 效果较好，在训练时采用该策略进行物体增广后的点云修正，作 Inference 时就直接计算 Freespace 即可。</p><h3 id="online-occupancy-mapping">2.3. Online Occupancy Mapping</h3><p>　　栅格内点云提取特征时融合了时序信息，Visibility 也需要融合时序信息，最直观的方式是将 3D Occupancy Map 进行时间维度的堆叠，获得 4D Map，这样对后续的计算量较大。本文采用 OctoMap<a href="#3" id="3ref"><sup>[3]</sup></a> 计算方式，作贝叶斯滤波，得到时序滤波的 3D Occupancy Map。原理与 <a href="/Grid-Mapping/" title="Grid-Mapping">Grid-Mapping</a> 一样，只不过这里是 3D 的。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Hu, Peiyun, et al. &quot;What You See is What You Get: Exploiting Visibility for 3D Object Detection.&quot; arXiv preprint arXiv:1912.04986 (2019).<br><a id="2" href="#2ref">[2]</a> Amanatides, John, and Andrew Woo. &quot;A fast voxel traversal algorithm for ray tracing.&quot; Eurographics. Vol. 87. No. 3. 1987.<br><a id="3" href="#3ref">[3]</a> Hornung, Armin, et al. &quot;OctoMap: An efficient probabilistic 3D mapping framework based on octrees.&quot; Autonomous robots 34.3 (2013): 189-206.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Bird-View 3D Detection 都是将点云离散化到 Voxel，有点的 Voxel 提取区域特征，无点的 Voxel 则置为空。而 LiDAR 的测量特性其实还包含更多的信息，&lt;a href=&quot;/Grid-Mapping/&quot; title=&quot;Grid-Map
      
    
    </summary>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/categories/3D-Detection/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Reconfigurable Voxels, A New Representation for LiDAR-Based Point Clouds&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-Reconfigurable-Voxels/"/>
    <id>https://leijiezhang001.github.io/paper-reading-Reconfigurable-Voxels/</id>
    <published>2020-04-20T01:54:31.000Z</published>
    <updated>2020-04-21T03:38:54.663Z</updated>
    
    <content type="html"><![CDATA[<p>　　Voxel-based 点云特征提取虽然损失了一定的信息，但是计算高效。Voxel-based 方法一个比较大的问题是，由于<strong>点云分布的不均匀性</strong>，作卷积时会导致可能计算的区域没有点，从而不能有效提取局部信息。为了解决栅格化后栅格中点云分布的不均匀问题，目前看到的有以下几种方法：</p><ol type="1"><li>Deformable Convolution，采用可变形卷积方法，自动学习卷积核的连接范围，理论上应该能更有效得使卷积核连接到点密度较高的栅格；</li><li><a href="/paper-reading-PolarNet/" title="PolarNet">PolarNet</a>提出了一种极坐标栅格化方式，因为点云获取的特性，这种方法获得的栅格中点数较为均匀;</li><li>手动设计不同分辨率的栅格，作特征提取，然后融合。比如近处分辨率较高，远处较低的方式；</li><li>本文<a href="#1" id="1ref"><sup>[1]</sup></a> 提出了一种自动选择栅格领域及分辨率，从而最大化卷积区域点数的方法；</li></ol><p><img src="/paper-reading-Reconfigurable-Voxels/reconfig.png" width="80%" height="80%" title="图 1. Reconfig Voxels"> 　　如图 1. 所示，本文提出的 Reconfigurable Voxel 方法，能自动选择领域内点数较多的栅格特征提取，进而作卷积运算，避免点数较少，从而信息量较少的栅格作特征提取操作；此外还可根据点数自动调整分辨率以获得合适的栅格点数。通过这种方法，每个栅格输入到网络前都能有效提取周围点数较多区域的特征信息。</p><h2 id="framework">1. Framework</h2><p><img src="/paper-reading-Reconfigurable-Voxels/pipeline.png" width="80%" height="80%" title="图 2. Framework"> 　　如图 2. 所示，本文以检测任务为例，分三部分：Voxel/Pillar Feature Extraction，Backbone，RPN/Detection Head。后两个采用传统的方法，本文主要是改进 Voxel/Pillar Feature Extraction，这是输入到网络前的特征提取阶段。</p><h2 id="voxelpillar-feature-extraction">2. Voxel/Pillar Feature Extraction</h2><p>　　传统的输入到 2D 卷积网络的特征要么是手工提取的，要么是用 <a href="/paperreading-PointPillars/" title="PointPillars">PointPillars</a> 网络去学习每个 Voxel 的特征。由此输入到网络的特征不是最优的，因为点云的稀疏性会导致后面的 2D 卷积网络作特征提取时遇到很多“空”的 Voxel。本文提出的方法就能显式得搜索每个 Voxel 周围有点的区域作特征提取，使得之后 2D 卷积特征提取更加有效。其步骤为：</p><ul><li>点云栅格化，并存储每个 Voxel 周围 Voxel 的索引；</li><li>每个 Voxel 周围 Voxel 作 Biased Random Walk，去搜索有更稠密点云的 Voxel；</li><li>将每个 Voxel 与新搜索到的周围 Voxel 作特征提取与融合，得到该 Voxel 特征；</li></ul><h3 id="biased-random-walking-neighbors">2.1. Biased Random Walking Neighbors</h3><p>　　邻域 Voxel 搜索目标是：<strong>在距离较近的情况下寻找较稠密的 Voxel</strong>。由此设计几种策略：</p><ul><li>点数越少的 Voxel，有更高概率作 Random Walk，以及更多 Step 去周围相邻的 Voxel；</li><li>点数越多的 Voxel，有更高概率被其它 Voxel Random Walk 到；</li></ul><p>　　将以上策略数学化。设第 \(j\) 个 Voxel 有 \(N(j)\) 个点，最大点数为 \(n\)，其作 Random Walk 的概率为 \(P _ w(j)\)，步数 Step 为 \(S(j)\)，第 \(i\) 步到达的 Voxel 为 \(w _ j(i)\)，其四领域 Voxel 集合为 \(V(w _ j(i))\)，从该 Voxel 走到下一个 Voxel 的概率为 \(P(w _ j(i+1)|w _ j(i))\)。由此得到以上策略的数学描述： <span class="math display">\[P _ w(j)=\frac{1}{N(j)} \tag{1}\]</span> <span class="math display">\[S(j)=n-N(j)\tag{2}\]</span> <span class="math display">\[P\left(w _ j(i+1)|w _ j(i)\right) = \frac{N\left(w _ j(i+1)\right)}{\sum _ {v\in V(w _ j(i))}N(v)}\tag{3}\]</span> 需要注意的是，\(S(j)\) 是在开始时计算的，此后每走一步就减1。 <img src="/paper-reading-Reconfigurable-Voxels/random_walk.png" width="90%" height="90%" title="图 3. Random walk"> 　　如图 3. 所示，左边为单分辨率下 Voxel 搜索过程。</p><h3 id="reconfigurable-voxels-encoder">2.2. Reconfigurable Voxels Encoder</h3><p>　　每个 Voxel \(v _ i\) 搜索到最优的 4 领域 Voxel 集 \(V(v _ i)\) 后，需要融合得到该 Voxel 的特征： <span class="math display">\[\begin{align}F(v _ i) &amp;= \psi\left(f _ {v _ i}, f _ {V(v _ i)}\right)\\&amp;= \varphi _ 1\left[\varphi _ 2(f _ {v _ i}), \varphi _ 2\left(\sum _ {j=1}^4 W _ j(f _ {v _ i})f _ {V _ {j(v _ i)}}\right)\right] _ f\tag{4}\end{align}\]</span> 其中 \(\varphi _ 1\) 为 low-level 操作，如 average pooling，\(\varphi _ 2\) 为 high-level 操作，如 MLP。</p><h3 id="multi-resolution-reconfigurable-voxels">2.3. Multi-resolution Reconfigurable Voxels</h3><p>　　图 3. 左边是单分辨率情况，Random Walking 可以拓展到多分辨率情形。当点云非常稀疏的时候，就很有必要降低栅格的分辨率。如图 3. 所示，\(P _ w\) 计算时除以 4，以维持与高分辨率的一致性；高分辨率到低分辨率搜索概率为 \(0.25P _ w\)，低分辨率到高分辨率搜索概率为 \(0.5P _ w\)。其余准则与单分辨率一致。实验结果表面多分辨率有一定提升，但是相比单分辨率提升不明显。</p><h2 id="reference">3. Reference</h2><p><a id="1" href="#1ref">[1]</a> Wang, Tai, Xinge Zhu, and Dahua Lin. &quot;Reconfigurable Voxels: A New Representation for LiDAR-Based Point Clouds.&quot; arXiv preprint arXiv:2004.02724 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Voxel-based 点云特征提取虽然损失了一定的信息，但是计算高效。Voxel-based 方法一个比较大的问题是，由于&lt;strong&gt;点云分布的不均匀性&lt;/strong&gt;，作卷积时会导致可能计算的区域没有点，从而不能有效提取局部信息。为了解决栅格化后栅格中点云分布
      
    
    </summary>
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/categories/Deep-Learning/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;PolarNet&quot;</title>
    <link href="https://leijiezhang001.github.io/paper-reading-PolarNet/"/>
    <id>https://leijiezhang001.github.io/paper-reading-PolarNet/</id>
    <published>2020-04-16T01:19:12.000Z</published>
    <updated>2020-04-17T02:09:42.539Z</updated>
    
    <content type="html"><![CDATA[<p>　　Point-wise 特征提取在 <a href="/PointCloud-Feature-Extraction/" title="PointCloud-Feature-Extraction">PointCloud-Feature-Extraction</a> 中已经有较为详细的描述，虽然 Point-wise 提取的特征更加精细，但是一般都有 KNN 构建及索引操作，计算量较大，而且实践中发现学习收敛较慢。Voxel-based 虽然理论上损失了一定的信息，但是能直接应用 2D 卷积网络，网络学习效率很高。本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种在极坐标下栅格化后进行点云 Semantic Segmentation 的方法，相比传统的笛卡尔坐标系下栅格化有一定的优势。</p><h2 id="voxelization">1. Voxelization</h2><p><img src="/paper-reading-PolarNet/pts.png" width="98%" height="98%" title="图 1. Cartesian VS. Polar"> 　　如图 1. 所示，传统的笛卡尔坐标系下栅格化的栅格是矩形，而极坐标系下栅格是饼状的。激光雷达是在极坐标方式下获取点云的，所以由图可知，<strong>极坐标栅格化下，每个栅格拥有的点数更加均匀</strong>，有利于网络学习并减少计算量。此外，本文统计后显示，相比笛卡尔坐标栅格，极坐标的栅格内点属于同一目标的概率更大。</p><h2 id="polarnet-framework">2. PolarNet Framework</h2><p><img src="/paper-reading-PolarNet/framework.png" width="98%" height="98%" title="图 2. PolarNet"> 　　如图 2. 所示，点云经过 Polar 栅格化后，对每个栅格首先进行 PointNet 特征提取，然后对所有栅格作 ring-convolution 操作。<br>　　ring-convolution 是指卷积在环形方向进行，没有边缘截断效应。实现上，将栅格从某处展开，然后边缘处用另一边对应的栅格进行 padding，即可用普通的卷积进行运算。<br>　　网络是作 Voxel-wise 的分割，然后直接将预测的类别应用到栅格内的点云中。统计上，同一栅格内的点云属于不同类别的概率很低，所以本文并没进一步作 Point-wise 的分割。</p><h2 id="rethinking">3. Rethinking</h2><p>　　PolarNet 作 Semantic Segmentation 比其它方法提升很多。但是实际应用时，PolarNet 不能指定各个方向的范围，所以计算效率较低。比如，自动驾驶中，我们可以设定前 100m，后 60m，左右各 30m 的检测范围，笛卡尔坐标系下很容易进行栅格化，而极坐标下则没法搞。所以为了解决点云的分布不均匀问题，另一种思路是在笛卡尔坐标系下，近处打高分辨率的栅格，远处打低分辨率的栅格。具体实现，可以先用低分辨率过一遍网络，然后再对感兴趣的特定区域作高分辨率检测。</p><h2 id="reference">4. Reference</h2><p><a id="1" href="#1ref">[1]</a> Zhang, Yang, et al. &quot;PolarNet: An Improved Grid Representation for Online LiDAR Point Clouds Semantic Segmentation.&quot; arXiv preprint arXiv:2003.14032 (2020).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Point-wise 特征提取在 &lt;a href=&quot;/PointCloud-Feature-Extraction/&quot; title=&quot;PointCloud-Feature-Extraction&quot;&gt;PointCloud-Feature-Extraction&lt;/a&gt; 中已经有
      
    
    </summary>
    
      <category term="Semantic Segmentation" scheme="https://leijiezhang001.github.io/categories/Semantic-Segmentation/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="Segmentation" scheme="https://leijiezhang001.github.io/tags/Segmentation/"/>
    
  </entry>
  
</feed>
