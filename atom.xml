<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LeijieZhang</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://leijiezhang001.github.io/"/>
  <updated>2019-06-19T10:51:52.879Z</updated>
  <id>https://leijiezhang001.github.io/</id>
  
  <author>
    <name>Leijie</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>[paper_reading]-&quot;Visual Odometry Part I&amp;II&quot;</title>
    <link href="https://leijiezhang001.github.io/%5Bpaper_reading%5D-Visual_Odometry_Part_I_II/"/>
    <id>https://leijiezhang001.github.io/[paper_reading]-Visual_Odometry_Part_I_II/</id>
    <published>2019-06-17T07:16:53.000Z</published>
    <updated>2019-06-19T10:51:52.879Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-ensp-Overview-of-VO"><a href="#1-ensp-Overview-of-VO" class="headerlink" title="1.&ensp;Overview of VO"></a>1.&ensp;Overview of VO</h2><p>　　SFM(Structure from Motion) 是解决从一堆图片中将场景以及相机姿态进行 3-D 重建的问题，最后的场景以及相机姿态可以通过离线优化方法（bundle adjustment）来 refine。VO &amp; VSLAM 都属于 SFM 的特殊情况，SfM 处理的图像时间上可以是无序的，而 VO &amp; VSLAM 则要求图像时间上有序。VO 只关心轨迹的局部一致性，而 VSLAM 关心全局轨迹和地图的一致性。VO 可以作为 VSLAM 的一个模块，用于重建相机的增量运动，Bundle Adjustment 可以用来 refine 相机的轨迹。如果用户只对相机路径感兴趣，不需要环境地图，且需要较高的实时性，那么一般 VO 就能满足需求。<br>　　视觉里程计（VO）最早应用于 NASA 火星地面探测器，相比于车轮里程计的优势：</p><ul><li>不受车轮打滑的影响；</li><li>不受拐弯影响，拐弯时左右轮速度不一样；</li><li>更加准确，相对位置误差大概在 0.1% 到 2%，可作为车轮里程计、GPS，IMU等其它测量装置的补充；</li><li>在某些领域是必须的，比如无法使用车轮里程计的无人机，GPS 失效的水下环境等；</li></ul><p>　　根据视觉传感器数量，VO 可分为 Stereo VO，与 Monocular VO。当场景距离远远大于双目基线时，Stereo VO 也需要退化成 Monocular VO 来处理。</p><h3 id="1-1-ensp-Stereo-VO-amp-Monocular-VO"><a href="#1-1-ensp-Stereo-VO-amp-Monocular-VO" class="headerlink" title="1.1.&ensp;Stereo VO &amp; Monocular VO"></a>1.1.&ensp;Stereo VO &amp; Monocular VO</h3><p>　　特征点匹配可以通过特征跟踪（Feature Tracking）或特征检测（Feature Detect）再匹配两种方式实现。特征跟踪计算量小，但是容易漂移；特征检测再匹配计算量大，需要用 RANSAC 去除无匹配点，但是特征点不容易漂移。<br>　　Motion Estimation 可通过 3D-3D，3D-2D，2D-2D 三种方式实现。Stereo 系统可以获得每个点的深度信息，所以这三种方式都可以用来做相机的运动估计。实验表明，直接在原始的 2-D 点上进行相机运动的估计，更加准确（？存疑）。<br>　　之所以研究单目 VO，是因为当场景距离相机很远的时候（相对于双目的基线），双目就退化为单目了。单目 VO 中绝对深度（尺度）是未知的，刚开始两帧相机移动的距离通常设定为 1，之后的相对位姿都基于此。相关方法可分为：</p><ul><li>Feature-based Methods，用每一帧的特征点来估计运动。</li><li>Appearance-based Methods，用图像中所有的像素点或是子区域中的像素点来估计运动。</li><li>Hybrid Methods，结合以上两种形式。</li></ul><p>第一种方法较好，运动估计用 five-point RANSAC 来求解。</p><h3 id="1-2-ensp-Reducing-the-Drift"><a href="#1-2-ensp-Reducing-the-Drift" class="headerlink" title="1.2.&ensp;Reducing the Drift"></a>1.2.&ensp;Reducing the Drift</h3><p>　　由于 VO 是一步步计算相机的运动轨迹然后作累加的，那么误差就有累积性，使得估计的运动轨迹会漂移。这可以用 Sliding Window(Windowed) Bundle Adjustment 局部优化方法来解决。也可以用 GPS 或 laser 或 IMU 融合来解决。Windowed Bundle Adjustment，是通过 m 个窗口下的信息来优化求解这 m 个相机位姿。</p><h3 id="1-3-ensp-VO-Versus-V-SLAM"><a href="#1-3-ensp-VO-Versus-V-SLAM" class="headerlink" title="1.3.&ensp;VO Versus V-SLAM"></a>1.3.&ensp;VO Versus V-SLAM</h3><p>　　V-SLAM 两大方法：</p><ul><li><strong>Filtering Methods</strong><br>概率法，以一定的概率分布融合所有图像信息；</li><li><strong>Keyframe Methods</strong><br>关键帧法，使用全局 Bundle Adjustment 优化被选择的关键帧；</li></ul><p>　　VO 只关心相机轨迹的一致性，而 SLAM 关注轨迹与地图整体的一致性。SLAM 中两大问题是，检测 loop closure 的发生以及用这个约束来更好的优化当下的地图和轨迹。而 VO 只对历史中以往 n 个轨迹中的位姿进行优化（windowed bundle adjustment），这可以认为与 SLAM 中建立局部地图与轨迹是等价的。但是这两者的 philosophy 不同：</p><ul><li>VO 只关心局部轨迹的一致性，局部地图只是用来（在 bundle ajustment）更精确的估计局部轨迹；</li><li>SLAM 关心整个地图的一致性，当然也包括轨迹，轨迹的精确性能使地图更加精确；</li></ul><p>　　VO 可以是 SLAM 的一个模块（相机运动轨迹的重建），SLAM 还需要一个闭环检测，以及一个全局的地图优化策略。V-SLAM 重建相机运动轨迹理论上比 VO 更精确（加入了更多的约束），但是不一定更鲁棒，因为闭环检测中的奇异值对地图的一致性有较大影响。此外 SLAM 更加复杂以及耗计算资源。VO 牺牲了全局一致性，来达到实时运行的目的，因为不需要记录所有的地图信息。</p><h2 id="2-ensp-Formulation-of-the-VO-Problem"><a href="#2-ensp-Formulation-of-the-VO-Problem" class="headerlink" title="2.&ensp;Formulation of the VO Problem"></a>2.&ensp;Formulation of the VO Problem</h2><p>　　在时间 \(k\) 下，相机拍摄的图像集记为：\(I_{0:n}=\{I_0,…,I_k\}\)。相机在时间 \(k-1\) 与 \(k\) 的位姿转换矩阵为 \(T_{k,k-1}\in \mathbb{R}^{4\times 4}\)。VO 所要求解的问题就是 \(T=T_{1,0}T_{2,1}…T_{k,k-1}\)。由此可知 VO 是计算相邻帧的相机位姿，然后对之前 m 个位姿做一个局部优化从而估计更准确的轨迹。<br><img src="/[paper_reading]-Visual_Odometry_Part_I_II/VO流程.png" width="50%" height="50%" title="图 1. VO流程图"><br>　　大多数 VO 算法是基于特征点来估计运动的，特征点法的流程如图 1. 所示：</p><ol><li><strong>Feature Detection(Extraction) and Matching/Feature Tracking</strong><br>特征提取并与上一帧的特征进行匹配，或者直接用上一帧的特征在这一帧进行跟踪；</li><li><strong>Motion Estimation</strong><br>在 \(k,k-1\) 帧之间求解 \(T_{k,k-1}\) 的过程，根据匹配的特征点对是 2D 还是 3D，运动估计可分为 3D-3D，3D-2D，2D-2D 三种方式实现；</li><li><strong>Local Optimization</strong><br>在 \(k,k-m\) 帧用 Bundle Adjustment 迭代优化求解最优的局部轨迹；</li></ol><p>本文会重点阐述 <strong><em>Camera Model</em></strong><a href="#1" id="1ref"><sup>[1]</sup></a>，<strong><em>Feature Detection and Matching</em></strong><a href="#2" id="2ref"><sup>[2]</sup></a>，<strong><em>Motion Estimation</em></strong><a href="#1" id="1ref"><sup>[1]</sup></a>，<strong><em>Robust Estimation</em></strong><a href="#2" id="2ref"><sup>[2]</sup></a>，<strong><em>Local Optimization</em></strong><a href="#2" id="2ref"><sup>[2]</sup></a>。</p><h2 id="3-ensp-Camera-Modeling-and-Calibration"><a href="#3-ensp-Camera-Modeling-and-Calibration" class="headerlink" title="3.&ensp;Camera Modeling and Calibration"></a>3.&ensp;Camera Modeling and Calibration</h2><p>　　<a href>相机模型及标定</a>，另文详述。</p><h2 id="4-ensp-Feature-Detection-and-Matching-Feature-Tracking"><a href="#4-ensp-Feature-Detection-and-Matching-Feature-Tracking" class="headerlink" title="4.&ensp;Feature Detection and Matching/Feature Tracking"></a>4.&ensp;Feature Detection and Matching/Feature Tracking</h2><p>　　生成前后帧特征点的匹配对，有两种方法：</p><ul><li>feature tracking<br>用局部搜索的方法，较适用于相邻两帧视角变化不大的情况，会有漂移（drift）的现象；</li><li>feature detection and matching<br>独立在每个图像上进行检测，然后用某种度量准则进行匹配。在视野变化较大的情况下，只能用这种方法；</li></ul><h3 id="4-1-ensp-Feature-Tracking"><a href="#4-1-ensp-Feature-Tracking" class="headerlink" title="4.1.&ensp;Feature Tracking"></a>4.1.&ensp;Feature Tracking</h3><p>　　主要采用 <a href>KLT</a> 方法进行特征点跟踪。</p><h3 id="4-2-ensp-Feature-Detection-and-Matching"><a href="#4-2-ensp-Feature-Detection-and-Matching" class="headerlink" title="4.2.&ensp;Feature Detection and Matching"></a>4.2.&ensp;Feature Detection and Matching</h3><p>　　特征点包含特征检测子与特征描述子。一个好的特征点应该有如下性质：</p><ul><li>可重复性(Repeatability)，不同图像下相同特征点可再次检测出；</li><li>可区别性(Distinctiveness)，不同特征点表达形式不一样，可以更好匹配；</li><li>高效率(Efficiency)，计算高效；</li><li>本地性(Locality)，特征仅与一小片图像区域有关；</li><li>定位准确(Localization Accuracy)，不同尺度下定位都要准确；</li><li>鲁棒性(Robustness)，对噪声，模糊，压缩有较好的鲁棒；</li><li>不变性(Invariance)，对光照(photometric)，旋转，尺度，投影畸变(geometric)有不变性；</li></ul><h4 id="4-2-1-ensp-Feature-Detector"><a href="#4-2-1-ensp-Feature-Detector" class="headerlink" title="4.2.1.&ensp;Feature Detector"></a>4.2.1.&ensp;Feature Detector</h4><p>　　特征检测子（feature detector）的计算过程包含两步，首先将图像进行一个特征响应函数的变换，比如 Harris 中的 角点响应函数，SIFT 中的 DoG 变换；然后应用非极大值抑制，提取最小或最大值。<br>　　特征检测子可分为两类：</p><ul><li>角点(corners)<br>角点检测子被定义为至少两个边缘相交的地方；角点计算快，定位精度高，但是区分度低，大尺度下定位精度低；</li><li>斑点(blobs)<br>斑点检测子被定义为一种与周围区域在亮度、颜色、纹理下不同的模式；区分度较高，但是速度较慢；</li></ul><p><img src="/[paper_reading]-Visual_Odometry_Part_I_II/detectors.png" width="80%" height="80%" title="图 2. 检测子比较"><br>　　如图2. 所示，常用的角点检测子有 ORB 特征中的 FAST 关键点，Harris 角点等；常用的斑点检测子有 SIFT，SURF，CENSURE 等。</p><h4 id="4-2-2-ensp-Feature-Descriptor"><a href="#4-2-2-ensp-Feature-Descriptor" class="headerlink" title="4.2.2.&ensp;Feature Descriptor"></a>4.2.2.&ensp;Feature Descriptor</h4><p>　　有了特征检测子后，为了特征点匹配，还需要描述这个检测子，描述量称为特征描述子。描述子可分为以下几类：</p><ol><li>Appearance，检测子周围的像素信息<ul><li>SSD 匹配，sum of squared difference，计算检测子周围像素亮度与其的误差和；</li><li>NCC 匹配，normalized cross correlation，相比 SSD，有一定的光照不变性；</li><li>Census Transform，将检测子周围的 patch 像素与其进行对比，合成 0,1 向量；</li></ul></li><li>Histogram of Local Gradient Orientations<ul><li>SIFT，光照，旋转，尺度，均具有不变性；不适用于角点，适用于斑点；</li></ul></li><li>Much Faster<ul><li>BRIEF，二进制描述子，用于 ORB；对于旋转和尺度有较强的区分性，并且提取以及比较速度都很快；</li></ul></li></ol><p>　　目前常用的 ORB 特征，采用的是 Oriented FAST 角点，以及 BRIEF 描述子。</p><h4 id="4-2-3-ensp-Feature-Matching"><a href="#4-2-3-ensp-Feature-Matching" class="headerlink" title="4.2.3.&ensp;Feature Matching"></a>4.2.3.&ensp;Feature Matching</h4><p>　　通过比较特征点中的描述子部分，来完成特征点的匹配。如果是 appearance 描述子，那么一般通过 SSD/NNC 来计算描述子之间的相似度，其它二进制描述子，可通过欧氏距离或汉明距离来度量。<br>　　基于相似性度量的特征匹配，最简单的就是暴力匹配，两组特征点挨个计算相似度。暴力匹配时间复杂度较高，通常我们采用<strong>快速近似最近邻算法（FLANN）</strong>，也可以加入运动估计模型（通过 IMU 等装置获得的大致运动位姿）来缩小搜索范围。特殊的如果是双目系统，因为左右目图像都是矫正过的，所以左右目的特征点匹配可通过行矩阵搜索解决。<br>　　匹配结束后，我们还得进一步验证匹配的正确性，去除误匹配的情况。比如相互一致性验证，每个特征点只能匹配一个特征点。<br>　　实验表明特征点的分布也很影响匹配效果，特征应尽量均匀分布，可以将图像栅格化，然后对不同的栅格用不同的特征检测阈值即可，保证栅格之间特征数量相等。</p><h2 id="5-ensp-Motion-Estimation"><a href="#5-ensp-Motion-Estimation" class="headerlink" title="5.&ensp;Motion Estimation"></a>5.&ensp;Motion Estimation</h2><h3 id="5-1-ensp-2D-2D"><a href="#5-1-ensp-2D-2D" class="headerlink" title="5.1.&ensp;2D-2D"></a>5.1.&ensp;2D-2D</h3><p>　　这种情况下特征点 \(f_{k-1},f_k\) 分别是在 2D 图像 \(I_{k-1},I_k\) 坐标系上。<br>　　<a href>对极约束推倒过程可详见这里</a>。根据对极约束，可推导出同一 3D 点投影到两个相机视角图像下后，其坐标之间的关系：<br>$$p_2^TK^{-T}t^{\wedge} RK^{-1}p_1=0$$<br>记<strong>本质矩阵(Essential Matrix)</strong>\(E=t^{\land} R\)，记<strong>基础矩阵(Fundamental Matrix)</strong>\(F=K^{-T}EK^{-1}\)。基础矩阵描述的是两幅图像对应点的像素坐标的关系；本质矩阵描述的是世界中的某点分别在两个相机坐标系下坐标的相对关系。<br>　　一般相机内参是已知的，所以我们求解本质矩阵。可采用五点法或者八点法来求解，五点法只能处理已知相机标定参数的情况，所以我们一般采用八点法来求解本质矩阵 \(E\)，大于八点即可用最小二乘求解线性方程。然后对本质矩阵进行奇异值分解，即可求出相机的位姿 \(R,t\)。<br>　　当选取的点共面时，基础矩阵的自由度下降，即出现退化的现象，这个时候需要同时求解单应矩阵\(H\)，选择重投影误差较小的那个作为最终的运动估计矩阵。<br>　　此外，还需计算当前运动的相对尺度，可由 3D 点的位置信息求解相对尺度。绝对尺度的求解需要三角化求解。<br>　　总结过程如下：</p><ol><li>得到新的当前帧 \(I_K\);</li><li>提取当前帧的特征点，并与上一帧的特征进行匹配；</li><li>根据匹配的特征点对，计算本质矩阵\(E\)；</li><li>奇异值分解本质矩阵，得到相机运动 \(R_K,t_k\)；</li><li>该相邻帧的相机运动信息与之前相机运动信息进行累计；</li><li>重复 1.；</li></ol><h3 id="5-2-ensp-3D-2D"><a href="#5-2-ensp-3D-2D" class="headerlink" title="5.2.&ensp;3D-2D"></a>5.2.&ensp;3D-2D</h3><p>　　这种情况下，特征点 \(f_{k-1}\) 是 3D 坐标点，\(f_k\) 是其投影到 2D 图像 \(I_K\) 上的匹配点。对于单目的情况，\(f_{k-1}\) 需要从相邻的前面帧中（比如 \(I_{k-2},I_{k-1}\)）三角化出 3D 坐标，然后与当前帧进行匹配，至少需要三帧的视角。3D-2D 比 3D-3D 更加精确，因为 3D-3D 直接优化相机运动，没有优化投影的过程。<br>　　该问题也称为 <strong>PnP(Perspective from n Points)</strong>。PnP 问题有很多种求解方法：</p><ul><li>P3P<br>只是用 3 个点对进行求解，容易受误匹配的影响；</li><li>直接线性变换<br>需要 6 对匹配点才能求解，如果大于 6 对，则可用 SVD 等方法求线性方程的最小二乘解；</li><li>EPnP</li><li>UPnP</li><li>非线性优化(Bundle Adjustment)<br>记 \(p_{k-1}^i\) 为 \(k-1\) 时刻下第 \(i\) 个特征点在相机坐标系下的坐标，定义重投影的误差项：<br>$$\xi=\mathop{\arg\min}\limits_{T_{k,k-1}} \sum_i \left\Vert uv^i_k-K \, T_{k,k-1} \, p_{k-1}^i \right\Vert^2$$</li></ul><p>　　总结过程如下：</p><ol><li>初始化，在 \(I_{k-2},I_{k-1}\) 两张图里提取特征并匹配，三角花得到特征点的 3D 坐标；</li><li>在 \(I_k\) 图像中提取特征点，并与上一帧的特征进行匹配；</li><li>用 PnP 求解相机运动；</li><li>在 \(I_{k-1},I_{k}\) 中三角化所有特征点；</li><li>重复 2.；</li></ol><h3 id="5-3-ensp-3D-3D"><a href="#5-3-ensp-3D-3D" class="headerlink" title="5.3.&ensp;3D-3D"></a>5.3.&ensp;3D-3D</h3><p>　　这种情况下特征点都是 3D 坐标点，都需要三角花得到，可以使用一个立体视觉系统。<br>　　已知两组匹配好的 3D 点，可以用 <strong>ICP(Iterative Closest Point)</strong> 来求解位姿。ICP 有两种求解方式：</p><ul><li>线性求解</li><li>非线性优化(类似 Bundle Adjustment)<br>定义重投影的误差项：<br>$$\xi=\mathop{\arg\min}\limits_{T_{k,k-1}} \sum_i \left\Vert p_{k}^i - T_{k,k-1} \, p_{k-1}^i \right\Vert^2$$</li></ul><p>　　ICP 问题存在唯一解或无穷多解的情况，所以非线性优化时，只要找到极小值，那一定是全局最优解，这也意味着 ICP 非线性优化时可以任意选定初始值。<br>　　在匹配已知的情况下，ICP 问题是有解析解的。不过如果有些特征点观察不到深度，那么可以混合着使用 PnP 和 ICP 优化：对于深度已知的特征点，建模 3D-3D 误差，对于深度未知的特征点，建模 3D-2D 的重投影误差。两个误差项，用非线性优化求解。</p><h3 id="5-4-ensp-Triangulation-and-Keyframe-Selection"><a href="#5-4-ensp-Triangulation-and-Keyframe-Selection" class="headerlink" title="5.4.&ensp;Triangulation and Keyframe Selection"></a>5.4.&ensp;Triangulation and Keyframe Selection</h3><p>　　对于 stereo camera， 3D-2D 比 3D-3D 更准确；3D-2D 比 2D-2D 计算更快，前者是 P3P 问题，后者则至少需要 5 个点。当场景中物体相比基线很大时，那么立体视觉系统就失效了，这时候用单目视觉系统比较靠谱。<br>　　对于 monocular camera，2D-2D 比 3D-2D 看样子更好，因为避免了三角测量；然后实际中，3D-2D 用得更多，因为数据关联更快。<br>　　当两帧之间相隔很短时间时，可以认为基线非常小，这种情况，获得的深度信息不确定性很高，所以需要选择某些 keyframes 来计算。</p><h2 id="6-ensp-Robust-Estimation-Outlier-Rejection"><a href="#6-ensp-Robust-Estimation-Outlier-Rejection" class="headerlink" title="6.&ensp;Robust Estimation/Outlier Rejection"></a>6.&ensp;Robust Estimation/Outlier Rejection</h2><p>　　匹配的特征点可能因为噪音、遮挡、模糊、视角变化、光照变化等原因成为外点（outliers），这时候该匹配对对运动估计来说就是个外点，估计的时候应该想办法去除掉。<br>　　<strong>RANSAC</strong> 目前已是在含有噪声的数据中进行模型估计的标准方法。其思想是随机选取一些数据进行建模，涵盖数据最多的模型即被选择是最终模型。对于相机运动估计来说，模型就是相机的运动 \(R,t\)，数据就是特征匹配对。RANSAC 流程为：</p><ol><li>初始化，记 A 为特征点对集；</li><li>从 A 中随机选取一些点对 s；</li><li>用 s 估计运动模型；</li><li>计算所有的点对与这个模型的距离误差，可使用 point-to-epipolar 距离或是 directional 误差(Sampson distance)；</li><li>统计距离误差小于一定阈值的点对的数量，并存储标记这些内点(inliers)；</li><li>重复 2.，直到达到最大迭代次数；</li><li>选取数量最多的内点点对集，用这些点估计最终模型；</li></ol><p><img src="/[paper_reading]-Visual_Odometry_Part_I_II/ransac.png" width="60%" height="60%" tit le="图 3. RANSAC 迭代次数比较"></p><p>　　为保证得到正确解，迭代次数要求：<br>$$N=\frac{log(1-p)}{log(1-(1-\epsilon)^s)}$$<br>其中，\(p\) 表示得到正确解的概率，\(\epsilon\) 表示外点的百分比，\(s\) 表示每次模型估计取出的点数。如图 3. 所示，选出的点数越少，迭代次数就可以越少。这个角度来讲，五点法比八点法有优势，但是五点法的前提是相机都是标定过的。不过不考虑速度的话，还是选择更多的点，因为可以平滑噪声。</p><h2 id="7-ensp-Local-Optimization"><a href="#7-ensp-Local-Optimization" class="headerlink" title="7.&ensp;Local Optimization"></a>7.&ensp;Local Optimization</h2><p>　　每次估计的相机运动都有误差，随着运动的累计，误差也会累计。这就要求做局部最优化，消除轨迹的漂移。优化方式有 Pose-Graph Optimization（需要回环检测） 以及 Windowed Bundle Adjustment 两种，这里主要介绍 BA。定义误差函数：<br>$$\xi=\mathop{\arg\min}\limits_{X^i,C_k} \sum_{i,k} \left\Vert uv_{k}^i - g(X^i,C_k) \right\Vert^2$$<br>其中 \(X^i\) 为世界坐标系下特征点的 3D 坐标，\(C_k = T_{1,0}…T_{k,k-1}\)，\(g(X^i,C_k)\)为特征点投影到图像的映射函数。该非线性问题可用 Newton-Gauss 或 LM 法解决。为了加速运算，如果 3D 特征点是准确的(如立体视觉获得的)，那么可以固定特征点的 3D 量，只优化相机的轨迹。</p><p><a id="1" href="#1ref">[1]</a> Scaramuzza, Davide, and Friedrich Fraundorfer. “Visual odometry [tutorial].” IEEE robotics &amp; automation magazine 18.4 (2011): 80-92.<br><a id="2" href="#2ref">[2]</a> Fraundorfer, Friedrich, and Davide Scaramuzza. “Visual odometry: Part ii: Matching, robustness, optimization, and applications.” IEEE Robotics &amp; Automation Magazine 19.2 (2012): 78-90.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-ensp-Overview-of-VO&quot;&gt;&lt;a href=&quot;#1-ensp-Overview-of-VO&quot; class=&quot;headerlink&quot; title=&quot;1.&amp;ensp;Overview of VO&quot;&gt;&lt;/a&gt;1.&amp;ensp;Overview of VO
      
    
    </summary>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/categories/paper-reading/"/>
    
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/tags/SLAM/"/>
    
      <category term="VO" scheme="https://leijiezhang001.github.io/tags/VO/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Stereo R-CNN based 3D Object Detection for Autonomous Driving&quot;</title>
    <link href="https://leijiezhang001.github.io/%5Bpaper_reading%5D-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/"/>
    <id>https://leijiezhang001.github.io/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/</id>
    <published>2019-06-08T06:21:14.000Z</published>
    <updated>2019-06-11T01:43:51.655Z</updated>
    
    <content type="html"><![CDATA[<p>　　Learning 方法有什么致命缺点吗？我认为目前 Learning 方法还存在的较为棘手的问题是，有时候结果会出现非常低级的错误，或是说不可思议不合常理的 cornercases。所以我认为一个工程系统或是一个鲁棒的算法系统，在 Learning 之后做一个基于常理（如 geometry 约束或专家系统）的验证，能有效抑制这个问题。本文就是一个比较好的 learning+geometry 想结合的方法。<br>　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>基于图像语义及几何信息，通过 3D 目标的稀疏与密集约束，提出了一种准确的 3D 目标检测方法。根据输入数据的类型，作者将 3D 检测分为三大类：</p><ul><li>LiDAR-based，近期被研究的较多，基本是自动驾驶所必须的；</li><li>Monocular-based，低成本方案；</li><li>Stereo-based，相比 Monocular-based，有优势，但是研究较少；</li></ul><p>本文就是 Stereo-based 3D 检测方案。不同于一般的 rgb+depth 作为输入的方案，本文直接将左右目 rgb 作为输入，没有显示地 depth 生成过程。工程上来说，这也极大地缩短了 3D Detection 的时延(latency)。<br>　　本文方法如图 1 所示，主要有三部分组成：</p><ol><li>&ensp;Network，又有三部分构成：<ul><li>Stereo RPN Module，输出左右图的 RoI；</li><li>Classification and Regression branches，输出目标类别，朝向，尺寸；</li><li>Keypoint branch，输出左目目标的关键点；</li></ul></li><li>&ensp;Sparse constraints，3D 框-2D 框的稀疏约束；</li><li>&ensp;Dense constraints，准确定位的关键模块；</li></ol><p><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/net_arch.png" width="100%" height="100%" title="图 1. 网络结构"></p><h2 id="1-ensp-Stereo-R-CNN-Network"><a href="#1-ensp-Stereo-R-CNN-Network" class="headerlink" title="1.&ensp;Stereo R-CNN Network"></a>1.&ensp;Stereo R-CNN Network</h2><p>　　Stereo R-CNN 是在 Faster R-CNN 基础上，同时检测与关联左右目图像 2D 框的微小差异。</p><h3 id="1-1-ensp-Stereo-RPN"><a href="#1-1-ensp-Stereo-RPN" class="headerlink" title="1.1.&ensp;Stereo RPN"></a>1.1.&ensp;Stereo RPN</h3><p>　　在传统 RPN 网络的基础上，本文先对左右图做 paramid features 提取，然后将不同尺度的特征 concatenate 一起，进入 RPN 网络。<br><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/target.png" width="60%" height="60%" title="图 2. 真值框定义方式"><br>　　关键的一点是 objectness classification与 stereo box regression 的真值框定义不一样。如图 2 所示，</p><ul><li>对于 objectness classification，真值框定义为左右目真值框的外接合并（union GT box），一个 anchor 在与真值框的交并比（Intersection-over-Union）大于 0.7 时标记为正样本，小于 0.3 时标记为负样本。分类任务的候选框包含了左右目真值框区域的信息。</li><li>对于 stereo box regression，真值框定义为左右目分别的真值框。待回归的参数定义为 \([\Delta u, \Delta w, \Delta u’, \Delta w’, \Delta v, \Delta h]\)，分别为左目的水平位置及宽，右目的水平位置及宽，垂直位置及高。因为输入为矫正过的左右目图像，所以可认为左右目的垂直方向上已经对齐。</li></ul><p>每个左右目的 proposal 都是通过同一个 anchor 产生的，自然左右目的 proposal 是关联的。通过 NMS 后，保留左右目都还存在的 proposal 关联对，取前 2000 个用于训练，测试时取前 300 个。</p><h3 id="1-2-ensp-Stereo-R-CNN"><a href="#1-2-ensp-Stereo-R-CNN" class="headerlink" title="1.2.&ensp;Stereo R-CNN"></a>1.2.&ensp;Stereo R-CNN</h3><p><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/viewpoint.png" width="50%" height="50%" title="图 3. 各角度关系"><br>　　网络头包含两大部分：</p><ol><li>&ensp;<strong>Stereo Regression</strong><br>左右目的 proposal 关联对，分别在左右目的 feature 上进行 RoI Align 的操作，然后 concatenate 输入到全链接层。左右目的 RoI 对与真值框的 IoU 均大于 0.5 时定位正样本，左右目的 RoI 对与真值框的 IoU 有一个小于 0.5 且大于 0.1，则定位负样本。用四个分支分别预测：<ul><li>object class；</li><li>stereo bounding boxes，与 stereo rpn 中一致，左右目的高度已对齐；</li><li>dimension，先统计平均的尺寸，然后预测相对量；</li><li>viewpoint angle，如图 3 所示，\(\theta\) 为相机坐标系下的朝向角，\(\beta\) 为相机中心点下的方位角(azimuth)，这三个目标在相机视野下是一样的，所以我们回归的量是视野角(viewpoint angle) \(\alpha=\theta+\beta\)，其中 \(\beta=arctan\left(-\frac{x}{z} \right) \)。并且为了连续性，回归量为 \([sin\,\alpha,cos\,\alpha]\)。<br><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/keypoints.png" width="70%" height="70%" title="图 4. 语义关键点"></li></ul></li><li>&ensp;<strong>Keypoint Prediction</strong><br>如图 4 所示，考虑 3D 框底部矩形的四个关键点，投影到图像平面后，最多只有一个关键点会在图像 2D 矩形框内。对左目图像进行关键点预测，类似 Mask R-CNN，在 6×28×28 的基础上，因为关键点只有图像坐标 u 方向才提供了额外的信息，所以对每列进行累加，最终输出 6×28 的向量。前 4 个通道代表每个关键点作为 perspective keypoint 投影到该 u 坐标下的概率；后 2 个通道代表该 u 坐标是左右边缘关键点(boundary keypoints)的概率。为了找出 perspective keypoint，softmax 应用于 4×28 的输出上；为了找出左右边缘关键点，softmax 分别应用于后两个 1×28 的输出上。训练的时候，4×28 中只有一个被赋予 perspective keypoint，忽略没有 perspective keypoint 的情况（遮挡等），然后最小化 cross-entropy loss；对于边缘关键点，则分别最小化 1×28 维度上的 cross-entropy loss，前景中也会被赋予边缘关键点。</li></ol><h2 id="2-ensp-3D-Box-Estimation"><a href="#2-ensp-3D-Box-Estimation" class="headerlink" title="2.&ensp;3D Box Estimation"></a>2.&ensp;3D Box Estimation</h2><p><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/projection.png" width="70%" height="70%" title="图 5. 关键点投影关系"><br>　　已知关键点，2D 框，尺寸，朝向角，我们可以求解出 3D 框 \(\{x,y,z,\theta\}\)。求解目标是最小化 3D 框投影到 2D 框以及关键点的误差。如图 5 所示，已知 7 个观测量 \(z = \{u_l,v_t,u_r,v_b,u’_l,u’_r,u_p\}\)，分别代表左目 2D 框的左上坐标，右下坐标，右目 2D 框的左右 u 方向坐标，以及 perspective keypoint 的 u 方向坐标。在图 5 的情况下（其它视角下，注意符号变化），左上点投影关系如下：<br>$$\require{cancel}<br>\begin{bmatrix}<br>u_l\\<br>v_t\\<br>1\\<br>\end{bmatrix}=K\cdot<br>\begin{bmatrix}<br>x_{cam}^{tl}\\<br>y_{cam}^{tl}\\<br>z_{cam}^{tl}\\<br>\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot<br>\begin{bmatrix}<br>x_{obj}^{tl}\\<br>y_{obj}^{tl}\\<br>z_{obj}^{tl}\\<br>\end{bmatrix}=\begin{bmatrix}<br>x\\<br>y\\<br>z\\<br>\end{bmatrix}+<br>\begin{bmatrix}<br>cos\theta &amp; 0 &amp;sin\theta\\<br>0 &amp; 1 &amp; 0\\<br>-sin\theta &amp; 0 &amp; cos\theta\\<br>\end{bmatrix} \cdot<br>\begin{bmatrix}<br>-\frac{w}{2}\\<br>-\frac{h}{2}\\<br>-\frac{l}{2}\\<br>\end{bmatrix}$$<br>其中 \(K\) 为相机内参，\(T_{cam}^{obj}\) 为目标中心坐标系在相机坐标系下的表示，\((\cdot)_{cam/obj}\) 分别为点在相机坐标系，目标中心坐标系下的表示。同样的，这个视野下，右下点为：<br>$$\require{cancel}<br>\begin{bmatrix}<br>u_l\\<br>v_t\\<br>1\\<br>\end{bmatrix}=K\cdot<br>\begin{bmatrix}<br>x_{cam}^{tl}\\<br>y_{cam}^{tl}\\<br>z_{cam}^{tl}\\<br>\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot<br>\begin{bmatrix}<br>x_{obj}^{tl}\\<br>y_{obj}^{tl}\\<br>z_{obj}^{tl}\\<br>\end{bmatrix}=\begin{bmatrix}<br>x\\<br>y\\<br>z\\<br>\end{bmatrix}+<br>\begin{bmatrix}<br>cos\theta &amp; 0 &amp;sin\theta\\<br>0 &amp; 1 &amp; 0\\<br>-sin\theta &amp; 0 &amp; cos\theta\\<br>\end{bmatrix} \cdot<br>\begin{bmatrix}<br>\frac{w}{2}\\<br>\frac{h}{2}\\<br>-\frac{l}{2}\\<br>\end{bmatrix}$$<br>右目两个边缘点以及 perspective keypoint 点也可同样得到，由此可整理出 7 个方程组（论文中第一个公式符号有错）：<br>$$\left\{\begin{array}{l}<br>u_l=(x- \frac{w}{2} cos\theta- \frac{l}{2} sin\theta) / (z+ \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\<br>v_t=(y- \frac{h}{2}) / (z+ \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\<br>u_r=(x+ \frac{w}{2} cos\theta+ \frac{l}{2} sin\theta) / (z- \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)\\<br>v_b=(y+ \frac{h}{2}) / (z- \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)\\<br>u’_l=(x-b- \frac{w}{2} cos\theta- \frac{l}{2} sin\theta) / (z+ \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\<br>u’_r=(x-b+ \frac{w}{2} cos\theta+ \frac{l}{2} sin\theta) / (z- \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)\\<br>u_p=(x+ \frac{w}{2} cos\theta- \frac{l}{2} sin\theta) / (z- \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\<br>\end{array}\right.$$<br>其中 \(b\) 为双目的基线长(baseline)。以上方程组可用 Gauss-Newton 法求解。</p><h2 id="3-ensp-Dense-3D-Box-Alignment"><a href="#3-ensp-Dense-3D-Box-Alignment" class="headerlink" title="3.&ensp;Dense 3D Box Alignment"></a>3.&ensp;Dense 3D Box Alignment</h2><p>　　以上得到的目标 3D 位置是 object-level 求解得到的，利用像素信息，还可以进行优化精确求解。首先在图像 2D 目标框内扣取一块 RoI，要使 RoI 能较为确定的在目标上，扣取方式定义为：</p><ul><li>目标一半以下区域；</li><li>perspective keypoint 与边缘关键点包围区域；</li></ul><p>关键点预测的时候只预测了 u 方向的坐标，边缘关键点无 v 方向的信息，看起来会使某些背景像素被划入为目标像素，更好的方法是加入 instance segmentation 信息。定义误差函数为：<br>$$E=\sum_{i=0}^N e_i=\sum_{i=0}^N \left\| I_l(u_i,v_i)-I_r(u_i-\frac{b}{z+\Delta z_i},v_i)\right\|$$<br>可由三角测量关系 \(z=\frac{bf}{d}\) 推出。上式中，\(\Delta z_i=z_i-z\) 表示某个像素点 \(i\) 所对应的 3D 点与目标中心点之间的距离。最小化总误差即可求得最优的中心点距离 \(z\)。优化过程可以用 coarse-to-fine 的策略，先以 0.5m 的精度找 50 步，再以 0.05m 的精度找 20 次。<br>　　这个 dense alignment 模块是独立的，可以应用到任意的左右目 3D 检测的后处理中。因为目标 RoI 是物理约束，所以这个方法避免了深度估计中不连续、病态的问题，且对光照是鲁棒的，因为每个像素都会对估计起作用。这里，本文只做了中心点的 align，尺寸，甚至朝向角是否能加入优化?</p><h2 id="4-ensp-Other-Details"><a href="#4-ensp-Other-Details" class="headerlink" title="4.&ensp;Other Details"></a>4.&ensp;Other Details</h2><p><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/r1.png" width="110%" height="110%"><br><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/r2.png" width="70%" height="70%"><br><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/r3.png" width="100%" height="100%"><br><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/r4.png" width="90%" height="90%"></p><p><a id="1" href="#1ref">[1]</a> Li, Peiliang, Xiaozhi Chen, and Shaojie Shen. “Stereo R-CNN based 3D Object Detection for Autonomous Driving.” arXiv preprint arXiv:1902.09738 (2019).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Learning 方法有什么致命缺点吗？我认为目前 Learning 方法还存在的较为棘手的问题是，有时候结果会出现非常低级的错误，或是说不可思议不合常理的 cornercases。所以我认为一个工程系统或是一个鲁棒的算法系统，在 Learning 之后做一个基于常理（
      
    
    </summary>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/categories/paper-reading/"/>
    
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving&quot;</title>
    <link href="https://leijiezhang001.github.io/%5Bpaper_reading%5D-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/"/>
    <id>https://leijiezhang001.github.io/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/</id>
    <published>2019-06-08T06:21:14.000Z</published>
    <updated>2019-06-13T15:15:25.250Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>结合 Semantic SLAM 与 Learning-based 3D Det 技术，提出了一种用于自动驾驶的动态目标定位与本车状态估计的方法。本文系统性较强，集成了较多成熟的模块，对工程应用也有较强的指导意义。<br><img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/arch.png" width="100%" height="100%" title="图 1. 语义跟踪系统框架"><br>　　如图 1. 所示，整个系统框架由三部分组成：</p><ul><li>2D object detection and viewpoint classification，目标位姿通过 2D-3D 约束求解出来；</li><li>feature extraction and matching，双目及前后帧的特征提取与匹配；</li><li>ego-motion and object tracking，将语义信息及特征量加入到优化中，并且加入车辆动力学约束以获得平滑的运动估计。</li></ul><h2 id="1-ensp-Viewpoint-Classification-and-3D-Box-Inference"><a href="#1-ensp-Viewpoint-Classification-and-3D-Box-Inference" class="headerlink" title="1.&ensp;Viewpoint Classification and 3D Box Inference"></a>1.&ensp;Viewpoint Classification and 3D Box Inference</h2><h3 id="1-1-ensp-Viewpoint-Classification"><a href="#1-1-ensp-Viewpoint-Classification" class="headerlink" title="1.1.&ensp;Viewpoint Classification"></a>1.1.&ensp;Viewpoint Classification</h3><p>　　选用 Faster R-CNN 作为 2D 检测框架，在此基础上，加入车辆视野（viewpoint）分类分支。由图 2. 所示，水平视野分为八类，垂直视野分为两类，总共 16 类。<br><img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/view.png" width="90%" height="90%" title="图 2. 车辆视野分类"></p><h3 id="1-2-ensp-3D-Box-Inference-Based-on-Viewpoint"><a href="#1-2-ensp-3D-Box-Inference-Based-on-Viewpoint" class="headerlink" title="1.2.&ensp;3D Box Inference Based on Viewpoint"></a>1.2.&ensp;3D Box Inference Based on Viewpoint</h3><p>　　网络输出图像 2D 框以及目标车辆的视野类别（viewpoint），此时我们假设：</p><ul><li>2D 框准确；</li><li>每种车辆的尺寸相同；</li><li>2D 框能紧密包围 3D 框；</li></ul><p>在以上假设条件下，我们可以求得 3D 框，该 3D 框作为后续优化的初始值。约束方程的表示在论文中比较晦涩，在这里我做细致的推倒。 3D 框可表示为 \(\{x,y,z,\theta,w,h,l\}\)，其中 \(\{w,h,l\}\) 分别对应 \(\{x,y,z\}\) 维度。如图 2.(b) 所示，这个视角下，四个 3D 框的顶点，可得四个约束方程。推倒过程为：<br>$$\require{cancel}<br>\begin{bmatrix}<br>u_{min}\\<br>v_1\\<br>1\\<br>\end{bmatrix}=K\cdot<br>\begin{bmatrix}<br>x_{1}^{cam}\\<br>y_{1}^{cam}\\<br>z_{1}^{cam}\\<br>\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot<br>\begin{bmatrix}<br>x_{1}^{obj}\\<br>y_{1}^{obj}\\<br>z_{1}^{obj}\\<br>\end{bmatrix}=\begin{bmatrix}<br>x\\<br>y\\<br>z\\<br>\end{bmatrix}+<br>\begin{bmatrix}<br>cos\theta &amp; 0 &amp;sin\theta\\<br>0 &amp; 1 &amp; 0\\<br>-sin\theta &amp; 0 &amp; cos\theta\\<br>\end{bmatrix} \cdot<br>\begin{bmatrix}<br>\frac{w}{2}\\<br>\frac{h}{2}\\<br>\frac{l}{2}\\<br>\end{bmatrix}$$<br>其中 \(K\) 为相机内参，做归一化处理消去；\(T_{cam}^{obj}\) 为目标中心坐标系在相机坐标系下的表示，\((\cdot)^{cam/obj}\) 分别为点在相机坐标系，目标中心坐标系下的表示。同样的，这个视野下，②，③，④ 点都可以由此获得：<br>$$\left\{\begin{array}{l}<br>\require{cancel}<br>\begin{bmatrix}<br>u_{min}\\<br>v_1\\<br>1\\<br>\end{bmatrix}=K\cdot<br>\begin{bmatrix}<br>x_{1}^{cam}\\<br>y_{1}^{cam}\\<br>z_{1}^{cam}\\<br>\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot<br>\begin{bmatrix}<br>x_{1}^{obj}\\<br>y_{1}^{obj}\\<br>z_{1}^{obj}\\<br>\end{bmatrix}=\begin{bmatrix}<br>x\\<br>y\\<br>z\\<br>\end{bmatrix}+<br>\begin{bmatrix}<br>cos\theta &amp; 0 &amp;sin\theta\\<br>0 &amp; 1 &amp; 0\\<br>-sin\theta &amp; 0 &amp; cos\theta\\<br>\end{bmatrix} \cdot<br>\begin{bmatrix}<br>\frac{w}{2}\\<br>\frac{h}{2}\\<br>\frac{l}{2}\\<br>\end{bmatrix}\\<br>\begin{bmatrix}<br>u_{max}\\<br>v_2\\<br>1\\<br>\end{bmatrix}=K\cdot<br>\begin{bmatrix}<br>x_{2}^{cam}\\<br>y_{2}^{cam}\\<br>z_{2}^{cam}\\<br>\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot<br>\begin{bmatrix}<br>x_{2}^{obj}\\<br>y_{2}^{obj}\\<br>z_{2}^{obj}\\<br>\end{bmatrix}=\begin{bmatrix}<br>x\\<br>y\\<br>z\\<br>\end{bmatrix}+<br>\begin{bmatrix}<br>cos\theta &amp; 0 &amp;sin\theta\\<br>0 &amp; 1 &amp; 0\\<br>-sin\theta &amp; 0 &amp; cos\theta\\<br>\end{bmatrix} \cdot<br>\begin{bmatrix}<br>-\frac{w}{2}\\<br>\frac{h}{2}\\<br>-\frac{l}{2}\\<br>\end{bmatrix}\\<br>\begin{bmatrix}<br>u_3\\<br>v_{min}\\<br>1\\<br>\end{bmatrix}=K\cdot<br>\begin{bmatrix}<br>x_{3}^{cam}\\<br>y_{3}^{cam}\\<br>z_{3}^{cam}\\<br>\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot<br>\begin{bmatrix}<br>x_{3}^{obj}\\<br>y_{3}^{obj}\\<br>z_{3}^{obj}\\<br>\end{bmatrix}=\begin{bmatrix}<br>x\\<br>y\\<br>z\\<br>\end{bmatrix}+<br>\begin{bmatrix}<br>cos\theta &amp; 0 &amp;sin\theta\\<br>0 &amp; 1 &amp; 0\\<br>-sin\theta &amp; 0 &amp; cos\theta\\<br>\end{bmatrix} \cdot<br>\begin{bmatrix}<br>\frac{w}{2}\\<br>-\frac{h}{2}\\<br>-\frac{l}{2}\\<br>\end{bmatrix}\\<br>\begin{bmatrix}<br>u_4\\<br>v_{max}\\<br>1\\<br>\end{bmatrix}=K\cdot<br>\begin{bmatrix}<br>x_{4}^{cam}\\<br>y_{4}^{cam}\\<br>z_{4}^{cam}\\<br>\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot<br>\begin{bmatrix}<br>x_{4}^{obj}\\<br>y_{4}^{obj}\\<br>z_{4}^{obj}\\<br>\end{bmatrix}=\begin{bmatrix}<br>x\\<br>y\\<br>z\\<br>\end{bmatrix}+<br>\begin{bmatrix}<br>cos\theta &amp; 0 &amp;sin\theta\\<br>0 &amp; 1 &amp; 0\\<br>-sin\theta &amp; 0 &amp; cos\theta\\<br>\end{bmatrix} \cdot<br>\begin{bmatrix}<br>-\frac{w}{2}\\<br>\frac{h}{2}\\<br>\frac{l}{2}\\<br>\end{bmatrix}<br>\end{array}\right.$$</p><p>将 \(z\) 方向归一化后，进一步得到最终的四个约束式子：<br>$$\left\{\begin{array}{l}<br>u_{min}=(x+ \frac{w}{2} cos\theta+ \frac{l}{2} sin\theta) / (z- \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)\\<br>u_{max}=(x- \frac{w}{2} cos\theta- \frac{l}{2} sin\theta) / (z+ \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\<br>v_{min}=(y- \frac{h}{2}) / (z- \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\<br>v_{max}=(y+ \frac{h}{2}) / (z+ \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)<br>\end{array}\right.$$<br>以上四个方程可以闭式求解 3D 框 \(\{x,y,z,\theta\}\)。该方法将 3D 框的回归求解分解成了 2D 框回归，视野角分类以及解方程组的过程，强依赖于前面的三点假设，实际情况 3D 框与 2D 框不会贴的很紧。这个 3D 框结果只用来作后续的特征提取区域及最大后验概率估计的初始化。</p><h2 id="2-ensp-Feature-Extraction-and-Matching"><a href="#2-ensp-Feature-Extraction-and-Matching" class="headerlink" title="2.&ensp;Feature Extraction and Matching"></a>2.&ensp;Feature Extraction and Matching</h2><p>　　这一部分做的是左右目及前后帧特征提取及匹配。选用 ORB 特征，目标区域由投影到图像的 3D 框确定。</p><ul><li><strong>目标区域内左右目的立体匹配</strong><br>由于已知目标的距离及尺寸，所以只需要在一定小范围内进行特征点的行搜索匹配。</li><li><strong>目标及背景区域下前后帧的时序匹配</strong><br>首先进行 2D 框的关联，2D 框经过相机旋转补偿后，最小化关联框的中心点距离及框形状相似度值。然后在关联上的目标框区域以及背景区域里，分别作 ORB 特征的匹配，异常值在 RANSAC 下通过基础矩阵测试去除。</li></ul><h2 id="3-ensp-Ego-motion-and-Object-Tracking"><a href="#3-ensp-Ego-motion-and-Object-Tracking" class="headerlink" title="3.&ensp;Ego-motion and Object Tracking"></a>3.&ensp;Ego-motion and Object Tracking</h2><p>　　首先进行本车运动状态估计，可在传统 SLAM 框架下做，不同的是将动态障碍物中的特征点去除。有了本车的位姿后，再估计动态障碍物的运动状态。文中符号定义较为复杂，这里不做赘述。</p><h3 id="3-1-ensp-Ego-motion-Tracking"><a href="#3-1-ensp-Ego-motion-Tracking" class="headerlink" title="3.1.&ensp;Ego-motion Tracking"></a>3.1.&ensp;Ego-motion Tracking</h3><p>　　给定左目前后帧背景区域特征点的观测，本车状态估计可以通过极大似然估计（Maximum Likelihood Estimation）得到。MLE 可以转化为非线性最小二乘问题，也就是 Bundle Adjustment 过程，这是典型的 SLAM 问题。文中给出的误差方程：<br><img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/3.png" width="50%" height="50%"><br>需要求解的是本车位姿以及背景特征点坐标，这是后验概率，可转为似然函数求解，然后转化为非线性优化问题。可参考《视觉 SLAM 十四讲》(107-108)来理解。</p><h3 id="3-2-ensp-Semantic-Object-Tracking"><a href="#3-2-ensp-Semantic-Object-Tracking" class="headerlink" title="3.2.&ensp;Semantic Object Tracking"></a>3.2.&ensp;Semantic Object Tracking</h3><p>　　得到本车相机的位姿后，运动目标的状态估计可以通过最大后验概率估计（Maximum-a-posterior, MAP）得到。类似的，可转为非线性优化问题进行求解，联合优化每个车辆的<strong>位姿</strong>，<strong>尺寸</strong>，<strong>速度</strong>，<strong>方向盘转角</strong>，<strong>所有特征点 3D 位置</strong>。有四个 loss 项：<br><img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/6.png" width="80%" height="80%"><br><img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/10.png" width="80%" height="80%"><br>\(r_Z,r_P,r_M,r_S\) 分别代表：</p><ul><li><strong>Sparse Feature Observation</strong><br>目标上的特征点重投影到左右目图像的误差，注意有左右目两个误差项；</li><li><strong>Semantic 3D Object Measurement</strong><br>3D 框投影到图像上与 2D 框的尺寸约束投影误差，即 1.2 节中的形式，区别在车辆尺寸与位姿作为了优化项；</li><li><strong>Vehicle Motion Model</strong><br>对于车辆，前后时刻的状态要有连续性，即误差最小；</li><li><strong>Point Cloud Alignment</strong><br>为了减少 3D 框的整体偏移，引入特征点到 3D 观察面的最小距离误差；</li></ul><p>这里只对车辆运动模型进行分析，其它几项基本在前文已经有描述或者比较常识化，就不展开，具体公式可参见论文。<br>　　由实验可知 Sparse Feature Observation 与 Point Cloud Alignment 对性能提升较明显，Motion Model 对困难情景性能才有提升。</p><h4 id="3-2-1-ensp-Vehicle-Motion-Model"><a href="#3-2-1-ensp-Vehicle-Motion-Model" class="headerlink" title="3.2.1.&ensp;Vehicle Motion Model"></a>3.2.1.&ensp;Vehicle Motion Model</h4><p>　　<a href="#2" id="2ref">[2]</a> 中介绍了前转向车的两种模型：运动学模型(Kinematic Bicycle Model)，以及更复杂的动力学模型(Dynamic Bicycle Model)。运动学模型假设车辆不存在滑动，这在大多数情况下都是满足的，所以我们只介绍运动学模型。<br><img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/kinematic.png" width="30%" height="30%" title="图 3. 车辆运动学模型"><br>　　如图 3. 所示，前后轮无滑动的约束下，可得方程组：<br>$$\left\{\begin{array}{rl}<br>\dot{x}_fsin(\theta+\delta)-\dot{y}_fcos(\theta+\delta)=&amp;0\\<br>\dot{x}sin(\theta)-\dot{y}cos(\theta)=&amp;0\\<br>x+Lcos(\theta)=&amp;x_f  \quad\Rightarrow \quad \dot{x}-\dot{\theta}Lsin(\theta)=\dot{x}_f\\<br>y+Lsin(\theta)=&amp;y_f \quad\Rightarrow \quad \dot{y}+\dot{\theta}Lcos(\theta)=\dot{y}_f<br>\end{array}\right.$$<br>由此可得到:<br>$$\dot{x}sin(\theta+\delta)-\dot{y}cos(\theta+\delta)-\dot{\theta}Lcos(\delta)=0$$<br>用 \(\left(v \cdot cos(\theta),v\cdot sin(\theta)\right)\) 代替 \((\dot{x},\dot{y})\) 可得：<br>$$\dot{\theta}=\frac{tan(\delta)}{L}\cdot v$$<br>最终可整理成矩阵形式：<br>$$<br>\begin{bmatrix}<br>\dot{x}\\<br>\dot{y}\\<br>\dot{\theta}\\<br>\dot{\delta}\\<br>\dot{v}\\<br>\end{bmatrix}=<br>\begin{bmatrix}<br>0 &amp;0 &amp;0 &amp;0 &amp;cos(\theta)\\<br>0 &amp;0 &amp;0 &amp;0 &amp;sin(\theta)\\<br>0 &amp;0 &amp;0 &amp;0 &amp;\frac{tan(\delta)}{L}\\<br>0 &amp;0 &amp;0 &amp;0 &amp;0\\<br>0 &amp;0 &amp;0 &amp;0 &amp;0\\<br>\end{bmatrix}<br>\begin{bmatrix}<br>x\\<br>y\\<br>\theta\\<br>\delta\\<br>v\\<br>\end{bmatrix}+<br>\begin{bmatrix}<br>0 &amp;0\\<br>0 &amp;0\\<br>0 &amp;0\\<br>1 &amp;0\\<br>0 &amp;1\\<br>\end{bmatrix}<br>\begin{bmatrix}<br>\gamma\\<br>\alpha\\<br>\end{bmatrix}<br>$$<br>其中 \(L\) 为车辆参数。观测量有：</p><ul><li>\((x,y,\theta)\) 为车辆的位置及朝向角；</li><li>\(\delta\) 为方向盘/车轮转角；</li><li>\(v\) 为车辆速度；</li></ul><p>控制量有：</p><ul><li>\(\gamma\) 为方向盘角度比率；</li><li>\(\alpha\) 为加速度；</li></ul><p>本文的目的是要约束车辆时序上运动(速度及朝向)的平滑一致性，令控制量 \(\gamma,\alpha\) 为 0，然后可得状态量在相邻时刻的关系应满足：<br>$$\left\{\begin{array}{l}<br>\hat{x}^t=x^{t-1}+cos(\theta^{t-1})v^{t-1}\Delta t\\<br>\hat{y}^t=y^{t-1}+sin(\theta^{t-1})v^{t-1}\Delta t\\<br>\hat{\theta}^t=\theta^{t-1}+\frac{tan(\delta^{t-1})}{L}v^{t-1}\Delta t\\<br>\hat{\delta}^t=\delta^{t-1}\\<br>\hat{v}^t=v^{t-1}<br>\end{array}\right.$$<br>由此可整理成论文中矩阵的形式及误差项：<br><img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/15.png" width="80%" height="80%"></p><p><a id="1" href="#1ref">[1]</a> Li, Peiliang, and Tong Qin. “Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving.” Proceedings of the European Conference on Computer Vision (ECCV). 2018.<br><a id="2" href="#2ref">[2]</a> Gu, Tianyu. Improved trajectory planning for on-road self-driving vehicles via combined graph search, optimization &amp; topology analysis. Diss. Carnegie Mellon University, 2017.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　本文&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt;结合 Semantic SLAM 与 Learning-based 3D Det 技术，提出了一种用于自动驾驶的动态目标定位与本车状态估计的方法。本文系统性较强，集成了较多成熟的模块，
      
    
    </summary>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/categories/paper-reading/"/>
    
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>MOT Metrics in Academia and Industry</title>
    <link href="https://leijiezhang001.github.io/MOT-Metrics-in-Academia-and-Industry/"/>
    <id>https://leijiezhang001.github.io/MOT-Metrics-in-Academia-and-Industry/</id>
    <published>2019-06-03T05:47:00.000Z</published>
    <updated>2019-06-10T13:53:32.627Z</updated>
    
    <content type="html"><![CDATA[<p>　　MOT 是一个比较基本的技术模块，在视频监控中，常用于行人行为分析、姿态估计等任务的前序模块；在自动驾驶中，MOT 是动态目标状态估计的重要环节。在学术界，MOT 算法性能的评价准则已经较为完善，其指标主要关注，尽可能地覆盖所有性能维度，以及指标的简洁性（上一篇有较多介绍，<a href="https://leijiezhang001.github.io/MOT-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87-Evaluating-Multiple-Object-Tracking-Performance-the-CLEAR-MOT-Metrics/#more">the CLEAR MOT Metrics</a>）。而工业界则尚无统一的标准，实际的指标需求情况也比学术界复杂。<br>　　指标的计算过程可由三部分组成，真值过滤(Filter)，匹配构建(Establishing Correspondences)与指标计算(Calculating Metrics)。其中真值过滤，更多的是工程细节，学术界没有文章对这一部分进行讨论研究。本文首先介绍学术界各评价指标详情，然后讨论工业界需要的评价指标又是怎样的。</p><h2 id="1-ensp-Metrics-in-Academia"><a href="#1-ensp-Metrics-in-Academia" class="headerlink" title="1.&ensp;Metrics in Academia"></a>1.&ensp;Metrics in Academia</h2><p>　　在学术界，因为数据集质量较高，噪声相对较小，匹配构建中距离的度量偏向于严格且简单的方式。对于区域(框)跟踪器，采用重叠区域来度量；对于点跟踪器，采用中心点的欧式距离来度量。指标汇总如下：<br>A.&ensp;<strong>检测指标</strong><br>　\(\lozenge\)　准确性(Accuracy)</p><ul><li><strong>Recall</strong> = \(\frac{TP}{GT}\)；</li><li><strong>Precision</strong> = \(\frac{TP}{TP+FP}\)；</li><li><strong>FAF/FPPI</strong><a href="#1" id="1ref"><sup>[1]</sup></a><a href="#2" id="2ref"><sup>[2]</sup></a> ，Average False Alarms per Frame；False Positive Per Image;</li><li><strong>MODA</strong><a href="#3" id="3ref"><sup>[3]</sup></a>，Multipe Object Detection Precision，整合了 FN 与 FP，设 \(c_m, c_f\) 分别为 FN，FP 的权重：<br>$$MODA=1-\frac{\sum_{t=1}^{N_frames}(c_m(fn_t)+c_f(fp_t))}{\sum_{t=1}^{N_frames}gt_t}$$</li></ul><p>　\(\lozenge\)　精确性(Precision)</p><ul><li><strong>MODP</strong><a href="#3" id="3ref"><sup>[3]</sup></a>，Multiple Object Detection Accuracy，<br>$$MODP=\frac{\sum_{t=1}^{N_frames} \sum_{i=1}^{N_{mapped}^{(t)}} \;\; dist}{\sum_{t=1}^{N_frames} N_{mapped}^{(t)}}$$<br>其中 \(N_{mapped}^{(t)}\) 为第 \(t\) 帧匹配的目标数；\(dist\) 为距离度量方法，如框的交并比度量法：<br>$$Mapped Overlap Ratio = \frac{\lvert G_i^{(t)}\bigcap D_i^{(t)}\rvert}{|G_i^{(t)}\bigcup D_i^{(t)}|}$$</li></ul><p>B.&ensp;<strong>跟踪指标</strong><br>　\(\lozenge\)　准确性(Accuracy)</p><ul><li><strong>IDS</strong><a href="#4" id="4ref"><sup>[4]</sup></a>，ID switch，a tracked target changes its ID with another target(预测关联真值)；</li><li><strong>MOTA</strong><a href="#5" id="5ref"><sup>[5]</sup></a>，Multiple Object Tracking Accuracy，整合了 FN，FP，ID-Switch：<br>$$MOTA=1-\frac{\sum_{t=1}^{N_{frames}} \;\; (c_m(fn_t)+c_f(fp_t)+c_s(ID-SWITCHES_t))}{\sum_{t=1}^{N_{frames}} \;\; gt_t}$$<br>其中权重方程一般可设为：\(c_m=c_f=1, \quad c_s=log_{10}\)；</li></ul><p>　\(\lozenge\)　精确性(Precision)</p><ul><li><strong>MOTP</strong><a href="#5" id="5ref"><sup>[5]</sup></a>，Multiple Object Tracking Precision，<br>$$MODP=\frac{\sum_{t=1}^{N_frames} \sum_{i=1}^{N_{mapped}^{(t)}} \;\; \left(\frac{\lvert G_i^{(t)}\bigcap D_i^{(t)}\rvert}{|G_i^{(t)}\bigcup D_i^{(t)}|} \right)}{\sum_{t=1}^{N_frames} N_{mapped}^{(t)}}$$</li><li><strong>TDE</strong><a href="#6" id="6ref"><sup>[6]</sup></a>，Distance between the ground-truth annotation and the tracking result；像素级别的误差计算，适用于人群跟踪；</li><li><strong>OSPA</strong><a href="#7" id="7ref"><sup>[7]</sup></a><a href="#8" id="8ref"><sup>[8]</sup></a>，Optimal Subpattern assignment，由定位 (localization) 误差及基数 (cardinality) 误差构成，对于第 \(t\) 帧：<br>$$e^t=\left[\frac{1}{n^t}\left( \mathop{\min}_{\pi\in\Pi_n} \sum_{i=1}^{m^t} d^{(c)}(x_i^t,y_{\pi(i)}^t)^p + (n^t-m^t)\cdot c^p \right) \right]^{1/p}$$<br>其中，\(n^t\) 为目标真值与算法输出中数量较大者。\(\Pi_n\) 为从 \(n^t\) 中取出的 \(m\) 个目标。\(p\) 为距离指数范数。其中定位截断误差为：<br>$$d^{(c)}(x_i^t,y_{\pi(i)}^t) = \mathop{\min}\left(c,d(x_i^t,y_{\pi(i)}^t)\right)$$<br>\(c\) 为截断参数。定位误差又由距离误差和标签误差组成：<br>$$d(x_i^t,y_{\pi(i)}^t=\parallel x_i^t-y_{\pi(i)}^t\parallel + \alpha \; \bar{\delta}(l_x, l_y)$$<br>其中 \(\alpha\in[0,c]\)，为标签误差的权重系数。如果 \(l_x=l_y\)，\(\bar{\delta}(l_x, l_y)=0\)，否则 \(\bar{\delta}(l_x, l_y)=1\).</li></ul><p>　\(\lozenge\)　完整性(Completeness)</p><ul><li><strong>MT</strong><a href="#9" id="9ref"><sup>[9]</sup></a>，Mostly Tracked，真值轨迹长度被跟踪大于80%的比例；</li><li><strong>ML</strong><a href="#9" id="9ref"><sup>[9]</sup></a>，Mostly Lost，真值轨迹长度被跟踪小于20%的比例；</li><li><strong>PT</strong><a href="#9" id="9ref"><sup>[9]</sup></a>，Partially Tracked，\(1-MT-ML\);</li><li><strong>FM</strong><a href="#9" id="9ref"><sup>[9]</sup></a>，Fragments，ID of a target changed along a GT trajectory, or no ID(真值关联预测)；</li></ul><p>　\(\lozenge\)　鲁棒性(Robustne)</p><ul><li><strong>RS</strong><a href="#10" id="10ref"><sup>[10]</sup></a>，Recover from short term occlusion;</li><li><strong>RL</strong><a href="#10" id="10ref"><sup>[10]</sup></a>，Recover from long term occlusion;</li></ul><h2 id="2-ensp-Metrics-in-Industry"><a href="#2-ensp-Metrics-in-Industry" class="headerlink" title="2.&ensp;Metrics in Industry"></a>2.&ensp;Metrics in Industry</h2><p>　　工业界的数据噪声较大，传感器配置也比较多样，不同的产品（传感器+算法），对 MOT 性能维度要求也不一样。更重要的是，评价指标应该从功能层面进行定义，在模块层面 (MOT) 进行调整及细化。可以说，工业界是以学术界为基础来设计 MOT 指标的，不同的产品没有统一的标准，但有比较通用的设计准则。<br>　　这里以自动驾驶/辅助驾驶中动态目标状态估计模块为例，模块详细分析<a href>日后再写</a>。该模块的基本输入为：</p><ul><li><strong>传感器数据</strong>，可以是图像，激光等；</li><li><strong><em>自定位系统</em></strong>，可以是基于视觉的 VO，基于视觉-IMU 的 VINS等；<br>其中自定位系统能使目标状态估计在世界坐标系（惯性系）下优化，否则只能在本体（ego）非惯性系下优化，会减少一些约束量。该功能的基本输出为：</li><li><strong>位置</strong>，本体坐标系下目标的三维位置，\(x,y,z\)；</li><li><strong>尺寸</strong>，目标的物理尺寸大小，包括立方体的长宽高；或者图像坐标系下的像素大小；或者图像/点云下目标的 mask，即分割后的目标；</li><li><strong><em>朝向</em></strong>，一般只考虑目标的航向角；</li><li><strong>速度</strong>，本体坐标系或世界坐标系下的三维速度，一般只考虑航向平面的速度；  </li></ul><p>其中朝向是非必须项，有了朝向后，能更有效地进行状态优化。该模块的子模块有（注意，MOT 只包含前三者）：</p><ul><li><strong>检测(Detection)</strong>，进行多目标检测；</li><li><strong>跟踪(Tracking)</strong>，根据上一帧结果，进行多目标跟踪；</li><li><strong>数据关联(Association)</strong>，检测结果与跟踪结果的融合，出目标的 tracklets，生成 ID；</li><li><strong>状态估计(State Estimation)</strong>，不同的方法包括不同的部分；  </li></ul><p>　　工业界设计产品时，基本遵循自顶向下的策略：产品需求-功能需求-模块需求，层层推倒。所以我们设计评价准则时，一般会问几个问题：</p><ul><li>该模块服务的产品功能，其需求及对应的指标是什么？</li><li>要达到功能指标，本模块的输出需要哪些指标来评测？</li><li>各个子模块对模块的影响是怎样的，对应需要增加哪些指标？  </li></ul><p>这里提到了功能指标，模块指标，子模块指标三层概念。功能指标及部分模块指标是可以写入产品手册的，所以需要突出重点，易于理解；部分模块及子模块指标则主要是为了产品上工程优化迭代，这就要求这部分指标要相当细致，将模块的不足尽可能解耦，且完全暴露出来。以下通过两个例子来分析设计过程。</p><h3 id="2-1-ensp-ADAS-中的-FCW-功能"><a href="#2-1-ensp-ADAS-中的-FCW-功能" class="headerlink" title="2.1.&ensp;ADAS 中的 FCW 功能"></a>2.1.&ensp;ADAS 中的 FCW 功能</h3><p>　　FCW 基本功能要求为：</p><ul><li>不允许误报，尽可能不漏报；</li><li>在 V km/h 下，以一定的刹车加速度 a，能避免与静止的前车相碰撞；  </li></ul><p>　　由以上两个功能需求，可确定必须的功能指标：</p><ul><li>（百公里）误报率；</li><li>（百公里）漏报率；</li><li>观测距离，可由第二项功能要求推到出（人反应时间已知）；  </li></ul><p>　　相应的 MOT +状态估计模块输出的指标为<strong>各距离维度各类别维度</strong>下的：</p><ul><li>误检率；</li><li>漏检率；</li><li>ID Switch；</li><li>定位精度；</li><li>速度估计精度；  </li></ul><p>　　其中 MOT 主要涉及误检率，漏检率，ID Switch（直接影响状态估计模块）。这些指标的计算方式可以在学术界定义的基础上做进一步改进，比如漏检率，就需要体现出百公里漏报率的性能，所以可以考虑将连续 N 帧漏检的目标才归为漏检，分母可以定义为每多少帧。此外，要在各距离维度各类别维度下进行计算，这就涉及到过滤（filter）策略。对于 FCW 而言，首要关注的是本车前方近距离位置，距离维度上的功能重要程度要突显出来，类别维度也要区别对待，以便算法模块可以重点优化。</p><h3 id="2-2-ensp-自动驾驶中的动态障碍物检测功能"><a href="#2-2-ensp-自动驾驶中的动态障碍物检测功能" class="headerlink" title="2.2.&ensp;自动驾驶中的动态障碍物检测功能"></a>2.2.&ensp;自动驾驶中的动态障碍物检测功能</h3><p>　　自动驾驶中动态障碍物检测的要求就高了，子模块也较为复杂，指标除了评估功能模块的性能，还需要指导迭代各子模块算法，包括本子模块的迭代比较，以及上下游模块相关指标的对比。<br>　　功能需求，我们简单列举几项：</p><ul><li>不允许漏检，尽可能不误检；</li><li>前向，后向，侧向观测距离分别要达到 x, y, z；  </li></ul><p>　　相应的功能指标为：</p><ul><li>漏检率；</li><li>误检率；</li><li>观测距离；</li><li>观测精度；</li><li>观测时延(delay)；  </li></ul><p>　　MOT +状态估计模块输出的指标依然在<strong>各距离维度各类别维度</strong>下：</p><ul><li>误检率；</li><li>漏检率；</li><li>ID Switch；</li><li>定位精度；</li><li>尺寸，朝向，速度估计精度；</li><li>状态估计收敛时间；</li><li>一系列描述时序稳定性的指标；  </li></ul><p>　　与前述 FCW 功能类似，只是多了较多的指标。过滤操作也做的更加细致，我们还可以将目标做重要性等级划分，比如本车道前车多少米内，那指标基本都要达到 99%+；还可以将地面区域做重要性划分（比距离维度更加细致，可以认为是三维层面），周围几米内，那误检率肯定要非常低。除了过滤策略需要仔细设计外，匹配策略也需要进一步思考。如果传感器本身精度就有限，那么匹配策略就要相应放宽。还需注意的是引入过滤策略后，FP与FN计算的细微差别，比如有个过滤条件为去除目标像素面积小于一定阈值的目标集 A，观测值与真值匹配时，如果与 A 中的目标匹配上，那么不应该记为 FP，如果没匹配上 A 中的目标，那么 A 中地目标也不应该被记为 FN。这种类似的情况逻辑要思考清楚。</p><h2 id="3-ensp-Summary"><a href="#3-ensp-Summary" class="headerlink" title="3.&ensp;Summary"></a>3.&ensp;Summary</h2><p>　　以上设计的出发点是，我们要承认<strong>算法的不完美性</strong>以及<strong>传感器的局限性</strong>，在工程领域，一定要首先解决主要矛盾，再打磨细节。本文还对以下内容未作进一步分析（以后有机会再写文细究）：</p><ul><li>状态估计时序相关指标，描述估计的时序稳定性，也可以用于 MOT 的评估；</li><li>标注与过滤策略的关系，过滤策略往往依赖于标注策略；</li><li>各个指标的阈值确定，确定阈值也是产品中一件重要而又系统的事，有时候比指标设计更复杂；　　</li></ul><p><a id="1" href="#1ref">[1]</a> Yang B, Huang C, Nevatia R. <a href="https://scholar.google.com/scholar?lookup=0&amp;q=Learning+affinities+and+dependencies+for+multi-target+tracking+using+a+CRF+model&amp;hl=zh-CN&amp;as_sdt=0,5&amp;as_vis=1" target="_blank" rel="noopener">Learning affinities and dependencies for multi-target tracking using a CRF model</a>[C]//CVPR 2011. IEEE, 2011: 1233-1240.<br><a id="2" href="#2ref">[2]</a> Choi W, Savarese S. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=Multiple+target+tracking+in+world+coordinate+with+single%2C+minimally+calibrated+camera&amp;btnG=" target="_blank" rel="noopener">Multiple target tracking in world coordinate with single, minimally calibrated camera</a>[C]//European Conference on Computer Vision. Springer, Berlin, Heidelberg, 2010: 553-567.<br><a id="3" href="#3ref">[3]</a> Kasturi, Rangachar, et al. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=Framework+for+performance+evaluation+of+face%2C+text%2C+and+vehicle+detection+and+tracking+in+video%3A+Data%2C+metrics%2C+and+protocol&amp;btnG=" target="_blank" rel="noopener">Framework for performance evaluation of face, text, and vehicle detection and tracking in video: Data, metrics, and protocol</a> IEEE transactions on Pattern Analysis and Machine intelligence 31.2 (2008): 319-336.<br><a id="4" href="#4ref">[4]</a> Yamaguchi K, Berg A C, Ortiz L E, et al. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=who+are+you+with+and+where+are+you+going&amp;btnG=" target="_blank" rel="noopener">Who are you with and where are you going?</a>[C]//CVPR 2011. IEEE, 2011: 1345-1352.<br><a id="5" href="#5ref">[5]</a> Bernardin K, Stiefelhagen R. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=evaluating+multiple+object+tracking+performance+the+clear+mot+metrics&amp;btnG=" target="_blank" rel="noopener">Evaluating multiple object tracking performance: the CLEAR MOT metrics</a>[J]. Journal on Image and Video Processing, 2008, 2008: 1.<br><a id="6" href="#6ref">[6]</a> Kratz L, Nishino K. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=%E2%80%9CTracking+with+local+spatio-temporal+motion+patterns+in+extremely+crowded+scenes&amp;btnG=" target="_blank" rel="noopener">Tracking with local spatio-temporal motion patterns in extremely crowded scenes</a>[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE, 2010: 693-700.<br><a id="7" href="#7ref">[7]</a> Ristic B, Vo B N, Clark D, et al. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=a+metric+for+performance+evaluation+of+multi-target+tracking+algorithms&amp;btnG=" target="_blank" rel="noopener">A metric for performance evaluation of multi-target tracking algorithms</a>[J]. IEEE Transactions on Signal Processing, 2011, 59(7): 3452-3457.<br><a id="8" href="#8ref">[8]</a> Schuhmacher D, Vo B T, Vo B N. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=A+Consistent+Metric+for+Performance+Evaluation+of+Multi-Object+Filters&amp;btnG=" target="_blank" rel="noopener">A consistent metric for performance evaluation of multi-object filters</a>[J]. IEEE transactions on signal processing, 2008, 56(8): 3447-3457.<br><a id="9" href="#9ref">[9]</a> Li Y, Huang C, Nevatia R. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=Learning+to+associate%3A+Hybridboosted+multi-target+tracker+for+crowded+scene&amp;btnG=" target="_blank" rel="noopener">Learning to associate: Hybridboosted multi-target tracker for crowded scene</a>[C]//2009 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2009: 2953-2960.<br><a id="10" href="#10ref">[10]</a> Song B, Jeng T Y, Staudt E, et al. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=A+stochastic+graph+evolution+framework+for+robust+multi-target+tracking&amp;btnG=" target="_blank" rel="noopener">A stochastic graph evolution framework for robust multi-target tracking</a>[C]//European Conference on Computer Vision. Springer, Berlin, Heidelberg, 2010: 605-619.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　MOT 是一个比较基本的技术模块，在视频监控中，常用于行人行为分析、姿态估计等任务的前序模块；在自动驾驶中，MOT 是动态目标状态估计的重要环节。在学术界，MOT 算法性能的评价准则已经较为完善，其指标主要关注，尽可能地覆盖所有性能维度，以及指标的简洁性（上一篇有较多介
      
    
    </summary>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/categories/MOT/"/>
    
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/tags/MOT/"/>
    
      <category term="tracking" scheme="https://leijiezhang001.github.io/tags/tracking/"/>
    
  </entry>
  
  <entry>
    <title>MOT 评价指标-&quot;Evaluating Multiple Object Tracking Performance, the CLEAR MOT Metrics&quot;</title>
    <link href="https://leijiezhang001.github.io/MOT-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87-Evaluating-Multiple-Object-Tracking-Performance-the-CLEAR-MOT-Metrics/"/>
    <id>https://leijiezhang001.github.io/MOT-评价指标-Evaluating-Multiple-Object-Tracking-Performance-the-CLEAR-MOT-Metrics/</id>
    <published>2019-06-02T06:23:46.000Z</published>
    <updated>2019-06-05T14:32:36.448Z</updated>
    
    <content type="html"><![CDATA[<p>　　这篇文章介绍了两个综合性指标 MOTA 以及 MOTP 的计算过程，这两个指标有优劣势，但是作为综合性指标至今在学术界仍广泛应用。本文主要介绍其设计思想及计算过程。<br>　　一个理想的 MOT 算法，我们期望每一帧：</p><ul><li>准确检测目标的数量；</li><li>准确估计每个目标的状态，如位置，朝向，速度等；</li><li>准确估计每个目标的轨迹，即目标的 ID 不变性；  </li></ul><p>这就要求评价准则：</p><ul><li>能评估目标定位的精度；</li><li>能反映目标轨迹的追踪能力，即同一个目标产生唯一的 ID；  </li></ul><p>此外，为了提高评价准则的实用性：</p><ul><li>参数尽可能少，阈值可调；</li><li>易于理解，表现方式符合人们的直觉；</li><li>有较强的通用性，能评估各种跟踪算法；</li><li>指标个数少，但是能足够反映算法不同维度的性能；  </li></ul><p>假设第 \(t\) 帧，有目标集 \(\{o_1,…,o_n\}\)，跟踪算法的输出(hypotheses)：\(\{h_1,…h_m\}\)。根据上述设计准则，设计评价计算过程：</p><ol><li>&ensp;构建 \(h_j\) 与 \(o_i\) 的最优匹配；</li><li>&ensp;对于每个匹配对，计算位置估计误差；</li><li>&ensp;累加所有匹配对的误差，包括：<br>a. &ensp;计算漏检数(FN)；<br>b. &ensp;计算误检数(FP)；<br>c. &ensp;计算 ID swith 次数，包括两个邻近目标的 ID 交换，以及遮挡后，同一目标的 ID 跳变；  </li></ol><p>由此可得到两大指标：</p><ul><li>tracking precision，目标位置的估计精度；</li><li>tracking accuracy，包括 misses(FN), FP, mismatches(IDs), failures to recover；  </li></ul><p>下面分两块做细节分析，匹配构建 (Establishing Correspondences) 与评价指标 (Metrics)。</p><h2 id="1-ensp-匹配构建"><a href="#1-ensp-匹配构建" class="headerlink" title="1.&ensp;匹配构建"></a>1.&ensp;匹配构建</h2><p>　　算法估计与目标真值的匹配，大致还是基于匹配最近 object-hypothesis 的思想，没匹配上的估计就是 FP，没匹配上的真值就是 FN。但是这中间需要进一步考虑一些问题。</p><h3 id="1-1-ensp-有效匹配"><a href="#1-1-ensp-有效匹配" class="headerlink" title="1.1.&ensp;有效匹配"></a>1.1.&ensp;有效匹配</h3><p>　　如果算法估计 \(h_j\) 与目标 \(o_i\) 的最近距离 \(dist_{i,j}\) 超过了一定的阈值 \(T\)，那么这个匹配也是不合理的，因为这个距离误差加入到定位误差中是不合理的，所以只能说这个跟踪的结果不是这个目标。关于距离的度量：</p><ul><li>区域（框）跟踪器，距离可用两者的重叠区域来度量，\(T\) 可以设为 0；</li><li>点跟踪器，距离可用两者中心点的欧氏距离来度量，\(T\) 可以根据目标的尺寸来设定；</li></ul><h3 id="1-2-ensp-跟踪一致性"><a href="#1-2-ensp-跟踪一致性" class="headerlink" title="1.2.&ensp;跟踪一致性"></a>1.2.&ensp;跟踪一致性</h3><p>　　统计目标与算法输出的匹配跳变的次数，也就是目标 ID 的跳变数。文章还提到，当目标有两个有效地匹<br>配时，选择之前的匹配，即使那个匹配的距离大于另一个匹配，这点当存在两个很近的目标时，可能会有问题，需要全局来看。</p><h3 id="1-3-ensp-匹配过程"><a href="#1-3-ensp-匹配过程" class="headerlink" title="1.3.&ensp;匹配过程"></a>1.3.&ensp;匹配过程</h3><ol><li>&ensp;对 \(t\) 帧，考虑 \(M_{t-1}\) 中所有匹配是否还依然有效，包括目标真值及算法输出是否还存在，如果都存在，那么距离是否超出阈值 \(T\)；</li><li>&ensp;对于剩下的没找到匹配的真值目标，在唯一匹配以及阈值约束下，可采用匹配算法或者贪心算法来求解，使得距离误差的总和最小（文章的意思是排除了从上一帧继承的已有匹配，当目标密集时，这部分也应该加入进来优化）。统计当前帧目标真值匹配的跳变数 \(mme_t\)，作为 mismatch errors；</li><li>&ensp;经过之前两步后，找到了所有的匹配，统计匹配个数为 \(c_t\)，计算匹配上的目标真值与算法输出的定位误差 \(d_t^i\)；</li><li>&ensp;统计没有匹配上的算法输出 (hypotheses) 为 \(fp_t\)，没有匹配上的目标真值为 \(m_t\)，目标真值个数为 \(g_t\)；</li><li>&ensp;每一帧重复步骤１，第一帧没有 mismatch；</li></ol><h2 id="2-ensp-评价指标"><a href="#2-ensp-评价指标" class="headerlink" title="2.&ensp;评价指标"></a>2.&ensp;评价指标</h2><p>　　基于以上的匹配策略，得出两个合理的指标：</p><ul><li><strong>MOTP</strong>(multiple object tracking precision)，跟踪定位精度指标：$$MOTP=\frac{\sum_{i,t}d_t^i}{\sum_tc_t}$$</li><li><strong>MOTA</strong>(multiple object tracking accuracy)，综合了漏检率，误检率，以及 ID 跳变率：$$MOTA=1-\frac{\sum_t(m_t+fp_t+mme_t)}{\sum_tg_t}$$</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　这篇文章介绍了两个综合性指标 MOTA 以及 MOTP 的计算过程，这两个指标有优劣势，但是作为综合性指标至今在学术界仍广泛应用。本文主要介绍其设计思想及计算过程。&lt;br&gt;　　一个理想的 MOT 算法，我们期望每一帧：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;准确检测目标的数量；&lt;/
      
    
    </summary>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/categories/paper-reading/"/>
    
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/tags/MOT/"/>
    
      <category term="tracking" scheme="https://leijiezhang001.github.io/tags/tracking/"/>
    
  </entry>
  
  <entry>
    <title>MOT 综述-&#39;Multiple Object Tracking: A Literature Review&#39;</title>
    <link href="https://leijiezhang001.github.io/MOT-%E7%BB%BC%E8%BF%B0-Multiple-Object-Tracking-A-Literature-Review/"/>
    <id>https://leijiezhang001.github.io/MOT-综述-Multiple-Object-Tracking-A-Literature-Review/</id>
    <published>2019-05-28T08:22:58.000Z</published>
    <updated>2019-06-05T15:17:36.085Z</updated>
    
    <content type="html"><![CDATA[<p>　　之前做 MOT 还是沿着 SOT 的思路，这篇文章对 MOT 有一个很深入且很有框架性的综述，以下对这篇文章做一个提炼，并加入一些自己的想法。<br>　　MOT 作为一个中层任务，是一些高层任务的基础，比如行人的 pose estimation，action recognition，behavior analysis，车辆的 state estimation。单目标跟踪(SOT)主要关注 appearance model 以及 motion model 的设计，解决尺度、旋转、光照等影响因素。而 MOT 包含两个任务：目标数量以及目标ID，这就要求 MOT 还需要解决其它问题：</p><ul><li>frequent occlusions</li><li>initialization and termination of tracks</li><li>similar appearance</li><li>interactions among multiple objects</li></ul><h2 id="1-ensp-问题描述"><a href="#1-ensp-问题描述" class="headerlink" title="1.&ensp;问题描述"></a>1.&ensp;问题描述</h2><p>　　多目标跟踪实际上是多参数估计问题。给定图像序列\(\{I_1,I_2,…,I_t,…\}\)，第\(t\)帧中目标个数为\(M_t\)，第\(t\)帧中所有目标的状态表示为\(S_t=\{s_t^1,s_t^2,…,s_t^{M_t}\}\)，第\(i\)个目标的轨迹表示为\(s_{1:t}^i=\{s_1^i,s_2^i,…,s_t^i\}\)，所有图像中所有目标的状态序列为\(S_{1:t}=\{S_1,S_2,…,S_t\}\)。相应的，所有图像中所有目标观测到的状态序列为\(O_{1:t}=\{O_1,O_2,…,O_t\}\)。多目标跟踪的优化目标是求解最优的各目标状态，即求解一个后验概率问题，$$ \widehat{S} _ {1:t}=\mathop{\arg\max}_{S_{1:t}}P(S_{1:t}|O_{1:t})$$<br>这种形式有两种实现方法：</p><ul><li><strong>probabilistic inference</strong><br>适合用于 online tracking 任务，Dynamic Model 为 \(P(S_t|S_{t-1})\)，Observation Model 为 \(P(O_t|S_t)\)，两步求解过程：<br>　\(\circ\)　Predict: \(P(S_t|O_{1:t-1})=\int P(S_t|S_{t-1})dS_{t-1}\)<br>　\(\circ\)　Update: \(P(S_t|O_{1:t}) \propto P(O_t|S_t)P(S_t|O_{1:t-1})\)</li><li><strong>deterministic optimization</strong><br>适合用于 offline tracking 任务，直接利用多帧信息进行最优化求解。</li></ul><h2 id="2-ensp-分类方法"><a href="#2-ensp-分类方法" class="headerlink" title="2.&ensp;分类方法"></a>2.&ensp;分类方法</h2><ul><li><strong>initialization method</strong><br>初始化方式分为：<br>　\(\circ\)　Detection-Based Tracking，优势明显，除了只能处理特定的目标类型；<br>　\(\circ\)　Detection-Free Tracking，能处理任何目标类型；</li><li><strong>processing mode</strong><br>根据是否使用未来的观测，处理方式可分为：<br>　\(\circ\)　online tracking，适合在线任务，缺点是观测量会比较少；<br>　\(\circ\)　offline tracking，输出结果存在时延，理论上能获得全局最优解；</li><li><p><strong>type of output</strong><br>根据问题求解方式输出是否存在随机性：<br>　\(\circ\)　probabilistic inference，概率性推断；<br>　\(\circ\)　deterministic inference，求解最大后验概率；</p><p><strong>自动驾驶等在线任务主要关注 Detection-Based，online tracking。</strong></p></li></ul><h2 id="3-ensp-框架"><a href="#3-ensp-框架" class="headerlink" title="3.&ensp;框架"></a>3.&ensp;框架</h2><p>　　MOT 主要考虑两个问题：</p><ul><li>目标在不同帧之间的相似性度量，即对appearance, motion, interaction, exclusion, occlusion的建模；</li><li>恢复出目标的ID，即 inference 过程；  </li></ul><h3 id="3-1-ensp-Appearance-Model"><a href="#3-1-ensp-Appearance-Model" class="headerlink" title="3.1.&ensp;Appearance Model"></a>3.1.&ensp;Appearance Model</h3><h4 id="3-1-1-ensp-Visual-Representation"><a href="#3-1-1-ensp-Visual-Representation" class="headerlink" title="3.1.1.&ensp;Visual Representation"></a>3.1.1.&ensp;Visual Representation</h4><p>　　视觉表达即目标的特征表示方式：</p><ol><li><strong>local features</strong><br>本质上是点特征，点特征由 corner+descriptor(角点+描述子) 组成。KLT(good features to track)在 SOT 中应用广泛，用它可以生成短轨迹，估计相机运动位姿，运动聚类等；Optical Flow也是一种局部特征，在数据关联之前也可用于将检测目标连接到短轨迹中去。</li><li><strong>region features</strong><br>在一个块区域内提取特征，根据像素间作差的次数，可分为：<ul><li>zero-order, color histogram &amp; raw pixel template</li><li>first-order, HOG &amp; level-set formulation(?)</li><li>up-to-second-order, Region covariance matrix</li></ul></li><li><strong>others</strong><br>其它特征本质上也需要 local 或 region 的方式提取，只是原始信息并不是灰度或彩图。如 depth,probabilistic occupancy map, gait feature.  </li></ol><p>　　Local features，比如颜色特征，在计算上比较高效，但是对遮挡，旋转比较敏感；Region features 里，HOG 对光照有一定的鲁棒性，但是对遮挡及形变效果较差；Region covariance matrix 更加鲁棒，但是需要更高的计算量；深度特征也比较有效，但是需要额外的获取深度信息的代价。</p><h4 id="3-1-2-ensp-Statistical-Measuring"><a href="#3-1-2-ensp-Statistical-Measuring" class="headerlink" title="3.1.2.&ensp;Statistical Measuring"></a>3.1.2.&ensp;Statistical Measuring</h4><p>　　有了目标的特征表示方式之后，就可以评价两个观察的目标的相似性。特征表示的线索(cue)可分为：</p><ol><li><strong>single cue</strong><br>因为只有一个线索，相似性(similarity)可以直接通过两个向量的距离转换得到。可以将距离指数化，高斯化。也可以将不相似度转为可能性，用协方差矩阵表示。</li><li><strong>multiple cues</strong><br>多线索，即多种特征的融合，能极大提高鲁棒性，融合的策略有：<ul><li>Boosting, 选取一系列的特征，用 boost 算法选取表达能力最强的特征；</li><li>Concatenation, 各个特征直接在空间维度上串起来，形成一个 cue 的表达方式；</li><li>Summation, 加权融合各个特征，形成一个 cue 的表达方式；</li><li>Product, 各个特征相乘的方式，比如目标 \(s_0\) 的某个潜在匹配 \(s_1\) 的颜色，形状特征为 \(color\), \(shape\) 的概率为 \(p(color|s_0)\), \(p(shape|s_0)\), 假设特征独立，那么，<br>　　　　　　　$$p(s_1|s_0)=p(color, shape|s_0)=p(color|s_0)\cdot p(shape|s_0)$$</li><li>Cascading, coarse-to-fine 的方式，逐步精细化搜索；</li></ul></li></ol><h3 id="3-2-ensp-Motion-Model"><a href="#3-2-ensp-Motion-Model" class="headerlink" title="3.2.&ensp;Motion Model"></a>3.2.&ensp;Motion Model</h3><p>　　运动模型对关联两个 tracklets 比较管用，而 online tracking 任务，对输出的时延要求较高，所以其中一个 tracklet 可以任务就是当前帧与上一帧形成的轨迹，所以这里很难去计算两个 tracklets 的相似度。能看到的一个应用点就是，通过 motion model 模型，预测下一时刻目标的位置，作为一个线索项目。以下讨论的各模型主要是为了度量 tracklets 的相似性，从而做 tracklets 的匹配。  </p><h4 id="3-2-1-ensp-Linear"><a href="#3-2-1-ensp-Linear" class="headerlink" title="3.2.1.&ensp;Linear"></a>3.2.1.&ensp;Linear</h4><ul><li>Velocity Smoothness. N 帧 M 个目标轨迹: \(C_{dyn}=\sum_{t=1}^{N-2}\sum_{i=1}^{M}\parallel v_i^t-v_i^{t+1}\parallel^2\)</li><li>Position Smoothness. \(G(p^{tail}+v^{tail}\Delta t-p^{head}, \sum_p)\cdot G(p^{head}-v^{head}\Delta t-p^{tail}, \sum_p)\)</li><li>Acceleration Smoothness.  </li></ul><h4 id="3-2-2-ensp-Non-linear"><a href="#3-2-2-ensp-Non-linear" class="headerlink" title="3.2.2.&ensp;Non-linear"></a>3.2.2.&ensp;Non-linear</h4><p>　　运动模型假设是非线性的，相似度计算还是按照以上高斯形式。引为中提到，非线性运动模型并不作为目标的惩罚因子，因为目标并不需要满足该模型，但是只要有目标满足，就降低惩罚系数。</p><h3 id="3-3-ensp-Interaction-Model"><a href="#3-3-ensp-Interaction-Model" class="headerlink" title="3.3.&ensp;Interaction Model"></a>3.3.&ensp;Interaction Model</h3><h4 id="3-3-1-ensp-Social-Force-Models"><a href="#3-3-1-ensp-Social-Force-Models" class="headerlink" title="3.3.1.&ensp;Social Force Models"></a>3.3.1.&ensp;Social Force Models</h4><ol><li><strong>Individual Force</strong><ul><li>fidelity, 目标不会改变它的目的地方向；</li><li>constancy, 目标不会突然改变速度和方向；</li></ul></li><li><strong>Group Force</strong><ul><li>attraction, 目标间应该尽量靠近；</li><li>repulsion, 目标间也得保留适当的距离；</li><li>coherence, 同一个 group 里面的目标速度应该差不多；</li></ul></li></ol><h4 id="3-3-2-ensp-Crowd-Motion-Pattern-Models"><a href="#3-3-2-ensp-Crowd-Motion-Pattern-Models" class="headerlink" title="3.3.2.&ensp;Crowd Motion Pattern Models"></a>3.3.2.&ensp;Crowd Motion Pattern Models</h4><p>　　当一个 group 比较密集的时候，单个目标的运动模型不太显著了，这时候群体的运动模型更加有效，可以用一些方法来构建群体运动模型。</p><h3 id="3-4-ensp-Exclusion-Model"><a href="#3-4-ensp-Exclusion-Model" class="headerlink" title="3.4.&ensp;Exclusion Model"></a>3.4.&ensp;Exclusion Model</h3><h4 id="3-4-1-ensp-Detection-level"><a href="#3-4-1-ensp-Detection-level" class="headerlink" title="3.4.1.&ensp;Detection-level"></a>3.4.1.&ensp;Detection-level</h4><p>　　同一帧两个检测量不能指向同一个目标。匹配 tracklets 时，可以将这一项作为惩罚项。不过目前的检测技术都做了 NMS，基本可以消除这种情况。  </p><h4 id="3-4-2-ensp-Trajectory-level"><a href="#3-4-2-ensp-Trajectory-level" class="headerlink" title="3.4.2.&ensp;Trajectory-level"></a>3.4.2.&ensp;Trajectory-level</h4><p>　　两个轨迹不能非常靠近。对于 online tracking 来说，就是 tracking 结果的两个量不能挨在一起，如果挨在一起，就说明有问题，比如遮挡，或跟丢。</p><h3 id="3-5-ensp-Occlusion-Handling"><a href="#3-5-ensp-Occlusion-Handling" class="headerlink" title="3.5.&ensp;Occlusion Handling"></a>3.5.&ensp;Occlusion Handling</h3><ul><li>Part-to-whole, 将目标分成栅格来处理；</li><li>Hypothesize-and-test, </li><li>Buffer-and-recover, 在遮挡产生前，记录一定量的观测，遮挡后恢复；</li><li>Others</li></ul><h3 id="3-6-ensp-Inference"><a href="#3-6-ensp-Inference" class="headerlink" title="3.6.&ensp;Inference"></a>3.6.&ensp;Inference</h3><h4 id="3-6-1-ensp-Probabilistic-Inference"><a href="#3-6-1-ensp-Probabilistic-Inference" class="headerlink" title="3.6.1.&ensp;Probabilistic Inference"></a>3.6.1.&ensp;Probabilistic Inference</h4><p>　　概率法只需要用到当前时刻之前的信息，所以适合用于 online tracking 任务。首先，如果假设一阶马尔科夫，当前目标的状态之依赖于前一时刻目标的状态，即 <em>dynamic model</em>：<br>$$P(S_t|S_{1:t-1})=P(S_t|S_{t-1})$$<br>其次，观测是独立的，即当前目标的观测只由当前目标的状态决定，<em>observation model</em>：<br>$$P(O_{1:t}|S_{1:t})=\prod_{i=1}^t P(O_t|S_t)$$<br>dynamic model 对应的就是跟踪算法策略，observation model 是状态观测手段，包括检测方法。目标状态估计的迭代过程为：</p><ul><li><strong>predict step</strong><br>根据 dynamic model，由目标的上一状态预测当前状态的后验概率分布；</li><li><strong>update step</strong><br>根据 observation model，更新当前目标状态的后验概率分布；</li></ul><p>　　状态估计的过程伴随着噪音等因素的影响，常用的概率推断模型有：</p><ul><li>Kalman filter</li><li>Extended Kalman filter</li><li>Particle filter</li></ul><h4 id="3-6-2-ensp-Deterministic-Optimization"><a href="#3-6-2-ensp-Deterministic-Optimization" class="headerlink" title="3.6.2.&ensp;Deterministic Optimization"></a>3.6.2.&ensp;Deterministic Optimization</h4><p>  　确定性优化法需要至少一个时间窗口的观测量，所以适合 offline tracking 任务。优化方法有：</p><ul><li>Bipartite graph matching</li><li>Dynamic Programming</li><li>Min-cost max-flow network flow</li><li>Conditional random field</li><li>MWIS(Maximum-weight independent set)</li></ul><h2 id="4-ensp-评价方法"><a href="#4-ensp-评价方法" class="headerlink" title="4.&ensp;评价方法"></a>4.&ensp;评价方法</h2><p>　　评价方法是非常重要的，一方面对算法系统进行调参优化，另一方面比较各个不同算法的优劣。评价方法 (evaluation) 包括评价指标 (metrics) 以及数据集 (datasets)，多类别的数据集主要有：</p><ul><li><a href="https://motchallenge.net/results/MOT17/" target="_blank" rel="noopener">MOT Challenge</a></li><li><a href="http://www.cvlibs.net/datasets/kitti/eval_tracking.php" target="_blank" rel="noopener">KITTI</a>　　</li></ul><p>评价指标可分为：<br>A.&ensp;<strong>检测指标</strong><br>　\(\lozenge\)　准确性(Accuracy)</p><ul><li>Recall &amp; Precision</li><li>False Alarme per Frame(FAF) rate, from <a href="https://www.google.com/search?q=Learning+affinities+and+dependencies+for+multi-target+tracking+using+a+CRF+model&amp;oq=Learning+affinities+and+dependencies+for+multi-target+tracking+using+a+CRF+model&amp;aqs=chrome..69i57.1077j0j9&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">paper</a></li><li>False Positive Per Image(FPPI), from <a href="https://www.google.com/search?q=Multiple+target+tracking+in+world+coordinate+with+single%2C+minimally+calibrated+camera&amp;oq=Multiple+target+tracking+in+world+coordinate+with+single%2C+minimally+calibrated+camera&amp;aqs=chrome..69i57j0.1134j0j9&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">paper</a></li><li>MODA(Multiple Object Detection Accuracy), 包含了 false positive &amp; miss dets. from <a href="https://www.google.com/search?q=Framework+for+performance+evaluation+of+face%2C+text%2C+and+vehicle+detection+and+tracking+in+video%3A+Data%2C+metrics%2C+and+protocol&amp;oq=Framework+for+performance+evaluation+of+face%2C+text%2C+and+vehicle+detection+and+tracking+in+video%3A+Data%2C+metrics%2C+and+protocol&amp;aqs=chrome..69i57j69i61.973j0j9&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">paper</a></li></ul><p>　\(\lozenge\)　精确性(Precision)</p><ul><li>MODP(Multiple Object Detection Precision), 衡量检测框与真值框的位置对齐程度；from <a href="https://www.google.com/search?q=Framework+for+performance+evaluation+of+face%2C+text%2C+and+vehicle+detection+and+tracking+in+video%3A+Data%2C+metrics%2C+and+protocol&amp;oq=Framework+for+performance+evaluation+of+face%2C+text%2C+and+vehicle+detection+and+tracking+in+video%3A+Data%2C+metrics%2C+and+protocol&amp;aqs=chrome..69i57j69i61.973j0j9&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">paper</a></li></ul><p>B.&ensp;<strong>跟踪指标</strong><br>　\(\lozenge\)　准确性(Accuracy)</p><ul><li>ID switches(IDs), from <a href="https://www.google.com/search?safe=strict&amp;ei=agXyXMaQEKyl_Qa4zJrQCg&amp;q=who+are+you+with+and+where+are+you+going&amp;oq=Who+are+you+with+and+where+are+you+going&amp;gs_l=psy-ab.1.0.0i203.53050.53050..55771...0.0..0.559.559.5-1......0....2j1..gws-wiz.nigYYAJc4jQ" target="_blank" rel="noopener">paper</a></li><li>MOTA(Multiple Object Tracking Accuracy), 包含了FP，FN，mismatch；from <a href="https://www.google.com/search?safe=strict&amp;ei=0ATyXP6lPIO6ggfIk6GAAQ&amp;q=evaluating+multiple+object+tracking+performance+the+clear+mot+metrics&amp;oq=Evaluating+Multiple+Object+Tracking+Performance&amp;gs_l=psy-ab.1.1.35i39j0j0i30j0i67.46576.46576..50024...0.0..0.436.436.4-1......0....2j1..gws-wiz.KAREeooiDMo" target="_blank" rel="noopener">paper</a></li></ul><p>　\(\lozenge\)　精确性(Precision)</p><ul><li>MOTP(Multiple Object Tracking Precision), from <a href="https://www.google.com/search?safe=strict&amp;ei=0ATyXP6lPIO6ggfIk6GAAQ&amp;q=evaluating+multiple+object+tracking+performance+the+clear+mot+metrics&amp;oq=Evaluating+Multiple+Object+Tracking+Performance&amp;gs_l=psy-ab.1.1.35i39j0j0i30j0i67.46576.46576..50024...0.0..0.436.436.4-1......0....2j1..gws-wiz.KAREeooiDMo" target="_blank" rel="noopener">paper</a></li><li>TDE(Tracking Distance Error), from <a href="https://www.google.com/search?safe=strict&amp;ei=fATyXNnwEvCH_QaG17fwDA&amp;q=%E2%80%9CTracking+with+local+spatio-temporal+motion+patterns+in+extremely+crowded+scenes&amp;oq=%E2%80%9CTracking+with+local+spatio-temporal+motion+patterns+in+extremely+crowded+scenes&amp;gs_l=psy-ab.12..0i30.82181.82181..83291...0.0..0.292.292.2-1......0....2j1..gws-wiz.hs0Je90zzHU" target="_blank" rel="noopener">paper</a></li><li>OSPA(optimal subpattern assignment), from <a href="https://www.google.com/search?safe=strict&amp;ei=_gDyXPKINY21ggeKtb2oDg&amp;q=a+metric+for+performance+evaluation+of+multi-target+tracking+algorithms&amp;oq=A_Metric_for_Performance_Evaluation_of_Multi-Targe&amp;gs_l=psy-ab.1.0.0i30.106502.106502..109413...0.0..0.303.303.3-1......0....2j1..gws-wiz.vrzc0MG18OM" target="_blank" rel="noopener">paper</a></li></ul><p>　\(\lozenge\)　完整性(Completeness)</p><ul><li>MT, the numbers of Mostly Tracked, from <a href="https://www.google.com/search?q=Learning+to+associate%3A+Hybridboosted+multi-target+tracker+for+crowded+scene&amp;oq=Learning+to+associate%3A+Hybridboosted+multi-target+tracker+for+crowded+scene&amp;aqs=chrome..69i57.1261j0j9&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">paper</a></li><li>PT, the numbers of Partly Tracked</li><li>ML, the numbers of Mostly Lost</li><li>FM, the numbers of Fragmentation</li></ul><p>　\(\lozenge\)　鲁棒性(Robustness)</p><ul><li>RS(Recover from Short-term occlusion), from <a href="https://www.google.com/search?safe=strict&amp;ei=_gDyXPKINY21ggeKtb2oDg&amp;q=A+stochastic+graph+evolution+framework+for+robust+multi-target+tracking&amp;oq=A+stochastic+graph+evolution+framework+for+robust+multi-target+tracking&amp;gs_l=psy-ab.12..0i30.453442.453442..454691...0.0..0.315.315.3-1......0....2j1..gws-wiz.OPYJ8mRFgYg" target="_blank" rel="noopener">paper</a></li><li>RL(Recover from Long-term occlusion)  </li></ul><p>评价指标汇总：<br><img src="/MOT-综述-Multiple-Object-Tracking-A-Literature-Review/metrics.png" width="50%" height="50%"></p><h2 id="5-ensp-总结"><a href="#5-ensp-总结" class="headerlink" title="5.&ensp;总结"></a>5.&ensp;总结</h2><h3 id="5-1-ensp-还存在的问题"><a href="#5-1-ensp-还存在的问题" class="headerlink" title="5.1.&ensp;还存在的问题"></a>5.1.&ensp;还存在的问题</h3><p>　　MOT 算法模块较多，参数也较复杂，但是最依赖于检测模块的性能，所以算法间比较性能时，需要注意按模块进行变量控制。</p><h3 id="5-2-ensp-未来研究方向"><a href="#5-2-ensp-未来研究方向" class="headerlink" title="5.2.&ensp;未来研究方向"></a>5.2.&ensp;未来研究方向</h3><ul><li><strong>MOT with video adaptation</strong>，检测模块式预先训练的，需要在线更新学习；</li><li><strong>MOT under multiple camera</strong>:<br>\(\circ\)　multiple views，不同视野相同场景信息的记录，<br>\(\circ\)　non-overlapping multi-camera，不同视野不同场景的 reidentification；</li><li><strong>Multiple 3D object tracking</strong>，能更准确预测位置，大小，更有效处理遮挡；</li><li><strong>MOT with scene understanding</strong>，拥挤场景，用场景理解来有效跟踪；</li><li><strong>MOT with deep learning</strong></li><li><strong>MOT with other cv tasks</strong>，和其他任务融合，比如目标分割等；</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　之前做 MOT 还是沿着 SOT 的思路，这篇文章对 MOT 有一个很深入且很有框架性的综述，以下对这篇文章做一个提炼，并加入一些自己的想法。&lt;br&gt;　　MOT 作为一个中层任务，是一些高层任务的基础，比如行人的 pose estimation，action recog
      
    
    </summary>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/categories/paper-reading/"/>
    
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/tags/MOT/"/>
    
      <category term="tracking" scheme="https://leijiezhang001.github.io/tags/tracking/"/>
    
  </entry>
  
  <entry>
    <title>3D Detection Paper List</title>
    <link href="https://leijiezhang001.github.io/3D-Detection-paper-list/"/>
    <id>https://leijiezhang001.github.io/3D-Detection-paper-list/</id>
    <published>2019-05-22T03:43:33.000Z</published>
    <updated>2019-05-26T08:38:20.552Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章从输入数据类别上进行 3D Detection paper 的归类。</p><h2 id="RGB"><a href="#RGB" class="headerlink" title="RGB"></a>RGB</h2><h2 id="RGB-D-双目，单目-点云"><a href="#RGB-D-双目，单目-点云" class="headerlink" title="RGB-D(双目，单目+点云)"></a>RGB-D(双目，单目+点云)</h2><h2 id="Lidar"><a href="#Lidar" class="headerlink" title="Lidar"></a>Lidar</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这篇文章从输入数据类别上进行 3D Detection paper 的归类。&lt;/p&gt;
&lt;h2 id=&quot;RGB&quot;&gt;&lt;a href=&quot;#RGB&quot; class=&quot;headerlink&quot; title=&quot;RGB&quot;&gt;&lt;/a&gt;RGB&lt;/h2&gt;&lt;h2 id=&quot;RGB-D-双目，单目-点云&quot;
      
    
    </summary>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/categories/paper-reading/"/>
    
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
  </entry>
  
  <entry>
    <title>Study Topic List</title>
    <link href="https://leijiezhang001.github.io/study-topic-list/"/>
    <id>https://leijiezhang001.github.io/study-topic-list/</id>
    <published>2019-05-20T05:30:54.000Z</published>
    <updated>2019-06-05T15:08:37.617Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文罗列了相关领域知识的学习资料。</p><h2 id="1-ensp-Detection"><a href="#1-ensp-Detection" class="headerlink" title="1.&ensp;Detection"></a>1.&ensp;Detection</h2><h3 id="1-1-ensp-2D-Detection"><a href="#1-1-ensp-2D-Detection" class="headerlink" title="1.1.&ensp;2D Detection"></a>1.1.&ensp;2D Detection</h3><ul><li><a href="https://zhuanlan.zhihu.com/p/34142321" target="_blank" rel="noopener">入门</a></li><li><a href="https://github.com/amusi/awesome-object-detection" target="_blank" rel="noopener">amusi</a></li><li><a href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#yolov3" target="_blank" rel="noopener">Object Detection @handong</a></li><li><a href="http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/" target="_blank" rel="noopener">Object Detection and Classification using R-CNNs</a></li><li><a href="https://paperswithcode.com/task/object-detection" target="_blank" rel="noopener">Paper with Code</a></li></ul><h3 id="1-2-ensp-3D-Detection"><a href="#1-2-ensp-3D-Detection" class="headerlink" title="1.2.&ensp;3D Detection"></a>1.2.&ensp;3D Detection</h3><ul><li><a href="https://paperswithcode.com/task/3d-object-detection" target="_blank" rel="noopener">Paper with Code</a></li><li><a href="http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d" target="_blank" rel="noopener">KITTI Leaderboard</a></li></ul><hr><h2 id="2-ensp-Tracking"><a href="#2-ensp-Tracking" class="headerlink" title="2.&ensp;Tracking"></a>2.&ensp;Tracking</h2><h3 id="2-1-ensp-Single-Object-Tracking"><a href="#2-1-ensp-Single-Object-Tracking" class="headerlink" title="2.1.&ensp;Single Object Tracking"></a>2.1.&ensp;Single Object Tracking</h3><ul><li><a href="https://paperswithcode.com/task/visual-object-tracking" target="_blank" rel="noopener">Paper with Code</a></li></ul><h3 id="2-2-ensp-Multi-Object-Tracking"><a href="#2-2-ensp-Multi-Object-Tracking" class="headerlink" title="2.2.&ensp;Multi Object Tracking"></a>2.2.&ensp;Multi Object Tracking</h3><ul><li><a href="https://paperswithcode.com/task/multiple-object-tracking" target="_blank" rel="noopener">Paper with Code</a></li><li><a href="https://zhuanlan.zhihu.com/p/65177442" target="_blank" rel="noopener">Paper List</a></li><li><a href="https://motchallenge.net/results/MOT17/" target="_blank" rel="noopener">MOT Challenge</a></li><li><a href="https://arxiv.org/abs/1409.7618" target="_blank" rel="noopener">综述：Multiple Object Tracking: A Literature Review</a></li><li><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2013/html/Wu_Online_Object_Tracking_2013_CVPR_paper.html" target="_blank" rel="noopener">综述：Online object tracking: A benchmark</a></li><li><a href="https://arxiv.org/abs/1504.01942" target="_blank" rel="noopener">综述：MOTChallenge 2015: Towards a benchmark for multi-target tracking</a></li></ul><hr><h2 id="3-ensp-Computational-Photography"><a href="#3-ensp-Computational-Photography" class="headerlink" title="3.&ensp;Computational Photography"></a>3.&ensp;Computational Photography</h2><ul><li><a href="http://graphics.cs.cmu.edu/courses/15-463/2017_fall/" target="_blank" rel="noopener">2017年秋季的计算摄影学课程15-463</a></li></ul><hr><h2 id="4-ensp-CNN-ACC"><a href="#4-ensp-CNN-ACC" class="headerlink" title="4.&ensp;CNN ACC"></a>4.&ensp;CNN ACC</h2><hr><h2 id="5-ensp-SLAM"><a href="#5-ensp-SLAM" class="headerlink" title="5.&ensp;SLAM"></a>5.&ensp;SLAM</h2><h3 id="5-1-ensp-理论知识"><a href="#5-1-ensp-理论知识" class="headerlink" title="5.1.&ensp;理论知识"></a>5.1.&ensp;理论知识</h3><ul><li><a href="http://cvrs.whu.edu.cn/downloads/ebooks/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6%E6%96%B9%E6%B3%95.pdf" target="_blank" rel="noopener">计算机视觉中的数学方法</a></li><li><a href="http://cvrs.whu.edu.cn/downloads/ebooks/Multiple%20View%20Geometry%20in%20Computer%20Vision%20%28Second%20Edition%29.pdf" target="_blank" rel="noopener">Multiple View Geometry in Computer Vision</a></li><li><a href="https://docs.ufpr.br/~danielsantos/ProbabilisticRobotics.pdf" target="_blank" rel="noopener">Probabilistic Robotics</a>(有中文版)</li><li><a href="http://asrl.utias.utoronto.ca/~tdb/bib/barfoot_ser17.pdf" target="_blank" rel="noopener">State Estimation for Robotics</a>(有中文版)</li><li><a href="https://github.com/gaoxiang12/slambook" target="_blank" rel="noopener">视觉SLAM十四讲</a></li></ul><h3 id="5-2-ensp-综述"><a href="#5-2-ensp-综述" class="headerlink" title="5.2.&ensp;综述"></a>5.2.&ensp;综述</h3><ul><li>[Visual Odometry Part I: Fundamentals]</li><li>[Visual Odometry Part II: Matching, Robustness, Optimization, Applications]</li><li><a href="https://link.springer.com/content/pdf/10.1186%2Fs40064-016-3573-7.pdf" target="_blank" rel="noopener">Review of Visual Odometry: Types, Approaches, Challenges, and Applications</a></li><li><a href="https://ipsjcva.springeropen.com/track/pdf/10.1186/s41074-017-0027-2" target="_blank" rel="noopener">Visual SLAM algorithms: a Survey from 2010 to 2016</a></li><li><a href="http://www.cvc.uab.es/~asappa/publications/C__IEEE_IV_2012_W3.pdf" target="_blank" rel="noopener">Visual SLAM for Driverless Cars: a Brief Survey</a></li><li><a href="https://link.springer.com/article/10.1007/s10462-012-9365-8" target="_blank" rel="noopener">Visual Simultaneous Locations and Mapping: a Survey</a></li></ul><h3 id="5-3-ensp-工具"><a href="#5-3-ensp-工具" class="headerlink" title="5.3.&ensp;工具"></a>5.3.&ensp;工具</h3><ul><li><a href="http://www.guyuehome.com/column/ros-explore" target="_blank" rel="noopener">ROS</a></li><li><a href="https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html" target="_blank" rel="noopener">Opencv Camera Calibration</a></li><li><a href="http://www.vision.caltech.edu/bouguetj/calib_doc/" target="_blank" rel="noopener">Matlab Camera Calibration Toolbox</a></li><li><a href="http://wiki.ros.org/camera_calibration" target="_blank" rel="noopener">ROS Wiki Camera Calibration</a></li></ul><h3 id="5-4-ensp-算法"><a href="#5-4-ensp-算法" class="headerlink" title="5.4.&ensp;算法"></a>5.4.&ensp;算法</h3><ul><li><a href="https://openslam-org.github.io/" target="_blank" rel="noopener">OpenSLAM</a></li></ul><h3 id="5-5-ensp-其它资料"><a href="#5-5-ensp-其它资料" class="headerlink" title="5.5.&ensp;其它资料"></a>5.5.&ensp;其它资料</h3><ul><li><a href="https://www.zhihu.com/people/cheng-xu-yuan-10/posts" target="_blank" rel="noopener">计算机视觉life</a></li><li><a href="https://paperswithcode.com/task/visual-odometry" target="_blank" rel="noopener">Paper with Code</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　本文罗列了相关领域知识的学习资料。&lt;/p&gt;
&lt;h2 id=&quot;1-ensp-Detection&quot;&gt;&lt;a href=&quot;#1-ensp-Detection&quot; class=&quot;headerlink&quot; title=&quot;1.&amp;ensp;Detection&quot;&gt;&lt;/a&gt;1.&amp;ensp;De
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://leijiezhang001.github.io/hello-world/"/>
    <id>https://leijiezhang001.github.io/hello-world/</id>
    <published>2019-05-20T04:58:47.933Z</published>
    <updated>2019-05-20T04:58:47.933Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
