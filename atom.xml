<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LeijieZhang</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://leijiezhang001.github.io/"/>
  <updated>2019-10-14T10:15:34.592Z</updated>
  <id>https://leijiezhang001.github.io/</id>
  
  <author>
    <name>Leijie</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>[paper_reading]-&quot;Multi-Task Multi-Sensor Fusion for 3D Object Detection&quot;&quot;</title>
    <link href="https://leijiezhang001.github.io/paperreading-MT-MS-Fusion-for-3D-Object-Detection/"/>
    <id>https://leijiezhang001.github.io/paperreading-MT-MS-Fusion-for-3D-Object-Detection/</id>
    <published>2019-10-14T02:42:54.000Z</published>
    <updated>2019-10-14T10:15:34.592Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种 3D 检测的多任务多传感器融合方法。输入数据为图像以及点云，输出为地面估计，2D/3D检测，稠密深度图。为了让其它任务来帮助提升 3D 检测效果，作者设计了很多方法，工作还是比较细致且系统。<br><img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/算法框架.png" width="90%" height="90%" title="图 1. 算法框架"> 　　整个算法框架如图 1. 所示。点云数据还是在俯视图(BEV)下进行栅格化处理，高度切割是在地面估计归一化后的基础上来做，因为要 3D 定位的目标都是在地面上的；另一方面，图像与投影到前视图的点云数据进行合并，作为网络的输入数据。 网络结构上作者提出了两种俯视图与前视图特征融合策略：1. Point-wise feature fusion; 2. ROI-wise feature fusion. 这也是文章比较重要的一个贡献点。<br>　　文章所提的 3D 检测方法大多数细节技巧并无新意，这里主要讨论分析文章中与传统方法不太一样的两大贡献点： 1. 俯视图与前视图特征融合策略； 2. 其它两个任务对检测任务提升的作用。</p><h2 id="俯视图与前视图特征融合策略">1. 俯视图与前视图特征融合策略</h2><p>　　由于网络输入有俯视图与前视图两个数据流，所以如何将这两个数据流进行特征级别的融合就显得尤为重要，文章提出了两种方式，backbone 网络级别的 point-wise feature fusion 以及第二阶段 ROI-wise feature fusion。</p><h3 id="point-wise-feature-fusion">1.1. Point-wise Feature Fusion</h3><p>　　3D 检测主体还是在俯视图下来做的，相比前视图对 3D 检测的处理，俯视图 3D 检测有天然的优势。因此，如何有效地将前视图的特征融合到俯视图的特征中，就显得尤为重要（俯视图特征融合到前视图相对比较简单）。<br><img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/point-wise.png" width="55%" height="55%" title="图 2. Point-wise Feature Fusion"> 　　如图 2. 所示，像素点级别的特征融合方式有两个模块，Multi-scale Fusion 以及 Continuous Fusion。Multi-scale Fusion 我们比较熟悉，可以采用类似 FPN 的结构实现。这里主要讨论 Continuous Fusion 模块。<br><img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/算法框架2.png" width="90%" height="90%" title="图 3. Deep Continuous Fusion 检测框架"> 　　Continuous Fusion 源自作者的另一篇文章<a href="#2" id="2ref"><sup>[2]</sup></a>。如图 3. 所示，该文检测框架基本就是本文的主干，其中 Fusion Layers 就是 Continuous Fusion 模块。而 continuous fusion 前身是作者团队提出的 Deep Parametric Continuous Convolution<a href="#3" id="3ref"><sup>[3]</sup></a>。</p><ul><li><p><strong>Deep Parametric Continuous Convolution</strong><br>传统的卷积只能作用于网格结构(gird-structured)的数据上，为了能处理点云这种非网格结构的数据，<a href="#3" id="3ref">[3]</a>提出了带参数的卷积(Parametric Continuous Convolution)。对于第 \(i\) 个需要计算的特征位置，其特征值 \(\mathrm{h}_i \in \mathbb{R}^N\) 数学形式为： <span class="math display">\[ \mathrm{h}_i=\sum_j \mathbf{MLP}(x_i-x_j)\cdot \mathrm{f}_j \]</span> 其中 \(j\) 表示第 \(i\) 个点周围的点，\(\mathrm{f}_j \in \mathbb{R}^N\) 为输入特征，\(x_j\in \mathbb{R}^3\) 是点的坐标值。多层感知机 \(\mathbf{MLP}\) 则起到了参数核函数的作用，将 \(\mathbb{R}^{J\times 3}\) 映射为 \(\mathbb{R}^{J\times N}\) 空间，用作特征计算的权重值。</p></li><li><p><strong>Continuous Fusion Layer</strong><br>Continuous Fusion 则没有显示得计算卷积权重的过程，这样使得特征提取能力更强，而且计算效率更高，不用存储权重值。其数学描述为： <span class="math display">\[ \mathrm{h}_i=\sum_j \mathbf{MLP}(\mathrm{concat}[\mathrm{f}_j,x_i-x_j]) \]</span> 多层感知机 \(\mathbf{MLP}\) 直接将 \(\mathbb{R}^{J\times (N+3)}\) 映射到 \(\mathbb{R}^{J\times M}\) 空间，最后再做一个 element-wise 的相加即得空间为 \(\mathbb{R}^{M}\) 的特征输出(<strong>这个和 PointNet 几乎一模一样，本质就是将每个点的特征空间升维，然后用对称函数(pooling, sum)消除无序点的影响, 只是这里输入的点的特征空间 \(N\) 可能已经很大了</strong>)。 <img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/continuous_fusion.png" width="70%" height="70%" title="图 4. Continuous Fusion"> 具体步骤如图 4. 所示：</p><ol type="1"><li>将点云投影到图像坐标系，在图像特征图上用双线性插值求取每个点对应的图像特征向量；</li><li>俯视图下对于每个需要求取特征的像素点，采样邻近的 \(K\) 个物理点，然后应用 Continuous Fusion，得到该像素点的特征向量；</li></ol></li></ul><h3 id="roi-wise-feature-fusion">1.2. ROI-wise Feature Fusion</h3><p>　　在俯视图上获得 3D 检测框后(见图 1.)，将其分别投影到图像特征图以及点云特征图上，图像特征图上用 ROIAlign 提取出目标框内的图像特征；点云特征图上用类似方法提取出带方向的目标框内的点云特征，两种特征合并到一起，再用网络进行 2D/3D 目标框的优化回归。 <img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/roi-wise.png" width="60%" height="60%" title="图 5. ROI-wise Fusion"> 　　如图 5. 所示，点云特征图上的目标框是带有一定方向的，准确提取特征时会有一些问题。由于旋转框有周期性，所以将目标框分成两种情况来考虑，这样提取的特征就没有奇异性了，如图 5.2 所示。此外 3D 优化回归是在目标框旋转后的坐标系下进行的。</p><h2 id="多任务对检测任务的提升作用">2. 多任务对检测任务的提升作用</h2><p><img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/ablation.png" width="100%" height="100%" title="图 6. Ablation on Kitti"> <img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/ablation2.png" width="50%" height="50%" title="图 7. Ablation on TOR4D"></p><h3 id="地面估计">2.1. 地面估计</h3><p>　　俯视图下点云进行栅格化手工提取特征之前，作者作了一个地面归一化的操作。地面估计是在栅格分辨率下进行的，所以自然能对点云的每个栅格进行地面归一化。作者认为自动驾驶 3D 检测的目标都是在地面上的，所以地面的先验知识应该有助于 3D 定位，与 HDNET<a href="#4" id="4ref"><sup>[4]</sup></a> 思想类似。而在线地面估计(地面估计是建图的其中一个任务)不依赖离线地图，能提高系统鲁棒性。<br><img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/ground_est.png" width="70%" height="70%" title="图 8. 目标定位误差"> 　　如图 6.,8 所示，地面估计的加入，确实使得 3D 检测性能有所提升。</p><h3 id="深度估计">2.2. 深度估计</h3><p>　　由于前视图输入的是图像以及点云的投影图，所以可进一步通过网络预测稠密的前视深度图。作者对点云的投影图作了精心的设计，这里不做展开，有可能直接投影的 \((x,y,z)\) 3 通道的投影图也够用。<br>　　获得了前视稠密深度图后，可将其反投影到点云俯视图下，这样稀疏的点云会变得更加稠密，更有利于图像到点云的 Point-wise Feature Fusion。这里作者只在邻近取不到点云的时候用这反投影的伪雷达点(pseudo LiDARP)。如图 7. 所示，在该数据集上效果提升还是比较明显，而 Kitti 上不太明显，因为两者的相机与雷达配置不太一样。在 TOR4D 数据集上，远距离的车上点云数量更小，所以该技术效果较好。</p><h2 id="其它细节">3. 其它细节</h2><p>　　Loss 设计为： <span class="math display">\[ Loss = L_{cls} + \lambda(L_{box}+L_{r2d}+L_{r3d}) + \gamma L_{depth} \]</span> 其中 \(\lambda\) 与 \(\gamma\) 为权重项，\(L_{box}\) 为俯视图下预测的 3D 框，\(L_{r2d},L_{r3d}\) 为优化回归的 2D/3D 框。每一项的 Loss 计算方式与传统无异。 <img src="/paperreading-MT-MS-Fusion-for-3D-Object-Detection/eval.png" width="90%" height="90%" title="图 9. 算法对比"> 　　本文方法与其它方法对比如图 9. 所示。</p><h2 id="参考文献">4. 参考文献</h2><p><a id="1" href="#1ref">[1]</a> Liang, Ming, et al. &quot;Multi-Task Multi-Sensor Fusion for 3D Object Detection.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.<br><a id="2" href="#2ref">[2]</a> Liang, Ming, et al. &quot;Deep continuous fusion for multi-sensor 3d object detection.&quot; Proceedings of the European Conference on Computer Vision (ECCV). 2018.<br><a id="3" href="#3ref">[3]</a> Wang, Shenlong, et al. &quot;Deep parametric continuous convolutional neural networks.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.<br><a id="4" href="#4ref">[4]</a> Yang, Bin, Ming Liang, and Raquel Urtasun. &quot;Hdnet: Exploiting hd maps for 3d object detection.&quot; Conference on Robot Learning. 2018.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　本文&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt;提出了一种 3D 检测的多任务多传感器融合方法。输入数据为图像以及点云，输出为地面估计，2D/3D检测，稠密深度图。为了让其它任务来帮助提升 3D 检测效果，作者设计了很多方法，工作还
      
    
    </summary>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/categories/paper-reading/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
  </entry>
  
  <entry>
    <title>Traveling-in-Serbia-Montenegro-Bosnia</title>
    <link href="https://leijiezhang001.github.io/traveling-in-Serbia-Montenegro-Bosnia/"/>
    <id>https://leijiezhang001.github.io/traveling-in-Serbia-Montenegro-Bosnia/</id>
    <published>2019-10-07T06:54:51.000Z</published>
    <updated>2019-10-08T04:31:28.244Z</updated>
    
    <content type="html"><![CDATA[<div id="dplayer0" class="dplayer hexo-tag-dplayer-mark" style="margin-bottom: 20px;"></div><script>(function(){var player = new DPlayer({"container":document.getElementById("dplayer0"),"theme":"#FADFA3","loop":true,"video":{"url":"https://leijie.oss-cn-shenzhen.aliyuncs.com/travel/Serbia-Montenegro-Bosnia.mp4","pic":"https://leijie.oss-cn-shenzhen.aliyuncs.com/travel/Serbia-Montenegro-Bosnia.mp4"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;div id=&quot;dplayer0&quot; class=&quot;dplayer hexo-tag-dplayer-mark&quot; style=&quot;margin-bottom: 20px;&quot;&gt;&lt;/div&gt;&lt;script&gt;(function(){var player = new DPlayer({&quot;c
      
    
    </summary>
    
      <category term="Travel" scheme="https://leijiezhang001.github.io/categories/Travel/"/>
    
    
      <category term="travel" scheme="https://leijiezhang001.github.io/tags/travel/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Fast and Furious&quot;</title>
    <link href="https://leijiezhang001.github.io/paperreading-Fast-and-Furious/"/>
    <id>https://leijiezhang001.github.io/paperreading-Fast-and-Furious/</id>
    <published>2019-09-24T02:23:23.000Z</published>
    <updated>2019-10-07T14:29:47.148Z</updated>
    
    <content type="html"><![CDATA[<p>　　动态目标状态估计传统的做法是将其分解为目标检测，目标跟踪，目标运动预测三个子问题进行链式求解，这回导致上游模块的误差在下游模块中会传递并放大。考虑到跟踪与预测能帮助提升检测的性能，比如对于遮挡或远距离目标，跟踪与预测能减少检测的漏检(FN)；而误检(FP)则可通过时域相关信息消除，由此本文<a href="#1" id="1ref"><sup>[1]</sup></a>提出了一种联合 3D 检测，跟踪，运动预测的多任务网络。</p><h2 id="model-architecture">1. Model Architecture</h2><h3 id="data-representation">1.1. Data Representation</h3><p>　　雷达坐标系下，每帧点云限定范围为\((x_{min}, x_{max}, y_{min}, y_{max}, z_{min}, z_{max})\)，那么在分辨率 \(r = (dx,dy,dz)\) 下进行栅格化，可得到体素 \((C, H, W) = (\frac{z_{max}-z_{min}}{dz}, \frac{y_{max}-y_{min}}{dy}, \frac{x_{max}-x_{min}}{dx})\), 如果体素中有点云那么该体素值置为1，否则置为0，这样就得到了俯视图下的伪图像。<br>　　此外将历史 \(T-1\) 帧点云先转换到当前本体坐标系(需要 ego motion 信息)，然后串成一起，就获得 \((T, C, H, W) \) 维的模型数据输入表示。</p><h3 id="model-formulation">1.2. Model Formulation</h3><p>　　实际输入网络的应该是 \((N, T, C, H, W) \) 维的数据，首先需要经过一个 fusion 层将数据映射到 \((N, C', H', W') \) 维，然后用一个类似与 SSD 结构的 backbone+head 网络即可。</p><h4 id="fusion">1.2.1. Fusion</h4><p><img src="/paperreading-Fast-and-Furious/fusion.png" width="80%" height="80%" title="图 1. Fusion 结构"> 　　本文提出了两种融合方式：</p><ul><li>Early Fusion<br>如图 1. 所示，直接在 T 维度上进行一维卷积，卷积 \(kernel_ size = T\)，由此得到 \((N, C, H, W) \) 维的特征。</li><li>Late Fusion<br>如图 1. 所示，通过两次 3D 卷积将 \(T=5\) 变换到 \(T=1\)，\(kernel size = (3, 3, 3)\),由此也得到 \((N, C, H, W) \) 维的特征。</li></ul><p>相比 Early Fusion，Late fusion 有更深的特征提取。</p><h4 id="backbonehead">1.2.2. Backbone+Head</h4><p><img src="/paperreading-Fast-and-Furious/head.png" width="80%" height="80%" title="图 2. Fusion 结构"> 　　backbone 采用 VGG16 结构，图 1. 可见。<br>　　head 采用类似 SSD 检测头的形式。anchor 也是有不同比例不同尺寸的矩形组成(另一种方法是，由于俯视图下同种类别的尺寸相似性，所以针对不同类别采用同一尺寸的 anchor 即够用)，角度回归则采用 \(cos, sin\) 形式。<br>　　如图 2. 所示，检测头有两个分之分支，第一个输出预测的分类 score map(n 个预测的 score map 是共享的)，第二个输出 n 个预测的 3D 框编码信息。</p><h3 id="decoding-tracklets">1.3. Decoding Tracklets</h3><p>　　由于有检测及预测的信息，所以可用简单的方法解析出跟踪 ID。历史的预测框信息可认为是当前的跟踪框，所以就自然得在 MOT 问题里进行求解。这里可直接计算跟踪框(历史预测框)与当前检测框的 overlap 误差项，然后将重合度高的目标框标记为同一 ID 即可。</p><h3 id="loss-function">1.4. Loss Function</h3><p>　　总的误差由分类误差与回归误差构成： <span class="math display">\[\xi = \sum\left(\alpha \cdot \xi_{cla} + \sum_{i=t,t+1,...,t+n}\xi_{reg}^t\right)\]</span> 这两项误差具体计算与传统的并无很大差别，此外作者还用了 OHEM 的策略，来平衡正负样本量巨大的差异。</p><h2 id="experimental-evaluation">2. Experimental Evaluation</h2><p><img src="/paperreading-Fast-and-Furious/test.png" width="80%" height="80%" title="图 3. ablation study"> 　　作者用了比 kitti 大的数据集，图 3. 所示，late fusion 比 early fusion 效果好一点，但是 late fusion 需要 3D 卷积。其它实验结果可参见文章。</p><p><a id="1" href="#1ref">[1]</a> Luo, Wenjie, Bin Yang, and Raquel Urtasun. &quot;Fast and furious: Real time end-to-end 3d detection, tracking and motion forecasting with a single convolutional net.&quot; Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. 2018.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　动态目标状态估计传统的做法是将其分解为目标检测，目标跟踪，目标运动预测三个子问题进行链式求解，这回导致上游模块的误差在下游模块中会传递并放大。考虑到跟踪与预测能帮助提升检测的性能，比如对于遮挡或远距离目标，跟踪与预测能减少检测的漏检(FN)；而误检(FP)则可通过时域相
      
    
    </summary>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/categories/paper-reading/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-PointPillars</title>
    <link href="https://leijiezhang001.github.io/paperreading-PointPillars/"/>
    <id>https://leijiezhang001.github.io/paperreading-PointPillars/</id>
    <published>2019-09-03T14:08:12.000Z</published>
    <updated>2019-10-07T14:29:47.148Z</updated>
    
    <content type="html"><![CDATA[<h2 id="voxelnet-second-pointpillars">1. VoxelNet-&gt;SECOND-&gt;PointPillars</h2><p>　　相比于图像，激光点云数据是 3D 的，且有稀疏性，所以对点云的前期编码预处理尤其重要，目前大多数算法都是在鸟瞰图下进行点云物体检测，由此对点云的编码预处理主要有两大类方法：</p><ol type="1"><li>以一定的分辨率将点云体素化，每个垂直列中的体素集合被编码成一个固定长度，手工制作的特征，最终形成一个三维的伪图像，以此为代表的方法有 MV3D，AVOD，PIXOR，Complex YOLO；</li><li>PointNet 无序点云处理方式，以此为代表的方法 Frustum PointNet<a href="#1" id="1ref"><sup>[1]</sup></a>, VoxelNet<a href="#2" id="2ref"><sup>[2]</sup></a>，SECOND<a href="#3" id="3ref"><sup>[3]</sup></a>，后两者是在鸟瞰图下进行编码的，需要 3D 卷积运算；</li></ol><p>　　本文提出的 PointPillar<a href="#4" id="4ref"><sup>[4]</sup></a> 是延续 VoxelNet，SECOND 的工作，VoxelNet 将 PointNet(<a href="/PointNet-系列论文详读/" title="PointNet-系列论文详读">PointNet-系列论文详读</a>) 思想引入体素化后的体素特征编码中，然后采用 3D 卷积做特征提取，再用传统的 2D 卷积进行目标检测；SECOND 则考虑到点云特征的稀疏性，用 2D 稀疏卷积代替传统卷积，速度得到了很大的提示。而 PointPillar 则在体素的垂直列上不做分割，从而移除了 3D 卷积的操作，其优点有：</p><ul><li>无手工编码的过程，利用了点云的所有信息，且无需要调节的参数；</li><li>运算均为 2D 卷积，高效；</li><li>可迁移至其它点云数据；</li></ul><p>　　这三篇工作框架结构基本一致，由三部分组成：</p><ol type="1"><li>特征编码网络(Encoder，作特征编码)，在鸟瞰图下，将点云编码为稀疏的伪图像；</li><li>卷积中间网络(Middle，作特征提取)，将伪图像用 backbone 网络进行特征提取；</li><li>区域生成网络(RPN)，也可以是 SSD FPN 等检测头的改进，用于分类和回归 3D 框，与图像检测不一样的地方是，点云鸟瞰图下的最后一层特征层不能很小；</li></ol><p><img src="/paperreading-PointPillars/PointPillar.png" width="100%" height="100%" title="图 3. PointPillar 网络框架"> 　　如图 1. 所示，本文 Pointpillar 主要的工作集中在特征编码网络，所以以下主要介绍其特征编码网络方式，以及实现细节。</p><h2 id="特征编码">2. 特征编码</h2><p>　　Pointpillar 只对 \(x-y\) 平面作 \(H\times W\) 栅格化，栅格化后形成 \(H\times W=P\) 个柱子(Pillar)，每个柱子采样出 \(N\) 个点，每个点编码为 \(D=9\) 维的向量：\(\{x,y,z,r,x_c,y_c,z_c,x_p,y_p \}\)，其中 \(\{x_c,y_c,z_c\}\) 为该点与柱子内所有点的均值点的距离，\(x_p,y_p \) 为该点与柱子中心的距离。综上最后形成\((D,P,N )\) 维的张量，然后用 PointNet 网络输出 \((C,P,N )\) 维的张量，最后用 \(MAX\) 操作输出 \((C,P) = (C,H,W)\) 的伪图像。</p><h2 id="实现细节">3. 实现细节</h2><ol type="1"><li><p>特征编码</p><ul><li>只取有点的柱子，所以 \(P &lt; H\times W\)</li><li>计算量较大，需要并行加速，我复现的时候是将柱子信息离线存下来的</li><li>pointpillar 方式可能只比高度体素采样方式效果高一点</li></ul></li><li><p>训练</p><ul><li>针对不同的类别设定唯一尺寸的 anchor，角度上旋转 90 度，所以每个点上每个类别是有两个 anchor</li><li>正负样本严重不均衡，所以需要 OHEM 或者 focalloss 技术</li></ul></li></ol><p><a id="1" href="#1ref">[1]</a> Qi, Charles R., et al. &quot;Frustum pointnets for 3d object detection from rgb-d data.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.<br><a id="2" href="#2ref">[2]</a> Zhou, Yin, and Oncel Tuzel. &quot;Voxelnet: End-to-end learning for point cloud based 3d object detection.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.<br><a id="3" href="#3ref">[3]</a> Yan, Yan, Yuxing Mao, and Bo Li. &quot;Second: Sparsely embedded convolutional detection.&quot; Sensors 18.10 (2018): 3337.<br><a id="4" href="#4ref">[4]</a> Lang, Alex H., et al. &quot;PointPillars: Fast encoders for object detection from point clouds.&quot; Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;voxelnet-second-pointpillars&quot;&gt;1. VoxelNet-&amp;gt;SECOND-&amp;gt;PointPillars&lt;/h2&gt;
&lt;p&gt;　　相比于图像，激光点云数据是 3D 的，且有稀疏性，所以对点云的前期编码预处理尤其重要，目前大多数算法都是
      
    
    </summary>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/categories/paper-reading/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="Point Cloud" scheme="https://leijiezhang001.github.io/tags/Point-Cloud/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
  </entry>
  
  <entry>
    <title>卡尔曼滤波器在三维目标状态估计中的应用</title>
    <link href="https://leijiezhang001.github.io/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%E5%99%A8%E5%9C%A8%E4%B8%89%E7%BB%B4%E7%9B%AE%E6%A0%87%E7%8A%B6%E6%80%81%E4%BC%B0%E8%AE%A1%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/"/>
    <id>https://leijiezhang001.github.io/卡尔曼滤波器在三维目标状态估计中的应用/</id>
    <published>2019-07-01T12:48:40.000Z</published>
    <updated>2019-10-07T14:29:47.148Z</updated>
    
    <content type="html"><![CDATA[<p>　　目前主流的三维目标的状态估计方法（也可称为 MOT 问题）主要包括三部分：<strong>1. 检测</strong>，出单帧三维目标信息；<strong>2. 跟踪</strong>，前后帧数据关联出 ID 信息；<strong>3. 滤波</strong>，平滑估计状态信息。这里的“跟踪”只是狭义地指出 ID 的过程，“滤波”也就是综述 <a href="/MOT-综述-Multiple-Object-Tracking-A-Literature-Review/" title="Multiple-Object-Tracking-A-Literature-Review">Multiple-Object-Tracking-A-Literature-Review</a> 中提到的 Inference 过程。Inference 过程还可以是基于优化的方法，本文我们主要讨论在自动驾驶中估计动态障碍物状态的扩展卡尔曼滤波方法。</p><h2 id="扩展卡尔曼滤波">1. 扩展卡尔曼滤波</h2><p>　　文章<a href="/卡尔曼滤波详解/" title="卡尔曼滤波详解">卡尔曼滤波详解</a>中已经详细推导了卡尔曼滤波相关原理，这里摘抄如下。非线性系统： <span class="math display">\[\left\{\begin{array}{l}运动方程:\quad x_k=f(x_{k-1},u_k)+w_k \\测量方程:\quad z_k=h(x_k)+v_k\end{array}\tag{1}\right.\]</span> 滤波的两个步骤：</p><ol type="1"><li><strong>预测（Predict）</strong><br>计算先验： <span class="math display">\[\begin{align}\bar{x} _k&amp;=f(\hat{x} _{k-1},u _k) \tag{2}\\\bar{P} _k&amp;=F\hat{P} _kF^T+Q _k \tag{3}\end{align}\]</span></li><li><strong>更新（Update）</strong><br>先计算卡尔曼增益： <span class="math display">\[K_k=\bar{P}_kH_k^T(H_k\bar{P}_kH_k^T+R_k)^{-1} \tag{4}\]</span> 再计算后验概率分布： <span class="math display">\[\begin{align}\hat{x}_k &amp;=\bar{x}_k+K(z_k-h(\bar{x})) \tag{5}\\\hat{P}_k &amp;=(I-KH_k)\bar{P}_k \tag{6}\end{align}\]</span></li></ol><h2 id="非线性系统构建">2. 非线性系统构建</h2><p>　　要构建三维目标状态估计系统，我们得分析状态量 \(x_k\)，测量量 \(z_k\)，输入量 \(u_k\)，状态转移函数（运动学方程）\(f(\cdot)\)，观测函数 \(h(\cdot)\)，以及雅克比矩阵 \(F\)，\(H\) 各是什么。 <img src="/卡尔曼滤波器在三维目标状态估计中的应用/状态量.png" width="25%" height="25%" title="图 1. 目标状态"> 　　如图1所示，<strong>我们严格限定要构建的非线性系统场景：动态目标的状态估计</strong>。对于自动驾驶中的动态目标状态估计，我们关心的状态量有水平面上目标物理位置，朝向，速度，转向速度，加速度，记为： <span class="math display">\[x_k=\begin{bmatrix}x\\y\\\psi\\v\\\dot{\psi}\\a\end{bmatrix}\tag{7}\]</span> 目前主流的 3D 检测方法，能出位置，尺寸，朝向。所以测量量： <span class="math display">\[z_k=\begin{bmatrix}x\\y\\\psi\\\end{bmatrix}\tag{8}\]</span> 以上针对的是目标三维状态估计，如果是本车的状态估计，那么测量量可能可以加上本车的速度等（故限定场景）。文章 <a href="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/" title="[paper_reading]-" stereo vision-based semantic 3d object and ego-motion tracking for autonomous driving"">[paper_reading]-"Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving"</a> 的 3.2.1 章节中提到过一种车辆运动学模型，<strong>该模型定义了输入量：方向盘比率以及加速度（实际使用中均设为0，类似下述质点模型中的 CTRV 模型）；并增加了状态量：方向盘转角</strong>。虽然那篇文章处理的场景与本篇讨论的一致，但是那篇文章采用的是优化方法，方向盘转角可作为优化参数进行求解；而本篇讨论的滤波迭代方法，则很难确定方向盘转角。所以该模型虽然能更好的描述目标，但是可能并不是更有效的（实际中可做实验对比），这里引出几种质点模型。<br>　　<a href="#1" id="1ref">[1]</a>中介绍了几种非线性车辆质点模型：CHCV(Constant Heading and Constant Velocity)，CTRV(Constant Turn Rate and Velocity)，CTRA(Constant Turn Rate and Acceleration)，此外应该还有 CHA(Constant Heading and Acceleration)。这些模型均没有考虑输入量，即： <span class="math display">\[u_k=\mathbb{0}\tag{9}\]</span> 这里我们依次介绍各模型（为了完整性，重写文章 <a href="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/" title="[paper_reading]-" stereo vision-based semantic 3d object and ego-motion tracking for autonomous driving"">[paper_reading]-"Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving"</a> 中的前转向车运动学模型），并导出状态转移函数（运动学方程）\(f(\cdot)\)，观测函数 \(h(\cdot)\)，以及雅克比矩阵 \(F\)，\(H\)。</p><h3 id="chcvconstant-heading-and-constant-velocity">2.1. CHCV(Constant Heading and Constant Velocity)</h3><p>　　该模型下，目标车辆的朝向及速度不变，即 \(\dot{\psi}=0\)。则容易写出，在时间 \(\Delta T\) 内，运动方程\(f(\cdot)\)为： <span class="math display">\[\begin{align}&amp; \begin{bmatrix}x\\y\\\psi\\v\\\dot{\psi}\end{bmatrix} _k=\begin{bmatrix}x+v\,cos(\psi)\Delta T\\y+v\,sin(\psi)\Delta T\\\psi\\v\\0\end{bmatrix} _{k-1} \\\iff &amp;\begin{bmatrix}x\\y\\\psi\\v\\\end{bmatrix} _k=\begin{bmatrix}x+v\,cos(\psi)\Delta T\\y+v\,sin(\psi)\Delta T\\\psi\\v\\\end{bmatrix} _{k-1} \tag{10}\end{align}\]</span> 观测方程\(h(\cdot)\)也可得到： <span class="math display">\[\begin{bmatrix}x\\y\\\psi\\\end{bmatrix} _k=\begin{bmatrix}1 &amp;0 &amp;0 &amp;0\\0 &amp;1 &amp;0 &amp;0\\0 &amp;0 &amp;1 &amp;0\\\end{bmatrix}\begin{bmatrix}x\\y\\\psi\\v\\\end{bmatrix} _{k} \tag{11} \]</span> 由此得到雅克比矩阵： <span class="math display">\[\begin{align}F&amp;=\begin{bmatrix}1 &amp;0 &amp;-v\,sin(\psi)\Delta T &amp;cos(\psi)\Delta T\\0 &amp;1 &amp;v\,cos(\psi)\Delta T &amp;sin(\psi)\Delta T\\0 &amp;0 &amp;1 &amp;0\\0 &amp;0 &amp;0 &amp;1\end{bmatrix} \tag{12} \\H&amp;=\begin{bmatrix}1 &amp;0 &amp;0 &amp;0\\0 &amp;1 &amp;0 &amp;0\\0 &amp;0 &amp;1 &amp;0\\\end{bmatrix} \tag{13}\end{align}\]</span></p><h3 id="ctrvconstant-turn-rate-and-velocity">2.2. CTRV(Constant Turn Rate and Velocity)</h3><p>　　该模型下，目标车辆的(朝向)转向速度及线速度不变，即 \(a=0\)。则分别在 \(x,y\) 方向上，位移积分为： <span class="math display">\[\left\{\begin{array}{l}x=\int_0^{\Delta T} v\,cos(\dot{\psi}t+\psi)dt=\frac{v}{\dot{\psi}}sin(\dot{\psi}t+\psi)\vert_0^{\Delta T}&amp;= \frac{v}{\dot{\psi}}(sin(\dot{\psi}\Delta T+\psi)-sin(\psi)) \\y=\int_0^{\Delta T} v\,sin(\dot{\psi}t+\psi)dt=-\frac{v}{\dot{\psi}}cos(\dot{\psi}t+\psi)\vert_0^{\Delta T}&amp;=\frac{v}{\dot{\psi}}(cos(\psi)-cos(\dot{\psi}\Delta T+\psi)) \\\end{array}\tag{14}\right.\]</span> 由此得到运动方程\(f(\cdot)\)为： <span class="math display">\[\begin{bmatrix}x\\y\\\psi\\v\\\dot{\psi}\end{bmatrix} _k=\begin{bmatrix}x+\frac{v}{\dot{\psi}}(sin(\dot{\psi}\Delta T+\psi)-sin(\psi))\\y+\frac{v}{\dot{\psi}}(cos(\psi)-cos(\dot{\psi}\Delta T+\psi)) \\\psi+\dot{\psi}\Delta T\\v\\\dot{\psi}\end{bmatrix} _{k-1}  \tag{15}\]</span> 观测方程则还是线性方程： <span class="math display">\[\begin{bmatrix}x\\y\\\psi\\\end{bmatrix} _k=\begin{bmatrix}1 &amp;0 &amp;0 &amp;0 &amp;0\\0 &amp;1 &amp;0 &amp;0 &amp;0\\0 &amp;0 &amp;1 &amp;0 &amp;0\\\end{bmatrix}\begin{bmatrix}x\\y\\\psi\\v\\\dot{\psi}\end{bmatrix} _{k} \tag{16} \]</span> 由此得到雅克比矩阵： <span class="math display">\[\begin{align}F&amp;=\begin{bmatrix}1 &amp;0 &amp;\frac{v}{\dot{\psi}}(cos(\dot{\psi}+\psi)-cos(\psi)) &amp;\frac{1}{\dot{\psi}}(sin(\dot{\psi}\Delta T+\psi)-sin(\psi)) &amp;\frac{v\Delta T}{\dot{\psi}}cos(\dot{\psi}\Delta T+\psi)-\frac{v}{\dot{\psi}^2}(sin(\dot{\psi}\Delta T+\psi)-sin(\psi)) \\0 &amp;1 &amp;\frac{v}{\dot{\psi}}(sin(\dot{\psi}+\psi)-sin(\psi)) &amp;\frac{1}{\dot{\psi}}(cos(\psi)-cos(\dot{\psi}\Delta T+\psi)) &amp;\frac{v\Delta T}{\dot{\psi}}sin(\dot{\psi}\Delta T+\psi)-\frac{v}{\dot{\psi}^2}(cos(\psi)-cos(\dot{\psi}\Delta T+\psi)) \\0 &amp;0 &amp;1 &amp;0 &amp;\Delta T\\0 &amp;0 &amp;0 &amp;1 &amp;0\\0 &amp;0 &amp;0 &amp;0 &amp;1\\\end{bmatrix} \tag{17} \\H&amp;=\begin{bmatrix}1 &amp;0 &amp;0 &amp;0 &amp;0\\0 &amp;1 &amp;0 &amp;0 &amp;0\\0 &amp;0 &amp;1 &amp;0 &amp;0\\\end{bmatrix} \tag{18}\end{align}\]</span></p><h3 id="ctraconstant-turn-rate-and-acceleration">2.3. CTRA(Constant Turn Rate and Acceleration)</h3><p>　　该模型下，目标车辆的(朝向)转向速度及线加速度不变。\(x,y\) 方向上的位移积分为： <span class="math display">\[\left\{\begin{array}{l}x&amp;=&amp;\int_0^{\Delta T} (v+at)\,cos(\dot{\psi}t+\psi)dt= \frac{a}{\dot{\psi}^2}cos(\dot{\psi}t+\psi)+\frac{v+at}{\dot{\psi}}sin(\dot{\psi}t+\psi)\vert_0^{\Delta T}\\ &amp;=&amp; \frac{a}{\dot{\psi}^2}cos(\dot{\psi}\Delta T+\psi)+\frac{v+a\Delta T}{\dot{\psi}}sin(\dot{\psi}\Delta T+\psi)-\frac{a}{\dot{\psi}^2}cos(\psi)-\frac{v}{\dot{\psi}}sin(\psi)\\y&amp;=&amp;\int_0^{\Delta T} (v+at)\,sin(\dot{\psi}t+\psi)dt= \frac{a}{\dot{\psi}^2}sin(\dot{\psi}t+\psi)-\frac{v+at}{\dot{\psi}}cos(\dot{\psi}t+\psi)\vert_0^{\Delta T}\\ &amp;=&amp; \frac{a}{\dot{\psi}^2}sin(\dot{\psi}\Delta T+\psi)-\frac{v+a\Delta T}{\dot{\psi}}cos(\dot{\psi}\Delta T+\psi)-\frac{a}{\dot{\psi}^2}sin(\psi)+\frac{v}{\dot{\psi}}cos(\psi)\\\end{array}\tag{19}\right.\]</span> 由此得到运动方程\(f(\cdot)\)为： <span class="math display">\[\begin{bmatrix}x\\y\\\psi\\v\\\dot{\psi}\\a\end{bmatrix} _k=\begin{bmatrix}x+\frac{a}{\dot{\psi}^2}cos(\dot{\psi}\Delta T+\psi)+\frac{v+a\Delta T}{\dot{\psi}}sin(\dot{\psi}\Delta T+\psi)-\frac{a}{\dot{\psi}^2}cos(\psi)-\frac{v}{\dot{\psi}}sin(\psi)\\y+\frac{a}{\dot{\psi}^2}sin(\dot{\psi}\Delta T+\psi)-\frac{v+a\Delta T}{\dot{\psi}}cos(\dot{\psi}\Delta T+\psi)-\frac{a}{\dot{\psi}^2}sin(\psi)+\frac{v}{\dot{\psi}}cos(\psi) \\\psi+\dot{\psi}\Delta T\\v+a\Delta T\\\dot{\psi}\\a\end{bmatrix} _{k-1}  \tag{20}\]</span> 观测方程则还是线性方程： <span class="math display">\[\begin{bmatrix}x\\y\\\psi\\\end{bmatrix} _k=\begin{bmatrix}1 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0\\0 &amp;1 &amp;0 &amp;0 &amp;0 &amp;0\\0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;0\\\end{bmatrix}\begin{bmatrix}x\\y\\\psi\\v\\\dot{\psi}\\a\end{bmatrix} _{k} \tag{21} \]</span> 同理可得到雅克比矩阵，由于页面限制，这里不再写出。</p><h3 id="chaconstant-heading-and-acceleration">2.4. CHA(Constant Heading and Acceleration)</h3><p>　　该模型下，此时目标车辆的朝向及线加速度不变，即 \(\dot{\psi}=0\)。\(x,y\) 方向上的位移积分为： <span class="math display">\[\left\{\begin{array}{l}x=\int_0^{\Delta T} (v+at)\,cos(\psi)dt= \left(vt+\frac{1}{2}at^2\right)cos(\psi)\vert_0^{\Delta T}&amp;= \left(v\Delta T+\frac{1}{2}a\Delta T^2 \right)cos(\psi) \\y=\int_0^{\Delta T} (v+at)\,sin(\psi)dt= \left(vt+\frac{1}{2}at^2\right)sin(\psi)\vert_0^{\Delta T}&amp;= \left(v\Delta T+\frac{1}{2}a\Delta T^2 \right)sin(\psi)  \\\end{array}\tag{22}\right.\]</span> 由此得到运动方程\(f(\cdot)\)为： <span class="math display">\[\begin{bmatrix}x\\y\\\psi\\v\\a\end{bmatrix} _k=\begin{bmatrix}x+\left(v\Delta T+\frac{1}{2}a\Delta T^2 \right)cos(\psi)\\y+\left(v\Delta T+\frac{1}{2}a\Delta T^2 \right)sin(\psi) \\\psi\\v+a\Delta T\\a\end{bmatrix} _{k-1}  \tag{23}\]</span> 观测方程则还是线性方程： <span class="math display">\[\begin{bmatrix}x\\y\\\psi\\\end{bmatrix} _k=\begin{bmatrix}1 &amp;0 &amp;0 &amp;0 &amp;0\\0 &amp;1 &amp;0 &amp;0 &amp;0\\0 &amp;0 &amp;1 &amp;0 &amp;0\\\end{bmatrix}\begin{bmatrix}x\\y\\\psi\\v\\a\end{bmatrix} _{k} \tag{24} \]</span> 由此得到雅克比矩阵： <span class="math display">\[\begin{align}F&amp;=\begin{bmatrix}1 &amp;0 &amp;-\left(v\Delta T+\frac{1}{2}a\Delta T^2 \right)sin(\psi) &amp;\Delta Tcos(\psi) &amp;\frac{1}{2}\Delta T^2cos(\psi) \\0 &amp;1 &amp;\left(v\Delta T+\frac{1}{2}a\Delta T^2 \right)cos(\psi) &amp;\Delta Tsin(\psi) &amp;\frac{1}{2}\Delta T^2sin(\psi)\\0 &amp;0 &amp;1 &amp;0 &amp;0\\0 &amp;0 &amp;0 &amp;1 &amp;\Delta T \\0 &amp;0 &amp;0 &amp;0 &amp;1\end{bmatrix} \tag{25} \\H&amp;=\begin{bmatrix}1 &amp;0 &amp;0 &amp;0 &amp;0\\0 &amp;1 &amp;0 &amp;0 &amp;0\\0 &amp;0 &amp;1 &amp;0 &amp;0\\\end{bmatrix} \tag{26}\end{align}\]</span></p><h3 id="前转向车模型">2.5. 前转向车模型</h3><p>　　这里给出文章 <a href="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/" title="[paper_reading]-" stereo vision-based semantic 3d object and ego-motion tracking for autonomous driving"">[paper_reading]-"Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving"</a> 中的前转向车运动学模型，推导过程可见文章。这里令方向盘角度比率 \(\gamma\) 以及加速度 \(a\) 为 0，所以本质上也是个 CTRV 模型。<br>　　引入状态变量方向盘/车轮角度 \(\delta\)（与朝向转速 \(\psi\) 类似），运动方程\(f(\cdot)\)为： <span class="math display">\[\begin{bmatrix}x\\y\\\psi\\v\\\delta\end{bmatrix} _k=\begin{bmatrix}x+cos(\psi)v\Delta T\\y+sin(\psi)v\Delta T \\\psi+\frac{tan(\delta)}{L}v\Delta T\\v\\\delta\end{bmatrix} _{k-1}  \tag{27}\]</span> 观测方程则还是线性方程： <span class="math display">\[\begin{bmatrix}x\\y\\\psi\\\end{bmatrix} _k=\begin{bmatrix}1 &amp;0 &amp;0 &amp;0 &amp;0\\0 &amp;1 &amp;0 &amp;0 &amp;0\\0 &amp;0 &amp;1 &amp;0 &amp;0\\\end{bmatrix}\begin{bmatrix}x\\y\\\psi\\v\\\delta\end{bmatrix} _{k} \tag{28} \]</span> 由此得到雅克比矩阵： <span class="math display">\[\begin{align}F&amp;=\begin{bmatrix}1 &amp;0 &amp;-sin(\psi)v\Delta T &amp;cos(\psi)\Delta T &amp;0 \\0 &amp;1 &amp;cos(\psi)v\Delta T  &amp;sin(\psi)\Delta T &amp;0  \\0 &amp;0 &amp;1 &amp;\frac{tan(\delta)}{L}\Delta T &amp;\frac{v}{Lcos^2(\delta)}\Delta T\\0 &amp;0 &amp;0 &amp;1 &amp;0\\0 &amp;0 &amp;0 &amp;0 &amp;1\\\end{bmatrix} \tag{29} \\H&amp;=\begin{bmatrix}1 &amp;0 &amp;0 &amp;0 &amp;0\\0 &amp;1 &amp;0 &amp;0 &amp;0\\0 &amp;0 &amp;1 &amp;0 &amp;0\\\end{bmatrix} \tag{30}\end{align}\]</span></p><h2 id="状态及参数初始化">3. 状态及参数初始化</h2><p>　　以上介绍了四个质点模型以及一个前转向模型，当然还有更复杂的模型，但是对于目标车辆的状态估计，由于观测量有限，而且也不能知道输入量（如果 V2X 能够实现，那就知道目标车辆的更多状态信息了），所以这些模型也基本够用。<br>　　模型构建好之后，为了迭代，还需初始化各个状态量及协方差参数矩阵。初始化值不对，会导致迭代发散，这里初始化就会有几个问题：</p><ul><li>无法观测的状态量较难初始化，如转向速度，线加速度等；</li><li>观测不稳定的状态量较难初始化，如目标有截断的情况下；</li><li>协方差矩阵较难初始化，如状态量的协方差矩阵；</li></ul><p>　　前两个问题需要在工程实践中优化；最后一个问题（<strong>非常重要</strong>）可以让检测网络同时出预测值的不确定性(Uncertainty)，这也是深度学习中一个较为系统性的工作，后面文章再做介绍。</p><p><a id="1" href="#1ref">[1]</a> https://github.com/balzer82/Kalman</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　目前主流的三维目标的状态估计方法（也可称为 MOT 问题）主要包括三部分：&lt;strong&gt;1. 检测&lt;/strong&gt;，出单帧三维目标信息；&lt;strong&gt;2. 跟踪&lt;/strong&gt;，前后帧数据关联出 ID 信息；&lt;strong&gt;3. 滤波&lt;/strong&gt;，平滑估计
      
    
    </summary>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/categories/MOT/"/>
    
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/tags/MOT/"/>
    
      <category term="tracking" scheme="https://leijiezhang001.github.io/tags/tracking/"/>
    
  </entry>
  
  <entry>
    <title>卡尔曼滤波详解</title>
    <link href="https://leijiezhang001.github.io/%E5%8D%A1%E5%B0%94%E6%9B%BC%E6%BB%A4%E6%B3%A2%E8%AF%A6%E8%A7%A3/"/>
    <id>https://leijiezhang001.github.io/卡尔曼滤波详解/</id>
    <published>2019-06-25T07:54:26.000Z</published>
    <updated>2019-10-07T14:29:47.148Z</updated>
    
    <content type="html"><![CDATA[<p>　　状态估计问题是指，基于初始状态信息，一系列观测量，一系列输入量，以及系统的运动模型和观测模型，来计算系统在某时刻的真实状态的估计值。卡尔曼滤波及其相关卡尔曼滤波算法是状态估计的重要方法。本文介绍卡尔曼滤波(Kalman Filter)，扩展卡尔曼滤波(Extended Kalman Filter)。</p><h2 id="卡尔曼滤波">1. 卡尔曼滤波</h2><h3 id="线性高斯系统">1.1. 线性高斯系统</h3><p>　　卡尔曼滤波是线性高斯系统的最优无偏估计，定义离散线性高斯系统： <span class="math display">\[\left\{\begin{array}{l}运动方程:\quad x_k=A_kx_{k-1}+B_ku_k+w_k\\测量方程:\quad z_k=C_kx_k+v_k\end{array}\tag{1}\right.\]</span> 其中矩阵 \(A_k\) 为转移矩阵（transition matrix），设矩阵 \(B_k=I\) 为控制矩阵，矩阵 \(C_k\) 为观测矩阵(observation matrix)。并且所有状态和噪声均满足高斯分布： <span class="math display">\[\begin{align}过程噪声: \quad &amp; w_k \sim N(0,Q_k)\\测量噪声: \quad &amp; v_k \sim N(0,R_k)\end{align}\]</span> 卡尔曼滤波估计线性高斯系统的状态分为两个步骤：</p><ol type="1"><li><strong>预测（Predict）</strong><br>计算先验： <span class="math display">\[\begin{align}\bar{x}_ k &amp;=A_ k\hat{x}_ {k-1}+u_ k \tag{2}\\\bar{P}_ k &amp;=A_ k\hat{P}_ {k-1}A_ k^T+Q_ k \tag{3}\end{align}\]</span></li><li><strong>更新（Update）</strong><br>先计算卡尔曼增益： <span class="math display">\[K_k=\bar{P}_kC_k^T(C_k\bar{P}_kC_k^T+R_k)^{-1} \tag{4}\]</span> 再计算后验概率分布： <span class="math display">\[\begin{align}\hat{x}_k &amp;=\bar{x}_k+K(z_k-C_k\bar{x}) \tag{5}\\\hat{P}_k &amp;=(I-KC_k)\bar{P}_k \tag{6}\end{align}\]</span></li></ol><p>以下通过三种方式来推导出卡尔曼滤波器。</p><h3 id="通过-map贝叶斯推断推导123">1.2. 通过 MAP/贝叶斯推断推导<a href="#1" id="1ref"><sup>[1]</sup></a><a href="#2" id="2ref"><sup>[2]</sup></a><a href="#3" id="3ref"><sup>[3]</sup></a></h3><p>　　状态估计问题的概率解释就是用 \(0\) 到 \(k\) 的数据（包括初始状态，观测量，输入量）来估计当前时刻的状态分布：\(P(x_k\vert x_0,u_{1:k},z_{1:k})\)。根据贝叶斯法则： <span class="math display">\[P(x_k\vert x_0,u_{1:k},z_{1:k}) \propto P(z_k\vert x_k)P(x_k\vert x_0,u_{1:k},z_{1:k-1})\tag{7}\]</span> 这三项分别为后验概率，似然，先验概率。所以状态估计可转换为该后验概率最大化（Maximize a Posterior，MAP）问题。MAP 相当于最大化似然与先验的乘积。似然由测量方程给出，先验有运动方程给出。先验部分如果考虑历史所有信息，那么可以用非线性优化框架来解；如果只考虑一阶马尔科夫性，那么就是卡尔曼滤波方法，前述线性高斯系统就满足一阶马尔科夫性。<br>　　该系统下，假设已知 \(k-1\) 时刻的后验状态估计 \(\hat{x}_ {k-1}\) 及其协方差 \(\hat{P}_ {k-1}\)，现在要根据 \(k\) 时刻的输入和观测数据，确定 \(x_k\) 的后验分布。这里以 \((\hat{\cdot})\) 表示后验分布，\((\bar{\cdot})\) 表示先验分布。<br>　　卡尔曼滤波器首先通过(1)中的运动方程确定 \(x_k\) 的先验分布，即预测过程。将 \(k-1\) 时刻的分布通过运动方程传递，对于均值有： <span class="math display">\[\begin{align}\bar{x}_k &amp;=E[x_k]=E[A_kx_{k-1}+u_k+w_k]\\&amp;=A_kE[x_{k-1}]+u_k+E[w_k]\\&amp;=A_k\hat{x}_{k-1}+u_k\end{align}\]</span> 对于协方差有： <span class="math display">\[\begin{align}\bar{P}_k &amp;=E\left[(x_k-E[x_k])(x_k-E[x_k])^T\right]\\&amp;=E\left[(A_kx_{k-1}+u_k+w_k-A_k\hat{x}_{k-1}-u_k)\cdot(A_kx_{k-1}+u_k+w_k-A_k\hat{x}_{k-1}-u_k)^T\right]\\&amp;=A_kE\left[(x _{k-1}-\hat{x} _{k-1})\cdot(x _{k-1}-\hat{x} _{k-1})^T\right]A_k^T+E[w_kw_k^T]\\&amp;=A_k\hat{P} _{k-1}A _{k-1}^T+Q _k\end{align}\]</span></p><p>由此可得<strong>预测过程</strong>： <span class="math display">\[\begin{align}&amp;P(x_k\vert x_0,u_{1:k},z_{1:k-1})=N\left(A_k\hat{x}_{k-1}+u_k,A_k\hat{P}_{k-1}A_k^T+Q_k\right)\tag{8}\\\iff &amp;公式 (2),(3)\end{align}\]</span> 　　另一方面，通过(1)中的观测方程，可以得到在某个状态下观测数据应该为： <span class="math display">\[P(z_k\vert x_k)=N(C_kx_k,R)\tag{9}\]</span> 由公式(7)可知，状态的后验概率分布由预测量以及测量量融合得到，这个融合的过程是两个高斯状的概率分布进行相乘，即 \(x_k\) 的后验概率： <span class="math display">\[N(\hat{x}_k,\hat{P}_k)=N(C_kx_k,R)\cdot N(\bar{x}_k,\bar{P}_k)\tag{10}\]</span> 比较该式指数部分即可得到<strong>更新过程</strong>： <span class="math display">\[\begin{align}&amp; (x_k-\hat{x}_k)^T\hat{P}_k^{-1}(x_k-\hat{x}_k)=(z_k-C_kx_k)^TR^{-1}(z_k-C_kx_k)+(x_k-\bar{x}_k)^T\bar{P}_k^{-1}(x_k-\bar{x}_k)\\\iff &amp; \left\{\begin{array}{l}二次项系数:\quad \hat{P}_k^{-1}=C_k^TR^{-1}C_k+\bar{P}_k^{-1}\\一次项系数:\quad 2\hat{x}_k^T\hat{P}_k^{-1}x_k=2z_k^TR^{-1}C_kx_k+2\bar{x}_k^T\bar{P}_k^{-1}x_k\end{array}\right. \tag{11} \\\iff &amp; \left\{\begin{array}{l}I=\hat{P}_kC_k^TR^{-1}C_k+\hat{P}_k\bar{P}_k^{-1}\\\hat{x}_k=\hat{P}_kC_k^TR^{-1}z_k+\hat{P}_k\bar{P}_k^{-1}\bar{x}_k\end{array}\right. 令 K=\hat{P}_kC_k^TR^{-1} \\\iff &amp; \left\{\begin{array}{l}I=KC_k+\hat{P}_k\bar{P}_k^{-1}\\\hat{x}_k=Kz_k+(I-KC_k)\bar{x}_k\end{array}\right. \\\iff &amp; 式 (4),(5),(6)\end{align}\]</span> 　　对于更新过程，<a href="#3" id="3ref">[3]</a>中提出了另一种更加形象的证明方法。如图1所示，容易得到小车模型的运动方程： <span class="math display">\[\begin{bmatrix}x _k\\\dot{x} _k\\\end{bmatrix}=\begin{bmatrix}1 &amp; \Delta k\\0 &amp; 1\\\end{bmatrix}\begin{bmatrix}x _{k-1}\\\dot{x} _{k-1}\\\end{bmatrix}+\begin{bmatrix}\frac{(\Delta k)^2}{2}\\\Delta k\\\end{bmatrix}a_k\]</span> 其中 \(a_k\) 为加速度输入量，对比式(1)也容易得到转移矩阵与控制矩阵。预测过程的证明方式与上述一致，下面简述其更新过程的证明，详见<a href="#3" id="3ref">[3]</a>。 <img src="/卡尔曼滤波详解/小车.png" width="100%" height="100%" title="图 1. 小车模型"> 　　如图1所示，红色区域代表预测量 \({x}_k\) 的概率分布高斯函数；蓝色代表测量量 \(z_k\) 概率分布的高斯函数，测量装置为左侧的 ToF 装置，单位为秒。绿色代表状态的后验概率分布 \(_k\)，由预测量的概率(先验)与测量量的概率(似然)相乘得到。由式(10)可知，两个高斯函数相乘还是高斯函数（但是是尺度变化的高斯函数，Scaled Gaussian<a href="#4" id="4ref"><sup>[4]</sup></a>），上面的证明过程直接比较二次项与一次项，这里是直接写出新的高斯分布均值方差与另两个高斯分布均值方差的关系，<strong>本质上都是比较自变量前面的系数</strong>，非系数是不相等的，还有 Scaled 项。由此可得到更新过程。要注意的是，高斯分布相乘时，要注意单位的转换(<strong>即需要满足式(10)的单位形式</strong>)，这里的观察矩阵就是基于测量装置的测量单位(秒)与状态单位(米，米/秒)的转换值。</p><h2 id="扩展卡尔曼滤波">2. 扩展卡尔曼滤波</h2><h3 id="非线性非高斯系统">2.1. 非线性非高斯系统</h3><p>　　通常系统（如 SLAM）的运动方程和观测方程是非线性函数，写成一般形式： <span class="math display">\[\left\{\begin{array}{l}运动方程:\quad x_k=f(x_{k-1},u_k)+w_k\\测量方程:\quad z_k=h(x_k)+v_k\end{array}\tag{12}\right.\]</span> 扩展卡尔曼滤波估计非线性系统的状态与卡尔曼滤波类似，也分为两个步骤：</p><ol type="1"><li><strong>预测（Predict）</strong> 计算先验： <span class="math display">\[\begin{align}\bar{x} _k&amp;=f(\hat{x} _{k-1},u _k) \tag{13}\\\bar{P} _k&amp;=F\hat{P} _kF^T+Q _k \tag{14}\end{align}\]</span></li><li><strong>更新（Update）</strong> 先计算卡尔曼增益： <span class="math display">\[K_k=\bar{P}_kH_k^T(H_k\bar{P}_kH_k^T+R_k)^{-1} \tag{15}\]</span> 再计算后验概率分布： <span class="math display">\[\begin{align}\hat{x}_k &amp;=\bar{x}_k+K(z_k-h(\bar{x})) \tag{16}\\\hat{P}_k &amp;=(I-KH_k)\bar{P}_k \tag{17}\end{align}\]</span></li></ol><h3 id="通过-map贝叶斯推断推导12">2.2. 通过 MAP/贝叶斯推断推导<a href="#1" id="1ref"><sup>[1]</sup></a><a href="#2" id="2ref"><sup>[2]</sup></a></h3><p>　　在某个点附件考虑运动方程与观测方程的一阶泰勒展开，只保留一阶项，即线性部分，然后按照线性系统进行推导。在 \(k\) 时刻，将运动方程和观测方程在 \(\hat{x}_ {k-1},\hat{P}_ {k-1}\) 处进行线性化： <span class="math display">\[\left\{\begin{array}{l}运动方程:\quad x_k\approx f(\hat{x}_{k-1},u_k)+F(x_{k-1}-\hat{x}_{k-1})+w_k\\测量方程:\quad z_k\approx h(\bar{x}_k)+H(x_k-\bar{x}_k)+v_k\end{array}\tag{18}\right.\]</span> 其中 \(F=\left.\frac{\partial f}{\partial x_{k-1}}\right\arrowvert_{\hat{x}_ {k-1}}\)， \(H=\left.\frac{\partial h}{\partial x_k}\right\arrowvert_{\bar{x}_ k}\)。<br>　　由此可得<strong>预测过程</strong>: <span class="math display">\[\begin{align}&amp;P(x_k\vert x_0,u_{1:k},z_{1:k-1})=N\left(f(\hat{x}_{k-1},u_k),F\hat{P}_{k-1}F^T+Q_k\right)\tag{19}\\\iff &amp;公式 (13),(14)\end{align}\]</span> 　　另一方面，通过(18)中的观测方程，可以得到在某个状态下观测数据应该为： <span class="math display">\[P(z_k\vert x_k)=N(h(\bar{x})+H(x_k-\bar{x}_k),R)\tag{20}\]</span> 由贝叶斯公式，可得 \(x_k\) 的后验概率： <span class="math display">\[N(\hat{x}_k,\hat{P}_k)=N(h(\bar{x})+H(x_k-\bar{x}_k),R))\cdot N(\bar{x}_k,\bar{P}_k)\tag{21}\]</span> 类似卡尔曼推导过程，由此可得到更新过程式(15)，(16)，(17)。</p><p><a id="1" href="#1ref">[1]</a> 高翔, 张涛, 颜沁睿, 刘毅, 视觉SLAM十四讲：从理论到实践, 电子工业出版社, 2017<br><a id="2" href="#2ref">[2]</a> T. D. Barfoot. State Estimation for Robotics. Cambridge University Press, 2017.<br><a id="3" href="#3ref">[3]</a> Faragher, Ramsey. &quot;Understanding the basis of the Kalman filter via a simple and intuitive derivation.&quot; IEEE Signal processing magazine 29.5 (2012): 128-132.<br><a id="4" href="#4ref">[4]</a> Bromiley, Paul. &quot;Products and convolutions of Gaussian probability density functions.&quot; Tina-Vision Memo 3.4 (2003): 1.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　状态估计问题是指，基于初始状态信息，一系列观测量，一系列输入量，以及系统的运动模型和观测模型，来计算系统在某时刻的真实状态的估计值。卡尔曼滤波及其相关卡尔曼滤波算法是状态估计的重要方法。本文介绍卡尔曼滤波(Kalman Filter)，扩展卡尔曼滤波(Extended 
      
    
    </summary>
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/categories/SLAM/"/>
    
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/tags/SLAM/"/>
    
  </entry>
  
  <entry>
    <title>KLT 光流算法详解</title>
    <link href="https://leijiezhang001.github.io/KLT/"/>
    <id>https://leijiezhang001.github.io/KLT/</id>
    <published>2019-06-19T10:58:47.000Z</published>
    <updated>2019-10-07T14:29:47.140Z</updated>
    
    <content type="html"><![CDATA[<p>　　光流（Optical Flow）是物体在三维空间中的运动（运动场）在二维图像平面上的投影，由物体与相机的相对速度产生，反映了微小时间内物体对应的图像像素的运动方向和速度。<br>　　KLT 是基于光流原理的一种特征点跟踪算法，本文首先介绍光流原理，然后介绍 KLT 及相关 KLT 变种算法。</p><h2 id="optical-flow">1. Optical Flow</h2><p>　　光流法假设：</p><ul><li>亮度恒定，图像中物体的像素亮度在连续帧之间不会发生变化；</li><li>短距离(短时)运动，相邻帧之间的时间足够短，物体运动较小；</li><li>空间一致性，相邻像素具有相似的运动；</li></ul><p>　　记 \(I(x,y,t)\) 为 \(t\) 时刻像素点 \((x,y)\) 的像素值，那么根据前两个假设，可得到： <span class="math display">\[I(x,y,t)=I(x+dx,y+dy,t+dt)\]</span> 一阶泰勒展开： <span class="math display">\[I(x+dx,y+dy,t+dt)=I(x,y,t)+\frac{\partial I}{\partial x}dx+\frac{\partial I}{\partial y}dy+\frac{\partial I}{\partial t}dt\]</span> 由此可得： <span class="math display">\[\frac{\partial I}{\partial x}dx+\frac{\partial I}{\partial y}dy+\frac{\partial I}{\partial t}dt=0 \iff \frac{\partial I}{\partial x}\frac{dx}{dt}+\frac{\partial I}{\partial y}\frac{dy}{dt}=-\frac{\partial I}{\partial t}\]</span> 记 \(\left(\frac{dx}{dt},\frac{dy}{dt}\right)=(u,v)\)，即为所要求解的像素光流；\(\left(\frac{\partial I}{\partial x},\frac{\partial I}{\partial y}\right)=(I_x,I_y)\) 为像素灰度空间微分；\(\frac{\partial I}{\partial t}=I_x\) 为像素坐标点的时间灰度微分。整理成矩阵形式： <span class="math display">\[\begin{bmatrix}I_x &amp;I_y\\\end{bmatrix}\begin{bmatrix}u\\v\\\end{bmatrix}=-I_t\]</span> 该式表示相同坐标位置的时间灰度微分是空间灰度微分与这个位置上相对于观察者的速度的乘积。由空间一致性假设，对于周围多个点，有： <span class="math display">\[\begin{bmatrix}I_{x1} &amp;I_{y1}\\I_{x2} &amp;I_{y2}\\I_{x3} &amp;I_{y3}\\\vdots &amp;\vdots \\\end{bmatrix}\begin{bmatrix}u\\v\\\end{bmatrix}=-\begin{bmatrix}I_{t1}\\I_{t2}\\\vdots\\\end{bmatrix} \iff A\vec{u}=b\]</span> 这是标准的线性方程组，可用最小二乘法求解 \(\vec{u}=\left(A^ TA\right)^ {-1}A^ Tb\)，也可以迭代求解。这种方式得到的光流，称为 Lucas-Kanade 算法。</p><h2 id="klt">2. KLT</h2><p>　　KLT 算法本质上也基于光流的三个假设，不同于前述直接比较像素点灰度值的作法，KLT 比较像素点周围的窗口像素，来寻找最相似的像素点。由光流假设，在很短时间 \(\tau\) 内，前后两帧图像满足： <span class="math display">\[J(A\mathrm{x}+d)=I(\mathrm{x}), 其中 A=1+D=1+\begin{bmatrix}d_{xx} &amp; d_{xy}\\d_{yx} &amp; d_{yy}\\\end{bmatrix}\]</span> 像素位移(displacement)向量满足仿射运动模型(Affine Motion) \(=Dx+d\)，其中 \(D\) 称为变形矩阵(Deformation Matrix)，\(d\) 称为位移向量(Displacement Vector)。\(D\) 表示两个像素窗口块运动后的变形量，所以当窗口较小时，会比较难估计。通常 \(D\) 可以用来衡量两个像素窗口的相似度，即衡量特征点有没有漂移。而对于光流跟踪量，一般只考虑平移模型(Translation Model)： <span class="math display">\[J(\mathrm{x}+d)=I(\mathrm{x})\]</span> 　　为了普遍性，我们用仿射运动模型来推到 KLT 算法原理。在像素窗口下，构造误差函数： <span class="math display">\[\epsilon=\iint_W [J(A\mathrm{x}+d)-I(x)]^2 w(\mathrm{x})d\mathrm{x}\]</span> 其中 \(w(\mathrm{x})\) 是权重函数，可定义为高斯形式。上式分别对变量 \(D\) 和 \(d\) 求导： <span class="math display">\[\left\{\begin{array}{l}\frac{\partial \epsilon}{\partial D}=2\iint_W[J(A\mathrm{x}+d)-I(\mathrm{x})]g\,\mathrm{x}^T\,w\,d\mathrm{x}&amp;=0\\\frac{\partial \epsilon}{\partial d}=2\iint_W[J(A\mathrm{x}+d)-I(\mathrm{x})]g\,w\,d\mathrm{x}&amp;=0\\\end{array}\right.\]</span> 其中 \(g=\left(\frac{\partial J}{\partial x},\frac{\partial J}{\partial y}\right)^ T\)。记光流 \(u=D\mathrm{x}+d\)，则对运动后的像素点进行泰勒展开： <span class="math display">\[J(A\mathrm{x}+d)=J(x)+g^T(u)\]</span> 仿射运动模型结果可见<a href="#1" id="1ref">[1]</a><a href="#5" id="5ref">[5]</a>，这里给出平移运动模型结果。令 \(D=0\)： <span class="math display">\[\begin{align}&amp;\iint_W[J(A\mathrm{x}+d)-I(\mathrm{x})]g\,w\,d\mathrm{x}=0\\\iff &amp;\iint_W[J(\mathrm{x})-I(\mathrm{x})]g\,w\,d\mathrm{x}=-\iint_Wg^T\,\mathrm{d}\,g\,w\,d\mathrm{x}=-\left[\iint_Wg\,g^T\,w\,d\mathrm{x}\right]\mathrm{d}\\\iff &amp;Z\mathrm{d}=e\end{align}\]</span> 其中 \(Z\) 是 \(2\times 2\) 矩阵，\(e\) 是 \(2\times 1\) 向量。这是线性方程组优化问题，当 \(Z\) 可逆时，这个方程可容易求解。因为推导过程用到了泰勒展开，所以只有当像素位移较小时，才成立。实际操作中，一般迭代式的来求解，每次用上次结果做初始化，进一步求解(In a Newton-Raphson Fasion)。</p><h2 id="pyramidal-iterative-klt">3. Pyramidal Iterative KLT</h2><p>　　以上标准的迭代式 KLT 计算过程只在位移较小时成立（泰勒展开），所以需要更优的金字塔式迭代求解。图像金字塔有多重定义方式，这里定义： <span class="math display">\[\begin{align}I^L(x,y)&amp;=\frac{1}{4}I^{L-1}(2x,2y)\\&amp;+\frac{1}{8}\left(I^{L-1}(2x-1,2y)+I^{L-1}(2x+1,2y)+I^{L-1}(2x,2y-1)+I^{L-1}(2x,2y+1)\right)\\&amp;+\frac{1}{16}\left(I^{L-1}(2x-1,2y-1)+I^{L-1}(2x+1,2y+1)+I^{L-1}(2x-1,2y+1)+I^{L-1}(2x+1,2y-1)\right)\end{align}\]</span> 　　特征点跟踪有两个关键指标：<strong>准确性(accuracy)</strong>，以及<strong>鲁棒性(robustness)</strong>。大的窗口，对大的运动量比较鲁棒，但是为了提高准确性，又不得不减小窗口。所以窗口的选择需要权衡跟踪准确性与鲁棒性。金字塔迭代 KLT 则能有效弱化窗口的局限性。这里介绍平移模型下金字塔迭代 KLT 算法，仿射模型算法过程可见<a href="#1" id="1ref">[1]</a><a href="#5" id="5ref">[5]</a>。<br>　　定义金字塔迭代 KLT 算法的目标：图像 \(I\) 中某坐标点 \(\mathrm{x}\)，在图像 \(J\) 中找到其对应点 \(\mathrm{}\)。算法流程为：</p><blockquote><p>建立图像金字塔：\(\{I^ L\}_ {L=0,...,L_m}\)，\(\{J^ L\}_ {L=0,...,L_m}\)<br>初始化光流在金字塔之间的传递值：\(g^ {L_m}=[g_x^ {L_m},g_y^ {L_m}]^ T=[0,0]^ T\)<br><strong>for \(L=L_m\) down to 0 with step of -1</strong></p><blockquote><p>计算图像 \(I^ L\) 中的 \(\mathrm{x}\) 坐标: \(\mathrm{x}^ L=[x,y]^ T=\mathrm{x}/2^ L\)<br>计算空间梯度矩阵 \(Z\)<br>初始化 KLT 迭代值：\(v^ 0=[0,0]^ T\)<br><strong>for \(k=1\) to \(K\) with step of 1</strong> or until \(\Vert\eta^ k\Vert\) &lt; accuracy threshold</p><blockquote><p>计算图像差矩阵 \(I^ L(\mathrm{x}^ L)-J^ L(\mathrm{x}^ L)=I^ L(x,y)-J^ L(x+g_x^ L+v_x^ {k-1},y+g_y^ L+v_y^ {k-1})\)<br>计算图像差矩阵 \(e_k\)<br>计算光流 \(\eta^ k=Z^ {-1}e_k\)<br>更新下次迭代的初值 \(v^ k=v^ {k-1}+\eta^ k\)</p></blockquote><p><strong>end of for-loop on k</strong><br>第 \(L\) 层金字塔下光流为：\(\mathrm{d}^ L=v^ K\)<br>初始化第 \(L-1\) 层金字塔的光流： \(g^ {L-1}=[g_x^ {L-1}, g_y^ {L-1}]^ T=2(g^ L+\mathrm{d}^ L)\)</p></blockquote><p><strong>end of for-loop on L</strong> 最终的光流结果：\(\mathrm{d}=g^ 0+\mathrm{d}^ 0\)<br>对应的 \(J\) 上的坐标点为：\(\hat{\mathrm{x}}=\mathrm{x}+\mathrm{d}\)</p></blockquote><h2 id="feature-selection">4. Feature Selection</h2><p>　　在特征点跟踪之前，特征点的选择也很重要，以上计算过程中，我们期望 \(Z\) 可逆，也就是其最小特征值要足够大。如果已经提取了角点，则可进一步做选择。因此特征点选择准则为：</p><ol type="1"><li>计算图像每个像素(或已提取的角点)的 \(Z\) 矩阵，及其最小的特征值 \(\lambda_m\)</li><li>从所有 \(\lambda_m\) 中取最大值为 \(\lambda_{max}\)</li><li>保留 \(\lambda_m\) 大于一定百分比(10%) \(\lambda_{max}\) 的像素(角点)</li><li>在这些像素(角点)中，保留局部最大值</li><li>视计算能力，保留其中的子集</li></ol><p>以上特征点提取的过程类似于 <a href="https://blog.csdn.net/u010103202/article/details/73331440" target="_blank" rel="noopener">Harris 角点</a>。要注意的是选择特征计算 \(Z\) 时，\(3\times3\) 窗口足够，但是跟踪时，一般大于 \(3\times3\)。</p><h2 id="dissimilarity">5. Dissimilarity</h2><p>　　相似性度量决定该特征点是否已经漂移而不能使用了，即外点检测(Outlier Detection)，所以非常重要。相比于平移模型，仿射模型对特征点的相似性度量更有效果。在长距离跟踪下，相似性度量可能解决不了是否漂移的问题，但是好的相似性度量能从一开始就剔除漂移的特征点。此外，也可用其它更高层面的外点检测技术替代。</p><p><a id="1" href="#1ref">[1]</a> Shi, Jianbo, and Carlo Tomasi. Good features to track. Cornell University, 1993.<br><a id="2" href="#2ref">[2]</a> Birchfield, Stan. &quot;Derivation of kanade-lucas-tomasi tracking equation.&quot; unpublished notes (1997).<br><a id="3" href="#3ref">[3]</a> Bouguet, J.-Y.. “Pyramidal implementation of the lucas kanade feature tracker.” (2000).<br><a id="4" href="#4ref">[4]</a> Suhr, Jae Kyu. &quot;Kanade-lucas-tomasi (klt) feature tracker.&quot; Computer Vision (EEE6503) (2009): 9-18.<br><a id="5" href="#5ref">[5]</a> Bouguet, Jean-Yves. &quot;Pyramidal implementation of the affine lucas kanade feature tracker description of the algorithm.&quot; Intel Corporation 5.1-10 (2001): 4.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　光流（Optical Flow）是物体在三维空间中的运动（运动场）在二维图像平面上的投影，由物体与相机的相对速度产生，反映了微小时间内物体对应的图像像素的运动方向和速度。&lt;br&gt;
　　KLT 是基于光流原理的一种特征点跟踪算法，本文首先介绍光流原理，然后介绍 KLT 及
      
    
    </summary>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/categories/MOT/"/>
    
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="tracking" scheme="https://leijiezhang001.github.io/tags/tracking/"/>
    
      <category term="ADAS" scheme="https://leijiezhang001.github.io/tags/ADAS/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Visual Odometry Part I&amp;II&quot;</title>
    <link href="https://leijiezhang001.github.io/%5Bpaper_reading%5D-Visual_Odometry_Part_I_II/"/>
    <id>https://leijiezhang001.github.io/[paper_reading]-Visual_Odometry_Part_I_II/</id>
    <published>2019-06-17T07:16:53.000Z</published>
    <updated>2019-10-07T14:29:47.148Z</updated>
    
    <content type="html"><![CDATA[<h2 id="overview-of-vo">1. Overview of VO</h2><p>　　SFM(Structure from Motion) 是解决从一堆图片中将场景以及相机姿态进行 3-D 重建的问题，最后的场景以及相机姿态可以通过离线优化方法（bundle adjustment）来 refine。VO &amp; VSLAM 都属于 SFM 的特殊情况，SfM 处理的图像时间上可以是无序的，而 VO &amp; VSLAM 则要求图像时间上有序。VO 只关心轨迹的局部一致性，而 VSLAM 关心全局轨迹和地图的一致性。VO 可以作为 VSLAM 的一个模块，用于重建相机的增量运动，Bundle Adjustment 可以用来 refine 相机的轨迹。如果用户只对相机路径感兴趣，不需要环境地图，且需要较高的实时性，那么一般 VO 就能满足需求。<br>　　视觉里程计（VO）最早应用于 NASA 火星地面探测器，相比于车轮里程计的优势：</p><ul><li>不受车轮打滑的影响；</li><li>不受拐弯影响，拐弯时左右轮速度不一样；</li><li>更加准确，相对位置误差大概在 0.1% 到 2%，可作为车轮里程计、GPS，IMU等其它测量装置的补充；</li><li>在某些领域是必须的，比如无法使用车轮里程计的无人机，GPS 失效的水下环境等；</li></ul><p>　　根据视觉传感器数量，VO 可分为 Stereo VO，与 Monocular VO。当场景距离远远大于双目基线时，Stereo VO 也需要退化成 Monocular VO 来处理。</p><h3 id="stereo-vo-monocular-vo">1.1. Stereo VO &amp; Monocular VO</h3><p>　　特征点匹配可以通过特征跟踪（Feature Tracking）或特征检测（Feature Detect）再匹配两种方式实现。特征跟踪计算量小，但是容易漂移；特征检测再匹配计算量大，需要用 RANSAC 去除无匹配点，但是特征点不容易漂移。<br>　　Motion Estimation 可通过 3D-3D，3D-2D，2D-2D 三种方式实现。Stereo 系统可以获得每个点的深度信息，所以这三种方式都可以用来做相机的运动估计。实验表明，直接在原始的 2-D 点上进行相机运动的估计，更加准确（？存疑）。<br>　　之所以研究单目 VO，是因为当场景距离相机很远的时候（相对于双目的基线），双目就退化为单目了。单目 VO 中绝对深度（尺度）是未知的，刚开始两帧相机移动的距离通常设定为 1，之后的相对位姿都基于此。相关方法可分为：</p><ul><li>Feature-based Methods，用每一帧的特征点来估计运动。</li><li>Appearance-based Methods，用图像中所有的像素点或是子区域中的像素点来估计运动。</li><li>Hybrid Methods，结合以上两种形式。</li></ul><p>第一种方法较好，运动估计用 five-point RANSAC 来求解。</p><h3 id="reducing-the-drift">1.2. Reducing the Drift</h3><p>　　由于 VO 是一步步计算相机的运动轨迹然后作累加的，那么误差就有累积性，使得估计的运动轨迹会漂移。这可以用 Sliding Window(Windowed) Bundle Adjustment 局部优化方法来解决。也可以用 GPS 或 laser 或 IMU 融合来解决。Windowed Bundle Adjustment，是通过 m 个窗口下的信息来优化求解这 m 个相机位姿。</p><h3 id="vo-versus-v-slam">1.3. VO Versus V-SLAM</h3><p>　　V-SLAM 两大方法：</p><ul><li><strong>Filtering Methods</strong> 概率法，以一定的概率分布融合所有图像信息；</li><li><strong>Keyframe Methods</strong> 关键帧法，使用全局 Bundle Adjustment 优化被选择的关键帧；</li></ul><p>　　VO 只关心相机轨迹的一致性，而 SLAM 关注轨迹与地图整体的一致性。SLAM 中两大问题是，检测 loop closure 的发生以及用这个约束来更好的优化当下的地图和轨迹。而 VO 只对历史中以往 n 个轨迹中的位姿进行优化（windowed bundle adjustment），这可以认为与 SLAM 中建立局部地图与轨迹是等价的。但是这两者的 philosophy 不同：</p><ul><li>VO 只关心局部轨迹的一致性，局部地图只是用来（在 bundle ajustment）更精确的估计局部轨迹；</li><li>SLAM 关心整个地图的一致性，当然也包括轨迹，轨迹的精确性能使地图更加精确；</li></ul><p>　　VO 可以是 SLAM 的一个模块（相机运动轨迹的重建），SLAM 还需要一个闭环检测，以及一个全局的地图优化策略。V-SLAM 重建相机运动轨迹理论上比 VO 更精确（加入了更多的约束），但是不一定更鲁棒，因为闭环检测中的奇异值对地图的一致性有较大影响。此外 SLAM 更加复杂以及耗计算资源。VO 牺牲了全局一致性，来达到实时运行的目的，因为不需要记录所有的地图信息。</p><h2 id="formulation-of-the-vo-problem">2. Formulation of the VO Problem</h2><p>　　在时间 \(k\) 下，相机拍摄的图像集记为：\(I_{0:n}=\{I_0,...,I_k\}\)。相机在时间 \(k-1\) 与 \(k\) 的位姿转换矩阵为 \(T_{k,k-1}\in \mathbb{R}^{4\times 4}\)。VO 所要求解的问题就是 \(T=T_{1,0}T_{2,1}...T_{k,k-1}\)。由此可知 VO 是计算相邻帧的相机位姿，然后对之前 m 个位姿做一个局部优化从而估计更准确的轨迹。 <img src="/[paper_reading]-Visual_Odometry_Part_I_II/VO流程.png" width="50%" height="50%" title="图 1. VO流程图"> 　　大多数 VO 算法是基于特征点来估计运动的，特征点法的流程如图 1. 所示：</p><ol type="1"><li><strong>Feature Detection(Extraction) and Matching/Feature Tracking</strong><br>特征提取并与上一帧的特征进行匹配，或者直接用上一帧的特征在这一帧进行跟踪；</li><li><strong>Motion Estimation</strong><br>在 \(k,k-1\) 帧之间求解 \(T_{k,k-1}\) 的过程，根据匹配的特征点对是 2D 还是 3D，运动估计可分为 3D-3D，3D-2D，2D-2D 三种方式实现；</li><li><strong>Local Optimization</strong><br>在 \(k,k-m\) 帧用 Bundle Adjustment 迭代优化求解最优的局部轨迹；</li></ol><p>本文会重点阐述 <strong><em>Camera Model</em></strong><a href="#1" id="1ref"><sup>[1]</sup></a>，<strong><em>Feature Detection and Matching</em></strong><a href="#2" id="2ref"><sup>[2]</sup></a>，<strong><em>Motion Estimation</em></strong><a href="#1" id="1ref"><sup>[1]</sup></a>，<strong><em>Robust Estimation</em></strong><a href="#2" id="2ref"><sup>[2]</sup></a>，<strong><em>Local Optimization</em></strong><a href="#2" id="2ref"><sup>[2]</sup></a>。</p><h2 id="camera-modeling-and-calibration">3. Camera Modeling and Calibration</h2><p>　　<a href>相机模型及标定</a>，另文详述。</p><h2 id="feature-detection-and-matchingfeature-tracking">4. Feature Detection and Matching/Feature Tracking</h2><p>　　生成前后帧特征点的匹配对，有两种方法：</p><ul><li>feature tracking<br>用局部搜索的方法，较适用于相邻两帧视角变化不大的情况，会有漂移（drift）的现象；</li><li>feature detection and matching<br>独立在每个图像上进行检测，然后用某种度量准则进行匹配。在视野变化较大的情况下，只能用这种方法；</li></ul><h3 id="feature-tracking">4.1. Feature Tracking</h3><p>　　主要采用 KLT（详见 <a href="/KLT/" title="KLT 算法详解">KLT 算法详解</a>）方法进行特征点跟踪。</p><h3 id="feature-detection-and-matching">4.2. Feature Detection and Matching</h3><p>　　特征点包含特征检测子与特征描述子。一个好的特征点应该有如下性质：</p><ul><li>可重复性(Repeatability)，不同图像下相同特征点可再次检测出；</li><li>可区别性(Distinctiveness)，不同特征点表达形式不一样，可以更好匹配；</li><li>高效率(Efficiency)，计算高效；</li><li>本地性(Locality)，特征仅与一小片图像区域有关；</li><li>定位准确(Localization Accuracy)，不同尺度下定位都要准确；</li><li>鲁棒性(Robustness)，对噪声，模糊，压缩有较好的鲁棒；</li><li>不变性(Invariance)，对光照(photometric)，旋转，尺度，投影畸变(geometric)有不变性；</li></ul><h4 id="feature-detector">4.2.1. Feature Detector</h4><p>　　特征检测子（feature detector）的计算过程包含两步，首先将图像进行一个特征响应函数的变换，比如 Harris 中的 角点响应函数，SIFT 中的 DoG 变换；然后应用非极大值抑制，提取最小或最大值。<br>　　特征检测子可分为两类：</p><ul><li>角点(corners)<br>角点检测子被定义为至少两个边缘相交的地方；角点计算快，定位精度高，但是区分度低，大尺度下定位精度低；</li><li>斑点(blobs)<br>斑点检测子被定义为一种与周围区域在亮度、颜色、纹理下不同的模式；区分度较高，但是速度较慢；</li></ul><p><img src="/[paper_reading]-Visual_Odometry_Part_I_II/detectors.png" width="80%" height="80%" title="图 2. 检测子比较"> 　　如图2. 所示，常用的角点检测子有 ORB 特征中的 FAST 关键点，Harris 角点等；常用的斑点检测子有 SIFT，SURF，CENSURE 等。</p><h4 id="feature-descriptor">4.2.2. Feature Descriptor</h4><p>　　有了特征检测子后，为了特征点匹配，还需要描述这个检测子，描述量称为特征描述子。描述子可分为以下几类：</p><ol type="1"><li>Appearance，检测子周围的像素信息<ul><li>SSD 匹配，sum of squared difference，计算检测子周围像素亮度与其的误差和；</li><li>NCC 匹配，normalized cross correlation，相比 SSD，有一定的光照不变性；</li><li>Census Transform，将检测子周围的 patch 像素与其进行对比，合成 0,1 向量；</li></ul></li><li>Histogram of Local Gradient Orientations<ul><li>SIFT，光照，旋转，尺度，均具有不变性；不适用于角点，适用于斑点；</li></ul></li><li>Much Faster<ul><li>BRIEF，二进制描述子，用于 ORB；对于旋转和尺度有较强的区分性，并且提取以及比较速度都很快；</li></ul></li></ol><p>　　目前常用的 ORB 特征，采用的是 Oriented FAST 角点，以及 BRIEF 描述子。</p><h4 id="feature-matching">4.2.3. Feature Matching</h4><p>　　通过比较特征点中的描述子部分，来完成特征点的匹配。如果是 appearance 描述子，那么一般通过 SSD/NNC 来计算描述子之间的相似度，其它二进制描述子，可通过欧氏距离或汉明距离来度量。<br>　　基于相似性度量的特征匹配，最简单的就是暴力匹配，两组特征点挨个计算相似度。暴力匹配时间复杂度较高，通常我们采用<strong>快速近似最近邻算法（FLANN）</strong>，也可以加入运动估计模型（通过 IMU 等装置获得的大致运动位姿）来缩小搜索范围。特殊的如果是双目系统，因为左右目图像都是矫正过的，所以左右目的特征点匹配可通过行矩阵搜索解决。<br>　　匹配结束后，我们还得进一步验证匹配的正确性，去除误匹配的情况。比如相互一致性验证，每个特征点只能匹配一个特征点。<br>　　实验表明特征点的分布也很影响匹配效果，特征应尽量均匀分布，可以将图像栅格化，然后对不同的栅格用不同的特征检测阈值即可，保证栅格之间特征数量相等。</p><h2 id="motion-estimation">5. Motion Estimation</h2><h3 id="d-2d">5.1. 2D-2D</h3><p>　　这种情况下特征点 \(f_{k-1},f_k\) 分别是在 2D 图像 \(I_{k-1},I_k\) 坐标系上。<br>　　<a href>对极约束推倒过程可详见这里</a>。根据对极约束，可推导出同一 3D 点投影到两个相机视角图像下后，其坐标之间的关系： <span class="math display">\[p_2^TK^{-T}t^{\wedge} RK^{-1}p_1=0\]</span> 记<strong>本质矩阵(Essential Matrix)</strong>\(E=t^{\land} R\)，记<strong>基础矩阵(Fundamental Matrix)</strong>\(F=K^ {-T}EK^ {-1}\)。基础矩阵描述的是两幅图像对应点的像素坐标的关系；本质矩阵描述的是世界中的某点分别在两个相机坐标系下坐标的相对关系。<br>　　一般相机内参是已知的，所以我们求解本质矩阵。可采用五点法或者八点法来求解，五点法只能处理已知相机标定参数的情况，所以我们一般采用八点法来求解本质矩阵 \(E\)，大于八点即可用最小二乘求解线性方程。然后对本质矩阵进行奇异值分解，即可求出相机的位姿 \(R,t\)。<br>　　当选取的点共面时，基础矩阵的自由度下降，即出现退化的现象，这个时候需要同时求解单应矩阵\(H\)，选择重投影误差较小的那个作为最终的运动估计矩阵。<br>　　此外，还需计算当前运动的相对尺度，可由 3D 点的位置信息求解相对尺度。绝对尺度的求解需要三角化求解。<br>　　总结过程如下：</p><ol type="1"><li>得到新的当前帧 \(I_K\);</li><li>提取当前帧的特征点，并与上一帧的特征进行匹配；</li><li>根据匹配的特征点对，计算本质矩阵\(E\)；</li><li>奇异值分解本质矩阵，得到相机运动 \(R_K,t_k\)；</li><li>该相邻帧的相机运动信息与之前相机运动信息进行累计；</li><li>重复 1.；</li></ol><h3 id="d-2d-1">5.2. 3D-2D</h3><p>　　这种情况下，特征点 \(f_{k-1}\) 是 3D 坐标点，\(f_k\) 是其投影到 2D 图像 \(I_K\) 上的匹配点。对于单目的情况，\(f_{k-1}\) 需要从相邻的前面帧中（比如 \(I_{k-2},I_{k-1}\)）三角化出 3D 坐标，然后与当前帧进行匹配，至少需要三帧的视角。3D-2D 比 3D-3D 更加精确，因为 3D-3D 直接优化相机运动，没有优化投影的过程。<br>　　该问题也称为 <strong>PnP(Perspective from n Points)</strong>。PnP 问题有很多种求解方法：</p><ul><li>P3P 只是用 3 个点对进行求解，容易受误匹配的影响；</li><li>直接线性变换 需要 6 对匹配点才能求解，如果大于 6 对，则可用 SVD 等方法求线性方程的最小二乘解；</li><li>EPnP</li><li>UPnP</li><li>非线性优化(Bundle Adjustment)</li></ul><p>记 \(p_{k-1}^ i\) 为 \(k-1\) 时刻下第 \(i\) 个特征点在相机坐标系下的坐标，定义重投影的误差项： <span class="math display">\[\xi=\mathop{\arg\min}\limits_{T_{k,k-1}} \sum_i \left\Vert uv^i_k-K \, T_{k,k-1} \, p_{k-1}^i \right\Vert^2\]</span></p><p>　　总结过程如下：</p><ol type="1"><li>初始化，在 \(I_{k-2},I_{k-1}\) 两张图里提取特征并匹配，三角花得到特征点的 3D 坐标；</li><li>在 \(I_k\) 图像中提取特征点，并与上一帧的特征进行匹配；</li><li>用 PnP 求解相机运动；</li><li>在 \(I_{k-1},I_{k}\) 中三角化所有特征点；</li><li>重复 2.；</li></ol><h3 id="d-3d">5.3. 3D-3D</h3><p>　　这种情况下特征点都是 3D 坐标点，都需要三角花得到，可以使用一个立体视觉系统。<br>　　已知两组匹配好的 3D 点，可以用 <strong>ICP(Iterative Closest Point)</strong> 来求解位姿。ICP 有两种求解方式：</p><ul><li>线性求解</li><li>非线性优化(类似 Bundle Adjustment)</li></ul><p>定义重投影的误差项： <span class="math display">\[\xi=\mathop{\arg\min}\limits_{T_{k,k-1}} \sum_i \left\Vert p_{k}^i - T_{k,k-1} \, p_{k-1}^i \right\Vert^2\]</span></p><p>　　ICP 问题存在唯一解或无穷多解的情况，所以非线性优化时，只要找到极小值，那一定是全局最优解，这也意味着 ICP 非线性优化时可以任意选定初始值。<br>　　在匹配已知的情况下，ICP 问题是有解析解的。不过如果有些特征点观察不到深度，那么可以混合着使用 PnP 和 ICP 优化：对于深度已知的特征点，建模 3D-3D 误差，对于深度未知的特征点，建模 3D-2D 的重投影误差。两个误差项，用非线性优化求解。</p><h3 id="triangulation-and-keyframe-selection">5.4. Triangulation and Keyframe Selection</h3><p>　　对于 stereo camera， 3D-2D 比 3D-3D 更准确；3D-2D 比 2D-2D 计算更快，前者是 P3P 问题，后者则至少需要 5 个点。当场景中物体相比基线很大时，那么立体视觉系统就失效了，这时候用单目视觉系统比较靠谱。<br>　　对于 monocular camera，2D-2D 比 3D-2D 看样子更好，因为避免了三角测量；然后实际中，3D-2D 用得更多，因为数据关联更快。<br>　　当两帧之间相隔很短时间时，可以认为基线非常小，这种情况，获得的深度信息不确定性很高，所以需要选择某些 keyframes 来计算。</p><h2 id="robust-estimationoutlier-rejection">6. Robust Estimation/Outlier Rejection</h2><p>　　匹配的特征点可能因为噪音、遮挡、模糊、视角变化、光照变化等原因成为外点（outliers），这时候该匹配对对运动估计来说就是个外点，估计的时候应该想办法去除掉。<br>　　<strong>RANSAC</strong> 目前已是在含有噪声的数据中进行模型估计的标准方法。其思想是随机选取一些数据进行建模，涵盖数据最多的模型即被选择是最终模型。对于相机运动估计来说，模型就是相机的运动 \(R,t\)，数据就是特征匹配对。RANSAC 流程为：</p><ol type="1"><li>初始化，记 A 为特征点对集；</li><li>从 A 中随机选取一些点对 s；</li><li>用 s 估计运动模型；</li><li>计算所有的点对与这个模型的距离误差，可使用 point-to-epipolar 距离或是 directional 误差(Sampson distance)；</li><li>统计距离误差小于一定阈值的点对的数量，并存储标记这些内点(inliers)；</li><li>重复 2.，直到达到最大迭代次数；</li><li>选取数量最多的内点点对集，用这些点估计最终模型；</li></ol><p><img src="/[paper_reading]-Visual_Odometry_Part_I_II/ransac.png" width="60%" height="60%" tit le="图 3. RANSAC 迭代次数比较"></p><p>　　为保证得到正确解，迭代次数要求： <span class="math display">\[N=\frac{log(1-p)}{log(1-(1-\epsilon)^s)}\]</span> 其中，\(p\) 表示得到正确解的概率，\(\epsilon\) 表示外点的百分比，\(s\) 表示每次模型估计取出的点数。如图 3. 所示，选出的点数越少，迭代次数就可以越少。这个角度来讲，五点法比八点法有优势，但是五点法的前提是相机都是标定过的。不过不考虑速度的话，还是选择更多的点，因为可以平滑噪声。</p><h2 id="local-optimization">7. Local Optimization</h2><p>　　每次估计的相机运动都有误差，随着运动的累计，误差也会累计。这就要求做局部最优化，消除轨迹的漂移。优化方式有 Pose-Graph Optimization（需要回环检测） 以及 Windowed Bundle Adjustment 两种，这里主要介绍 BA。定义误差函数： <span class="math display">\[\xi=\mathop{\arg\min}\limits_{X^i,C_k} \sum_{i,k} \left\Vert uv_{k}^i - g(X^i,C_k) \right\Vert^2\]</span> 其中 \(X^i\) 为世界坐标系下特征点的 3D 坐标，\(C_k = T_{1,0}...T_{k,k-1}\)，\(g(X^i,C_k)\)为特征点投影到图像的映射函数。该非线性问题可用 Newton-Gauss 或 LM 法解决。为了加速运算，如果 3D 特征点是准确的(如立体视觉获得的)，那么可以固定特征点的 3D 量，只优化相机的轨迹。</p><p><a id="1" href="#1ref">[1]</a> Scaramuzza, Davide, and Friedrich Fraundorfer. &quot;Visual odometry [tutorial].&quot; IEEE robotics &amp; automation magazine 18.4 (2011): 80-92.<br><a id="2" href="#2ref">[2]</a> Fraundorfer, Friedrich, and Davide Scaramuzza. &quot;Visual odometry: Part ii: Matching, robustness, optimization, and applications.&quot; IEEE Robotics &amp; Automation Magazine 19.2 (2012): 78-90.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;overview-of-vo&quot;&gt;1. Overview of VO&lt;/h2&gt;
&lt;p&gt;　　SFM(Structure from Motion) 是解决从一堆图片中将场景以及相机姿态进行 3-D 重建的问题，最后的场景以及相机姿态可以通过离线优化方法（bundle a
      
    
    </summary>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/categories/paper-reading/"/>
    
    
      <category term="SLAM" scheme="https://leijiezhang001.github.io/tags/SLAM/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="VO" scheme="https://leijiezhang001.github.io/tags/VO/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving&quot;</title>
    <link href="https://leijiezhang001.github.io/%5Bpaper_reading%5D-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/"/>
    <id>https://leijiezhang001.github.io/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/</id>
    <published>2019-06-08T06:21:14.000Z</published>
    <updated>2019-10-07T14:29:47.144Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>结合 Semantic SLAM 与 Learning-based 3D Det 技术，提出了一种用于自动驾驶的动态目标定位与本车状态估计的方法。本文系统性较强，集成了较多成熟的模块，对工程应用也有较强的指导意义。 <img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/arch.png" width="100%" height="100%" title="图 1. 语义跟踪系统框架"> 　　如图 1. 所示，整个系统框架由三部分组成：</p><ul><li>2D object detection and viewpoint classification，目标位姿通过 2D-3D 约束求解出来；</li><li>feature extraction and matching，双目及前后帧的特征提取与匹配；</li><li>ego-motion and object tracking，将语义信息及特征量加入到优化中，并且加入车辆动力学约束以获得平滑的运动估计。</li></ul><h2 id="viewpoint-classification-and-3d-box-inference">1. Viewpoint Classification and 3D Box Inference</h2><h3 id="viewpoint-classification">1.1. Viewpoint Classification</h3><p>　　选用 Faster R-CNN 作为 2D 检测框架，在此基础上，加入车辆视野（viewpoint）分类分支。由图 2. 所示，水平视野分为八类，垂直视野分为两类，总共 16 类。 <img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/view.png" width="90%" height="90%" title="图 2. 车辆视野分类"></p><h3 id="d-box-inference-based-on-viewpoint">1.2. 3D Box Inference Based on Viewpoint</h3><p>　　网络输出图像 2D 框以及目标车辆的视野类别（viewpoint），此时我们假设：</p><ul><li>2D 框准确；</li><li>每种车辆的尺寸相同；</li><li>2D 框能紧密包围 3D 框；</li></ul><p>在以上假设条件下，我们可以求得 3D 框，该 3D 框作为后续优化的初始值。约束方程的表示在论文中比较晦涩，在这里我做细致的推倒。 3D 框可表示为 \(\{x,y,z,\theta,w,h,l\}\)，其中 \(\{w,h,l\}\) 分别对应 \({x,y,z}\) 维度。如图 2.(b) 所示，这个视角下，四个 3D 框的顶点，可得四个约束方程。推倒过程为： <span class="math display">\[\require{cancel}\begin{bmatrix}u_{min}\\v_1\\1\\\end{bmatrix}=K\cdot\begin{bmatrix}x_{1}^{cam}\\y_{1}^{cam}\\z_{1}^{cam}\\\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot \begin{bmatrix}x_{1}^{obj}\\y_{1}^{obj}\\z_{1}^{obj}\\\end{bmatrix}=\begin{bmatrix}x\\y\\z\\\end{bmatrix}+\begin{bmatrix}cos\theta &amp; 0 &amp;sin\theta\\0 &amp; 1 &amp; 0\\-sin\theta &amp; 0 &amp; cos\theta\\\end{bmatrix} \cdot\begin{bmatrix}\frac{w}{2}\\\frac{h}{2}\\\frac{l}{2}\\\end{bmatrix}\]</span> 其中 \(K\) 为相机内参，做归一化处理消去；\(T_{cam}^{obj}\) 为目标中心坐标系在相机坐标系下的表示，\((\cdot)^{cam/obj}\) 分别为点在相机坐标系，目标中心坐标系下的表示。同样的，这个视野下，②，③，④ 点都可以由此获得： <span class="math display">\[\left\{\begin{array}{l}\require{cancel}\begin{bmatrix}u_{min}\\v_1\\1\\\end{bmatrix}=K\cdot\begin{bmatrix}x_{1}^{cam}\\y_{1}^{cam}\\z_{1}^{cam}\\\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot \begin{bmatrix}x_{1}^{obj}\\y_{1}^{obj}\\z_{1}^{obj}\\\end{bmatrix}=\begin{bmatrix}x\\y\\z\\\end{bmatrix}+\begin{bmatrix}cos\theta &amp; 0 &amp;sin\theta\\0 &amp; 1 &amp; 0\\-sin\theta &amp; 0 &amp; cos\theta\\\end{bmatrix} \cdot\begin{bmatrix}\frac{w}{2}\\\frac{h}{2}\\\frac{l}{2}\\\end{bmatrix}\\\begin{bmatrix}u_{max}\\v_2\\1\\\end{bmatrix}=K\cdot\begin{bmatrix}x_{2}^{cam}\\y_{2}^{cam}\\z_{2}^{cam}\\\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot \begin{bmatrix}x_{2}^{obj}\\y_{2}^{obj}\\z_{2}^{obj}\\\end{bmatrix}=\begin{bmatrix}x\\y\\z\\\end{bmatrix}+\begin{bmatrix}cos\theta &amp; 0 &amp;sin\theta\\0 &amp; 1 &amp; 0\\-sin\theta &amp; 0 &amp; cos\theta\\\end{bmatrix} \cdot\begin{bmatrix}-\frac{w}{2}\\\frac{h}{2}\\-\frac{l}{2}\\\end{bmatrix}\\\begin{bmatrix}u_3\\v_{min}\\1\\\end{bmatrix}=K\cdot\begin{bmatrix}x_{3}^{cam}\\y_{3}^{cam}\\z_{3}^{cam}\\\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot \begin{bmatrix}x_{3}^{obj}\\y_{3}^{obj}\\z_{3}^{obj}\\\end{bmatrix}=\begin{bmatrix}x\\y\\z\\\end{bmatrix}+\begin{bmatrix}cos\theta &amp; 0 &amp;sin\theta\\0 &amp; 1 &amp; 0\\-sin\theta &amp; 0 &amp; cos\theta\\\end{bmatrix} \cdot\begin{bmatrix}\frac{w}{2}\\-\frac{h}{2}\\-\frac{l}{2}\\\end{bmatrix}\\\begin{bmatrix}u_4\\v_{max}\\1\\\end{bmatrix}=K\cdot\begin{bmatrix}x_{4}^{cam}\\y_{4}^{cam}\\z_{4}^{cam}\\\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot \begin{bmatrix}x_{4}^{obj}\\y_{4}^{obj}\\z_{4}^{obj}\\\end{bmatrix}=\begin{bmatrix}x\\y\\z\\\end{bmatrix}+\begin{bmatrix}cos\theta &amp; 0 &amp;sin\theta\\0 &amp; 1 &amp; 0\\-sin\theta &amp; 0 &amp; cos\theta\\\end{bmatrix} \cdot\begin{bmatrix}-\frac{w}{2}\\\frac{h}{2}\\\frac{l}{2}\\\end{bmatrix}\end{array}\right.\]</span></p><p>将 \(z\) 方向归一化后，进一步得到最终的四个约束式子： <span class="math display">\[\left\{\begin{array}{l}u_{min}=(x+ \frac{w}{2} cos\theta+ \frac{l}{2} sin\theta) / (z- \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)\\u_{max}=(x- \frac{w}{2} cos\theta- \frac{l}{2} sin\theta) / (z+ \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\v_{min}=(y- \frac{h}{2}) / (z- \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\v_{max}=(y+ \frac{h}{2}) / (z+ \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)\end{array}\right.\]</span> 以上四个方程可以闭式求解 3D 框 \({x,y,z,}\)。该方法将 3D 框的回归求解分解成了 2D 框回归，视野角分类以及解方程组的过程，强依赖于前面的三点假设，实际情况 3D 框与 2D 框不会贴的很紧。这个 3D 框结果只用来作后续的特征提取区域及最大后验概率估计的初始化。</p><h2 id="feature-extraction-and-matching">2. Feature Extraction and Matching</h2><p>　　这一部分做的是左右目及前后帧特征提取及匹配。选用 ORB 特征，目标区域由投影到图像的 3D 框确定。</p><ul><li><strong>目标区域内左右目的立体匹配</strong> 由于已知目标的距离及尺寸，所以只需要在一定小范围内进行特征点的行搜索匹配。</li><li><strong>目标及背景区域下前后帧的时序匹配</strong> 首先进行 2D 框的关联，2D 框经过相机旋转补偿后，最小化关联框的中心点距离及框形状相似度值。然后在关联上的目标框区域以及背景区域里，分别作 ORB 特征的匹配，异常值在 RANSAC 下通过基础矩阵测试去除。</li></ul><h2 id="ego-motion-and-object-tracking">3. Ego-motion and Object Tracking</h2><p>　　首先进行本车运动状态估计，可在传统 SLAM 框架下做，不同的是将动态障碍物中的特征点去除。有了本车的位姿后，再估计动态障碍物的运动状态。文中符号定义较为复杂，这里不做赘述。</p><h3 id="ego-motion-tracking">3.1. Ego-motion Tracking</h3><p>　　给定左目前后帧背景区域特征点的观测，本车状态估计可以通过极大似然估计（Maximum Likelihood Estimation）得到。MLE 可以转化为非线性最小二乘问题，也就是 Bundle Adjustment 过程，这是典型的 SLAM 问题。文中给出的误差方程： <img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/3.png" width="50%" height="50%"> 需要求解的是本车位姿以及背景特征点坐标，这是后验概率，可转为似然函数求解，然后转化为非线性优化问题。可参考《视觉 SLAM 十四讲》(107-108)来理解。</p><h3 id="semantic-object-tracking">3.2. Semantic Object Tracking</h3><p>　　得到本车相机的位姿后，运动目标的状态估计可以通过最大后验概率估计（Maximum-a-posterior, MAP）得到。类似的，可转为非线性优化问题进行求解，联合优化每个车辆的<strong>位姿</strong>，<strong>尺寸</strong>，<strong>速度</strong>，<strong>方向盘转角</strong>，<strong>所有特征点 3D 位置</strong>。有四个 loss 项： <img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/6.png" width="80%" height="80%"> <img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/10.png" width="80%" height="80%"> \(r_Z,r_P,r_M,r_S\) 分别代表：</p><ul><li><strong>Sparse Feature Observation</strong><br>目标上的特征点重投影到左右目图像的误差，注意有左右目两个误差项；</li><li><strong>Semantic 3D Object Measurement</strong><br>3D 框投影到图像上与 2D 框的尺寸约束投影误差，即 1.2 节中的形式，区别在车辆尺寸与位姿作为了优化项；</li><li><strong>Vehicle Motion Model</strong><br>对于车辆，前后时刻的状态要有连续性，即误差最小；</li><li><strong>Point Cloud Alignment</strong><br>为了减少 3D 框的整体偏移，引入特征点到 3D 观察面的最小距离误差；</li></ul><p>这里只对车辆运动模型进行分析，其它几项基本在前文已经有描述或者比较常识化，就不展开，具体公式可参见论文。<br>　　由实验可知 Sparse Feature Observation 与 Point Cloud Alignment 对性能提升较明显，Motion Model 对困难情景性能才有提升。</p><h4 id="vehicle-motion-model">3.2.1. Vehicle Motion Model</h4><p>　　<a href="#2" id="2ref">[2]</a> 中介绍了前转向车的两种模型：运动学模型(Kinematic Bicycle Model)，以及更复杂的动力学模型(Dynamic Bicycle Model)。运动学模型假设车辆不存在滑动，这在大多数情况下都是满足的，所以我们只介绍运动学模型。 <img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/kinematic.png" width="30%" height="30%" title="图 3. 车辆运动学模型"> 　　如图 3. 所示，前后轮无滑动的约束下，可得方程组： <span class="math display">\[\left\{\begin{array}{rl}\dot{x}_fsin(\theta+\delta)-\dot{y}_fcos(\theta+\delta)=&amp;0\\\dot{x}sin(\theta)-\dot{y}cos(\theta)=&amp;0\\x+Lcos(\theta)=&amp;x_f  \quad\Rightarrow \quad \dot{x}-\dot{\theta}Lsin(\theta)=\dot{x}_f\\y+Lsin(\theta)=&amp;y_f \quad\Rightarrow \quad \dot{y}+\dot{\theta}Lcos(\theta)=\dot{y}_f\end{array}\right.\]</span> 由此可得到: <span class="math display">\[\dot{x}sin(\theta+\delta)-\dot{y}cos(\theta+\delta)-\dot{\theta}Lcos(\delta)=0\]</span> 用 \(\left(v \cdot cos(\theta),v\cdot sin(\theta)\right)\) 代替 \((\dot{x},\dot{y})\) 可得： <span class="math display">\[\dot{\theta}=\frac{tan(\delta)}{L}\cdot v\]</span> 最终可整理成矩阵形式： <span class="math display">\[\begin{bmatrix}\dot{x}\\\dot{y}\\\dot{\theta}\\\dot{\delta}\\\dot{v}\\\end{bmatrix}=\begin{bmatrix}0 &amp;0 &amp;0 &amp;0 &amp;cos(\theta)\\0 &amp;0 &amp;0 &amp;0 &amp;sin(\theta)\\0 &amp;0 &amp;0 &amp;0 &amp;\frac{tan(\delta)}{L}\\0 &amp;0 &amp;0 &amp;0 &amp;0\\0 &amp;0 &amp;0 &amp;0 &amp;0\\\end{bmatrix}\begin{bmatrix}x\\y\\\theta\\\delta\\v\\\end{bmatrix}+\begin{bmatrix}0 &amp;0\\0 &amp;0\\0 &amp;0\\1 &amp;0\\0 &amp;1\\\end{bmatrix}\begin{bmatrix}\gamma\\\alpha\\\end{bmatrix}\]</span> 其中 \(L\) 为车辆参数。观测量有：</p><ul><li>\((x,y,\theta)\) 为车辆的位置及朝向角；</li><li>\(\delta\) 为方向盘/车轮转角；</li><li>\(v\) 为车辆速度；</li></ul><p>控制量有：</p><ul><li>\(\gamma\) 为方向盘角度比率；</li><li>\(\alpha\) 为加速度；</li></ul><p>本文的目的是要约束车辆时序上运动(速度及朝向)的平滑一致性，令控制量 \(\gamma,\alpha\) 为 0，然后可得状态量在相邻时刻的关系应满足： <span class="math display">\[\left\{\begin{array}{l}\hat{x}^t=x^{t-1}+cos(\theta^{t-1})v^{t-1}\Delta t\\\hat{y}^t=y^{t-1}+sin(\theta^{t-1})v^{t-1}\Delta t\\\hat{\theta}^t=\theta^{t-1}+\frac{tan(\delta^{t-1})}{L}v^{t-1}\Delta t\\\hat{\delta}^t=\delta^{t-1}\\\hat{v}^t=v^{t-1}\end{array}\right.\]</span> 由此可整理成论文中矩阵的形式及误差项： <img src="/[paper_reading]-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving/15.png" width="80%" height="80%"></p><p><a id="1" href="#1ref">[1]</a> Li, Peiliang, and Tong Qin. &quot;Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving.&quot; Proceedings of the European Conference on Computer Vision (ECCV). 2018.<br><a id="2" href="#2ref">[2]</a> Gu, Tianyu. Improved trajectory planning for on-road self-driving vehicles via combined graph search, optimization &amp; topology analysis. Diss. Carnegie Mellon University, 2017.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　本文&lt;a href=&quot;#1&quot; id=&quot;1ref&quot;&gt;&lt;sup&gt;[1]&lt;/sup&gt;&lt;/a&gt;结合 Semantic SLAM 与 Learning-based 3D Det 技术，提出了一种用于自动驾驶的动态目标定位与本车状态估计的方法。本文系统性较强，集成了较多成熟的模块，
      
    
    </summary>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/categories/paper-reading/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
  </entry>
  
  <entry>
    <title>[paper_reading]-&quot;Stereo R-CNN based 3D Object Detection for Autonomous Driving&quot;</title>
    <link href="https://leijiezhang001.github.io/%5Bpaper_reading%5D-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/"/>
    <id>https://leijiezhang001.github.io/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/</id>
    <published>2019-06-08T06:21:14.000Z</published>
    <updated>2019-10-07T14:29:47.140Z</updated>
    
    <content type="html"><![CDATA[<p>　　Learning 方法有什么致命缺点吗？我认为目前 Learning 方法还存在的较为棘手的问题是，有时候结果会出现非常低级的错误，或是说不可思议不合常理的 cornercases。所以我认为一个工程系统或是一个鲁棒的算法系统，在 Learning 之后做一个基于常理（如 geometry 约束或专家系统）的验证，能有效抑制这个问题。本文就是一个比较好的 learning+geometry 想结合的方法。<br>　　本文<a href="#1" id="1ref"><sup>[1]</sup></a>基于图像语义及几何信息，通过 3D 目标的稀疏与密集约束，提出了一种准确的 3D 目标检测方法。根据输入数据的类型，作者将 3D 检测分为三大类：</p><ul><li>LiDAR-based，近期被研究的较多，基本是自动驾驶所必须的；</li><li>Monocular-based，低成本方案；</li><li>Stereo-based，相比 Monocular-based，有优势，但是研究较少；</li></ul><p>本文就是 Stereo-based 3D 检测方案。不同于一般的 rgb+depth 作为输入的方案，本文直接将左右目 rgb 作为输入，没有显示地 depth 生成过程。工程上来说，这也极大地缩短了 3D Detection 的时延(latency)。<br>　　本文方法如图 1 所示，主要有三部分组成：</p><ol type="1"><li> Network，又有三部分构成：<ul><li>Stereo RPN Module，输出左右图的 RoI；</li><li>Classification and Regression branches，输出目标类别，朝向，尺寸；</li><li>Keypoint branch，输出左目目标的关键点；</li></ul></li><li> Sparse constraints，3D 框-2D 框的稀疏约束；</li><li> Dense constraints，准确定位的关键模块；</li></ol><p><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/net_arch.png" width="100%" height="100%" title="图 1. 网络结构"></p><h2 id="stereo-r-cnn-network">1. Stereo R-CNN Network</h2><p>　　Stereo R-CNN 是在 Faster R-CNN 基础上，同时检测与关联左右目图像 2D 框的微小差异。</p><h3 id="stereo-rpn">1.1. Stereo RPN</h3><p>　　在传统 RPN 网络的基础上，本文先对左右图做 paramid features 提取，然后将不同尺度的特征 concatenate 一起，进入 RPN 网络。 <img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/target.png" width="60%" height="60%" title="图 2. 真值框定义方式"> 　　关键的一点是 objectness classification与 stereo box regression 的真值框定义不一样。如图 2 所示，</p><ul><li>对于 objectness classification，真值框定义为左右目真值框的外接合并（union GT box），一个 anchor 在与真值框的交并比（Intersection-over-Union）大于 0.7 时标记为正样本，小于 0.3 时标记为负样本。分类任务的候选框包含了左右目真值框区域的信息。</li><li>对于 stereo box regression，真值框定义为左右目分别的真值框。待回归的参数定义为 \([u, w, u', w', v, h]\)，分别为左目的水平位置及宽，右目的水平位置及宽，垂直位置及高。因为输入为矫正过的左右目图像，所以可认为左右目的垂直方向上已经对齐。</li></ul><p>每个左右目的 proposal 都是通过同一个 anchor 产生的，自然左右目的 proposal 是关联的。通过 NMS 后，保留左右目都还存在的 proposal 关联对，取前 2000 个用于训练，测试时取前 300 个。</p><h3 id="stereo-r-cnn">1.2. Stereo R-CNN</h3><p><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/viewpoint.png" width="50%" height="50%" title="图 3. 各角度关系"> 　　网络头包含两大部分：</p><ol type="1"><li><p> <strong>Stereo Regression</strong><br>左右目的 proposal 关联对，分别在左右目的 feature 上进行 RoI Align 的操作，然后 concatenate 输入到全链接层。左右目的 RoI 对与真值框的 IoU 均大于 0.5 时定位正样本，左右目的 RoI 对与真值框的 IoU 有一个小于 0.5 且大于 0.1，则定位负样本。用四个分支分别预测：</p><ul><li>object class；</li><li>stereo bounding boxes，与 stereo rpn 中一致，左右目的高度已对齐；</li><li>dimension，先统计平均的尺寸，然后预测相对量；</li><li>viewpoint angle，如图 3 所示，\(\theta\) 为相机坐标系下的朝向角，\(\beta\) 为相机中心点下的方位角(azimuth)，这三个目标在相机视野下是一样的，所以我们回归的量是视野角(viewpoint angle) \(\alpha=\theta+\beta\)，其中 \(\beta=arctan\left(-\frac{x}{z} \right) \)。并且为了连续性，回归量为 \([sin\,\alpha,cos\,\alpha]\)。</li></ul></li></ol><p><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/keypoints.png" width="70%" height="70%" title="图 4. 语义关键点"></p><ol start="2" type="1"><li> <strong>Keypoint Prediction</strong><br>如图 4 所示，考虑 3D 框底部矩形的四个关键点，投影到图像平面后，最多只有一个关键点会在图像 2D 矩形框内。对左目图像进行关键点预测，类似 Mask R-CNN，在 6×28×28 的基础上，因为关键点只有图像坐标 u 方向才提供了额外的信息，所以对每列进行累加，最终输出 6×28 的向量。前 4 个通道代表每个关键点作为 perspective keypoint 投影到该 u 坐标下的概率；后 2 个通道代表该 u 坐标是左右边缘关键点(boundary keypoints)的概率。为了找出 perspective keypoint，softmax 应用于 4×28 的输出上；为了找出左右边缘关键点，softmax 分别应用于后两个 1×28 的输出上。训练的时候，4×28 中只有一个被赋予 perspective keypoint，忽略没有 perspective keypoint 的情况（遮挡等），然后最小化 cross-entropy loss；对于边缘关键点，则分别最小化 1×28 维度上的 cross-entropy loss，前景中也会被赋予边缘关键点。</li></ol><h2 id="d-box-estimation">2. 3D Box Estimation</h2><p><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/projection.png" width="70%" height="70%" title="图 5. 关键点投影关系"> 　　已知关键点，2D 框，尺寸，朝向角，我们可以求解出 3D 框 \(\{x,y,z,\theta\}\)。求解目标是最小化 3D 框投影到 2D 框以及关键点的误差。如图 5 所示，已知 7 个观测量 \(z = \{u_l,v_t,u_r,v_b,u_l',u_r',u_p\}\)，分别代表左目 2D 框的左上坐标，右下坐标，右目 2D 框的左右 u 方向坐标，以及 perspective keypoint 的 u 方向坐标。在图 5 的情况下（其它视角下，注意符号变化），左上点投影关系如下： <span class="math display">\[\require{cancel}\begin{bmatrix}u_l\\v_t\\1\\\end{bmatrix}=K\cdot\begin{bmatrix}x_{cam}^{tl}\\y_{cam}^{tl}\\z_{cam}^{tl}\\\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot \begin{bmatrix}x_{obj}^{tl}\\y_{obj}^{tl}\\z_{obj}^{tl}\\\end{bmatrix}=\begin{bmatrix}x\\y\\z\\\end{bmatrix}+\begin{bmatrix}cos\theta &amp; 0 &amp;sin\theta\\0 &amp; 1 &amp; 0\\-sin\theta &amp; 0 &amp; cos\theta\\\end{bmatrix} \cdot\begin{bmatrix}-\frac{w}{2}\\-\frac{h}{2}\\-\frac{l}{2}\\\end{bmatrix}\]</span> 其中 \(K\) 为相机内参，\(T_{cam}^{obj}\) 为目标中心坐标系在相机坐标系下的表示，\((\cdot)_{cam/obj}\) 分别为点在相机坐标系，目标中心坐标系下的表示。同样的，这个视野下，右下点为： <span class="math display">\[\require{cancel}\begin{bmatrix}u_r\\v_b\\1\\\end{bmatrix}=K\cdot\begin{bmatrix}x_{cam}^{tl}\\y_{cam}^{tl}\\z_{cam}^{tl}\\\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot \begin{bmatrix}x_{obj}^{tl}\\y_{obj}^{tl}\\z_{obj}^{tl}\\\end{bmatrix}=\begin{bmatrix}x\\y\\z\\\end{bmatrix}+\begin{bmatrix}cos\theta &amp; 0 &amp;sin\theta\\0 &amp; 1 &amp; 0\\-sin\theta &amp; 0 &amp; cos\theta\\\end{bmatrix} \cdot\begin{bmatrix}\frac{w}{2}\\\frac{h}{2}\\-\frac{l}{2}\\\end{bmatrix}\]</span> 右目两个边缘点以及 perspective keypoint 点也可同样得到，由此可整理出 7 个方程组（论文中第一个公式符号有错）： <span class="math display">\[\left\{\begin{array}{l}u_l=(x- \frac{w}{2} cos\theta- \frac{l}{2} sin\theta) / (z+ \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\v_t=(y- \frac{h}{2}) / (z+ \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\u_r=(x+ \frac{w}{2} cos\theta+ \frac{l}{2} sin\theta) / (z- \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)\\v_b=(y+ \frac{h}{2}) / (z- \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)\\u&#39;_l=(x-b- \frac{w}{2} cos\theta- \frac{l}{2} sin\theta) / (z+ \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\u&#39;_r=(x-b+ \frac{w}{2} cos\theta+ \frac{l}{2} sin\theta) / (z- \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)\\u_p=(x+ \frac{w}{2} cos\theta- \frac{l}{2} sin\theta) / (z- \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\\end{array}\right.\]</span> 其中 \(b\) 为双目的基线长(baseline)。以上方程组可用 Gauss-Newton 法求解。</p><h2 id="dense-3d-box-alignment">3. Dense 3D Box Alignment</h2><p>　　以上得到的目标 3D 位置是 object-level 求解得到的，利用像素信息，还可以进行优化精确求解。首先在图像 2D 目标框内扣取一块 RoI，要使 RoI 能较为确定的在目标上，扣取方式定义为：</p><ul><li>目标一半以下区域；</li><li>perspective keypoint 与边缘关键点包围区域；</li></ul><p>关键点预测的时候只预测了 u 方向的坐标，边缘关键点无 v 方向的信息，看起来会使某些背景像素被划入为目标像素，更好的方法是加入 instance segmentation 信息。定义误差函数为： <span class="math display">\[E=\sum_{i=0}^N e_i=\sum_{i=0}^N \left\| I_l(u_i,v_i)-I_r(u_i-\frac{b}{z+\Delta z_i},v_i)\right\|\]</span> 可由三角测量关系 \(z=\) 推出。上式中，\(z_i=z_i-z\) 表示某个像素点 \(i\) 所对应的 3D 点与目标中心点之间的距离。最小化总误差即可求得最优的中心点距离 \(z\)。优化过程可以用 coarse-to-fine 的策略，先以 0.5m 的精度找 50 步，再以 0.05m 的精度找 20 次。<br>　　这个 dense alignment 模块是独立的，可以应用到任意的左右目 3D 检测的后处理中。因为目标 RoI 是物理约束，所以这个方法避免了深度估计中不连续、病态的问题，且对光照是鲁棒的，因为每个像素都会对估计起作用。这里，本文只做了中心点的 align，尺寸，甚至朝向角是否能加入优化?</p><h2 id="other-details">4. Other Details</h2><p><img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/r1.png" width="110%" height="110%"> <img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/r2.png" width="70%" height="70%"> <img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/r3.png" width="100%" height="100%"> <img src="/[paper_reading]-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving/r4.png" width="90%" height="90%"></p><p><a id="1" href="#1ref">[1]</a> Li, Peiliang, Xiaozhi Chen, and Shaojie Shen. &quot;Stereo R-CNN based 3D Object Detection for Autonomous Driving.&quot; arXiv preprint arXiv:1902.09738 (2019).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　Learning 方法有什么致命缺点吗？我认为目前 Learning 方法还存在的较为棘手的问题是，有时候结果会出现非常低级的错误，或是说不可思议不合常理的 cornercases。所以我认为一个工程系统或是一个鲁棒的算法系统，在 Learning 之后做一个基于常理（
      
    
    </summary>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/categories/paper-reading/"/>
    
    
      <category term="Deep Learning" scheme="https://leijiezhang001.github.io/tags/Deep-Learning/"/>
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
  </entry>
  
  <entry>
    <title>MOT Metrics in Academia and Industry</title>
    <link href="https://leijiezhang001.github.io/MOT-Metrics-in-Academia-and-Industry/"/>
    <id>https://leijiezhang001.github.io/MOT-Metrics-in-Academia-and-Industry/</id>
    <published>2019-06-03T05:47:00.000Z</published>
    <updated>2019-10-07T14:29:47.140Z</updated>
    
    <content type="html"><![CDATA[<p>　　MOT 是一个比较基本的技术模块，在视频监控中，常用于行人行为分析、姿态估计等任务的前序模块；在自动驾驶中，MOT 是动态目标状态估计的重要环节。在学术界，MOT 算法性能的评价准则已经较为完善，其指标主要关注，尽可能地覆盖所有性能维度，以及指标的简洁性（上一篇有较多介绍，<a href="https://leijiezhang001.github.io/MOT-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87-Evaluating-Multiple-Object-Tracking-Performance-the-CLEAR-MOT-Metrics/#more">the CLEAR MOT Metrics</a>）。而工业界则尚无统一的标准，实际的指标需求情况也比学术界复杂。<br>　　指标的计算过程可由三部分组成，真值过滤(Filter)，匹配构建(Establishing Correspondences)与指标计算(Calculating Metrics)。其中真值过滤，更多的是工程细节，学术界没有文章对这一部分进行讨论研究。本文首先介绍学术界各评价指标详情，然后讨论工业界需要的评价指标又是怎样的。</p><h2 id="metrics-in-academia">1. Metrics in Academia</h2><p>　　在学术界，因为数据集质量较高，噪声相对较小，匹配构建中距离的度量偏向于严格且简单的方式。对于区域(框)跟踪器，采用重叠区域来度量；对于点跟踪器，采用中心点的欧式距离来度量。指标汇总如下：</p><p>A. <strong>检测指标</strong><br>　\(\lozenge\)　准确性(Accuracy)</p><ul><li><strong>Recall</strong> = \(\frac{TP}{GT}\)；</li><li><strong>Precision</strong> = \(\frac{TP}{TP+FP}\)；</li><li><strong>FAF/FPPI</strong><a href="#1" id="1ref"><sup>[1]</sup></a><a href="#2" id="2ref"><sup>[2]</sup></a> ，Average False Alarms per Frame；False Positive Per Image;</li><li><strong>MODA</strong><a href="#3" id="3ref"><sup>[3]</sup></a>，Multipe Object Detection Precision，整合了 FN 与 FP，设 \(c_m, c_f\) 分别为 FN，FP 的权重： <span class="math display">\[MODA=1-\frac{\sum_{t=1}^{N_frames}(c_m(fn_t)+c_f(fp_t))}{\sum_{t=1}^{N_frames}gt_t}\]</span></li></ul><p>　\(\lozenge\)　精确性(Precision)</p><ul><li><strong>MODP</strong><a href="#3" id="3ref"><sup>[3]</sup></a>，Multiple Object Detection Accuracy， <span class="math display">\[MODP=\frac{\sum_{t=1}^{N_frames} \sum_{i=1}^{N_{mapped}^{(t)}} \;\; dist}{\sum_{t=1}^{N_frames} N_{mapped}^{(t)}}\]</span> 其中 \(N_{mapped}^{(t)}\) 为第 \(t\) 帧匹配的目标数；\(dist\) 为距离度量方法，如框的交并比度量法： <span class="math display">\[Mapped Overlap Ratio = \frac{\lvert G_i^{(t)}\bigcap D_i^{(t)}\rvert}{|G_i^{(t)}\bigcup D_i^{(t)}|}\]</span></li></ul><p>B. <strong>跟踪指标</strong><br>　\(\lozenge\)　准确性(Accuracy)</p><ul><li><strong>IDS</strong><a href="#4" id="4ref"><sup>[4]</sup></a>，ID switch，a tracked target changes its ID with another target(预测关联真值)；</li><li><strong>MOTA</strong><a href="#5" id="5ref"><sup>[5]</sup></a>，Multiple Object Tracking Accuracy，整合了 FN，FP，ID-Switch： <span class="math display">\[MOTA=1-\frac{\sum_{t=1}^{N_{frames}} \;\; (c_m(fn_t)+c_f(fp_t)+c_s(ID-SWITCHES_t))}{\sum_{t=1}^{N_{frames}} \;\; gt_t}\]</span> 其中权重方程一般可设为：\(c_m=c_f=1, \quad c_s=log_{10}\)；</li></ul><p>　\(\lozenge\)　精确性(Precision)</p><ul><li><strong>MOTP</strong><a href="#5" id="5ref"><sup>[5]</sup></a>，Multiple Object Tracking Precision， <span class="math display">\[MODP=\frac{\sum_{t=1}^{N_frames} \sum_{i=1}^{N_{mapped}^{(t)}} \;\; \left(\frac{\lvert G_i^{(t)}\bigcap D_i^{(t)}\rvert}{|G_i^{(t)}\bigcup D_i^{(t)}|} \right)}{\sum_{t=1}^{N_frames} N_{mapped}^{(t)}}\]</span></li><li><strong>TDE</strong><a href="#6" id="6ref"><sup>[6]</sup></a>，Distance between the ground-truth annotation and the tracking result；像素级别的误差计算，适用于人群跟踪；</li><li><strong>OSPA</strong><a href="#7" id="7ref"><sup>[7]</sup></a><a href="#8" id="8ref"><sup>[8]</sup></a>，Optimal Subpattern assignment，由定位 (localization) 误差及基数 (cardinality) 误差构成，对于第 \(t\) 帧： <span class="math display">\[e^t=\left[\frac{1}{n^t}\left( \mathop{\min}_{\pi\in\Pi_n} \sum_{i=1}^{m^t} d^{(c)}(x_i^t,y_{\pi(i)}^t)^p + (n^t-m^t)\cdot c^p \right) \right]^{1/p}\]</span> 其中，\(n^t\) 为目标真值与算法输出中数量较大者。\(\Pi_n\) 为从 \(n^t\) 中取出的 \(m\) 个目标。\(p\) 为距离指数范数。其中定位截断误差为： <span class="math display">\[d^{(c)}(x_i^t,y_{\pi(i)}^t) = \mathop{\min}\left(c,d(x_i^t,y_{\pi(i)}^t)\right)\]</span> \(c\) 为截断参数。定位误差又由距离误差和标签误差组成： <span class="math display">\[d(x_i^t,y_{\pi(i)}^t=\parallel x_i^t-y_{\pi(i)}^t\parallel + \alpha \; \bar{\delta}(l_x, l_y)\]</span> 其中 \(\alpha\in[0,c]\)，为标签误差的权重系数。如果 \(l_x=l_y\)，\(\bar{\delta}(l_x, l_y)=0\)，否则 \(\bar{\delta}(l_x, l_y)=1\).</li></ul><p>　\(\lozenge\)　完整性(Completeness)</p><ul><li><strong>MT</strong><a href="#9" id="9ref"><sup>[9]</sup></a>，Mostly Tracked，真值轨迹长度被跟踪大于80%的比例；</li><li><strong>ML</strong><a href="#9" id="9ref"><sup>[9]</sup></a>，Mostly Lost，真值轨迹长度被跟踪小于20%的比例；</li><li><strong>PT</strong><a href="#9" id="9ref"><sup>[9]</sup></a>，Partially Tracked，\(1-MT-ML\);</li><li><strong>FM</strong><a href="#9" id="9ref"><sup>[9]</sup></a>，Fragments，ID of a target changed along a GT trajectory, or no ID(真值关联预测)；</li></ul><p>　\(\lozenge\)　鲁棒性(Robustne)</p><ul><li><strong>RS</strong><a href="#10" id="10ref"><sup>[10]</sup></a>，Recover from short term occlusion;</li><li><strong>RL</strong><a href="#10" id="10ref"><sup>[10]</sup></a>，Recover from long term occlusion;</li></ul><h2 id="metrics-in-industry">2. Metrics in Industry</h2><p>　　工业界的数据噪声较大，传感器配置也比较多样，不同的产品（传感器+算法），对 MOT 性能维度要求也不一样。更重要的是，评价指标应该从功能层面进行定义，在模块层面 (MOT) 进行调整及细化。可以说，工业界是以学术界为基础来设计 MOT 指标的，不同的产品没有统一的标准，但有比较通用的设计准则。<br>　　这里以自动驾驶/辅助驾驶中动态目标状态估计模块为例，模块详细分析<a href>日后再写</a>。该模块的基本输入为：</p><ul><li><strong>传感器数据</strong>，可以是图像，激光等；</li><li><strong><em>自定位系统</em></strong>，可以是基于视觉的 VO，基于视觉-IMU 的 VINS等；</li></ul><p>其中自定位系统能使目标状态估计在世界坐标系（惯性系）下优化，否则只能在本体（ego）非惯性系下优化，会减少一些约束量。该功能的基本输出为：</p><ul><li><strong>位置</strong>，本体坐标系下目标的三维位置，\(x,y,z\)；</li><li><strong>尺寸</strong>，目标的物理尺寸大小，包括立方体的长宽高；或者图像坐标系下的像素大小；或者图像/点云下目标的 mask，即分割后的目标；</li><li><strong><em>朝向</em></strong>，一般只考虑目标的航向角；</li><li><strong>速度</strong>，本体坐标系或世界坐标系下的三维速度，一般只考虑航向平面的速度；</li></ul><p>其中朝向是非必须项，有了朝向后，能更有效地进行状态优化。该模块的子模块有（注意，MOT 只包含前三者）：</p><ul><li><strong>检测(Detection)</strong>，进行多目标检测；</li><li><strong>跟踪(Tracking)</strong>，根据上一帧结果，进行多目标跟踪；</li><li><strong>数据关联(Association)</strong>，检测结果与跟踪结果的融合，出目标的 tracklets，生成 ID；</li><li><strong>状态估计(State Estimation)</strong>，不同的方法包括不同的部分；</li></ul><p>　　工业界设计产品时，基本遵循自顶向下的策略：产品需求-功能需求-模块需求，层层推倒。所以我们设计评价准则时，一般会问几个问题：</p><ul><li>该模块服务的产品功能，其需求及对应的指标是什么？</li><li>要达到功能指标，本模块的输出需要哪些指标来评测？</li><li>各个子模块对模块的影响是怎样的，对应需要增加哪些指标？</li></ul><p>这里提到了功能指标，模块指标，子模块指标三层概念。功能指标及部分模块指标是可以写入产品手册的，所以需要突出重点，易于理解；部分模块及子模块指标则主要是为了产品上工程优化迭代，这就要求这部分指标要相当细致，将模块的不足尽可能解耦，且完全暴露出来。以下通过两个例子来分析设计过程。</p><h3 id="adas-中的-fcw-功能">2.1. ADAS 中的 FCW 功能</h3><p>　　FCW 基本功能要求为：</p><ul><li>不允许误报，尽可能不漏报；</li><li>在 V km/h 下，以一定的刹车加速度 a，能避免与静止的前车相碰撞；</li></ul><p>　　由以上两个功能需求，可确定必须的功能指标：</p><ul><li>（百公里）误报率；</li><li>（百公里）漏报率；</li><li>观测距离，可由第二项功能要求推到出（人反应时间已知）；</li></ul><p>　　相应的 MOT +状态估计模块输出的指标为<strong>各距离维度各类别维度</strong>下的：</p><ul><li>误检率；</li><li>漏检率；</li><li>ID Switch；</li><li>定位精度；</li><li>速度估计精度；</li></ul><p>　　其中 MOT 主要涉及误检率，漏检率，ID Switch（直接影响状态估计模块）。这些指标的计算方式可以在学术界定义的基础上做进一步改进，比如漏检率，就需要体现出百公里漏报率的性能，所以可以考虑将连续 N 帧漏检的目标才归为漏检，分母可以定义为每多少帧。此外，要在各距离维度各类别维度下进行计算，这就涉及到过滤（filter）策略。对于 FCW 而言，首要关注的是本车前方近距离位置，距离维度上的功能重要程度要突显出来，类别维度也要区别对待，以便算法模块可以重点优化。</p><h3 id="自动驾驶中的动态障碍物检测功能">2.2. 自动驾驶中的动态障碍物检测功能</h3><p>　　自动驾驶中动态障碍物检测的要求就高了，子模块也较为复杂，指标除了评估功能模块的性能，还需要指导迭代各子模块算法，包括本子模块的迭代比较，以及上下游模块相关指标的对比。<br>　　功能需求，我们简单列举几项：</p><ul><li>不允许漏检，尽可能不误检；</li><li>前向，后向，侧向观测距离分别要达到 x, y, z；</li></ul><p>　　相应的功能指标为：</p><ul><li>漏检率；</li><li>误检率；</li><li>观测距离；</li><li>观测精度；</li><li>观测时延(delay)；</li></ul><p>　　MOT +状态估计模块输出的指标依然在<strong>各距离维度各类别维度</strong>下：</p><ul><li>误检率；</li><li>漏检率；</li><li>ID Switch；</li><li>定位精度；</li><li>尺寸，朝向，速度估计精度；</li><li>状态估计收敛时间；</li><li>一系列描述时序稳定性的指标；</li></ul><p>　　与前述 FCW 功能类似，只是多了较多的指标。过滤操作也做的更加细致，我们还可以将目标做重要性等级划分，比如本车道前车多少米内，那指标基本都要达到 99%+；还可以将地面区域做重要性划分（比距离维度更加细致，可以认为是三维层面），周围几米内，那误检率肯定要非常低。除了过滤策略需要仔细设计外，匹配策略也需要进一步思考。如果传感器本身精度就有限，那么匹配策略就要相应放宽。还需注意的是引入过滤策略后，FP与FN计算的细微差别，比如有个过滤条件为去除目标像素面积小于一定阈值的目标集 A，观测值与真值匹配时，如果与 A 中的目标匹配上，那么不应该记为 FP，如果没匹配上 A 中的目标，那么 A 中地目标也不应该被记为 FN。这种类似的情况逻辑要思考清楚。</p><h2 id="summary">3. Summary</h2><p>　　以上设计的出发点是，我们要承认<strong>算法的不完美性</strong>以及<strong>传感器的局限性</strong>，在工程领域，一定要首先解决主要矛盾，再打磨细节。本文还对以下内容未作进一步分析（以后有机会再写文细究）：</p><ul><li>状态估计时序相关指标，描述估计的时序稳定性，也可以用于 MOT 的评估；</li><li>标注与过滤策略的关系，过滤策略往往依赖于标注策略；</li><li>各个指标的阈值确定，确定阈值也是产品中一件重要而又系统的事，有时候比指标设计更复杂； 　　</li></ul><p><a id="1" href="#1ref">[1]</a> Yang B, Huang C, Nevatia R. <a href="https://scholar.google.com/scholar?lookup=0&amp;q=Learning+affinities+and+dependencies+for+multi-target+tracking+using+a+CRF+model&amp;hl=zh-CN&amp;as_sdt=0,5&amp;as_vis=1" target="_blank" rel="noopener">Learning affinities and dependencies for multi-target tracking using a CRF model</a>[C]//CVPR 2011. IEEE, 2011: 1233-1240.<br><a id="2" href="#2ref">[2]</a> Choi W, Savarese S. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=Multiple+target+tracking+in+world+coordinate+with+single%2C+minimally+calibrated+camera&amp;btnG=" target="_blank" rel="noopener">Multiple target tracking in world coordinate with single, minimally calibrated camera</a>[C]//European Conference on Computer Vision. Springer, Berlin, Heidelberg, 2010: 553-567.<br><a id="3" href="#3ref">[3]</a> Kasturi, Rangachar, et al. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=Framework+for+performance+evaluation+of+face%2C+text%2C+and+vehicle+detection+and+tracking+in+video%3A+Data%2C+metrics%2C+and+protocol&amp;btnG=" target="_blank" rel="noopener">Framework for performance evaluation of face, text, and vehicle detection and tracking in video: Data, metrics, and protocol</a> IEEE transactions on Pattern Analysis and Machine intelligence 31.2 (2008): 319-336.<br><a id="4" href="#4ref">[4]</a> Yamaguchi K, Berg A C, Ortiz L E, et al. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=who+are+you+with+and+where+are+you+going&amp;btnG=" target="_blank" rel="noopener">Who are you with and where are you going?</a>[C]//CVPR 2011. IEEE, 2011: 1345-1352.<br><a id="5" href="#5ref">[5]</a> Bernardin K, Stiefelhagen R. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=evaluating+multiple+object+tracking+performance+the+clear+mot+metrics&amp;btnG=" target="_blank" rel="noopener">Evaluating multiple object tracking performance: the CLEAR MOT metrics</a>[J]. Journal on Image and Video Processing, 2008, 2008: 1.<br><a id="6" href="#6ref">[6]</a> Kratz L, Nishino K. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=%E2%80%9CTracking+with+local+spatio-temporal+motion+patterns+in+extremely+crowded+scenes&amp;btnG=" target="_blank" rel="noopener">Tracking with local spatio-temporal motion patterns in extremely crowded scenes</a>[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE, 2010: 693-700.<br><a id="7" href="#7ref">[7]</a> Ristic B, Vo B N, Clark D, et al. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=a+metric+for+performance+evaluation+of+multi-target+tracking+algorithms&amp;btnG=" target="_blank" rel="noopener">A metric for performance evaluation of multi-target tracking algorithms</a>[J]. IEEE Transactions on Signal Processing, 2011, 59(7): 3452-3457.<br><a id="8" href="#8ref">[8]</a> Schuhmacher D, Vo B T, Vo B N. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=A+Consistent+Metric+for+Performance+Evaluation+of+Multi-Object+Filters&amp;btnG=" target="_blank" rel="noopener">A consistent metric for performance evaluation of multi-object filters</a>[J]. IEEE transactions on signal processing, 2008, 56(8): 3447-3457.<br><a id="9" href="#9ref">[9]</a> Li Y, Huang C, Nevatia R. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=Learning+to+associate%3A+Hybridboosted+multi-target+tracker+for+crowded+scene&amp;btnG=" target="_blank" rel="noopener">Learning to associate: Hybridboosted multi-target tracker for crowded scene</a>[C]//2009 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2009: 2953-2960.<br><a id="10" href="#10ref">[10]</a> Song B, Jeng T Y, Staudt E, et al. <a href="https://scholar.google.com/scholar?hl=zh-CN&amp;as_sdt=0%2C5&amp;as_vis=1&amp;q=A+stochastic+graph+evolution+framework+for+robust+multi-target+tracking&amp;btnG=" target="_blank" rel="noopener">A stochastic graph evolution framework for robust multi-target tracking</a>[C]//European Conference on Computer Vision. Springer, Berlin, Heidelberg, 2010: 605-619.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　MOT 是一个比较基本的技术模块，在视频监控中，常用于行人行为分析、姿态估计等任务的前序模块；在自动驾驶中，MOT 是动态目标状态估计的重要环节。在学术界，MOT 算法性能的评价准则已经较为完善，其指标主要关注，尽可能地覆盖所有性能维度，以及指标的简洁性（上一篇有较多介
      
    
    </summary>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/categories/MOT/"/>
    
    
      <category term="autonomous driving" scheme="https://leijiezhang001.github.io/tags/autonomous-driving/"/>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/tags/MOT/"/>
    
      <category term="tracking" scheme="https://leijiezhang001.github.io/tags/tracking/"/>
    
  </entry>
  
  <entry>
    <title>MOT 评价指标-&quot;Evaluating Multiple Object Tracking Performance, the CLEAR MOT Metrics&quot;</title>
    <link href="https://leijiezhang001.github.io/MOT-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87-Evaluating-Multiple-Object-Tracking-Performance-the-CLEAR-MOT-Metrics/"/>
    <id>https://leijiezhang001.github.io/MOT-评价指标-Evaluating-Multiple-Object-Tracking-Performance-the-CLEAR-MOT-Metrics/</id>
    <published>2019-06-02T06:23:46.000Z</published>
    <updated>2019-10-07T14:29:47.140Z</updated>
    
    <content type="html"><![CDATA[<p>　　这篇文章介绍了两个综合性指标 MOTA 以及 MOTP 的计算过程，这两个指标有优劣势，但是作为综合性指标至今在学术界仍广泛应用。本文主要介绍其设计思想及计算过程。<br>　　一个理想的 MOT 算法，我们期望每一帧：</p><ul><li>准确检测目标的数量；</li><li>准确估计每个目标的状态，如位置，朝向，速度等；</li><li>准确估计每个目标的轨迹，即目标的 ID 不变性；</li></ul><p>这就要求评价准则：</p><ul><li>能评估目标定位的精度；</li><li>能反映目标轨迹的追踪能力，即同一个目标产生唯一的 ID；</li></ul><p>此外，为了提高评价准则的实用性：</p><ul><li>参数尽可能少，阈值可调；</li><li>易于理解，表现方式符合人们的直觉；</li><li>有较强的通用性，能评估各种跟踪算法；</li><li>指标个数少，但是能足够反映算法不同维度的性能；</li></ul><p>假设第 \(t\) 帧，有目标集 \(\{o_1,...,o_n\}\)，跟踪算法的输出(hypotheses)：\(\{h_1,...h_m\}\)。根据上述设计准则，设计评价计算过程：</p><ol type="1"><li> 构建 \(h_j\) 与 \(o_i\) 的最优匹配；</li><li> 对于每个匹配对，计算位置估计误差；</li><li> 累加所有匹配对的误差，包括：<ol type="a"><li> 计算漏检数(FN)；</li><li> 计算误检数(FP)；</li><li> 计算 ID swith 次数，包括两个邻近目标的 ID 交换，以及遮挡后，同一目标的 ID 跳变；</li></ol></li></ol><p>由此可得到两大指标：</p><ul><li>tracking precision，目标位置的估计精度；</li><li>tracking accuracy，包括 misses(FN), FP, mismatches(IDs), failures to recover；</li></ul><p>下面分两块做细节分析，匹配构建 (Establishing Correspondences) 与评价指标 (Metrics)。</p><h2 id="匹配构建">1. 匹配构建</h2><p>　　算法估计与目标真值的匹配，大致还是基于匹配最近 object-hypothesis 的思想，没匹配上的估计就是 FP，没匹配上的真值就是 FN。但是这中间需要进一步考虑一些问题。</p><h3 id="有效匹配">1.1. 有效匹配</h3><p>　　如果算法估计 \(h_j\) 与目标 \(o_i\) 的最近距离 \(dist_{i,j}\) 超过了一定的阈值 \(T\)，那么这个匹配也是不合理的，因为这个距离误差加入到定位误差中是不合理的，所以只能说这个跟踪的结果不是这个目标。关于距离的度量：</p><ul><li>区域（框）跟踪器，距离可用两者的重叠区域来度量，\(T\) 可以设为 0；</li><li>点跟踪器，距离可用两者中心点的欧氏距离来度量，\(T\) 可以根据目标的尺寸来设定；</li></ul><h3 id="跟踪一致性">1.2. 跟踪一致性</h3><p>　　统计目标与算法输出的匹配跳变的次数，也就是目标 ID 的跳变数。文章还提到，当目标有两个有效地匹配时，选择之前的匹配，即使那个匹配的距离大于另一个匹配，这点当存在两个很近的目标时，可能会有问题，需要全局来看。</p><h3 id="匹配过程">1.3. 匹配过程</h3><ol type="1"><li> 对 \(t\) 帧，考虑 \(M_{t-1}\) 中所有匹配是否还依然有效，包括目标真值及算法输出是否还存在，如果都存在，那么距离是否超出阈值 \(T\)；</li><li> 对于剩下的没找到匹配的真值目标，在唯一匹配以及阈值约束下，可采用匹配算法或者贪心算法来求解，使得距离误差的总和最小（文章的意思是排除了从上一帧继承的已有匹配，当目标密集时，这部分也应该加入进来优化）。统计当前帧目标真值匹配的跳变数 \(mme_t\)，作为 mismatch errors；</li><li> 经过之前两步后，找到了所有的匹配，统计匹配个数为 \(c_t\)，计算匹配上的目标真值与算法输出的定位误差 \(d_t^i\)；</li><li> 统计没有匹配上的算法输出 (hypotheses) 为 \(fp_t\)，没有匹配上的目标真值为 \(m_t\)，目标真值个数为 \(g_t\)；</li><li> 每一帧重复步骤１，第一帧没有 mismatch；</li></ol><h2 id="评价指标">2. 评价指标</h2><p>　　基于以上的匹配策略，得出两个合理的指标：</p><ul><li><strong>MOTP</strong>(multiple object tracking precision)，跟踪定位精度指标：<span class="math display">\[MOTP=\frac{\sum_{i,t}d_t^i}{\sum_tc_t}\]</span></li><li><strong>MOTA</strong>(multiple object tracking accuracy)，综合了漏检率，误检率，以及 ID 跳变率：<span class="math display">\[MOTA=1-\frac{\sum_t(m_t+fp_t+mme_t)}{\sum_tg_t}\]</span></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　这篇文章介绍了两个综合性指标 MOTA 以及 MOTP 的计算过程，这两个指标有优劣势，但是作为综合性指标至今在学术界仍广泛应用。本文主要介绍其设计思想及计算过程。&lt;br&gt;
　　一个理想的 MOT 算法，我们期望每一帧：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;准确检测目标的数量；&lt;
      
    
    </summary>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/categories/paper-reading/"/>
    
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/tags/MOT/"/>
    
      <category term="tracking" scheme="https://leijiezhang001.github.io/tags/tracking/"/>
    
  </entry>
  
  <entry>
    <title>MOT 综述-&#39;Multiple Object Tracking: A Literature Review&#39;</title>
    <link href="https://leijiezhang001.github.io/MOT-%E7%BB%BC%E8%BF%B0-Multiple-Object-Tracking-A-Literature-Review/"/>
    <id>https://leijiezhang001.github.io/MOT-综述-Multiple-Object-Tracking-A-Literature-Review/</id>
    <published>2019-05-28T08:22:58.000Z</published>
    <updated>2019-10-07T14:29:47.140Z</updated>
    
    <content type="html"><![CDATA[<p>　　之前做 MOT 还是沿着 SOT 的思路，这篇文章对 MOT 有一个很深入且很有框架性的综述，以下对这篇文章做一个提炼，并加入一些自己的想法。<br>　　MOT 作为一个中层任务，是一些高层任务的基础，比如行人的 pose estimation，action recognition，behavior analysis，车辆的 state estimation。单目标跟踪(SOT)主要关注 appearance model 以及 motion model 的设计，解决尺度、旋转、光照等影响因素。而 MOT 包含两个任务：目标数量以及目标ID，这就要求 MOT 还需要解决其它问题：</p><ul><li>frequent occlusions</li><li>initialization and termination of tracks</li><li>similar appearance</li><li>interactions among multiple objects</li></ul><h2 id="问题描述">1. 问题描述</h2><p>　　多目标跟踪实际上是多参数估计问题。给定图像序列\(\{I_1,I_2,...,I_t,...\}\)，第\(t\)帧中目标个数为\(M_t\)，第\(t\)帧中所有目标的状态表示为\(S_t=\{s_t^ 1,s_t^ 2,...,s_t^ {M_t}\}\)，第\(i\)个目标的轨迹表示为\(s_{1:t}^ i=\{s_1^ i,s_2^ i,...,s_t^ i\}\)，所有图像中所有目标的状态序列为\(S_{1:t}=\{S_1,S_2,...,S_t\}\)。相应的，所有图像中所有目标观测到的状态序列为\(O_{1:t}=\{O_1,O_2,...,O_t\}\)。多目标跟踪的优化目标是求解最优的各目标状态，即求解一个后验概率问题，<span class="math display">\[ \widehat{S}_{1:t}=\mathop{\arg\max}_{S_{1:t}}P(S_{1:t}|O_{1:t})\]</span> 这种形式有两种实现方法：</p><ul><li><p><strong>probabilistic inference</strong><br>适合用于 online tracking 任务，Dynamic Model 为 \(P(S_t|S_{t-1})\)，Observation Model 为 \(P(O_t|S_t)\)，两步求解过程：<br>　\(\circ\)　Predict: \(P(S_t|O_{1:t-1})=\int P(S_t|S_{t-1})dS_{t-1}\)<br>　\(\circ\)　Update: \(P(S_t|O_{1:t}) \propto P(O_t|S_t)P(S_t|O_{1:t-1})\)</p></li><li><p><strong>deterministic optimization</strong><br>适合用于 offline tracking 任务，直接利用多帧信息进行最优化求解。</p></li></ul><h2 id="分类方法">2. 分类方法</h2><ul><li><strong>initialization method</strong><br>初始化方式分为：<br>　\(\circ\)　Detection-Based Tracking，优势明显，除了只能处理特定的目标类型；<br>　\(\circ\)　Detection-Free Tracking，能处理任何目标类型；</li><li><strong>processing mode</strong><br>根据是否使用未来的观测，处理方式可分为：<br>　\(\circ\)　online tracking，适合在线任务，缺点是观测量会比较少；<br>　\(\circ\)　offline tracking，输出结果存在时延，理论上能获得全局最优解；</li><li><p><strong>type of output</strong><br>根据问题求解方式输出是否存在随机性：<br>　\(\circ\)　probabilistic inference，概率性推断；<br>　\(\circ\)　deterministic inference，求解最大后验概率；</p><p><strong>自动驾驶等在线任务主要关注 Detection-Based，online tracking。</strong></p></li></ul><h2 id="框架">3. 框架</h2><p>　　MOT 主要考虑两个问题：</p><ul><li>目标在不同帧之间的相似性度量，即对appearance, motion, interaction, exclusion, occlusion的建模；</li><li>恢复出目标的ID，即 inference 过程；</li></ul><h3 id="appearance-model">3.1. Appearance Model</h3><h4 id="visual-representation">3.1.1. Visual Representation</h4><p>　　视觉表达即目标的特征表示方式：</p><ol type="1"><li><strong>local features</strong><br>本质上是点特征，点特征由 corner+descriptor(角点+描述子) 组成。KLT(good features to track)在 SOT 中应用广泛，用它可以生成短轨迹，估计相机运动位姿，运动聚类等；Optical Flow也是一种局部特征，在数据关联之前也可用于将检测目标连接到短轨迹中去。</li><li><strong>region features</strong><br>在一个块区域内提取特征，根据像素间作差的次数，可分为：<ul><li>zero-order, color histogram &amp; raw pixel template</li><li>first-order, HOG &amp; level-set formulation(?)</li><li>up-to-second-order, Region covariance matrix</li></ul></li><li><strong>others</strong><br>其它特征本质上也需要 local 或 region 的方式提取，只是原始信息并不是灰度或彩图。如 depth,probabilistic occupancy map, gait feature.</li></ol><p>　　Local features，比如颜色特征，在计算上比较高效，但是对遮挡，旋转比较敏感；Region features 里，HOG 对光照有一定的鲁棒性，但是对遮挡及形变效果较差；Region covariance matrix 更加鲁棒，但是需要更高的计算量；深度特征也比较有效，但是需要额外的获取深度信息的代价。</p><h4 id="statistical-measuring">3.1.2. Statistical Measuring</h4><p>　　有了目标的特征表示方式之后，就可以评价两个观察的目标的相似性。特征表示的线索(cue)可分为：</p><ol type="1"><li><strong>single cue</strong><br>因为只有一个线索，相似性(similarity)可以直接通过两个向量的距离转换得到。可以将距离指数化，高斯化。也可以将不相似度转为可能性，用协方差矩阵表示。</li><li><strong>multiple cues</strong><br>多线索，即多种特征的融合，能极大提高鲁棒性，融合的策略有：<ul><li>Boosting, 选取一系列的特征，用 boost 算法选取表达能力最强的特征；</li><li>Concatenation, 各个特征直接在空间维度上串起来，形成一个 cue 的表达方式；</li><li>Summation, 加权融合各个特征，形成一个 cue 的表达方式；</li><li>Product, 各个特征相乘的方式，比如目标 \(s_0\) 的某个潜在匹配 \(s_1\) 的颜色，形状特征为 \(color\), \(shape\) 的概率为 \(p(color|s_0)\), \(p(shape|s_0)\), 假设特征独立，那么， 　　　　　　　<span class="math display">\[p(s_1|s_0)=p(color, shape|s_0)=p(color|s_0)\cdot p(shape|s_0)\]</span></li><li>Cascading, coarse-to-fine 的方式，逐步精细化搜索；</li></ul></li></ol><h3 id="motion-model">3.2. Motion Model</h3><p>　　运动模型对关联两个 tracklets 比较管用，而 online tracking 任务，对输出的时延要求较高，所以其中一个 tracklet 可以任务就是当前帧与上一帧形成的轨迹，所以这里很难去计算两个 tracklets 的相似度。能看到的一个应用点就是，通过 motion model 模型，预测下一时刻目标的位置，作为一个线索项目。以下讨论的各模型主要是为了度量 tracklets 的相似性，从而做 tracklets 的匹配。</p><h4 id="linear">3.2.1. Linear</h4><ul><li>Velocity Smoothness. N 帧 M 个目标轨迹: \(C_{dyn}=\sum_{t=1}^ {N-2}\sum_{i=1}^ {M}\parallel v_i^ t-v_i^ {t+1}\parallel^ 2\)</li><li>Position Smoothness. \(G(p^ {tail}+v^ {tail}\Delta t-p^ {head}, \sum_p)\cdot G(p^ {head}-v^ {head}\Delta t-p^ {tail}, \sum_p)\)</li><li>Acceleration Smoothness.</li></ul><h4 id="non-linear">3.2.2. Non-linear</h4><p>　　运动模型假设是非线性的，相似度计算还是按照以上高斯形式。引为中提到，非线性运动模型并不作为目标的惩罚因子，因为目标并不需要满足该模型，但是只要有目标满足，就降低惩罚系数。</p><h3 id="interaction-model">3.3. Interaction Model</h3><h4 id="social-force-models">3.3.1. Social Force Models</h4><ol type="1"><li><strong>Individual Force</strong><ul><li>fidelity, 目标不会改变它的目的地方向；</li><li>constancy, 目标不会突然改变速度和方向；</li></ul></li><li><strong>Group Force</strong><ul><li>attraction, 目标间应该尽量靠近；</li><li>repulsion, 目标间也得保留适当的距离；</li><li>coherence, 同一个 group 里面的目标速度应该差不多；</li></ul></li></ol><h4 id="crowd-motion-pattern-models">3.3.2. Crowd Motion Pattern Models</h4><p>　　当一个 group 比较密集的时候，单个目标的运动模型不太显著了，这时候群体的运动模型更加有效，可以用一些方法来构建群体运动模型。</p><h3 id="exclusion-model">3.4. Exclusion Model</h3><h4 id="detection-level">3.4.1. Detection-level</h4><p>　　同一帧两个检测量不能指向同一个目标。匹配 tracklets 时，可以将这一项作为惩罚项。不过目前的检测技术都做了 NMS，基本可以消除这种情况。</p><h4 id="trajectory-level">3.4.2. Trajectory-level</h4><p>　　两个轨迹不能非常靠近。对于 online tracking 来说，就是 tracking 结果的两个量不能挨在一起，如果挨在一起，就说明有问题，比如遮挡，或跟丢。</p><h3 id="occlusion-handling">3.5. Occlusion Handling</h3><ul><li>Part-to-whole, 将目标分成栅格来处理；</li><li>Hypothesize-and-test,</li><li>Buffer-and-recover, 在遮挡产生前，记录一定量的观测，遮挡后恢复；</li><li>Others</li></ul><h3 id="inference">3.6. Inference</h3><h4 id="probabilistic-inference">3.6.1. Probabilistic Inference</h4><p>　　概率法只需要用到当前时刻之前的信息，所以适合用于 online tracking 任务。首先，如果假设一阶马尔科夫，当前目标的状态之依赖于前一时刻目标的状态，即 <em>dynamic model</em>： <span class="math display">\[P(S_t|S_{1:t-1})=P(S_t|S_{t-1})\]</span> 其次，观测是独立的，即当前目标的观测只由当前目标的状态决定，<em>observation model</em>： <span class="math display">\[P(O_{1:t}|S_{1:t})=\prod_{i=1}^t P(O_t|S_t)\]</span> dynamic model 对应的就是跟踪算法策略，observation model 是状态观测手段，包括检测方法。目标状态估计的迭代过程为：</p><ul><li><strong>predict step</strong><br>根据 dynamic model，由目标的上一状态预测当前状态的后验概率分布；</li><li><strong>update step</strong><br>根据 observation model，更新当前目标状态的后验概率分布；</li></ul><p>　　状态估计的过程伴随着噪音等因素的影响，常用的概率推断模型有：</p><ul><li>Kalman filter</li><li>Extended Kalman filter</li><li>Particle filter</li></ul><h4 id="deterministic-optimization">3.6.2. Deterministic Optimization</h4><p>　确定性优化法需要至少一个时间窗口的观测量，所以适合 offline tracking 任务。优化方法有：</p><ul><li>Bipartite graph matching</li><li>Dynamic Programming</li><li>Min-cost max-flow network flow</li><li>Conditional random field</li><li>MWIS(Maximum-weight independent set)</li></ul><h2 id="评价方法">4. 评价方法</h2><p>　　评价方法是非常重要的，一方面对算法系统进行调参优化，另一方面比较各个不同算法的优劣。评价方法 (evaluation) 包括评价指标 (metrics) 以及数据集 (datasets)，多类别的数据集主要有：</p><ul><li><a href="https://motchallenge.net/results/MOT17/" target="_blank" rel="noopener">MOT Challenge</a></li><li><a href="http://www.cvlibs.net/datasets/kitti/eval_tracking.php" target="_blank" rel="noopener">KITTI</a>　　</li></ul><p>评价指标可分为：</p><p>A. <strong>检测指标</strong><br>　\(\lozenge\)　准确性(Accuracy)</p><ul><li>Recall &amp; Precision</li><li>False Alarme per Frame(FAF) rate, from <a href="https://www.google.com/search?q=Learning+affinities+and+dependencies+for+multi-target+tracking+using+a+CRF+model&amp;oq=Learning+affinities+and+dependencies+for+multi-target+tracking+using+a+CRF+model&amp;aqs=chrome..69i57.1077j0j9&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">paper</a></li><li>False Positive Per Image(FPPI), from <a href="https://www.google.com/search?q=Multiple+target+tracking+in+world+coordinate+with+single%2C+minimally+calibrated+camera&amp;oq=Multiple+target+tracking+in+world+coordinate+with+single%2C+minimally+calibrated+camera&amp;aqs=chrome..69i57j0.1134j0j9&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">paper</a></li><li>MODA(Multiple Object Detection Accuracy), 包含了 false positive &amp; miss dets. from <a href="https://www.google.com/search?q=Framework+for+performance+evaluation+of+face%2C+text%2C+and+vehicle+detection+and+tracking+in+video%3A+Data%2C+metrics%2C+and+protocol&amp;oq=Framework+for+performance+evaluation+of+face%2C+text%2C+and+vehicle+detection+and+tracking+in+video%3A+Data%2C+metrics%2C+and+protocol&amp;aqs=chrome..69i57j69i61.973j0j9&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">paper</a></li></ul><p>　\(\lozenge\)　精确性(Precision)</p><ul><li>MODP(Multiple Object Detection Precision), 衡量检测框与真值框的位置对齐程度；from <a href="https://www.google.com/search?q=Framework+for+performance+evaluation+of+face%2C+text%2C+and+vehicle+detection+and+tracking+in+video%3A+Data%2C+metrics%2C+and+protocol&amp;oq=Framework+for+performance+evaluation+of+face%2C+text%2C+and+vehicle+detection+and+tracking+in+video%3A+Data%2C+metrics%2C+and+protocol&amp;aqs=chrome..69i57j69i61.973j0j9&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">paper</a></li></ul><p>B. <strong>跟踪指标</strong><br>　\(\lozenge\)　准确性(Accuracy)</p><ul><li>ID switches(IDs), from <a href="https://www.google.com/search?safe=strict&amp;ei=agXyXMaQEKyl_Qa4zJrQCg&amp;q=who+are+you+with+and+where+are+you+going&amp;oq=Who+are+you+with+and+where+are+you+going&amp;gs_l=psy-ab.1.0.0i203.53050.53050..55771...0.0..0.559.559.5-1......0....2j1..gws-wiz.nigYYAJc4jQ" target="_blank" rel="noopener">paper</a></li><li>MOTA(Multiple Object Tracking Accuracy), 包含了FP，FN，mismatch；from <a href="https://www.google.com/search?safe=strict&amp;ei=0ATyXP6lPIO6ggfIk6GAAQ&amp;q=evaluating+multiple+object+tracking+performance+the+clear+mot+metrics&amp;oq=Evaluating+Multiple+Object+Tracking+Performance&amp;gs_l=psy-ab.1.1.35i39j0j0i30j0i67.46576.46576..50024...0.0..0.436.436.4-1......0....2j1..gws-wiz.KAREeooiDMo" target="_blank" rel="noopener">paper</a></li></ul><p>　\(\lozenge\)　精确性(Precision)</p><ul><li>MOTP(Multiple Object Tracking Precision), from <a href="https://www.google.com/search?safe=strict&amp;ei=0ATyXP6lPIO6ggfIk6GAAQ&amp;q=evaluating+multiple+object+tracking+performance+the+clear+mot+metrics&amp;oq=Evaluating+Multiple+Object+Tracking+Performance&amp;gs_l=psy-ab.1.1.35i39j0j0i30j0i67.46576.46576..50024...0.0..0.436.436.4-1......0....2j1..gws-wiz.KAREeooiDMo" target="_blank" rel="noopener">paper</a></li><li>TDE(Tracking Distance Error), from <a href="https://www.google.com/search?safe=strict&amp;ei=fATyXNnwEvCH_QaG17fwDA&amp;q=%E2%80%9CTracking+with+local+spatio-temporal+motion+patterns+in+extremely+crowded+scenes&amp;oq=%E2%80%9CTracking+with+local+spatio-temporal+motion+patterns+in+extremely+crowded+scenes&amp;gs_l=psy-ab.12..0i30.82181.82181..83291...0.0..0.292.292.2-1......0....2j1..gws-wiz.hs0Je90zzHU" target="_blank" rel="noopener">paper</a></li><li>OSPA(optimal subpattern assignment), from <a href="https://www.google.com/search?safe=strict&amp;ei=_gDyXPKINY21ggeKtb2oDg&amp;q=a+metric+for+performance+evaluation+of+multi-target+tracking+algorithms&amp;oq=A_Metric_for_Performance_Evaluation_of_Multi-Targe&amp;gs_l=psy-ab.1.0.0i30.106502.106502..109413...0.0..0.303.303.3-1......0....2j1..gws-wiz.vrzc0MG18OM" target="_blank" rel="noopener">paper</a></li></ul><p>　\(\lozenge\)　完整性(Completeness)</p><ul><li>MT, the numbers of Mostly Tracked, from <a href="https://www.google.com/search?q=Learning+to+associate%3A+Hybridboosted+multi-target+tracker+for+crowded+scene&amp;oq=Learning+to+associate%3A+Hybridboosted+multi-target+tracker+for+crowded+scene&amp;aqs=chrome..69i57.1261j0j9&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank" rel="noopener">paper</a></li><li>PT, the numbers of Partly Tracked</li><li>ML, the numbers of Mostly Lost</li><li>FM, the numbers of Fragmentation</li></ul><p>　\(\lozenge\)　鲁棒性(Robustness)</p><ul><li>RS(Recover from Short-term occlusion), from <a href="https://www.google.com/search?safe=strict&amp;ei=_gDyXPKINY21ggeKtb2oDg&amp;q=A+stochastic+graph+evolution+framework+for+robust+multi-target+tracking&amp;oq=A+stochastic+graph+evolution+framework+for+robust+multi-target+tracking&amp;gs_l=psy-ab.12..0i30.453442.453442..454691...0.0..0.315.315.3-1......0....2j1..gws-wiz.OPYJ8mRFgYg" target="_blank" rel="noopener">paper</a></li><li>RL(Recover from Long-term occlusion)</li></ul><p>评价指标汇总： <img src="/MOT-综述-Multiple-Object-Tracking-A-Literature-Review/metrics.png" width="50%" height="50%"></p><h2 id="总结">5. 总结</h2><h3 id="还存在的问题">5.1. 还存在的问题</h3><p>　　MOT 算法模块较多，参数也较复杂，但是最依赖于检测模块的性能，所以算法间比较性能时，需要注意按模块进行变量控制。</p><h3 id="未来研究方向">5.2. 未来研究方向</h3><ul><li><strong>MOT with video adaptation</strong>，检测模块式预先训练的，需要在线更新学习；</li><li><strong>MOT under multiple camera</strong>: \(\circ\)　multiple views，不同视野相同场景信息的记录， \(\circ\)　non-overlapping multi-camera，不同视野不同场景的 reidentification；</li><li><strong>Multiple 3D object tracking</strong>，能更准确预测位置，大小，更有效处理遮挡；</li><li><strong>MOT with scene understanding</strong>，拥挤场景，用场景理解来有效跟踪；</li><li><strong>MOT with deep learning</strong></li><li><strong>MOT with other cv tasks</strong>，和其他任务融合，比如目标分割等；</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　之前做 MOT 还是沿着 SOT 的思路，这篇文章对 MOT 有一个很深入且很有框架性的综述，以下对这篇文章做一个提炼，并加入一些自己的想法。&lt;br&gt;
　　MOT 作为一个中层任务，是一些高层任务的基础，比如行人的 pose estimation，action reco
      
    
    </summary>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/categories/paper-reading/"/>
    
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
      <category term="MOT" scheme="https://leijiezhang001.github.io/tags/MOT/"/>
    
      <category term="tracking" scheme="https://leijiezhang001.github.io/tags/tracking/"/>
    
  </entry>
  
  <entry>
    <title>3D Detection Paper List</title>
    <link href="https://leijiezhang001.github.io/3D-Detection-paper-list/"/>
    <id>https://leijiezhang001.github.io/3D-Detection-paper-list/</id>
    <published>2019-05-22T03:43:33.000Z</published>
    <updated>2019-10-07T14:29:47.140Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章从输入数据类别上进行 3D Detection paper 的归类。</p><h2 id="rgb">RGB</h2><h2 id="rgb-d双目单目点云">RGB-D(双目，单目+点云)</h2><h2 id="lidar">Lidar</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这篇文章从输入数据类别上进行 3D Detection paper 的归类。&lt;/p&gt;
&lt;h2 id=&quot;rgb&quot;&gt;RGB&lt;/h2&gt;
&lt;h2 id=&quot;rgb-d双目单目点云&quot;&gt;RGB-D(双目，单目+点云)&lt;/h2&gt;
&lt;h2 id=&quot;lidar&quot;&gt;Lidar&lt;/h2&gt;

      
    
    </summary>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/categories/paper-reading/"/>
    
    
      <category term="3D Detection" scheme="https://leijiezhang001.github.io/tags/3D-Detection/"/>
    
      <category term="paper reading" scheme="https://leijiezhang001.github.io/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>Study Topic List</title>
    <link href="https://leijiezhang001.github.io/study-topic-list/"/>
    <id>https://leijiezhang001.github.io/study-topic-list/</id>
    <published>2019-05-20T05:30:54.000Z</published>
    <updated>2019-10-07T14:29:47.148Z</updated>
    
    <content type="html"><![CDATA[<p>　　本文罗列了相关领域知识的学习资料。</p><h2 id="detection">1. Detection</h2><h3 id="d-detection">1.1. 2D Detection</h3><ul><li><a href="https://zhuanlan.zhihu.com/p/34142321" target="_blank" rel="noopener">入门</a></li><li><a href="https://github.com/amusi/awesome-object-detection" target="_blank" rel="noopener">amusi</a></li><li><span class="citation" data-cites="handong">[Object Detection @handong]</span>(https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html#yolov3)</li><li><a href="http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/" target="_blank" rel="noopener">Object Detection and Classification using R-CNNs</a></li><li><a href="https://paperswithcode.com/task/object-detection" target="_blank" rel="noopener">Paper with Code</a></li></ul><h3 id="d-detection-1">1.2. 3D Detection</h3><ul><li><a href="https://paperswithcode.com/task/3d-object-detection" target="_blank" rel="noopener">Paper with Code</a></li><li><a href="http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d" target="_blank" rel="noopener">KITTI Leaderboard</a></li></ul><hr><h2 id="tracking">2. Tracking</h2><h3 id="single-object-tracking">2.1. Single Object Tracking</h3><ul><li><a href="https://paperswithcode.com/task/visual-object-tracking" target="_blank" rel="noopener">Paper with Code</a></li></ul><h3 id="multi-object-tracking">2.2. Multi Object Tracking</h3><ul><li><a href="https://paperswithcode.com/task/multiple-object-tracking" target="_blank" rel="noopener">Paper with Code</a></li><li><a href="https://zhuanlan.zhihu.com/p/65177442" target="_blank" rel="noopener">Paper List</a></li><li><a href="https://motchallenge.net/results/MOT17/" target="_blank" rel="noopener">MOT Challenge</a></li><li><a href="https://arxiv.org/abs/1409.7618" target="_blank" rel="noopener">综述：Multiple Object Tracking: A Literature Review</a></li><li><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2013/html/Wu_Online_Object_Tracking_2013_CVPR_paper.html" target="_blank" rel="noopener">综述：Online object tracking: A benchmark</a></li><li><a href="https://arxiv.org/abs/1504.01942" target="_blank" rel="noopener">综述：MOTChallenge 2015: Towards a benchmark for multi-target tracking</a></li></ul><hr><h2 id="computational-photography">3. Computational Photography</h2><ul><li><a href="http://graphics.cs.cmu.edu/courses/15-463/2017_fall/" target="_blank" rel="noopener">2017年秋季的计算摄影学课程15-463</a></li></ul><hr><h2 id="cnn-acc">4. CNN ACC</h2><hr><h2 id="slam">5. SLAM</h2><h3 id="理论知识">5.1. 理论知识</h3><ul><li><a href="http://cvrs.whu.edu.cn/downloads/ebooks/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6%E6%96%B9%E6%B3%95.pdf" target="_blank" rel="noopener">计算机视觉中的数学方法</a></li><li><a href="http://cvrs.whu.edu.cn/downloads/ebooks/Multiple%20View%20Geometry%20in%20Computer%20Vision%20%28Second%20Edition%29.pdf" target="_blank" rel="noopener">Multiple View Geometry in Computer Vision</a></li><li><a href="https://docs.ufpr.br/~danielsantos/ProbabilisticRobotics.pdf" target="_blank" rel="noopener">Probabilistic Robotics</a>(有中文版)</li><li><a href="http://asrl.utias.utoronto.ca/~tdb/bib/barfoot_ser17.pdf" target="_blank" rel="noopener">State Estimation for Robotics</a>(有中文版)</li><li><a href="https://github.com/gaoxiang12/slambook" target="_blank" rel="noopener">视觉SLAM十四讲</a></li></ul><h3 id="综述">5.2. 综述</h3><ul><li>[Visual Odometry Part I: Fundamentals]</li><li>[Visual Odometry Part II: Matching, Robustness, Optimization, Applications]</li><li><a href="https://link.springer.com/content/pdf/10.1186%2Fs40064-016-3573-7.pdf" target="_blank" rel="noopener">Review of Visual Odometry: Types, Approaches, Challenges, and Applications</a></li><li><a href="https://ipsjcva.springeropen.com/track/pdf/10.1186/s41074-017-0027-2" target="_blank" rel="noopener">Visual SLAM algorithms: a Survey from 2010 to 2016</a></li><li><a href="http://www.cvc.uab.es/~asappa/publications/C__IEEE_IV_2012_W3.pdf" target="_blank" rel="noopener">Visual SLAM for Driverless Cars: a Brief Survey</a></li><li><a href="https://link.springer.com/article/10.1007/s10462-012-9365-8" target="_blank" rel="noopener">Visual Simultaneous Locations and Mapping: a Survey</a></li></ul><h3 id="工具">5.3. 工具</h3><ul><li><a href="http://www.guyuehome.com/column/ros-explore" target="_blank" rel="noopener">ROS</a></li><li><a href="https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html" target="_blank" rel="noopener">Opencv Camera Calibration</a></li><li><a href="http://www.vision.caltech.edu/bouguetj/calib_doc/" target="_blank" rel="noopener">Matlab Camera Calibration Toolbox</a></li><li><a href="http://wiki.ros.org/camera_calibration" target="_blank" rel="noopener">ROS Wiki Camera Calibration</a></li></ul><h3 id="算法">5.4. 算法</h3><ul><li><a href="https://openslam-org.github.io/" target="_blank" rel="noopener">OpenSLAM</a></li></ul><h3 id="其它资料">5.5. 其它资料</h3><ul><li><a href="https://www.zhihu.com/people/cheng-xu-yuan-10/posts" target="_blank" rel="noopener">计算机视觉life</a></li><li><a href="https://paperswithcode.com/task/visual-odometry" target="_blank" rel="noopener">Paper with Code</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;　　本文罗列了相关领域知识的学习资料。&lt;/p&gt;
&lt;h2 id=&quot;detection&quot;&gt;1. Detection&lt;/h2&gt;
&lt;h3 id=&quot;d-detection&quot;&gt;1.1. 2D Detection&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://zhuanl
      
    
    </summary>
    
    
  </entry>
  
</feed>
