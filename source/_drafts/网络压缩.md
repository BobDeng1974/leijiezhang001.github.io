---
title: 网络压缩
date: 2019-07-08 17:10:16
tags:
---

pruning


quantization


knowledge distillation
知识蒸馏是指利用已经训练的一个较大复杂的 teacher 模型，指导一个较轻量的 student 模型。