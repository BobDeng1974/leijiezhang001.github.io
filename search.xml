<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[[paper_reading]-"Stereo R-CNN based 3D Object Detection for Autonomous Driving"]]></title>
    <url>%2F%5Bpaper_reading%5D-Stereo-RCNN-based-3D-Object-Detection-for-Autonomous-Driving%2F</url>
    <content type="text"><![CDATA[Learning 方法有什么致命缺点吗？我认为目前 Learning 方法还存在的较为棘手的问题是，有时候结果会出现非常低级的错误，或是说不可思议不合常理的 cornercases。所以我认为一个工程系统或是一个鲁棒的算法系统，在 Learning 之后做一个基于常理（如 geometry 约束或专家系统）的验证，能有效抑制这个问题。本文就是一个比较好的 learning+geometry 想结合的方法。 本文[1]基于图像语义及几何信息，通过 3D 目标的稀疏与密集约束，提出了一种准确的 3D 目标检测方法。根据输入数据的类型，作者将 3D 检测分为三大类： LiDAR-based，近期被研究的较多，基本是自动驾驶所必须的； Monocular-based，低成本方案； Stereo-based，相比 Monocular-based，有优势，但是研究较少； 本文就是 Stereo-based 3D 检测方案。不同于一般的 rgb+depth 作为输入的方案，本文直接将左右目 rgb 作为输入，没有显示地 depth 生成过程。工程上来说，这也极大地缩短了 3D Detection 的时延(latency)。 本文方法如图 1 所示，主要有三部分组成： &ensp;Network，又有三部分构成： Stereo RPN Module，输出左右图的 RoI； Classification and Regression branches，输出目标类别，朝向，尺寸； Keypoint branch，输出左目目标的关键点； &ensp;Sparse constraints，3D 框-2D 框的稀疏约束； &ensp;Dense constraints，准确定位的关键模块； 1.&ensp;Stereo R-CNN Network Stereo R-CNN 是在 Faster R-CNN 基础上，同时检测与关联左右目图像 2D 框的微小差异。 1.1.&ensp;Stereo RPN 在传统 RPN 网络的基础上，本文先对左右图做 paramid features 提取，然后将不同尺度的特征 concatenate 一起，进入 RPN 网络。 关键的一点是 objectness classification与 stereo box regression 的真值框定义不一样。如图 2 所示， 对于 objectness classification，真值框定义为左右目真值框的外接合并（union GT box），一个 anchor 在与真值框的交并比（Intersection-over-Union）大于 0.7 时标记为正样本，小于 0.3 时标记为负样本。分类任务的候选框包含了左右目真值框区域的信息。 对于 stereo box regression，真值框定义为左右目分别的真值框。待回归的参数定义为 \([\Delta u, \Delta w, \Delta u’, \Delta w’, \Delta v, \Delta h]\)，分别为左目的水平位置及宽，右目的水平位置及宽，垂直位置及高。因为输入为矫正过的左右目图像，所以可认为左右目的垂直方向上已经对齐。 每个左右目的 proposal 都是通过同一个 anchor 产生的，自然左右目的 proposal 是关联的。通过 NMS 后，保留左右目都还存在的 proposal 关联对，取前 2000 个用于训练，测试时取前 300 个。 1.2.&ensp;Stereo R-CNN 网络头包含两大部分： &ensp;Stereo Regression左右目的 proposal 关联对，分别在左右目的 feature 上进行 RoI Align 的操作，然后 concatenate 输入到全链接层。左右目的 RoI 对与真值框的 IoU 均大于 0.5 时定位正样本，左右目的 RoI 对与真值框的 IoU 有一个小于 0.5 且大于 0.1，则定位负样本。用四个分支分别预测： object class； stereo bounding boxes，与 stereo rpn 中一致，左右目的高度已对齐； dimension，先统计平均的尺寸，然后预测相对量； viewpoint angle，如图 3 所示，\(\theta\) 为相机坐标系下的朝向角，\(\beta\) 为相机中心点下的方位角(azimuth)，这三个目标在相机视野下是一样的，所以我们回归的量是视野角(viewpoint angle) \(\alpha=\theta+\beta\)，其中 \(\beta=arctan\left(-\frac{x}{z} \right) \)。并且为了连续性，回归量为 \([sin\,\alpha,cos\,\alpha]\)。 &ensp;Keypoint Prediction如图 4 所示，考虑 3D 框底部矩形的四个关键点，投影到图像平面后，最多只有一个关键点会在图像 2D 矩形框内。对左目图像进行关键点预测，类似 Mask R-CNN，在 6×28×28 的基础上，因为关键点只有图像坐标 u 方向才提供了额外的信息，所以对每列进行累加，最终输出 6×28 的向量。前 4 个通道代表每个关键点作为 perspective keypoint 投影到该 u 坐标下的概率；后 2 个通道代表该 u 坐标是左右边缘关键点(boundary keypoints)的概率。为了找出 perspective keypoint，softmax 应用于 4×28 的输出上；为了找出左右边缘关键点，softmax 分别应用于后两个 1×28 的输出上。训练的时候，4×28 中只有一个被赋予 perspective keypoint，忽略没有 perspective keypoint 的情况（遮挡等），然后最小化 cross-entropy loss；对于边缘关键点，则分别最小化 1×28 维度上的 cross-entropy loss，前景中也会被赋予边缘关键点。 2.&ensp;3D Box Estimation 已知关键点，2D 框，尺寸，朝向角，我们可以求解出 3D 框 \(\{x,y,z,\theta\}\)。求解目标是最小化 3D 框投影到 2D 框以及关键点的误差。如图 5 所示，已知 7 个观测量 \(z = \{u_l,v_t,u_r,v_b,u’_l,u’_r,u_p\}\)，分别代表左目 2D 框的左上坐标，右下坐标，右目 2D 框的左右 u 方向坐标，以及 perspective keypoint 的 u 方向坐标。在图 5 的情况下（其它视角下，注意符号变化），左上点投影关系如下：$$\require{cancel}\begin{bmatrix}u_l\\v_t\\1\\\end{bmatrix}=K\cdot\begin{bmatrix}x_{cam}^{tl}\\y_{cam}^{tl}\\z_{cam}^{tl}\\\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot\begin{bmatrix}x_{obj}^{tl}\\y_{obj}^{tl}\\z_{obj}^{tl}\\\end{bmatrix}=\begin{bmatrix}x\\y\\z\\\end{bmatrix}+\begin{bmatrix}cos\theta &amp; 0 &amp;sin\theta\\0 &amp; 1 &amp; 0\\-sin\theta &amp; 0 &amp; cos\theta\\\end{bmatrix} \cdot\begin{bmatrix}-\frac{w}{2}\\-\frac{h}{2}\\-\frac{l}{2}\\\end{bmatrix}$$其中 \(K\) 为相机内参，\(T_{cam}^{obj}\) 为目标中心坐标系在相机坐标系下的表示，\((\cdot)_{cam/obj}\) 分别为点在相机坐标系，目标中心坐标系下的表示。同样的，这个视野下，右下点为：$$\require{cancel}\begin{bmatrix}u_l\\v_t\\1\\\end{bmatrix}=K\cdot\begin{bmatrix}x_{cam}^{tl}\\y_{cam}^{tl}\\z_{cam}^{tl}\\\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot\begin{bmatrix}x_{obj}^{tl}\\y_{obj}^{tl}\\z_{obj}^{tl}\\\end{bmatrix}=\begin{bmatrix}x\\y\\z\\\end{bmatrix}+\begin{bmatrix}cos\theta &amp; 0 &amp;sin\theta\\0 &amp; 1 &amp; 0\\-sin\theta &amp; 0 &amp; cos\theta\\\end{bmatrix} \cdot\begin{bmatrix}\frac{w}{2}\\\frac{h}{2}\\-\frac{l}{2}\\\end{bmatrix}$$右目两个边缘点以及 perspective keypoint 点也可同样得到，由此可整理出 7 个方程组（论文中第一个公式符号有错）：$$\left\{\begin{array}{l}u_l=(x- \frac{w}{2} cos\theta- \frac{l}{2} sin\theta) / (z+ \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\v_t=(y- \frac{h}{2}) / (z+ \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\u_r=(x+ \frac{w}{2} cos\theta+ \frac{l}{2} sin\theta) / (z- \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)\\v_b=(y+ \frac{h}{2}) / (z- \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)\\u’_l=(x-b- \frac{w}{2} cos\theta- \frac{l}{2} sin\theta) / (z+ \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\u’_r=(x-b+ \frac{w}{2} cos\theta+ \frac{l}{2} sin\theta) / (z- \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)\\u_p=(x+ \frac{w}{2} cos\theta- \frac{l}{2} sin\theta) / (z- \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\\end{array}\right.$$其中 \(b\) 为双目的基线长(baseline)。以上方程组可用 Gauss-Newton 法求解。 3.&ensp;Dense 3D Box Alignment 以上得到的目标 3D 位置是 object-level 求解得到的，利用像素信息，还可以进行优化精确求解。首先在图像 2D 目标框内扣取一块 RoI，要使 RoI 能较为确定的在目标上，扣取方式定义为： 目标一半以下区域； perspective keypoint 与边缘关键点包围区域； 关键点预测的时候只预测了 u 方向的坐标，边缘关键点无 v 方向的信息，看起来会使某些背景像素被划入为目标像素，更好的方法是加入 instance segmentation 信息。定义误差函数为：$$E=\sum_{i=0}^N e_i=\sum_{i=0}^N \left\| I_l(u_i,v_i)-I_r(u_i-\frac{b}{z+\Delta z_i},v_i)\right\|$$可由三角测量关系 \(z=\frac{bf}{d}\) 推出。上式中，\(\Delta z_i=z_i-z\) 表示某个像素点 \(i\) 所对应的 3D 点与目标中心点之间的距离。最小化总误差即可求得最优的中心点距离 \(z\)。优化过程可以用 coarse-to-fine 的策略，先以 0.5m 的精度找 50 步，再以 0.05m 的精度找 20 次。 这个 dense alignment 模块是独立的，可以应用到任意的左右目 3D 检测的后处理中。因为目标 RoI 是物理约束，所以这个方法避免了深度估计中不连续、病态的问题，且对光照是鲁棒的，因为每个像素都会对估计起作用。这里，本文只做了中心点的 align，尺寸，甚至朝向角是否能加入优化? 4.&ensp;Other Details [1] Li, Peiliang, Xiaozhi Chen, and Shaojie Shen. “Stereo R-CNN based 3D Object Detection for Autonomous Driving.” arXiv preprint arXiv:1902.09738 (2019).]]></content>
      <categories>
        <category>paper reading</category>
      </categories>
      <tags>
        <tag>paper reading</tag>
        <tag>3D Detection</tag>
        <tag>autonomous driving</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[paper_reading]-"Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving"]]></title>
    <url>%2F%5Bpaper_reading%5D-Stereo-Vision-based-Semantic-3D-Object-and-Ego-motion-Tracking-for-Autonomous-Driving%2F</url>
    <content type="text"><![CDATA[本文[1]结合 Semantic SLAM 与 Learning-based 3D Det 技术，提出了一种用于自动驾驶的动态目标定位与本车状态估计的方法。本文系统性较强，集成了较多成熟的模块，对工程应用也有较强的指导意义。 如图 1. 所示，整个系统框架由三部分组成： 2D object detection and viewpoint classification，目标位姿通过 2D-3D 约束求解出来； feature extraction and matching，双目及前后帧的特征提取与匹配； ego-motion and object tracking，将语义信息及特征量加入到优化中，并且加入车辆动力学约束以获得平滑的运动估计。 1.&ensp;Viewpoint Classification and 3D Box Inference1.1.&ensp;Viewpoint Classification 选用 Faster R-CNN 作为 2D 检测框架，在此基础上，加入车辆视野（viewpoint）分类分支。由图 2. 所示，水平视野分为八类，垂直视野分为两类，总共 16 类。 1.2.&ensp;3D Box Inference Based on Viewpoint 网络输出图像 2D 框以及目标车辆的视野类别（viewpoint），此时我们假设： 2D 框准确； 每种车辆的尺寸相同； 2D 框能紧密包围 3D 框； 在以上假设条件下，我们可以求得 3D 框，该 3D 框作为后续优化的初始值。约束方程的表示在论文中比较晦涩，在这里我做细致的推倒。 3D 框可表示为 \(\{x,y,z,\theta,w,h,l\}\)，其中 \(\{w,h,l\}\) 分别对应 \(\{x,y,z\}\) 维度。如图 2.(b) 所示，这个视角下，四个 3D 框的顶点，可得四个约束方程。推倒过程为：$$\require{cancel}\begin{bmatrix}u_{min}\\v_1\\1\\\end{bmatrix}=K\cdot\begin{bmatrix}x_{1}^{cam}\\y_{1}^{cam}\\z_{1}^{cam}\\\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot\begin{bmatrix}x_{1}^{obj}\\y_{1}^{obj}\\z_{1}^{obj}\\\end{bmatrix}=\begin{bmatrix}x\\y\\z\\\end{bmatrix}+\begin{bmatrix}cos\theta &amp; 0 &amp;sin\theta\\0 &amp; 1 &amp; 0\\-sin\theta &amp; 0 &amp; cos\theta\\\end{bmatrix} \cdot\begin{bmatrix}\frac{w}{2}\\\frac{h}{2}\\\frac{l}{2}\\\end{bmatrix}$$其中 \(K\) 为相机内参，做归一化处理消去；\(T_{cam}^{obj}\) 为目标中心坐标系在相机坐标系下的表示，\((\cdot)^{cam/obj}\) 分别为点在相机坐标系，目标中心坐标系下的表示。同样的，这个视野下，②，③，④ 点都可以由此获得：$$\left\{\begin{array}{l}\require{cancel}\begin{bmatrix}u_{min}\\v_1\\1\\\end{bmatrix}=K\cdot\begin{bmatrix}x_{1}^{cam}\\y_{1}^{cam}\\z_{1}^{cam}\\\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot\begin{bmatrix}x_{1}^{obj}\\y_{1}^{obj}\\z_{1}^{obj}\\\end{bmatrix}=\begin{bmatrix}x\\y\\z\\\end{bmatrix}+\begin{bmatrix}cos\theta &amp; 0 &amp;sin\theta\\0 &amp; 1 &amp; 0\\-sin\theta &amp; 0 &amp; cos\theta\\\end{bmatrix} \cdot\begin{bmatrix}\frac{w}{2}\\\frac{h}{2}\\\frac{l}{2}\\\end{bmatrix}\\\begin{bmatrix}u_{max}\\v_2\\1\\\end{bmatrix}=K\cdot\begin{bmatrix}x_{2}^{cam}\\y_{2}^{cam}\\z_{2}^{cam}\\\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot\begin{bmatrix}x_{2}^{obj}\\y_{2}^{obj}\\z_{2}^{obj}\\\end{bmatrix}=\begin{bmatrix}x\\y\\z\\\end{bmatrix}+\begin{bmatrix}cos\theta &amp; 0 &amp;sin\theta\\0 &amp; 1 &amp; 0\\-sin\theta &amp; 0 &amp; cos\theta\\\end{bmatrix} \cdot\begin{bmatrix}-\frac{w}{2}\\\frac{h}{2}\\-\frac{l}{2}\\\end{bmatrix}\\\begin{bmatrix}u_3\\v_{min}\\1\\\end{bmatrix}=K\cdot\begin{bmatrix}x_{3}^{cam}\\y_{3}^{cam}\\z_{3}^{cam}\\\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot\begin{bmatrix}x_{3}^{obj}\\y_{3}^{obj}\\z_{3}^{obj}\\\end{bmatrix}=\begin{bmatrix}x\\y\\z\\\end{bmatrix}+\begin{bmatrix}cos\theta &amp; 0 &amp;sin\theta\\0 &amp; 1 &amp; 0\\-sin\theta &amp; 0 &amp; cos\theta\\\end{bmatrix} \cdot\begin{bmatrix}\frac{w}{2}\\-\frac{h}{2}\\-\frac{l}{2}\\\end{bmatrix}\\\begin{bmatrix}u_4\\v_{max}\\1\\\end{bmatrix}=K\cdot\begin{bmatrix}x_{4}^{cam}\\y_{4}^{cam}\\z_{4}^{cam}\\\end{bmatrix}\doteq \xcancel{K} \cdot T_{cam}^{obj} \cdot\begin{bmatrix}x_{4}^{obj}\\y_{4}^{obj}\\z_{4}^{obj}\\\end{bmatrix}=\begin{bmatrix}x\\y\\z\\\end{bmatrix}+\begin{bmatrix}cos\theta &amp; 0 &amp;sin\theta\\0 &amp; 1 &amp; 0\\-sin\theta &amp; 0 &amp; cos\theta\\\end{bmatrix} \cdot\begin{bmatrix}-\frac{w}{2}\\\frac{h}{2}\\\frac{l}{2}\\\end{bmatrix}\end{array}\right.$$ 将 \(z\) 方向归一化后，进一步得到最终的四个约束式子：$$\left\{\begin{array}{l}u_{min}=(x+ \frac{w}{2} cos\theta+ \frac{l}{2} sin\theta) / (z- \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)\\u_{max}=(x- \frac{w}{2} cos\theta- \frac{l}{2} sin\theta) / (z+ \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\v_{min}=(y- \frac{h}{2}) / (z- \frac{w}{2} sin\theta - \frac{l}{2} cos\theta)\\v_{max}=(y+ \frac{h}{2}) / (z+ \frac{w}{2} sin\theta + \frac{l}{2} cos\theta)\end{array}\right.$$以上四个方程可以闭式求解 3D 框 \(\{x,y,z,\theta\}\)。该方法将 3D 框的回归求解分解成了 2D 框回归，视野角分类以及解方程组的过程，强依赖于前面的三点假设，实际情况 3D 框与 2D 框不会贴的很紧。这个 3D 框结果只用来作后续的特征提取区域及最大后验概率估计的初始化。 2.&ensp;Feature Extraction and Matching 这一部分做的是左右目及前后帧特征提取及匹配。选用 ORB 特征，目标区域由投影到图像的 3D 框确定。 目标区域内左右目的立体匹配由于已知目标的距离及尺寸，所以只需要在一定小范围内进行特征点的行搜索匹配。 目标及背景区域下前后帧的时序匹配首先进行 2D 框的关联，2D 框经过相机旋转补偿后，最小化关联框的中心点距离及框形状相似度值。然后在关联上的目标框区域以及背景区域里，分别作 ORB 特征的匹配，异常值在 RANSAC 下通过基础矩阵测试去除。 3.&ensp;Ego-motion and Object Tracking 首先进行本车运动状态估计，可在传统 SLAM 框架下做，不同的是将动态障碍物中的特征点去除。有了本车的位姿后，再估计动态障碍物的运动状态。文中符号定义较为复杂，这里不做赘述。 3.1.&ensp;Ego-motion Tracking 给定左目前后帧背景区域特征点的观测，本车状态估计可以通过极大似然估计（Maximum Likelihood Estimation）得到。MLE 可以转化为非线性最小二乘问题，也就是 Bundle Adjustment 过程，这是典型的 SLAM 问题。文中给出的误差方程：需要求解的是本车位姿以及背景特征点坐标，这是后验概率，可转为似然函数求解，然后转化为非线性优化问题。可参考《视觉 SLAM 十四讲》(107-108)来理解。 3.2.&ensp;Semantic Object Tracking 得到本车相机的位姿后，运动目标的状态估计可以通过最大后验概率估计（Maximum-a-posterior, MAP）得到。类似的，可转为非线性优化问题进行求解，联合优化每个车辆的位姿，尺寸，速度，方向盘转角，所有特征点 3D 位置。有四个 loss 项：\(r_Z,r_P,r_M,r_S\) 分别代表： Sparse Feature Observation目标上的特征点重投影到左右目图像的误差，注意有左右目两个误差项； Semantic 3D Object Measurement3D 框投影到图像上与 2D 框的尺寸约束投影误差，即 1.2 节中的形式，区别在车辆尺寸与位姿作为了优化项； Vehicle Motion Model对于车辆，前后时刻的状态要有连续性，即误差最小； Point Cloud Alignment为了减少 3D 框的整体偏移，引入特征点到 3D 观察面的最小距离误差； 这里只对车辆运动模型进行分析，其它几项基本在前文已经有描述或者比较常识化，就不展开，具体公式可参见论文。 由实验可知 Sparse Feature Observation 与 Point Cloud Alignment 对性能提升较明显，Motion Model 对困难情景性能才有提升。 3.2.1.&ensp;Vehicle Motion Model [2] 中介绍了前转向车的两种模型：运动学模型(Kinematic Bicycle Model)，以及更复杂的动力学模型(Dynamic Bicycle Model)。运动学模型假设车辆不存在滑动，这在大多数情况下都是满足的，所以我们只介绍运动学模型。 如图 3. 所示，前后轮无滑动的约束下，可得方程组：$$\left\{\begin{array}{rl}\dot{x}_fsin(\theta+\delta)-\dot{y}_fcos(\theta+\delta)=&amp;0\\\dot{x}sin(\theta)-\dot{y}cos(\theta)=&amp;0\\x+Lcos(\theta)=&amp;x_f \quad\Rightarrow \quad \dot{x}-\dot{\theta}Lsin(\theta)=\dot{x}_f\\y+Lsin(\theta)=&amp;y_f \quad\Rightarrow \quad \dot{y}+\dot{\theta}Lcos(\theta)=\dot{y}_f\end{array}\right.$$由此可得到:$$\dot{x}sin(\theta+\delta)-\dot{y}cos(\theta+\delta)-\dot{\theta}Lcos(\delta)=0$$用 \(\left(v \cdot cos(\theta),v\cdot sin(\theta)\right)\) 代替 \((\dot{x},\dot{y})\) 可得：$$\dot{\theta}=\frac{tan(\delta)}{L}\cdot v$$最终可整理成矩阵形式：$$\begin{bmatrix}\dot{x}\\\dot{y}\\\dot{\theta}\\\dot{\delta}\\\dot{v}\\\end{bmatrix}=\begin{bmatrix}0 &amp;0 &amp;0 &amp;0 &amp;cos(\theta)\\0 &amp;0 &amp;0 &amp;0 &amp;sin(\theta)\\0 &amp;0 &amp;0 &amp;0 &amp;\frac{tan(\delta)}{L}\\0 &amp;0 &amp;0 &amp;0 &amp;0\\0 &amp;0 &amp;0 &amp;0 &amp;0\\\end{bmatrix}\begin{bmatrix}x\\y\\\theta\\\delta\\v\\\end{bmatrix}+\begin{bmatrix}0 &amp;0\\0 &amp;0\\0 &amp;0\\1 &amp;0\\0 &amp;1\\\end{bmatrix}\begin{bmatrix}\gamma\\\alpha\\\end{bmatrix}$$其中 \(L\) 为车辆参数。观测量有： \((x,y,\theta)\) 为车辆的位置及朝向角； \(\delta\) 为方向盘/车轮转角； \(v\) 为车辆速度； 控制量有： \(\gamma\) 为方向盘角度比率； \(\alpha\) 为加速度； 本文的目的是要约束车辆时序上运动(速度及朝向)的平滑一致性，令控制量 \(\gamma,\alpha\) 为 0，然后可得状态量在相邻时刻的关系应满足：$$\left\{\begin{array}{l}\hat{x}^t=x^{t-1}+cos(\theta^{t-1})v^{t-1}\Delta t\\\hat{y}^t=y^{t-1}+sin(\theta^{t-1})v^{t-1}\Delta t\\\hat{\theta}^t=\theta^{t-1}+\frac{tan(\delta^{t-1})}{L}v^{t-1}\Delta t\\\hat{\delta}^t=\delta^{t-1}\\\hat{v}^t=v^{t-1}\end{array}\right.$$由此可整理成论文中矩阵的形式及误差项： [1] Li, Peiliang, and Tong Qin. “Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving.” Proceedings of the European Conference on Computer Vision (ECCV). 2018.[2] Gu, Tianyu. Improved trajectory planning for on-road self-driving vehicles via combined graph search, optimization &amp; topology analysis. Diss. Carnegie Mellon University, 2017.]]></content>
      <categories>
        <category>paper reading</category>
      </categories>
      <tags>
        <tag>paper reading</tag>
        <tag>3D Detection</tag>
        <tag>autonomous driving</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MOT Metrics in Academia and Industry]]></title>
    <url>%2FMOT-Metrics-in-Academia-and-Industry%2F</url>
    <content type="text"><![CDATA[MOT 是一个比较基本的技术模块，在视频监控中，常用于行人行为分析、姿态估计等任务的前序模块；在自动驾驶中，MOT 是动态目标状态估计的重要环节。在学术界，MOT 算法性能的评价准则已经较为完善，其指标主要关注，尽可能地覆盖所有性能维度，以及指标的简洁性（上一篇有较多介绍，the CLEAR MOT Metrics）。而工业界则尚无统一的标准，实际的指标需求情况也比学术界复杂。 指标的计算过程可由三部分组成，真值过滤(Filter)，匹配构建(Establishing Correspondences)与指标计算(Calculating Metrics)。其中真值过滤，更多的是工程细节，学术界没有文章对这一部分进行讨论研究。本文首先介绍学术界各评价指标详情，然后讨论工业界需要的评价指标又是怎样的。 1.&ensp;Metrics in Academia 在学术界，因为数据集质量较高，噪声相对较小，匹配构建中距离的度量偏向于严格且简单的方式。对于区域(框)跟踪器，采用重叠区域来度量；对于点跟踪器，采用中心点的欧式距离来度量。指标汇总如下：A.&ensp;检测指标 \(\lozenge\) 准确性(Accuracy) Recall = \(\frac{TP}{GT}\)； Precision = \(\frac{TP}{TP+FP}\)； FAF/FPPI[1][2] ，Average False Alarms per Frame；False Positive Per Image; MODA[3]，Multipe Object Detection Precision，整合了 FN 与 FP，设 \(c_m, c_f\) 分别为 FN，FP 的权重：$$MODA=1-\frac{\sum_{t=1}^{N_frames}(c_m(fn_t)+c_f(fp_t))}{\sum_{t=1}^{N_frames}gt_t}$$ \(\lozenge\) 精确性(Precision) MODP[3]，Multiple Object Detection Accuracy，$$MODP=\frac{\sum_{t=1}^{N_frames} \sum_{i=1}^{N_{mapped}^{(t)}} \;\; dist}{\sum_{t=1}^{N_frames} N_{mapped}^{(t)}}$$其中 \(N_{mapped}^{(t)}\) 为第 \(t\) 帧匹配的目标数；\(dist\) 为距离度量方法，如框的交并比度量法：$$Mapped Overlap Ratio = \frac{\lvert G_i^{(t)}\bigcap D_i^{(t)}\rvert}{|G_i^{(t)}\bigcup D_i^{(t)}|}$$ B.&ensp;跟踪指标 \(\lozenge\) 准确性(Accuracy) IDS[4]，ID switch，a tracked target changes its ID with another target(预测关联真值)； MOTA[5]，Multiple Object Tracking Accuracy，整合了 FN，FP，ID-Switch：$$MOTA=1-\frac{\sum_{t=1}^{N_{frames}} \;\; (c_m(fn_t)+c_f(fp_t)+c_s(ID-SWITCHES_t))}{\sum_{t=1}^{N_{frames}} \;\; gt_t}$$其中权重方程一般可设为：\(c_m=c_f=1, \quad c_s=log_{10}\)； \(\lozenge\) 精确性(Precision) MOTP[5]，Multiple Object Tracking Precision，$$MODP=\frac{\sum_{t=1}^{N_frames} \sum_{i=1}^{N_{mapped}^{(t)}} \;\; \left(\frac{\lvert G_i^{(t)}\bigcap D_i^{(t)}\rvert}{|G_i^{(t)}\bigcup D_i^{(t)}|} \right)}{\sum_{t=1}^{N_frames} N_{mapped}^{(t)}}$$ TDE[6]，Distance between the ground-truth annotation and the tracking result；像素级别的误差计算，适用于人群跟踪； OSPA[7][8]，Optimal Subpattern assignment，由定位 (localization) 误差及基数 (cardinality) 误差构成，对于第 \(t\) 帧：$$e^t=\left[\frac{1}{n^t}\left( \mathop{\min}_{\pi\in\Pi_n} \sum_{i=1}^{m^t} d^{(c)}(x_i^t,y_{\pi(i)}^t)^p + (n^t-m^t)\cdot c^p \right) \right]^{1/p}$$其中，\(n^t\) 为目标真值与算法输出中数量较大者。\(\Pi_n\) 为从 \(n^t\) 中取出的 \(m\) 个目标。\(p\) 为距离指数范数。其中定位截断误差为：$$d^{(c)}(x_i^t,y_{\pi(i)}^t) = \mathop{\min}\left(c,d(x_i^t,y_{\pi(i)}^t)\right)$$\(c\) 为截断参数。定位误差又由距离误差和标签误差组成：$$d(x_i^t,y_{\pi(i)}^t=\parallel x_i^t-y_{\pi(i)}^t\parallel + \alpha \; \bar{\delta}(l_x, l_y)$$其中 \(\alpha\in[0,c]\)，为标签误差的权重系数。如果 \(l_x=l_y\)，\(\bar{\delta}(l_x, l_y)=0\)，否则 \(\bar{\delta}(l_x, l_y)=1\). \(\lozenge\) 完整性(Completeness) MT[9]，Mostly Tracked，真值轨迹长度被跟踪大于80%的比例； ML[9]，Mostly Lost，真值轨迹长度被跟踪小于20%的比例； PT[9]，Partially Tracked，\(1-MT-ML\); FM[9]，Fragments，ID of a target changed along a GT trajectory, or no ID(真值关联预测)； \(\lozenge\) 鲁棒性(Robustne) RS[10]，Recover from short term occlusion; RL[10]，Recover from long term occlusion; 2.&ensp;Metrics in Industry 工业界的数据噪声较大，传感器配置也比较多样，不同的产品（传感器+算法），对 MOT 性能维度要求也不一样。更重要的是，评价指标应该从功能层面进行定义，在模块层面 (MOT) 进行调整及细化。可以说，工业界是以学术界为基础来设计 MOT 指标的，不同的产品没有统一的标准，但有比较通用的设计准则。 这里以自动驾驶/辅助驾驶中动态目标状态估计模块为例，模块详细分析日后再写。该模块的基本输入为： 传感器数据，可以是图像，激光等； 自定位系统，可以是基于视觉的 VO，基于视觉-IMU 的 VINS等；其中自定位系统能使目标状态估计在世界坐标系（惯性系）下优化，否则只能在本体（ego）非惯性系下优化，会减少一些约束量。该功能的基本输出为： 位置，本体坐标系下目标的三维位置，\(x,y,z\)； 尺寸，目标的物理尺寸大小，包括立方体的长宽高；或者图像坐标系下的像素大小；或者图像/点云下目标的 mask，即分割后的目标； 朝向，一般只考虑目标的航向角； 速度，本体坐标系或世界坐标系下的三维速度，一般只考虑航向平面的速度； 其中朝向是非必须项，有了朝向后，能更有效地进行状态优化。该模块的子模块有（注意，MOT 只包含前三者）： 检测(Detection)，进行多目标检测； 跟踪(Tracking)，根据上一帧结果，进行多目标跟踪； 数据关联(Association)，检测结果与跟踪结果的融合，出目标的 tracklets，生成 ID； 状态估计(State Estimation)，不同的方法包括不同的部分； 工业界设计产品时，基本遵循自顶向下的策略：产品需求-功能需求-模块需求，层层推倒。所以我们设计评价准则时，一般会问几个问题： 该模块服务的产品功能，其需求及对应的指标是什么？ 要达到功能指标，本模块的输出需要哪些指标来评测？ 各个子模块对模块的影响是怎样的，对应需要增加哪些指标？ 这里提到了功能指标，模块指标，子模块指标三层概念。功能指标及部分模块指标是可以写入产品手册的，所以需要突出重点，易于理解；部分模块及子模块指标则主要是为了产品上工程优化迭代，这就要求这部分指标要相当细致，将模块的不足尽可能解耦，且完全暴露出来。以下通过两个例子来分析设计过程。 2.1.&ensp;ADAS 中的 FCW 功能 FCW 基本功能要求为： 不允许误报，尽可能不漏报； 在 V km/h 下，以一定的刹车加速度 a，能避免与静止的前车相碰撞； 由以上两个功能需求，可确定必须的功能指标： （百公里）误报率； （百公里）漏报率； 观测距离，可由第二项功能要求推到出（人反应时间已知）； 相应的 MOT +状态估计模块输出的指标为各距离维度各类别维度下的： 误检率； 漏检率； ID Switch； 定位精度； 速度估计精度； 其中 MOT 主要涉及误检率，漏检率，ID Switch（直接影响状态估计模块）。这些指标的计算方式可以在学术界定义的基础上做进一步改进，比如漏检率，就需要体现出百公里漏报率的性能，所以可以考虑将连续 N 帧漏检的目标才归为漏检，分母可以定义为每多少帧。此外，要在各距离维度各类别维度下进行计算，这就涉及到过滤（filter）策略。对于 FCW 而言，首要关注的是本车前方近距离位置，距离维度上的功能重要程度要突显出来，类别维度也要区别对待，以便算法模块可以重点优化。 2.2.&ensp;自动驾驶中的动态障碍物检测功能 自动驾驶中动态障碍物检测的要求就高了，子模块也较为复杂，指标除了评估功能模块的性能，还需要指导迭代各子模块算法，包括本子模块的迭代比较，以及上下游模块相关指标的对比。 功能需求，我们简单列举几项： 不允许漏检，尽可能不误检； 前向，后向，侧向观测距离分别要达到 x, y, z； 相应的功能指标为： 漏检率； 误检率； 观测距离； 观测精度； 观测时延(delay)； MOT +状态估计模块输出的指标依然在各距离维度各类别维度下： 误检率； 漏检率； ID Switch； 定位精度； 尺寸，朝向，速度估计精度； 状态估计收敛时间； 一系列描述时序稳定性的指标； 与前述 FCW 功能类似，只是多了较多的指标。过滤操作也做的更加细致，我们还可以将目标做重要性等级划分，比如本车道前车多少米内，那指标基本都要达到 99%+；还可以将地面区域做重要性划分（比距离维度更加细致，可以认为是三维层面），周围几米内，那误检率肯定要非常低。除了过滤策略需要仔细设计外，匹配策略也需要进一步思考。如果传感器本身精度就有限，那么匹配策略就要相应放宽。还需注意的是引入过滤策略后，FP与FN计算的细微差别，比如有个过滤条件为去除目标像素面积小于一定阈值的目标集 A，观测值与真值匹配时，如果与 A 中的目标匹配上，那么不应该记为 FP，如果没匹配上 A 中的目标，那么 A 中地目标也不应该被记为 FN。这种类似的情况逻辑要思考清楚。 3.&ensp;Summary 以上设计的出发点是，我们要承认算法的不完美性以及传感器的局限性，在工程领域，一定要首先解决主要矛盾，再打磨细节。本文还对以下内容未作进一步分析（以后有机会再写文细究）： 状态估计时序相关指标，描述估计的时序稳定性，也可以用于 MOT 的评估； 标注与过滤策略的关系，过滤策略往往依赖于标注策略； 各个指标的阈值确定，确定阈值也是产品中一件重要而又系统的事，有时候比指标设计更复杂； [1] Yang B, Huang C, Nevatia R. Learning affinities and dependencies for multi-target tracking using a CRF model[C]//CVPR 2011. IEEE, 2011: 1233-1240.[2] Choi W, Savarese S. Multiple target tracking in world coordinate with single, minimally calibrated camera[C]//European Conference on Computer Vision. Springer, Berlin, Heidelberg, 2010: 553-567.[3] Kasturi, Rangachar, et al. Framework for performance evaluation of face, text, and vehicle detection and tracking in video: Data, metrics, and protocol IEEE transactions on Pattern Analysis and Machine intelligence 31.2 (2008): 319-336.[4] Yamaguchi K, Berg A C, Ortiz L E, et al. Who are you with and where are you going?[C]//CVPR 2011. IEEE, 2011: 1345-1352.[5] Bernardin K, Stiefelhagen R. Evaluating multiple object tracking performance: the CLEAR MOT metrics[J]. Journal on Image and Video Processing, 2008, 2008: 1.[6] Kratz L, Nishino K. Tracking with local spatio-temporal motion patterns in extremely crowded scenes[C]//2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. IEEE, 2010: 693-700.[7] Ristic B, Vo B N, Clark D, et al. A metric for performance evaluation of multi-target tracking algorithms[J]. IEEE Transactions on Signal Processing, 2011, 59(7): 3452-3457.[8] Schuhmacher D, Vo B T, Vo B N. A consistent metric for performance evaluation of multi-object filters[J]. IEEE transactions on signal processing, 2008, 56(8): 3447-3457.[9] Li Y, Huang C, Nevatia R. Learning to associate: Hybridboosted multi-target tracker for crowded scene[C]//2009 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2009: 2953-2960.[10] Song B, Jeng T Y, Staudt E, et al. A stochastic graph evolution framework for robust multi-target tracking[C]//European Conference on Computer Vision. Springer, Berlin, Heidelberg, 2010: 605-619.]]></content>
      <categories>
        <category>MOT</category>
      </categories>
      <tags>
        <tag>MOT</tag>
        <tag>tracking</tag>
        <tag>autonomous driving</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MOT 评价指标-"Evaluating Multiple Object Tracking Performance, the CLEAR MOT Metrics"]]></title>
    <url>%2FMOT-%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87-Evaluating-Multiple-Object-Tracking-Performance-the-CLEAR-MOT-Metrics%2F</url>
    <content type="text"><![CDATA[这篇文章介绍了两个综合性指标 MOTA 以及 MOTP 的计算过程，这两个指标有优劣势，但是作为综合性指标至今在学术界仍广泛应用。本文主要介绍其设计思想及计算过程。 一个理想的 MOT 算法，我们期望每一帧： 准确检测目标的数量； 准确估计每个目标的状态，如位置，朝向，速度等； 准确估计每个目标的轨迹，即目标的 ID 不变性； 这就要求评价准则： 能评估目标定位的精度； 能反映目标轨迹的追踪能力，即同一个目标产生唯一的 ID； 此外，为了提高评价准则的实用性： 参数尽可能少，阈值可调； 易于理解，表现方式符合人们的直觉； 有较强的通用性，能评估各种跟踪算法； 指标个数少，但是能足够反映算法不同维度的性能； 假设第 \(t\) 帧，有目标集 \(\{o_1,…,o_n\}\)，跟踪算法的输出(hypotheses)：\(\{h_1,…h_m\}\)。根据上述设计准则，设计评价计算过程： &ensp;构建 \(h_j\) 与 \(o_i\) 的最优匹配； &ensp;对于每个匹配对，计算位置估计误差； &ensp;累加所有匹配对的误差，包括：a. &ensp;计算漏检数(FN)；b. &ensp;计算误检数(FP)；c. &ensp;计算 ID swith 次数，包括两个邻近目标的 ID 交换，以及遮挡后，同一目标的 ID 跳变； 由此可得到两大指标： tracking precision，目标位置的估计精度； tracking accuracy，包括 misses(FN), FP, mismatches(IDs), failures to recover； 下面分两块做细节分析，匹配构建 (Establishing Correspondences) 与评价指标 (Metrics)。 1.&ensp;匹配构建 算法估计与目标真值的匹配，大致还是基于匹配最近 object-hypothesis 的思想，没匹配上的估计就是 FP，没匹配上的真值就是 FN。但是这中间需要进一步考虑一些问题。 1.1.&ensp;有效匹配 如果算法估计 \(h_j\) 与目标 \(o_i\) 的最近距离 \(dist_{i,j}\) 超过了一定的阈值 \(T\)，那么这个匹配也是不合理的，因为这个距离误差加入到定位误差中是不合理的，所以只能说这个跟踪的结果不是这个目标。关于距离的度量： 区域（框）跟踪器，距离可用两者的重叠区域来度量，\(T\) 可以设为 0； 点跟踪器，距离可用两者中心点的欧氏距离来度量，\(T\) 可以根据目标的尺寸来设定； 1.2.&ensp;跟踪一致性 统计目标与算法输出的匹配跳变的次数，也就是目标 ID 的跳变数。文章还提到，当目标有两个有效地匹配时，选择之前的匹配，即使那个匹配的距离大于另一个匹配，这点当存在两个很近的目标时，可能会有问题，需要全局来看。 1.3.&ensp;匹配过程 &ensp;对 \(t\) 帧，考虑 \(M_{t-1}\) 中所有匹配是否还依然有效，包括目标真值及算法输出是否还存在，如果都存在，那么距离是否超出阈值 \(T\)； &ensp;对于剩下的没找到匹配的真值目标，在唯一匹配以及阈值约束下，可采用匹配算法或者贪心算法来求解，使得距离误差的总和最小（文章的意思是排除了从上一帧继承的已有匹配，当目标密集时，这部分也应该加入进来优化）。统计当前帧目标真值匹配的跳变数 \(mme_t\)，作为 mismatch errors； &ensp;经过之前两步后，找到了所有的匹配，统计匹配个数为 \(c_t\)，计算匹配上的目标真值与算法输出的定位误差 \(d_t^i\)； &ensp;统计没有匹配上的算法输出 (hypotheses) 为 \(fp_t\)，没有匹配上的目标真值为 \(m_t\)，目标真值个数为 \(g_t\)； &ensp;每一帧重复步骤１，第一帧没有 mismatch； 2.&ensp;评价指标 基于以上的匹配策略，得出两个合理的指标： MOTP(multiple object tracking precision)，跟踪定位精度指标：$$MOTP=\frac{\sum_{i,t}d_t^i}{\sum_tc_t}$$ MOTA(multiple object tracking accuracy)，综合了漏检率，误检率，以及 ID 跳变率：$$MOTA=1-\frac{\sum_t(m_t+fp_t+mme_t)}{\sum_tg_t}$$]]></content>
      <categories>
        <category>paper reading</category>
      </categories>
      <tags>
        <tag>paper reading</tag>
        <tag>MOT</tag>
        <tag>tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MOT 综述-'Multiple Object Tracking: A Literature Review']]></title>
    <url>%2FMOT-%E7%BB%BC%E8%BF%B0-Multiple-Object-Tracking-A-Literature-Review%2F</url>
    <content type="text"><![CDATA[之前做 MOT 还是沿着 SOT 的思路，这篇文章对 MOT 有一个很深入且很有框架性的综述，以下对这篇文章做一个提炼，并加入一些自己的想法。 MOT 作为一个中层任务，是一些高层任务的基础，比如行人的 pose estimation，action recognition，behavior analysis，车辆的 state estimation。单目标跟踪(SOT)主要关注 appearance model 以及 motion model 的设计，解决尺度、旋转、光照等影响因素。而 MOT 包含两个任务：目标数量以及目标ID，这就要求 MOT 还需要解决其它问题： frequent occlusions initialization and termination of tracks similar appearance interactions among multiple objects 1.&ensp;问题描述 多目标跟踪实际上是多参数估计问题。给定图像序列\(\{I_1,I_2,…,I_t,…\}\)，第\(t\)帧中目标个数为\(M_t\)，第\(t\)帧中所有目标的状态表示为\(S_t=\{s_t^1,s_t^2,…,s_t^{M_t}\}\)，第\(i\)个目标的轨迹表示为\(s_{1:t}^i=\{s_1^i,s_2^i,…,s_t^i\}\)，所有图像中所有目标的状态序列为\(S_{1:t}=\{S_1,S_2,…,S_t\}\)。相应的，所有图像中所有目标观测到的状态序列为\(O_{1:t}=\{O_1,O_2,…,O_t\}\)。多目标跟踪的优化目标是求解最优的各目标状态，即求解一个后验概率问题，$$ \widehat{S} _ {1:t}=\mathop{\arg\max}_{S_{1:t}}P(S_{1:t}|O_{1:t})$$这种形式有两种实现方法： probabilistic inference适合用于 online tracking 任务，Dynamic Model 为 \(P(S_t|S_{t-1})\)，Observation Model 为 \(P(O_t|S_t)\)，两步求解过程： \(\circ\) Predict: \(P(S_t|O_{1:t-1})=\int P(S_t|S_{t-1})dS_{t-1}\) \(\circ\) Update: \(P(S_t|O_{1:t}) \propto P(O_t|S_t)P(S_t|O_{1:t-1})\) deterministic optimization适合用于 offline tracking 任务，直接利用多帧信息进行最优化求解。 2.&ensp;分类方法 initialization method初始化方式分为： \(\circ\) Detection-Based Tracking，优势明显，除了只能处理特定的目标类型； \(\circ\) Detection-Free Tracking，能处理任何目标类型； processing mode根据是否使用未来的观测，处理方式可分为： \(\circ\) online tracking，适合在线任务，缺点是观测量会比较少； \(\circ\) offline tracking，输出结果存在时延，理论上能获得全局最优解； type of output根据问题求解方式输出是否存在随机性： \(\circ\) probabilistic inference，概率性推断； \(\circ\) deterministic inference，求解最大后验概率； 自动驾驶等在线任务主要关注 Detection-Based，online tracking。 3.&ensp;框架 MOT 主要考虑两个问题： 目标在不同帧之间的相似性度量，即对appearance, motion, interaction, exclusion, occlusion的建模； 恢复出目标的ID，即 inference 过程； 3.1.&ensp;Appearance Model3.1.1.&ensp;Visual Representation 视觉表达即目标的特征表示方式： local features本质上是点特征，点特征由 corner+descriptor(角点+描述子) 组成。KLT(good features to track)在 SOT 中应用广泛，用它可以生成短轨迹，估计相机运动位姿，运动聚类等；Optical Flow也是一种局部特征，在数据关联之前也可用于将检测目标连接到短轨迹中去。 region features在一个块区域内提取特征，根据像素间作差的次数，可分为： zero-order, color histogram &amp; raw pixel template first-order, HOG &amp; level-set formulation(?) up-to-second-order, Region covariance matrix others其它特征本质上也需要 local 或 region 的方式提取，只是原始信息并不是灰度或彩图。如 depth,probabilistic occupancy map, gait feature. Local features，比如颜色特征，在计算上比较高效，但是对遮挡，旋转比较敏感；Region features 里，HOG 对光照有一定的鲁棒性，但是对遮挡及形变效果较差；Region covariance matrix 更加鲁棒，但是需要更高的计算量；深度特征也比较有效，但是需要额外的获取深度信息的代价。 3.1.2.&ensp;Statistical Measuring 有了目标的特征表示方式之后，就可以评价两个观察的目标的相似性。特征表示的线索(cue)可分为： single cue因为只有一个线索，相似性(similarity)可以直接通过两个向量的距离转换得到。可以将距离指数化，高斯化。也可以将不相似度转为可能性，用协方差矩阵表示。 multiple cues多线索，即多种特征的融合，能极大提高鲁棒性，融合的策略有： Boosting, 选取一系列的特征，用 boost 算法选取表达能力最强的特征； Concatenation, 各个特征直接在空间维度上串起来，形成一个 cue 的表达方式； Summation, 加权融合各个特征，形成一个 cue 的表达方式； Product, 各个特征相乘的方式，比如目标 \(s_0\) 的某个潜在匹配 \(s_1\) 的颜色，形状特征为 \(color\), \(shape\) 的概率为 \(p(color|s_0)\), \(p(shape|s_0)\), 假设特征独立，那么， $$p(s_1|s_0)=p(color, shape|s_0)=p(color|s_0)\cdot p(shape|s_0)$$ Cascading, coarse-to-fine 的方式，逐步精细化搜索； 3.2.&ensp;Motion Model 运动模型对关联两个 tracklets 比较管用，而 online tracking 任务，对输出的时延要求较高，所以其中一个 tracklet 可以任务就是当前帧与上一帧形成的轨迹，所以这里很难去计算两个 tracklets 的相似度。能看到的一个应用点就是，通过 motion model 模型，预测下一时刻目标的位置，作为一个线索项目。以下讨论的各模型主要是为了度量 tracklets 的相似性，从而做 tracklets 的匹配。 3.2.1.&ensp;Linear Velocity Smoothness. N 帧 M 个目标轨迹: \(C_{dyn}=\sum_{t=1}^{N-2}\sum_{i=1}^{M}\parallel v_i^t-v_i^{t+1}\parallel^2\) Position Smoothness. \(G(p^{tail}+v^{tail}\Delta t-p^{head}, \sum_p)\cdot G(p^{head}-v^{head}\Delta t-p^{tail}, \sum_p)\) Acceleration Smoothness. 3.2.2.&ensp;Non-linear 运动模型假设是非线性的，相似度计算还是按照以上高斯形式。引为中提到，非线性运动模型并不作为目标的惩罚因子，因为目标并不需要满足该模型，但是只要有目标满足，就降低惩罚系数。 3.3.&ensp;Interaction Model3.3.1.&ensp;Social Force Models Individual Force fidelity, 目标不会改变它的目的地方向； constancy, 目标不会突然改变速度和方向； Group Force attraction, 目标间应该尽量靠近； repulsion, 目标间也得保留适当的距离； coherence, 同一个 group 里面的目标速度应该差不多； 3.3.2.&ensp;Crowd Motion Pattern Models 当一个 group 比较密集的时候，单个目标的运动模型不太显著了，这时候群体的运动模型更加有效，可以用一些方法来构建群体运动模型。 3.4.&ensp;Exclusion Model3.4.1.&ensp;Detection-level 同一帧两个检测量不能指向同一个目标。匹配 tracklets 时，可以将这一项作为惩罚项。不过目前的检测技术都做了 NMS，基本可以消除这种情况。 3.4.2.&ensp;Trajectory-level 两个轨迹不能非常靠近。对于 online tracking 来说，就是 tracking 结果的两个量不能挨在一起，如果挨在一起，就说明有问题，比如遮挡，或跟丢。 3.5.&ensp;Occlusion Handling Part-to-whole, 将目标分成栅格来处理； Hypothesize-and-test, Buffer-and-recover, 在遮挡产生前，记录一定量的观测，遮挡后恢复； Others 3.6.&ensp;Inference3.6.1.&ensp;Probabilistic Inference 概率法只需要用到当前时刻之前的信息，所以适合用于 online tracking 任务。首先，如果假设一阶马尔科夫，当前目标的状态之依赖于前一时刻目标的状态，即 dynamic model：$$P(S_t|S_{1:t-1})=P(S_t|S_{t-1})$$其次，观测是独立的，即当前目标的观测只由当前目标的状态决定，observation model：$$P(O_{1:t}|S_{1:t})=\prod_{i=1}^t P(O_t|S_t)$$dynamic model 对应的就是跟踪算法策略，observation model 是状态观测手段，包括检测方法。目标状态估计的迭代过程为： predict step根据 dynamic model，由目标的上一状态预测当前状态的后验概率分布； update step根据 observation model，更新当前目标状态的后验概率分布； 状态估计的过程伴随着噪音等因素的影响，常用的概率推断模型有： Kalman filter Extended Kalman filter Particle filter 3.6.2.&ensp;Deterministic Optimization 确定性优化法需要至少一个时间窗口的观测量，所以适合 offline tracking 任务。优化方法有： Bipartite graph matching Dynamic Programming Min-cost max-flow network flow Conditional random field MWIS(Maximum-weight independent set) 4.&ensp;评价方法 评价方法是非常重要的，一方面对算法系统进行调参优化，另一方面比较各个不同算法的优劣。评价方法 (evaluation) 包括评价指标 (metrics) 以及数据集 (datasets)，多类别的数据集主要有： MOT Challenge KITTI 评价指标可分为：A.&ensp;检测指标 \(\lozenge\) 准确性(Accuracy) Recall &amp; Precision False Alarme per Frame(FAF) rate, from paper False Positive Per Image(FPPI), from paper MODA(Multiple Object Detection Accuracy), 包含了 false positive &amp; miss dets. from paper \(\lozenge\) 精确性(Precision) MODP(Multiple Object Detection Precision), 衡量检测框与真值框的位置对齐程度；from paper B.&ensp;跟踪指标 \(\lozenge\) 准确性(Accuracy) ID switches(IDs), from paper MOTA(Multiple Object Tracking Accuracy), 包含了FP，FN，mismatch；from paper \(\lozenge\) 精确性(Precision) MOTP(Multiple Object Tracking Precision), from paper TDE(Tracking Distance Error), from paper OSPA(optimal subpattern assignment), from paper \(\lozenge\) 完整性(Completeness) MT, the numbers of Mostly Tracked, from paper PT, the numbers of Partly Tracked ML, the numbers of Mostly Lost FM, the numbers of Fragmentation \(\lozenge\) 鲁棒性(Robustness) RS(Recover from Short-term occlusion), from paper RL(Recover from Long-term occlusion) 评价指标汇总： 5.&ensp;总结5.1.&ensp;还存在的问题 MOT 算法模块较多，参数也较复杂，但是最依赖于检测模块的性能，所以算法间比较性能时，需要注意按模块进行变量控制。 5.2.&ensp;未来研究方向 MOT with video adaptation，检测模块式预先训练的，需要在线更新学习； MOT under multiple camera:\(\circ\) multiple views，不同视野相同场景信息的记录，\(\circ\) non-overlapping multi-camera，不同视野不同场景的 reidentification； Multiple 3D object tracking，能更准确预测位置，大小，更有效处理遮挡； MOT with scene understanding，拥挤场景，用场景理解来有效跟踪； MOT with deep learning MOT with other cv tasks，和其他任务融合，比如目标分割等；]]></content>
      <categories>
        <category>paper reading</category>
      </categories>
      <tags>
        <tag>paper reading</tag>
        <tag>MOT</tag>
        <tag>tracking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3D Detection Paper List]]></title>
    <url>%2F3D-Detection-paper-list%2F</url>
    <content type="text"><![CDATA[这篇文章从输入数据类别上进行 3D Detection paper 的归类。 RGBRGB-D(双目，单目+点云)Lidar]]></content>
      <categories>
        <category>paper reading</category>
      </categories>
      <tags>
        <tag>paper reading</tag>
        <tag>3D Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Study Topic List]]></title>
    <url>%2Fstudy-topic-list%2F</url>
    <content type="text"><![CDATA[本文罗列了相关领域知识的学习资料。 1.&ensp;Detection1.1.&ensp;2D Detection 入门 amusi Object Detection @handong Object Detection and Classification using R-CNNs Paper with Code 1.2.&ensp;3D Detection Paper with Code KITTI Leaderboard 2.&ensp;Tracking2.1.&ensp;Single Object Tracking Paper with Code 2.2.&ensp;Multi Object Tracking Paper with Code Paper List MOT Challenge 综述：Multiple Object Tracking: A Literature Review 综述：Online object tracking: A benchmark 综述：MOTChallenge 2015: Towards a benchmark for multi-target tracking 3.&ensp;Computational Photography 2017年秋季的计算摄影学课程15-463 4.&ensp;CNN ACC 5.&ensp;SLAM5.1.&ensp;理论知识 计算机视觉中的数学方法 Multiple View Geometry in Computer Vision Probabilistic Robotics(有中文版) State Estimation for Robotics(有中文版) 视觉SLAM十四讲 5.2.&ensp;综述 [Visual Odometry Part I: Fundamentals] [Visual Odometry Part II: Matching, Robustness, Optimization, Applications] Review of Visual Odometry: Types, Approaches, Challenges, and Applications Visual SLAM algorithms: a Survey from 2010 to 2016 Visual SLAM for Driverless Cars: a Brief Survey Visual Simultaneous Locations and Mapping: a Survey 5.3.&ensp;工具 ROS Opencv Camera Calibration Matlab Camera Calibration Toolbox ROS Wiki Camera Calibration 5.4.&ensp;算法 OpenSLAM 5.5.&ensp;其它资料 计算机视觉life Paper with Code]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
